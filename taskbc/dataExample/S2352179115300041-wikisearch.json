[
    {
        "phraseCharEnd": "37", 
        "phraseIndex": "T1", 
        "phraseGoldStandardTag": "Process", 
        "phrase": "thermo-oxidation", 
        "wikiSearchResults": [
            {
                "snippet": "degrade initially by a process which includes both photo-oxidation and thermo-oxidation, so it can degrade in the dark. Resin identification code 7 is applicable", 
                "pageCategories": "All articles needing additional references\nArticles needing additional references from May 2013\nArticles with Wayback Machine links\nBags\nCanadian inventions\nCommons category with local link same as on Wikidata\nPlastics\nWaste containers", 
                "pageContent": "A bin bag or bin liner (British English) or garbage bag, or trash bag (American English) is a disposable bag used to contain rubbish (British English), or the North American equivalent trash or garbage. Such bags are useful to line the insides of waste containers to prevent the insides of the receptacle from becoming coated in waste material. Most bags these days are made out of plastic, and are typically black in color.\nPlastic bags are a convenient and sanitary way of handling garbage, and are widely used. Plastic garbage bags are fairly lightweight and are particularly useful for messy or wet rubbish, as is commonly the case with food waste, and are also useful for wrapping up garbage to minimize odour. Plastic bags are often used for lining litter or waste containers or bins. This serves to keep the container sanitary by avoiding container contact with the garbage. After the bag in the container is filled with litter, the bag can be pulled out by its edges, closed, and tied with minimal contact with the waste matter.\nCreated in 1950, this invention can be attributed to Canadians Harry Wasylyk, Larry Hansen and Frank Plomp. In a special on CBC television, the green garbage bag ranked 36th among the top 50 Canadian inventions.\nPlastic bags can be incinerated with their contents in appropriate facilities for waste-to-energy conversion. They are stable and benign in sanitary landfills; some are degradable under specified conditions.\n\n\n== Description ==\nPlastic bags for rubbish or litter are sold in a number of sizes at many stores in packets or rolls of a few tens of bags. Wire twist ties are sometimes supplied for closing the bag once full. Varying thicknesses are commonly manufactured - thicker bags are used for heavy duty applications such as construction waste, or in order to be able to withstand being compacted during recycling processes. In the mid-1990s bin bags with draw strings for closure were introduced. Some bags have handles which may be tied, or holes through which the neck of the bag can be pulled. Most commonly, the plastic used to make bin bags is the rather soft and flexible LDPE (Low Density Polyethylene) or, for strength, LLDPE (Linear Low Density Polyethylene) or HDPE (High Density Polyethylene) are sometimes used.\n\n\n=== Biodegradable plastic bags ===\n\nOxo-biodegradable plastic bags have the same strength as ordinary plastic and cost very little extra. They will degrade then biodegrade if they get into the open environment, but they can be recycled if collected during their useful life. They are designed so that they will not degrade deep in landfill and will not therefore generate methane. Oxo-biodegradable plastic does not degrade quickly in low temperature \"windrow\" composting, but it is suitable for \"in-vessel\" composting at the higher temperatures required by the animal by-products regulations. Oxo-biodegradable plastic is bio-assimilated by the same bacteria and fungi, which transform natural material such as twigs and leaves to cell biomass, like lignocellulosic materials. Oxo-biodegradable plastic is designed to degrade initially by a process which includes both photo-oxidation and thermo-oxidation, so it can degrade in the dark. Resin identification code 7 is applicable to biodegradable plastics.\n\n\n=== Drawstring and flexibility ===\nIn 1984, the drawstring garbage bag was introduced by GLAD and Hefty. In 2001, Hefty introduced a garbage bag with a drawstring designed to stretch around the garbage can's rim and stay in place. In July 2004, ForceFlex, a flexible plastic garbage bag, was introduced by GLAD (followed by Hefty's Ultra Flex brand in September).\n\n\n== See also ==\nBlue bag\nPackaging\nPlastic bag\nPlastic recycling\nThermal depolymerization, post consumer waste processing technologies\n\n\n== References ==\n\n\n=== Books ===\nBrody, A. L., and Marsh, K, S., Encyclopedia of Packaging Technology, John Wiley & Sons, 1997, ISBN 0-471-06397-5\nSelke, S, Packaging and the Environment, 1994, ISBN 1-56676-104-2\nSelke, S,. Plastics Packaging, 2004, ISBN 1-56990-372-7", 
                "titleUrl": "https://en.wikipedia.org/wiki/Bin_bag", 
                "title": "Bin bag"
            }, 
            {
                "snippet": "used in other applications requiring low volatility and excellent thermo-oxidative and ionizing radiation stability. Such applications include use as", 
                "pageCategories": "Lubricants\nPlastics\nPolyethers", 
                "pageContent": "Phenyl ether polymers are a class of polymers that contain a phenoxy and/or a thiophenoxy group as the repeating group in ether linkages. Commercial phenyl ether polymers belong to two chemical classes: polyphenyl ethers (PPEs) and polyphenylene oxides (PPOs). The phenoxy groups in the former class of polymers do not contain any substituents whereas those in the latter class contain 2 to 4 alkyl groups on the phenyl ring. The structure of an oxygen-containing PPE is provided in Figure 1 and that of a 2, 6-xylenol derived PPO is shown in Figure 2. Either class can have the oxygen atoms attached at various positions around the rings.\n\n\n== Structure and synthesis ==\n\nThe proper name for a phenyl ether polymer is poly(phenyl ether) or polyphenyl polyether, but the name polyphenyl ether is widely accepted. Polyphenyl ethers (PPEs) are obtained by repeated application of the Ullmann Ether Synthesis: reaction of an alkali-metal phenate with a halogenated benzene catalyzed by copper.\nPPEs of up to 6 phenyl rings, both oxy and thio ethers, are commercially available. See Table 1. They are characterized by indicating the substitution pattern of each ring, followed by the number of phenyl rings and the number of ether linkages. Thus, the structure in Figure 1 with n equal to 1 is identified as pmp5P4E, indicating para, meta, para substitution of the three middle rings, a total of 5 rings, and 4 ether linkages. Meta substitution of the aryl rings in these materials is most common and often desired. Longer chain analogues with up to 10 benzene rings are also known.\nThe simplest member of the phenyl ether family is diphenyl ether (DPE), also called diphenyl oxide, the structure of which is provided in Figure 4. Low molecular weight polyphenyl ethers and thioethers are used in a variety of applications, and include high-vacuum devices, optics, electronics, and in high-temperature and radiation-resistant fluids and greases. Figure 5 shows the structure of the sulfur analogue of 3-R polyphenyl ether shown in Figure 3.\n\n\n== Physical properties ==\nTypical physical properties of polyphenyl ethers are provided in Table 2. Physical properties of a particular PPE depend upon the number of aromatic rings, their substitution pattern, and whether it is an ether or a thioether. In the case of products of mixed structures, properties are hard to predict from only the structural features; hence, they must be determined via measurement.\nThe important attributes of PPEs include their thermal and oxidative stability and stability in the presence of ionizing radiation. PPEs have the disadvantage of having somewhat high pour points. For example, PPEs that contain two and three benzene rings are actually solids at room temperatures. The melting points of the ordinarily solid PPEs are lowered if they contain more m-phenylene rings, alkyl groups, or are mixtures of isomers. PPEs that contain only o- and p-substituted rings have the highest melting points.\n\n\n=== Thermo-oxidative stability ===\nPPEs have excellent high temperature properties and good oxidation stability. With respect to volatilities, p-derivatives have the lowest volatilities, and the o-derivatives have the highest volatilities. The opposite is true for flash points and fire points. Spontaneous ignition temperatures of polyphenyl ethers lie between 550 and 595 \u00b0C (1,022 and 1,103 \u00b0F), alkyl substitution reduces this value by ~50 \u00b0C (122 \u00b0F). PPEs are compatible with most metals and elastomers that are commonly used in high-temperature applications. They typically swell common seal materials.\nOxidation stability of un-substituted PPEs is quite good, partly because they lack easily oxidizable carbon-hydrogen bonds. Thermal decomposition temperature, as measured by the isoteniscope procedure, is between 440 and 465 \u00b0C (824 and 869 \u00b0F).\n\n\n=== Radiation stability ===\nIonizing radiation affects all organic compounds, causing a change in their properties because radiation disrupts covalent bonds that are most prevalent in organic compounds. One result of ionization is that the organic molecules disproportionate to form smaller hydrocarbon molecules as well as larger hydrocarbons molecules. This is reflected by increased evaporation loss, lowering of the flash and fire points, and increased viscosity. Other chemical reactions caused by radiation include oxidation and isomerization. The former leads to increased acidity, corrosivity, and coke formation; the latter causes a change in viscosity and volatility.\nPPEs have extremely high radiation resistance. Of all classes of synthetic lubricants (with the possible exception of perfluoropolyethers) the polyphenyl ethers are the most radiation resistant. Excellent radiation stability of PPEs can be ascribed to the limited number of ionizable carbon-carbon and carbon-hydrogen bonds. In one study, the performance of PPE under the influence of 1x1011 ergs/gram of radiation at 99 \u00b0C (210 \u00b0F) was compared with synthetic ester, synthetic hydrocarbon, and silicone fluids. PPE showed a viscosity increase of only 35%, while all other fluids showed a viscosity increase of 1700% and gelled. Further tests have shown PPEs to be resistant to gamma and associated neutron radiation dosages of 1x1010 erg/g at temperatures up to 315 \u00b0C (599 \u00b0F).\n\n\n=== Surface tension ===\nPPEs have high surface tension; hence these fluids have a lower tendency to wet metal surfaces. The surface tension of the commercially available 5R4E is 49.9 dynes/cm, one of the highest in pure organic liquids. This property is useful in applications where migration of the lubricant into the surrounding environment must be avoided.\n\n\n== Applications ==\nWhile originally PPEs were developed for use in extreme environments that were experienced in aerospace applications, they are now used in other applications requiring low volatility and excellent thermo-oxidative and ionizing radiation stability. Such applications include use as diffusion pump fluids; high vacuum fluids; and in formulating jet engine/turbine lubricants, high-temperature hydraulic lubricants and greases, and heat transfer fluids. In addition, because of excellent optical properties these fluids have found use in optical devices.\n\n\n=== Ultra-high-vacuum fluids ===\nVacuum pumps are devices that remove gases from an enclosed space to greatly reduce pressure. Oil diffusion pumps in combination with a fore pump are amongst the most popular. Diffusion pumps use a high boiling liquid of low vapor pressure to create a high-speed jet that strikes the gaseous molecules in the system to be evacuated and direct them into space that is being evacuated by the fore pump. A good diffusion fluid must therefore reflect low vapor pressure, high flash point, high thermal and oxidative stability and chemical resistance. If the diffusion pump is operating in the proximity of ionizing radiation source, good radiation stability is also desired.\nData presented in Table 3 demonstrates polyphenyl ether to be superior to other fluids that are commonly used in diffusion pumps. PPEs help achieve the highest vacuum of 4 x 10\u221210 torr at 25 \u00b0C. Such high vacuums are necessary in equipment such as electron microscopes, mass spectrometers and that used for various surface physics studies. Vacuum pumps are also used in the production of electric lamps, vacuum tubes, and cathode ray tubes (CRTs), semiconductor processing, and vacuum engineering.\n\n\n=== Electronic connector lubricants ===\n5R4E PPE has a surface tension of 49.9 dynes/cm, which is amongst the highest in pure organic liquids. Because of this, this PPE and the other PPEs do not effectively wet metal surfaces. This property is useful when migration of a lubricant from one part of the equipment to another part must be avoided, such as in certain electronic devices. A thin film of polyphenyl ether on a surface is not a thin contiguous film as one would envision, but rather comprises tiny droplets. This PPE property tends to keep the film stationary, or at least to cause it to remain in the area where the lubrication is needed, rather than migrating away by spreading and forming a new surface. As a result, contamination of other components and equipment, which do not require a lubricant, is avoided. The high surface tension of PPEs, therefore, makes them useful in lubricating electronic contacts.\nPolyphenyl ether lubricants have a 30-year history of commercial service for connectors with precious and base metal contacts in telecom, automotive, aerospace, instrumentation and general-purpose applications. In addition to maintaining the current flow and providing long-term lubrication, PPEs offer protection to connectors against aggressive acidic and oxidative environments. By providing a protective surface film, polyphenyl ethers not only protect connectors against corrosion but also against vibration-related wear and abrasion that leads to fretting wear. The devices that benefit from the specialized properties of PPEs include cell phones, printers, and a variety of other electronic appliances. The protection lasts for decades or for the life of the equipment.\n\n\n=== Optics ===\nPolyphenyl ethers (PPEs) possess good optical clarity, a high refractive index, and other beneficial optical properties. Because of these, PPEs have the ability to meet the rigorous performance demands of signal processing in advanced photonics systems. Optical clarity of PPEs resembles that of the other optical polymers, that is, they have refractive indices of between 1.5 and 1.7 and provide good propagation of light between approximately 400 nm and 1700 nm. Close refractive index (RI) matching between materials is important for proper propagation of light through them. Because of the ease of RI matching, PPEs are used in many optical devices as optical fluids. Extreme resistance to ionizing radiation gives PPEs an added advantage in the manufacture of solar cells and solid-state UV/blue emitters and telecommunication equipment made from high-index glasses and semiconductors.\n\n\n=== High-temperature and radiation-resistant lubricants ===\nPPEs, being of excellent thermo-oxidative stability and radiation resistance, have found extensive use in high temperature applications that also require radiation resistance. In addition, PPEs demonstrate better wear control and load-carrying ability than mineral oils, especially when used in bearings.\nAs noted earlier, PPEs were developed for use in jet engines that involved high speed-related frictional temperatures of as high as 320 \u00b0C (608 \u00b0F). While the use of PPEs in lubricating jet engines has somewhat subsided due to their higher cost, they are still used in some aerospace applications. PPEs are also used as base fluids for radiation-resistant greases used in nuclear power plant mechanisms. PPEs and their derivatives have also found use as vapor phase lubricants in gas turbines and custom bearings, and wherever extreme environmental conditions exist. Vapor phase lubrication is achieved by heating the liquid lubricant above its boiling point. The resultant vapors are then transported to the hot bearing surface. If the temperatures of the bearing surface are kept below the lubricant\u2019s boiling point, the vapors re-condense to provide liquid lubrication.\nPolyphenyl ether technology can also provide superior fire safety and fatigue life, depending on the specific bearing design. In this application, PPEs have the advantage of providing lubrication both as a liquid at low temperatures and as a vapor at temperatures above 315 \u00b0C (599 \u00b0F). Due to the low volatility and excellent high-temperature thermo-oxidative stability, PPEs have also found use as a lubricant for chains used in and around kilns, metal fabrication plants, and glass molding and manufacturing equipment. In these high-temperature applications, PPEs do not form any sludge and hard deposits. The low soft-carbon residue that is left behind is removed easily by wiping. PPEs' low volatility, low flammability, and good thermodynamic properties make them ideally suited for use as heat transfer fluids and in heat sink applications as well.\n\n\n== Polyphenylene oxides (PPOs) ==\n\nThese polymers are made through oxidative coupling of substituted phenol in the presence of oxygen and copper and amine containing catalysts, such as cuprous bromide and pyridine. See Figure 2 for the PPO structure. PPO polymers can be classified as plastic resins. They and their composites with polystyrene, glass, and nylon are used as high-strength, moisture-resistant engineering plastics in a number of industries, including computer, telecommunication, and automotive parts. PPOs are marketed by SABIC Innovative Plastics under the trademarked name of Noryl.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Polyphenyl_ether", 
                "title": "Polyphenyl ether"
            }, 
            {
                "snippet": " chemical and hydrolytic resistance, gaseous barrier, thermal and thermo-oxidative resistance and ultraviolet (UV) light barrier resistance compared to", 
                "pageCategories": "All stub articles\nPlastics\nPolyesters\nPolymer stubs", 
                "pageContent": "Polyethylene naphthalate (PEN) (Poly(ethylene 2,6-naphthalate) is a polyester with good barrier properties (even better than Polyethylene terephthalate). Because it provides a very good oxygen barrier, it is particularly well-suited for bottling beverages that are susceptible to oxidation, such as beer. It is also used in making high performance sailcloth. It also has been found to show supreme scintillation properties and is expected to replace classic plastic scintillators.\n\n\n== Production ==\nPolyethylene Naphthalate (PEN, Poly(ethylene-2,6-naphthalene dicarboxylate, CAS No: 25853-85-4) is a polyester polymer of naphthalene-2,6-dicarboxylate and ethylene glycol.\nThere are two major manufacturing routes for PEN, i.e. an ester or an acid process, named according to whether the starting monomer is a diester or a diacid of naphthalene, respectively. In both cases for PEN, the glycol monomer is ethylene glycol.\nSolid-state polymerization (SSP) of the melt-produced resin pellets is the preferred process to increase the average molecular weight of PEN.\n\n\n== Applications ==\nSignificant commercial markets have been developed for its application in textile and industrial fibers, films, and foamed articles, containers for carbonated beverages, water and other liquids, and thermoformed applications. It is also an emerging material for modern electronic devices.\nPEN was the medium for Advanced Photo System film (discontinued in 2011).\nPEN is used for manufacturing high performance fibers that have very high modulus and better dimensional stability than PET or Nylon fibers.\nPEN is used as the substrate for some Linear Tape-Open (LTO) cartridges.\n\n\n== Benefits when compared to PET (Polyethylene Terephthalate) ==\nThe two condensed aromatic rings of PEN confer on it improvements in strength and modulus, chemical and hydrolytic resistance, gaseous barrier, thermal and thermo-oxidative resistance and ultraviolet (UV) light barrier resistance compared to PET.\nPEN is intended as a PET replacement, especially when used as a substrate for flexible integrated circuits.\nIt is prepared from ethylene glycol and one or more naphthalene dicarboxylic acids by condensation polymerization.\n\n\n== External links ==\nAdvansa Sasa Polyester San A.S. [1]\nDuPont Teijin Films TEONEX Q83 Data sheet [2]\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Polyethylene_naphthalate", 
                "title": "Polyethylene naphthalate"
            }, 
            {
                "snippet": "bilayers, and biomembranes. (3) The preparation of high temperature thermo-oxidatively stable aerospace polymers. (4) The preparation, characterization and", 
                "pageCategories": "1946 births\nAmerican emigrants to Israel\nBar-Ilan University faculty\nBrooklyn College alumni\nHarvard University alumni\nIsraeli Orthodox Jews\nIsraeli Orthodox rabbis\nIsraeli chemists\nIsraeli feminists\nJewish chemists", 
                "pageContent": "Aryeh Abraham Frimer (Hebrew: \u05d0\u05e8\u05d9\u05d4 \u05d0\u05d1\u05e8\u05d4\u05dd \u05e4\u05e8\u05d9\u05de\u05e8) (born November 24, 1946) is a noted Israeli Active Oxygen Chemist and specialist on women and Jewish law.\n\n\n== Biography ==\nAryeh Abraham Frimer was born in Minneapolis Minn. to Rabbi Dr. Norman E. Frimer and Esther Miriam Frimer on November 24, 1946, but spent most of his formative years in Brooklyn, NY. He graduated Summa Cum Laude with honors from Brooklyn College (Phi Beta Kappa; Sigma Xi) with a B.S. degree in Chemistry in 1969. During his undergraduate years he also studied at Yeshivat Kerem B'Yavneh in Israel and then Yeshivat Eretz Israel, Brooklyn NY, with Rabbi Judah Gershuni (one of the last students of Rabbi Abraham Isaac Kook) - ultimately receiving Rabbinic Ordination. While a graduate student in organic chemistry at Harvard University with Prof. Paul Doughty Bartlett, he was a National Science Foundation and Danforth Foundation Fellow. He was also appointed Assistant to the Hillel Director, serving as Rabbi to the Harvard-Radcliffe Orthodox Minyan from 1969-1974. Upon completing his Ph.D. at Harvard in 1974, Aryeh and his family moved to Israel becoming a Post-Doctoral Fellow at The Weizmann Institute of Science with Prof. Dov Elad. He then joined the faculty of Bar Ilan University in 1975, where he is the Ethel and David Resnick Professor of Active Oxygen Chemistry and former Chemistry Department Chairman. From 1982-1983, Prof. Frimer was appointed Visiting Medical Scientist at Brookhaven National Laboratory in Upton, Long Island, NY. From 1990-2004, he spent a sabbatical year and consecutive summers as a National Research Council Fellow and an Ohio Aerospace Institute Senior Research Associate at NASA's Glenn Research Center in Cleveland, Ohio. In June 2015, he received Bar Ilan University\u2019s Excellence in Teaching Award, and in October 2015, he became a Professor Emeritus. Aryeh and his wife Esther (Neiman) Frimer have four children and twelve grandchildren and live in Rehovot, Israel.\n\n\n== Scientific Interests and Publications ==\nProf. Frimer\u2019s research interests include: (1) The organic chemistry of active oxygen species: singlet and triplet molecular oxygen, peroxides, oxy-radicals and superoxide anion radical. (2) The organic chemistry of active oxygen species within organic media, liposomal lipid bilayers, and biomembranes. (3) The preparation of high temperature thermo-oxidatively stable aerospace polymers. (4) The preparation, characterization and neutralization of \u201cgreen\u201d reduced sensitivity high energy compounds. (5) The formation and chemistry of lithium-oxygen species in Li-O\n  \n    \n      \n        \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle _{2}}\n   batteries. Prof. Frimer has published 140 scientific articles, reviews and books in the area of active oxygen chemistry, and given presentations at more than 120 scientific meetings. In 1985, together with Prof. Ionel Rosental, he edited two special issues of the Israeli Journal of Chemistry on \u201cActive Oxygen Chemistry\u201d. In the same year, the Chemical Rubber Company (CRC) published his four volume series on \"Singlet O\n  \n    \n      \n        \n          \n          \n            2\n          \n        \n      \n    \n    {\\displaystyle _{2}}\n  \".\n\n\n== Rabbinic Studies and Writings ==\nIn addition to his scientific work, Rabbi Frimer has lectured on Judaism, Zionism and Jewish Identity for officer training courses of the Israel Defense Forces. He has also published some 50 Torah articles and lectured internationally on various aspects of Jewish tradition and Halakha (Jewish law). He is known most prominently for his scholarly writings on the status of women in Jewish ritual and law. He serves on the Rabbinic Board of the Rabbi Jacob Berman Community Center \u2013 Tiferet Moshe Synagogue in Rehovot, Israel. The Community Center maintains a website with more than 80 recorded lectures by Rabbi Frimer given predominantly from 1997-2000 at the Tiferet Moshe Synagogue. The site contains assorted published papers, audio files, source material and unedited lecture notes on women and Jewish law. Rabbi Prof. Aryeh Frimer has published, inter alia, on women and minyan, women\u2019s megilla readings, women in Jewish leadership roles, on questions of liturgy and ritual, and Orthodox Feminism.,  Together with his brother Rabbi Prof. Dov Frimer, he has published several seminal papers on Women\u2019s Prayer (Tefilla) Groups and Partnership Minyanim including the issue of women\u2019s aliyyot., \n\n\n== References ==\n\n\n== See also ==\nKavod HaBriyot\nPartnership Minyan", 
                "titleUrl": "https://en.wikipedia.org/wiki/Aryeh_Frimer", 
                "title": "Aryeh Frimer"
            }
        ], 
        "phraseCharStart": "21"
    }, 
    {
        "phraseCharEnd": "69", 
        "phraseIndex": "T2", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "ITER", 
        "wikiSearchResults": [
            {
                "snippet": "43\u00b042\u203217.84\u2033N 5\u00b046\u20329.1\u2033E\ufeff / \ufeff43.7049556\u00b0N 5.769194\u00b0E\ufeff / 43.7049556; 5.769194 ITER (International Thermonuclear Experimental Reactor, and is also Latin for", 
                "pageCategories": "All articles with dead external links\nAll articles with unsourced statements\nArticles with French-language external links\nArticles with dead external links from August 2012\nArticles with unsourced statements from April 2008\nArticles with unsourced statements from December 2015\nArticles with unsourced statements from February 2007\nArticles with unsourced statements from March 2016\nBuildings and structures in Bouches-du-Rh\u00f4ne\nCommons category with local link same as on Wikidata", 
                "pageContent": "ITER (International Thermonuclear Experimental Reactor, and is also Latin for \"the way\") is an international nuclear fusion research and engineering megaproject, which will be the world's largest magnetic confinement plasma physics experiment. It is an experimental tokamak nuclear fusion reactor that is being built next to the Cadarache facility in Saint-Paul-l\u00e8s-Durance, south of France.\nThe ITER project aims to make the long-awaited transition from experimental studies of plasma physics to full-scale electricity-producing fusion power stations. The ITER fusion reactor has been designed to produce 500 megawatts of output power for several seconds while needing 50 megawatts to operate. Thereby the machine aims to demonstrate the principle of producing more energy from the fusion process than is used to initiate it, something that has not yet been achieved in any fusion reactor.\nThe project is funded and run by seven member entities\u2014the European Union, India, Japan, China, Russia, South Korea, and the United States. The EU, as host party for the ITER complex, is contributing about 45 percent of the cost, with the other six parties contributing approximately 9 percent each.\nConstruction of the ITER Tokamak complex started in 2013 and the building costs are now over US$14 billion as of June 2015. The facility is expected to finish its construction phase in 2019 and will start commissioning the reactor that same year and initiate plasma experiments in 2020 with full deuterium\u2013tritium fusion experiments starting in 2027. If ITER becomes operational, it will become the largest magnetic confinement plasma physics experiment in use, surpassing the Joint European Torus. The first commercial demonstration fusion power station, named DEMO, is proposed to follow on from the ITER project.\n\n\n== Background ==\nFusion power has the potential to provide sufficient energy to satisfy mounting demand, and to do so sustainably, with a relatively small impact on the environment.\nNuclear fusion has many potential attractions. Firstly, its hydrogen isotope fuels are relatively abundant \u2013 one of the necessary isotopes, deuterium, can be extracted from seawater, while the other fuel, tritium, would be bred from a lithium blanket using neutrons produced in the fusion reaction itself. Furthermore, a fusion reactor would produce virtually no CO2 or atmospheric pollutants, and its other radioactive waste products would be very short-lived compared to those produced by conventional nuclear reactors.\nOn 21 November 2006, the seven participants formally agreed to fund the creation of a nuclear fusion reactor. The program is anticipated to last for 30 years \u2013 10 for construction, and 20 of operation. ITER was originally expected to cost approximately \u20ac5billion, but the rising price of raw materials and changes to the initial design have seen that amount almost triple to \u20ac13billion. The reactor is expected to take 10 years to build with completion scheduled for 2019. Site preparation has begun in Cadarache, France, and procurement of large components has started.\nITER is designed to produce approximately 500 MW of fusion power sustained for up to 1,000 seconds (compared to JET's peak of 16 MW for less than a second) by the fusion of about 0.5 g of deuterium/tritium mixture in its approximately 840 m3 reactor chamber. Although ITER is expected to produce (in the form of heat) 10 times more energy than the amount consumed to heat up the plasma to fusion temperatures, the generated heat will not be used to generate any electricity.\nITER was originally an acronym for International Thermonuclear Experimental Reactor, but that title was eventually dropped due to the negative popular connotations of the word \"thermonuclear\", especially when used in conjunction with \"experimental\". \"Iter\" also means \"journey\", \"direction\" or \"way\" in Latin, reflecting ITER's potential role in harnessing nuclear fusion as a peaceful power source.\n\n\n== Organization history ==\n\nITER began in 1985 as a Reagan\u2013Gorbachev initiative with the equal participation of the Soviet Union, European Union (through European Atomic Energy Community), the United States, and Japan through the 1988\u20131998 initial design phases. Preparations for the first Gorbachev-Reagan Summit showed that there were no tangible agreements in the works for the summit.\nOne energy research project, however, was being considered quietly by two physicists, Alvin Trivelpiece and Evgeny Velikhov. The project involved collaboration on the next phase of magnetic fusion research \u2014 the construction of a demonstration model. At the time, magnetic fusion research was ongoing in Japan, Europe, the Soviet Union and the US. Velikhov and Trivelpiece believed that taking the next step in fusion research would be beyond the budget of any of the key nations and that collaboration would be useful internationally.\nA major bureaucratic fight erupted in the US government over the project. One argument against collaboration was that the Soviets would use it to steal US technology and know-how. A second was symbolic \u2014 the Soviet physicist Andrei Sakharov was in internal exile and the US was pushing the Soviet Union on its human rights record. The United States National Security Council convened a meeting under the direction of William Flynn Martin that resulted in a consensus that the US should go forward with the project.\nMartin and Velikhov concluded the agreement that was agreed at the summit and announced in the last paragraph of this historic summit meeting, \"... The two leaders emphasized the potential importance of the work aimed at utilizing controlled thermonuclear fusion for peaceful purposes and, in this connection, advocated the widest practicable development of international cooperation in obtaining this source of energy, which is essentially inexhaustible, for the benefit for all mankind.\"\nConceptual and engineering design phases carried out under the auspices of the IAEA led to an acceptable, detailed design in 2001, underpinned by US$650 million worth of research and development by the \"ITER Parties\" to establish its practical feasibility. These parties, namely EU, Japan, Russian Federation (replacing the Soviet Union), and United States (which opted out of the project in 1999 and returned in 2003), were joined in negotiations by China, South Korea, and Canada (who then terminated its participation at the end of 2003). India officially became part of ITER on December 2005.\nOn 28 June 2005, it was officially announced that ITER would be built in the European Union in Southern France. The negotiations that led to the decision ended in a compromise between the EU and Japan, in that Japan was promised 20% of the research staff on the French location of ITER, as well as the head of the administrative body of ITER. In addition, another research facility for the project will be built in Japan, and the European Union has agreed to contribute about 50% of the costs of this institution.\nOn 21 November 2006, an international consortium signed a formal agreement to build the reactor. On 24 September 2007, the People's Republic of China became the seventh party to deposit the ITER Agreement to the IAEA. Finally, on 24 October 2007, the ITER Agreement entered into force and the ITER Organization legally came into existence.\n\n\n== Objectives ==\nITER's mission is to demonstrate the feasibility of fusion power, and prove that it can work without negative impact. Specifically, the project aims:\nTo momentarily produce ten times more thermal energy from fusion heating than is supplied by auxiliary heating (a Q value equals 10).\nTo produce a steady-state plasma with a Q value greater than 5. (Q = 1 is breakeven.)\nTo maintain a fusion pulse for up to 8 minutes.\nTo ignite a \"burning\" (self-sustaining) plasma. (i.e. 'ignition' see Lawson criterion)\nTo develop technologies and processes needed for a fusion power station \u2014 including superconducting magnets and remote handling (maintenance by robot).\nTo verify tritium breeding concepts.\nTo refine neutron shield/heat conversion technology (most of the energy in the D+T fusion reaction is released in the form of fast neutrons).\n\n\n== Timeline and current status ==\nIn 1978, the EC, Japan, USA, and USSR joined in the International Tokamak Reactor (INTOR) Workshop, under the auspices of the International Atomic Energy Agency (IAEA), to assess the readiness of magnetic fusion to move forward to the experimental power reactor (EPR) stage, to identify the additional R&D that must be undertaken, and to define the characteristics of such an EPR by means of a conceptual design.\nHundreds of fusion scientists and engineers in each participating country took part in a detailed assessment of the then present status of the tokamak confinement concept vis-a-vis the requirements of an EPR, identified the required R&D by early 1980, and produced a conceptual design by mid-1981.\nTimeline:\n1985. At the Geneva summit meeting in 1985, Mikhail Gorbachev suggested to Ronald Reagan that the two countries jointly undertake the construction of a tokamak EPR as proposed by the INTOR Workshop. The ITER project was initiated in 1988.\n1988. Conceptual design activities ran from 1988 to 1990.\n1992. Engineering design activities started.\n1998. In June, the 'Final design' from the Engineering Design Activities was approved.\n2001. In July, the \"cost-cutting\" 'ITER-FEAT' design was agreed.\n2006. The ITER project was formally agreed to and funded with a cost estimate of \u20ac10 billion ($12.8 billion) projecting the start of construction in 2008 and completion a decade later.\n2007. In September, fourteen major design changes were agreed to the 2001 design.\n2013. The project had run into many delays and budget overruns. The facility is not expected to begin operations at the schedule initially anticipated.\n2014. In February, The New Yorker published the ITER Management Assessment report, listing 11 essential recommendations, for example: \"Create a Project Culture\", \"Instill a Nuclear Safety Culture\", \"Develop a realistic ITER Project Schedule\" and \"Simplify and Reduce the IO Bureaucracy\". The USA considered withdrawal, but is still participating in ITER.\n2015. In November a project review concludes that the schedule may need extending by at least six years; (e.g. first plasma in 2026).\n2016. Atomic Energy Organization of Iran completed the preliminary work for Iran to join ITER\n\n\n== Reactor overview ==\n\nWhen deuterium and tritium fuse, two nuclei come together to form a helium nucleus (an alpha particle), and a high-energy neutron.\n2\n1D + 3\n1T \u2192 4\n2He + 1\n0n + 6988281983061712000\u266017.6 MeV\nWhile nearly all stable isotopes lighter on the periodic table than iron-56 and nickel-62, which have the highest binding energy per nucleon, will fuse with some other isotope and release energy, deuterium and tritium are by far the most attractive for energy generation as they require the lowest activation energy (thus lowest temperature) to do so, while producing among the most energy per unit weight.\nAll proto- and mid-life stars radiate enormous amounts of energy generated by fusion processes. Mass for mass, the deuterium\u2013tritium fusion process releases roughly three times as much energy as uranium-235 fission, and millions of times more energy than a chemical reaction such as the burning of coal. It is the goal of a fusion power station to harness this energy to produce electricity.\nActivation energies for fusion reactions are generally high because the protons in each nucleus will tend to strongly repel one another, as they each have the same positive charge. A heuristic for estimating reaction rates is that nuclei must be able to get within 100 femtometer (1 \u00d7 10\u221213 meter) of each other, where the nuclei are increasingly likely to undergo quantum tunneling past the electrostatic barrier and the turning point where the strong nuclear force and the electrostatic force are equally balanced, allowing them to fuse. In ITER, this distance of approach is made possible by high temperatures and magnetic confinement. High temperatures give the nuclei enough energy to overcome their electrostatic repulsion (see Maxwell\u2013Boltzmann distribution). For deuterium and tritium, the optimal reaction rates occur at temperatures on the order of 100,000,000 K. The plasma is heated to a high temperature by ohmic heating (running a current through the plasma). Additional heating is applied using neutral beam injection (which cross magnetic field lines without a net deflection and will not cause a large electromagnetic disruption) and radio frequency (RF) or microwave heating.\nAt such high temperatures, particles have a large kinetic energy, and hence velocity. If unconfined, the particles will rapidly escape, taking the energy with them, cooling the plasma to the point where net energy is no longer produced. A successful reactor would need to contain the particles in a small enough volume for a long enough time for much of the plasma to fuse. In ITER and many other magnetic confinement reactors, the plasma, a gas of charged particles, is confined using magnetic fields. A charged particle moving through a magnetic field experiences a force perpendicular to the direction of travel, resulting in centripetal acceleration, thereby confining it to move in a circle or helix around the lines of magnetic flux.\nA solid confinement vessel is also needed, both to shield the magnets and other equipment from high temperatures and energetic photons and particles, and to maintain a near-vacuum for the plasma to populate. The containment vessel is subjected to a barrage of very energetic particles, where electrons, ions, photons, alpha particles, and neutrons constantly bombard it and degrade the structure. The material must be designed to endure this environment so that a power station would be economical. Tests of such materials will be carried out both at ITER and at IFMIF (International Fusion Materials Irradiation Facility).\nOnce fusion has begun, high energy neutrons will radiate from the reactive regions of the plasma, crossing magnetic field lines easily due to charge neutrality (see neutron flux). Since it is the neutrons that receive the majority of the energy, they will be ITER's primary source of energy output. Ideally, alpha particles will expend their energy in the plasma, further heating it.\nBeyond the inner wall of the containment vessel one of several test blanket modules will be placed. These are designed to slow and absorb neutrons in a reliable and efficient manner, limiting damage to the rest of the structure, and breeding tritium for fuel from lithium and the incoming neutrons. Energy absorbed from the fast neutrons is extracted and passed into the primary coolant. This heat energy would then be used to power an electricity-generating turbine in a real power station; in ITER this generating system is not of scientific interest, so instead the heat will be extracted and disposed of.\n\n\n== Technical design ==\n\n\n=== Vacuum vessel ===\nThe vacuum vessel is the central part of the ITER machine: a double walled steel container in which the plasma is contained by means of magnetic fields.\nThe ITER vacuum vessel will be twice as large and 16 times as heavy as any previously manufactured fusion vessel: each of the nine torus shaped sectors will weigh between 390 and 430 tonnes. When all the shielding and port structures are included, this adds up to a total of 5,116 tonnes. Its external diameter will measure 19.4 metres (64 ft), the internal 6.5 metres (21 ft). Once assembled, the whole structure will be 11.3 metres (37 ft) high.\nThe primary function of the vacuum vessel is to provide a hermetically sealed plasma container. Its main components are the main vessel, the port structures and the supporting system. The main vessel is a double walled structure with poloidal and toroidal stiffening ribs between 60-millimetre-thick (2.4 in) shells to reinforce the vessel structure. These ribs also form the flow passages for the cooling water. The space between the double walls will be filled with shield structures made of stainless steel. The inner surfaces of the vessel will act as the interface with breeder modules containing the breeder blanket component. These modules will provide shielding from the high-energy neutrons produced by the fusion reactions and some will also be used for tritium breeding concepts.\nThe vacuum vessel has 18 upper, 17 equatorial and 9 lower ports that will be used for remote handling operations, diagnostic systems, neutral beam injections and vacuum pumping.\n\n\n=== Breeder blanket ===\nOwing to very limited terrestrial resources of tritium, a key component of the ITER reactor design is the breeder blanket. This component, located adjacent to the vacuum vessel, serves to produce tritium through reaction of 6Li isotopes with high energy neutrons from the plasma. Concepts for the breeder blanket include helium cooled lithium lead (HCLL) and helium cooled pebble bed (HCPB) methods. Test blanket modules based on both concepts will be tested in ITER and will share a common box geometry. Materials for use as breeder pebbles in the HCPB concept include lithium metatitanate and lithium orthosilicate. Requirements of breeder materials include good tritium production and extraction, mechanical stability and low activation levels.\n\n\n=== Magnet system ===\nThe central solenoid coil will use superconducting niobium-tin to carry 46 kA and produce a field of up to 13.5 teslas. The 18 toroidal field coils will also use niobium-tin. At their maximum field strength of 11.8 teslas, they will be able to store 41 gigajoules. They have been tested at a record 80 kA. Other lower field ITER magnets (PF and CC) will use niobium-titanium for their superconducting elements.\n\n\n=== Additional heating ===\nThere will be three types of external heating in ITER:\nTwo Heating Neutral Beam injectors (HNB), each providing about 17MW to the burning plasma, with the possibility to add a third one. The requirements in terms of deuterium beam energy (1MeV), total current (40A) and beam pulse duration (up to 1h). The prototype is being built at the a Neutral Beam Test Facility (NBTF) prototype is being constructed in Padova\nIon Cyclotron Resonance Heating (ICRH)\nElectron Cyclotron Resonance Heating (ECRH)\n\n\n=== Cryostat ===\nThe cryostat is a large 3,800-tonne stainless steel structure surrounding the vacuum vessel and the superconducting magnets, in order to provide a super-cool vacuum environment. Its thickness ranging from 50 to 250 mm will allow it to withstand the atmospheric pressure on the area of a volume of 8,500 cubic meters. The total of 54 modules of the cryostat will be engineered, procured, manufactured, and installed by Larsen & Toubro Heavy Engineering.\n\n\n=== Cooling systems ===\nThe ITER tokamak will use three interconnected cooling systems. Most of the heat will be removed by a primary water cooling loop, itself cooled by water through a heat exchanger within the tokamak building's secondary confinement. The secondary cooling loop will be cooled by a larger complex, comprising a cooling tower, a 5 km pipeline supplying water from Canal de Provence, and basins that allow cooling water to be cooled and tested for chemical contamination and tritium before being released into the Durance River. This system will need to dissipate an average power of 450 MW during the tokamak's operation. A liquid nitrogen system will provide a further 1,300 kW of cooling to 80 kelvins, and a liquid helium system will provide 75 kW of cooling to 4.5 K. The liquid helium system will be designed, manufactured, installed and commissioned by Air Liquide.\n\n\n== Location ==\n\nThe process of selecting a location for ITER was long and drawn out. The most likely sites were Cadarache in Provence-Alpes-C\u00f4te-d'Azur, France, and Rokkasho, Aomori, Japan. Additionally, Canada announced a bid for the site in Clarington in May 2001, but withdrew from the race in 2003. Spain also offered a site at Vandell\u00f2s on 17 April 2002, but the EU decided to concentrate its support solely behind the French site in late November 2003. From this point on, the choice was between France and Japan. On 3 May 2005, the EU and Japan agreed to a process which would settle their dispute by July.\nAt the final meeting in Moscow on 28 June 2005, the participating parties agreed to construct ITER at Cadarache in Provence-Alpes-C\u00f4te-d'Azur, France. Construction of the ITER complex began in 2007, while assembly of the tokamak itself is scheduled to begin in 2015.\nFusion for Energy, the EU agency in charge of the European contribution to the project, is located in Barcelona, Spain. Fusion for Energy (F4E) is the European Union's Joint Undertaking for ITER and the Development of Fusion Energy. According to the agency's website:\n\n\"F4E is responsible for providing Europe's contribution to ITER, the world's largest scientific partnership that aims to demonstrate fusion as a viable and sustainable source of energy. [...] F4E also supports fusion research and development initiatives [...]\"\n\nThe ITER Neutral Beam Test Facility aimed at developing and optimizing the neutral beam injector prototype, is being constructed in Padova. It will be the only ITER facility out of the site in Cadarache.\n\n\n== Participants ==\n\nCurrently there are seven parties participating in the ITER program: the European Union (through the legally distinct organisation EURATOM), India, Japan, China, Russia, South Korea, and the United States. Canada was previously a full member, but has since pulled out due to a lack of funding from the federal government. The lack of funding also resulted in Canada withdrawing from its bid for the ITER site in 2003. The host member of the ITER project, and hence the member contributing most of the costs, is the EU.\nIn 2007, it was announced that participants in the ITER will consider Kazakhstan's offer to join the program and in March 2009, Switzerland, an associate member of EURATOM since 1979, also ratified the country's accession to the European Domestic Agency Fusion for Energy as a third country member.\nITER's work is supervised by the ITER Council, which has the authority to appoint senior staff, amend regulations, decide on budgeting issues, and allow additional states or organizations to participate in ITER. The present Chairman of the ITER Council is Dr Hideyuki Takatsu \nParticipating countries\n\n\n== Funding ==\nAs of 2016, the total price of constructing the experiment is expected to be in excess of \u20ac20 billion, an increase of \u20ac4.6 billion of its 2010 estimate, and of \u20ac9.6 billion from the 2009 estimate. Prior to that, the proposed costs for ITER were \u20ac5 billion for the construction and \u20ac5 billion for maintenance and the research connected with it during its 35-year lifetime. At the June 2005 conference in Moscow the participating members of the ITER cooperation agreed on the following division of funding contributions: 45% by the hosting member, the European Union, and the rest split between the non-hosting members \u2013 China, India, Japan, South Korea, the Russian Federation and the USA. During the operation and deactivation phases, Euratom will contribute to 34% of the total costs.\nAlthough Japan's financial contribution as a non-hosting member is one-eleventh of the total, the EU agreed to grant it a special status so that Japan will provide for two-elevenths of the research staff at Cadarache and be awarded two-elevenths of the construction contracts, while the European Union's staff and construction components contributions will be cut from five-elevenths to four-elevenths.\nIt was reported in December 2010 that the European Parliament had refused to approve a plan by member states to reallocate \u20ac1.4 billion from the budget to cover a shortfall in ITER building costs in 2012\u201313. The closure of the 2010 budget required this financing plan to be revised, and the European Commission (EC) was forced to put forward an ITER budgetary resolution proposal in 2011.\nThe U.S. withdrew from the ITER consortium in 2000. In 2006, Congress voted to rejoin, and again contribute financially. In June 2015, it appeared that the U.S. Senate might vote to stop the scheduled U.S. contribution of $150 million in the 2015\u20132016 fiscal year.\n\n\n== Criticism ==\n\nA technical concern is that the 14 MeV neutrons produced by the fusion reactions will damage the materials from which the reactor is built. Research is in progress to determine whether and how reactor walls can be designed to last long enough to make a commercial power station economically viable in the presence of the intense neutron bombardment. The damage is primarily caused by high energy neutrons knocking atoms out of their normal position in the crystal lattice. A related problem for a future commercial fusion power station is that the neutron bombardment will induce radioactivity in the reactor material itself. Maintaining and decommissioning a commercial reactor may thus be difficult and expensive. Another problem is that superconducting magnets are damaged by neutron fluxes. A new special research facility, IFMIF, is planned to investigate this problem.\nAnother source of concern comes from the recent tokamak parameters database interpolation which says that power load on tokamak divertors will be five times the expected value for ITER and much more for actual electricity-generating reactors. Given that the projected power load on the ITER divertor is already very high, these new findings mean that new divertor designs should be urgently tested. However, the corresponding test facility (ADX) still has not received any funding.\nA number of fusion researchers working on non-tokamak systems, such as Robert Bussard and Eric Lerner, have been critical of ITER for diverting funding from what they believe could be a potentially more viable and/or cost-effective path to fusion power, such as the polywell reactor. Many critics accuse ITER researchers of being unwilling to face up to the technical and economic potential problems posed by Tokamak fusion schemes. The expected cost of ITER has risen from $5 billion USD to $20 billion USD, and the timeline for operation at full power was moved from the original estimate of 2016 to 2027.\nA French association including about 700 anti-nuclear groups, Sortir du nucl\u00e9aire (Get Out of Nuclear Energy), claimed that ITER was a hazard because scientists did not yet know how to manipulate the high-energy deuterium and tritium hydrogen isotopes used in the fusion process.\nRebecca Harms, Green/EFA member of the European Parliament's Committee on Industry, Research and Energy, said: \"In the next 50 years, nuclear fusion will neither tackle climate change nor guarantee the security of our energy supply.\" Arguing that the EU's energy research should be focused elsewhere, she said: \"The Green/EFA group demands that these funds be spent instead on energy research that is relevant to the future. A major focus should now be put on renewable sources of energy.\" French Green party lawmaker No\u00ebl Mam\u00e8re claims that more concrete efforts to fight present-day global warming will be neglected as a result of ITER: \"This is not good news for the fight against the greenhouse effect because we're going to put ten billion euros towards a project that has a term of 30\u201350 years when we're not even sure it will be effective.\"\nITER is not designed to produce electricity, but made as a proof of concept reactor for the later DEMO project.\n\n\n=== Responses to criticism ===\nProponents believe that much of the ITER criticism is misleading and inaccurate, in particular the allegations of the experiment's \"inherent danger.\" The stated goals for a commercial fusion power station design are that the amount of radioactive waste produced should be hundreds of times less than that of a fission reactor, and that it should produce no long-lived radioactive waste, and that it is impossible for any such reactor to undergo a large-scale runaway chain reaction. A direct contact of the plasma with ITER inner walls would contaminate it, causing it to cool immediately and stop the fusion process. In addition, the amount of fuel contained in a fusion reactor chamber (one half gram of deuterium/tritium fuel) is only sufficient to sustain the fusion burn pulse from minutes up to an hour at most, whereas a fission reactor usually contains several years' worth of fuel. Moreover, some detritiation systems will be implemented, so that at a fuel cycle inventory level of about 2 kg, ITER will eventually need to recycle large amounts of tritium and at turnovers orders of magnitude higher than any preceding tritium facility worldwide.\nIn the case of an accident (or sabotage), it is expected that a fusion reactor might release far less radioactive pollution than would an ordinary fission nuclear station. Furthermore, ITER's type of fusion power has little in common with nuclear weapons technology, and does not produce the fissile materials necessary for the construction of a weapon. Proponents note that large-scale fusion power would be able to produce reliable electricity on demand, and with virtually zero pollution (no gaseous CO2, SO2, or NOx by-products are produced).\nAccording to researchers at a demonstration reactor in Japan, a fusion generator should be feasible in the 2030s and no later than the 2050s. Japan is pursuing its own research program with several operational facilities that are exploring several fusion paths.\nIn the United States alone, electricity accounts for US$210 billion in annual sales. Asia's electricity sector attracted US$93 billion in private investment between 1990 and 1999. These figures take into account only current prices. Proponents of ITER contend that an investment in research now should be viewed as an attempt to earn a far greater future return. Also, worldwide investment of less than US$1 billion per year into ITER is not incompatible with concurrent research into other methods of power generation, which in 2007 totaled US$16.9 billion.\nSupporters of ITER emphasize that the only way to test ideas for withstanding the intense neutron flux is to experimentally subject materials to that flux, which is one of the primary missions of ITER and the IFMIF, and both facilities will be vitally important to that effort. The purpose of ITER is to explore the scientific and engineering questions that surround potential fusion power stations. It is nearly impossible to acquire satisfactory data for the properties of materials expected to be subject to an intense neutron flux, and burning plasmas are expected to have quite different properties from externally heated plasmas. Supporters contend that the answer to these questions requires the ITER experiment, especially in the light of the monumental potential benefits.\nFurthermore, the main line of research via tokamaks has been developed to the point that it is now possible to undertake the penultimate step in magnetic confinement plasma physics research with a self-sustained reaction. In the tokamak research program, recent advances devoted to controlling the configuration of the plasma have led to the achievement of substantially improved energy and pressure confinement, which reduces the projected cost of electricity from such reactors by a factor of two to a value only about 50% more than the projected cost of electricity from advanced light-water reactors. In addition, progress in the development of advanced, low activation structural materials supports the promise of environmentally benign fusion reactors and research into alternate confinement concepts is yielding the promise of future improvements in confinement. Finally, supporters contend that other potential replacements to the fossil fuels have environmental issues of their own. Solar, wind, and hydroelectric power all have a relatively low power output per square kilometer compared to ITER's successor DEMO which, at 2,000 MW, would have an energy density that exceeds even large fission power stations.\n\n\n== Similar projects ==\nPrecursors to ITER were JET and Tore Supra. Other planned and proposed fusion reactors include DEMO, Wendelstein 7-X, NIF, HiPER, and MAST, as well as CFETR (China Fusion Engineering Test Reactor), a 200 MW tokamak.\n\n\n== See also ==\n\nTokamak\nITER Neutral Beam Test Facility, the facility dedicated to the development of the ITER neutral beam injector prototype\nFusion for Energy, the Domestic Agency in charge of managing EU contributions to the ITER project\nInternational Fusion Materials Irradiation Facility, proposed, construction not started\nJT-60/JT-60SA\nEAST (Experimental Advanced Superconducting Tokamak)\nNational Ignition Facility, inertial confinement using lasers\nNuclear power in France\nWendelstein 7-X (German experimental fusion reactor) - a stellarator\nFusenet, European Fusion Education Network, 2008-2013\n\n\n== References ==\n\n\n== External links ==\nOfficial website\nThe New Yorker, Mar. 3 2014, Star in a Bottle, by Raffi Khatchadourian\nArchival material collected by Prof. McCray relating to ITER\u2019s early phase (1979\u20131989) can be consulted at the Historical Archives of the European Union in Florence\n\"Way to New Energy\" video (23:24) at YouTube, by RT, on May 6, 2014.\nThe roles of the Host and the non-Host for the ITER Project. June 2005 The broader approach agreement with Japan.\nFusion Electricity - A roadmap to the realisation of fusion energy EFDA 2012 - 8 missions, ITER, project plan with dependancies, ...", 
                "titleUrl": "https://en.wikipedia.org/wiki/ITER", 
                "title": "ITER"
            }, 
            {
                "snippet": "Iter-pi\u0161a, inscribed in cuneiform as i-te-er-pi/pi4-\u0161a and meaning \"Her command is surpassing\", ca. 1769\u20131767 BC (short chronology) or ca. 1833\u20131831 BC", 
                "pageCategories": "18th-century BC rulers\nBabylonian kings", 
                "pageContent": "Iter-pi\u0161a, inscribed in cuneiform as i-te-er-pi/pi4-\u0161a and meaning \"Her command is surpassing\", ca. 1769\u20131767 BC (short chronology) or ca. 1833\u20131831 BC (middle chronology), was the 12th king of Isin during the Old Babylonian period. The Sumerian King List tells us that \"the divine Iter-pi\u0161a ruled for 4 years.\" The Ur-Isin King List which was written in the 4th year of the reign of Damiq-ili\u0161u gives a reign of just 3 years.\n\n\n== Biography ==\nHe was a contemporary of Warad-Sin (ca. 1770 BC to 1758 BC) the king of Larsa, whose brother and successor, Rim-Sin I would eventually come to overthrow the dynasty, ending the cities' bitter rivalry around 40 years later. He is only known from Kings lists and year-name date formulae.\nA letter from Iter-pi\u0161a to a deity was excavated in a scribal school, \"House F,\" in Nippur during the 1951\u201352 dig season. The scribal school had operated during the 1740s, early in the reign of king Samsu-iluna and the piece had become a belle letter.\n\n\n== External links ==\nIter-pi\u0161a year-names at CDLI, but note the tablet reference BM 85384 in year-name (b) is incorrect.\n\n\n== Inscriptions ==\n\n\n== Notes ==\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Iter-pisha", 
                "title": "Iter-pisha"
            }, 
            {
                "snippet": "European Union (EU) organisation responsible for Europe\u2019s contribution to ITER, the world\u2019s largest scientific partnership aiming to demonstrate fusion", 
                "pageCategories": "Agencies of the European Union\nAll articles needing additional references\nArticles needing additional references from October 2015\nEuropean Atomic Energy Community\nWikipedia articles with possible conflicts of interest from October 2015", 
                "pageContent": "Fusion for Energy (F4E) is the European Union (EU) organisation responsible for Europe\u2019s contribution to ITER, the world\u2019s largest scientific partnership aiming to demonstrate fusion as a viable and sustainable source of energy. The organisation \u2013 formally known as the European Joint Undertaking for ITER and the Development of Fusion Energy \u2013was created under article 45 of the Euratom Treaty by the decision of the Council of the European Union on 27 March 2007 for a period of 35 years.\nF4E counts 400 members of staff and its offices are located in Barcelona, in Spain. One of its main tasks is to work together with European industry and research organisations to develop and provide a wide range of high technology components for the ITER project. The European Union is the host party for the ITER project. Its contribution amounts to 45%, while the other six parties have an in-kind contribution of approximately 9% each. Since 2008, F4E has been collaborating with at least 250 companies and more than 50 R&D organisations.\n\n\n== Mission and governance ==\nF4E\u2019s primary mission is to manage the European contribution to the ITER project; therefore it provides financial funds, which mostly come from the European Community budget. Among other tasks, F4E oversees the preparation of the ITER construction site in Saint-Paul-l\u00e8s-Durance, in France. F4E is formed by Euratom (represented by the European Commission), the Member States of the European Union and Switzerland, which participates as a third country. To ensure the overall supervision of its activities, the members sit on a governing board, which has a wide range of responsibilities including appointing the director.\n\n\n== Fusion energy ==\nFusion is the process which powers the sun, producing energy by fusing together light atoms such as hydrogen at extremely high pressures and temperatures. Fusion reactors use two forms of hydrogen, deuterium and tritium, as fuel.\nThe benefits of fusion energy are that it is an inherently safe process and it does not create greenhouse gases or long-lasting radioactive waste.\n\n\n== The ITER project ==\nITER, meaning \u201cthe way\u201d in Latin, is an international experiment aiming to demonstrate the scientific and technical feasibility of fusion as an energy source. The machine is being constructed in Saint-Paul-l\u00e8s-Durance in the South of France and is funded by seven parties: China, the European Union, India, Japan, Russia, South Korea and the United States. Collectively, the parties taking part in the ITER project represent over one half of the world\u2019s population and 80% of the global GDP.\n\n\n== The Broader Approach activities ==\nThe Broader Approach (BA) activities are three research projects carried out under an agreement between the European Atomic Energy Community (Euratom) and Japan, which contribute equally financially. They are meant to complement the ITER project and accelerate the development of fusion energy through R&D by cooperating on a number of projects of mutual interest.\nThis agreement entered into force on 1 June 2007 and runs for at least 10 years. The Broader Approach consists of three main projects located in Japan: the Satellite Tokamak Programme project JT-60SA (super advanced), the International Fusion Materials Irradiation Facility - Engineering Validation and Engineering Design Activities (IFMIF/EVEDA) and the International Fusion Energy Research Centre (IFERC).\n\n\n== The DEMO project ==\nF4E also aims to contribute to DEMO (Demonstration Power Plant). This experiment is supposed to generate significant amounts of electricity over extended periods and will be self-sufficient in tritium, one of the necessary gases to create fusion. The first commercial fusion electricity power plants are set to be established following DEMO, which is set to be larger in size than ITER and to produce significantly larger fusion power over long periods: a continuous production of up to 500 megawatts of electricity.\n\n\n== Management difficulties ==\nA report by the consultancy Ernst & Young published in 2013 by the European Parliament's Budgetary Control Committee found that F4E has suffered from significant management difficulties. According to the report, \"the organisation faced a series of internal problems that have only been gradually addressed, notably an organisational structure ill-adapted for project-oriented activities.\" From 2010, a host of reforms were undertaken within F4E, including a reshuffling and reorientation of the governance and management structures, as well as a cost-savings programme.\n\n\n== See also ==\nITER\nFusenet\nFusion power\nEuratom\n\n\n== References ==\n\n\n== External links ==\nFusion for Energy, the agency's home page.\nFusion for Energy: Understanding Fusion\nEuratom/fusion, the Fusion page of the EURATOM\n[1], the Broader Approach agreement", 
                "titleUrl": "https://en.wikipedia.org/wiki/Fusion_for_Energy", 
                "title": "Fusion for Energy"
            }, 
            {
                "snippet": "build upon the ITER experimental nuclear fusion reactor. The objectives of DEMO are usually understood to lie somewhere between those of ITER and a \"first", 
                "pageCategories": "All articles containing potentially dated statements\nAll articles with unsourced statements\nArticles containing potentially dated statements from 2016\nArticles with unsourced statements from August 2011\nFusion power\nInterlanguage link template link number\nProposed fusion reactors\nProposed nuclear power stations\nTokamaks\nUse dmy dates from August 2011", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/DEMO", 
                "title": "DEMO"
            }, 
            {
                "snippet": "Iter.Viator is the debut solo album of ex-Peccatum member, (and wife of Ihsahn) Ihriel. Translated from Latin \"Iter Viator\" literally means \"road traveler\"", 
                "pageCategories": "2002 debut albums\nArticles with album ratings that need to be turned into prose\nArticles with hAudio microformats", 
                "pageContent": "Iter.Viator is the debut solo album of ex-Peccatum member, (and wife of Ihsahn) Ihriel. Translated from Latin \"Iter Viator\" literally means \"road traveler\".\n\n\n== Track listing ==\nAll Songs Written & Arranged By Ihriel.\n\"Chasm Blue\" \u2013 1:44\n\"Sanies\" \u2013 7:03\n\"Beautiful As Torment\" \u2013 6:38\n\"Death Salutes Atropos\" \u2013 5:27\n\"The Nudity Of Light\" \u2013 3:26\n\"Odie Et Amo\" \u2013 7:14\n\"In The Throws Of Guilt\" \u2013 11:00\n\n\n== Personnel ==\n\n\n=== Star of Ash ===\nHeidi S. Tveitan: Vocals, Keyboards, programming\n\n\n=== Additional Personnel ===\nVegard Sverre Tveitan: Guitar on all songs except \"Sanies\", bass on tracks 3\u20137, vocals on 4 & 7.\nEinar Solberg: Vocals on track 3.\nJostein Thomassen: Guitar on track 3 & 6.\nKnut Aalefj\u00e6r: Drums & percussion on tracks 3\u20136.\nKenneth Lia Solberg: Guitar on tracks 4 & 6.\nKris G. Rygg: Vocals on tracks 5 & 7.\nThe Star Of Ash Choir on track 7: Kaia Lia (conductor), Sanne Anundsk\u00e5s, Astrid Marie Lia, Inger Bronken, Elisabeth Lia, Marit B\u00f8e, Heidi S. Tveitan, P\u00e5l Solberg, Knut Bendik Breistein, Einar Solberg, Vegard Tveitan, Kenneth Lia Solberg\n\n\n== Production ==\nProduced By Heidi S. Tveitan, V. Tveitan, Tore Ylwizaker & Kris G. Rygg\nRecorded, Engineered & Mixed By Kristoffer G. Rygg, Tore Ylwizaker & Ihriel\nMastered By Tom Kvalsvoll\n\n\n== External links ==\n\"Iter.Viator\" at discogs", 
                "titleUrl": "https://en.wikipedia.org/wiki/Iter.Viator", 
                "title": "Iter.Viator"
            }, 
            {
                "snippet": "fusion energy which will be pertinent to the ITER fusion project as part of that country's contribution to the ITER effort. The project was approved in 1995", 
                "pageCategories": "Interlanguage link template link number\nTokamaks", 
                "pageContent": "The KSTAR, or Korea Superconducting Tokamak Advanced Research is a magnetic fusion device being built at the National Fusion Research Institute in Daejeon, South Korea. It is intended to study aspects of magnetic fusion energy which will be pertinent to the ITER fusion project as part of that country's contribution to the ITER effort. The project was approved in 1995 but construction was delayed by the East Asian financial crisis which weakened the South Korean economy considerably; however the construction phase of the project was completed on September 14, 2007. First plasma occurred on July 15, 2008. or more likely on June 30 2008.\nKSTAR will be one of the first research tokamaks in the world to feature fully superconducting magnets, which again will be of great relevance to ITER as this will also use SC magnets. The KSTAR magnet system consists of 16 niobium-tin direct current toroidal field magnets, 10 niobium-tin alternating current poloidal field magnets and 4 niobium-titanium alternating current poloidal field magnets. It is planned that the reactor will study plasma pulses of up to 20 seconds duration until 2011, when it will be upgraded to study pulses of up to 300 seconds duration. The reactor vessel will have a major radius of 1.8 m, a minor radius of 0.5 m, a maximum toroidal field of 3.5 tesla, and a maximum plasma current of 2 megaampere. As with other tokamaks, heating and current drive will be initiated using neutral beam injection, ion cyclotron resonance heating (ICRH), radio frequency heating and electron cyclotron resonance heating (ECRH). Initial heating power will be 8 megawatt from neutral beam injection upgradeable to 24 MW, 6 MW from ICRH upgradeable to 12 MW, and at present undetermined heating power from ECRH and RF heating. The experiment will use both hydrogen and deuterium fuels but not the deuterium-tritium mix which will be studied in ITER.\nIn 2012, it succeeded in maintaining high-temperature plasma (about 50 million degrees Celsius) for 17 seconds.\n\n\n== Timeline ==\nThe design was based on Tokomak Physics Experiment which was based on Compact Ignition Tokamak design - See Robert J. Goldston.\n1995 - Started Project KSTAR\n1997 - JET of EU emits 17 MW energy from itself.\n1998 - JT-60U went beyond energy junction successfully, and acknowledged possibility of commercialization of nuclear fusion.\n2006 - Life span of 3 Fusion Reactors (JT-60U, JET, and DIII-D) are terminated.\n2007, September - KSTAR's major devices are constructed.\n2008, July - First plasma occurred. Maintenance time: 0.865 seconds, Temperature: 2\u00d7106 K\n2009 - Maintained 320,000A plasma for 3.6 seconds.\n2010, November - First H-mode plasma run.\n2011 - Maintained high-temperature plasma for 5.2 seconds, Temperature: ~50\u00d7106 K, successfully fully deterred ELM (Edge-Localized Mode), first ever in the World.\n2012 - Maintained high-temperature plasma for 17 seconds, Temperature: 50\u00d7106 K\n2013 - Maintained high-temperature plasma for 20 seconds, Temperature: 50\u00d7106 K\n2014 - Maintained high-temperature plasma for 48 seconds, and successfully fully deterred ELM for 5 seconds.\n\n\n== References ==\n\n\n== External links ==\nKSTAR homepage\nEnglish KSTAR homepage\nKSTAR parameters re ITER and other tokamaks\n\nKSTAR Project Status PDF (undated - seems to be 2001. Includes slide-13 construction schedule to end 2004 and slide-16 operation from 2005 with upgrade planned 2010-11.)\nKSTAR Assembly Status, October 2006 PDF\nStatus and Result of the KSTAR Upgrade for the 2010\u2019s Campaign\nKSTAR ICRF transmission line system upgrade for load resilient operation. Jan 2013", 
                "titleUrl": "https://en.wikipedia.org/wiki/KSTAR", 
                "title": "KSTAR"
            }, 
            {
                "snippet": "journey from Rome to Brundisium. It is thus also known as the Iter Brundisium or Iter ad Brundisium. Alluding to a famous satire in which Horace\u2019s poetic", 
                "pageCategories": "1st-century BC Latin books\nAll articles lacking in-text citations\nArticles containing Latin-language text\nArticles lacking in-text citations from October 2012\nPoetry by Horace\nSatirical works", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Satires_(Horace)", 
                "title": "Satires (Horace)"
            }, 
            {
                "snippet": "series of experiments. Compared to the ITER international project, IGNITOR would be much smaller and cheaper: the ITER reactor is 19,000 tons[vague] in weight", 
                "pageCategories": "All Wikipedia articles needing clarification\nArticles with Italian-language external links\nFusion power\nInterlanguage link template link number\nNuclear research institutes\nResearch projects\nTokamaks\nWikipedia articles needing clarification from January 2010", 
                "pageContent": "IGNITOR is the Italian name for a nuclear research project of magnetic confinement fusion, developed by ENEA Laboratories in Frascati. Construction (in Russia) is not complete.\nThe project theory is based on ignited plasma in tokamak. Started in 1977 by Prof. Bruno Coppi of MIT, IGNITOR is based on the 1970s Alcator machine at MIT which pioneered the high magnetic field approach to plasma magnetic confinement, continued with the Alcator C/C-Mod at MIT and the FT/FTU series of experiments.\nCompared to the ITER international project, IGNITOR would be much smaller and cheaper: the ITER reactor is 19,000 tons in weight while the IGNITOR is only 500 tons in weight. IGNITOR is designed to produce approximately 100 MW of fusion power (and ITER to produce ~500 MW fusion power).\n\n\n== Development ==\nAt a meeting with the scientific attach\u00e9s of the European embassies in Moscow in early February 2010 Mikhail Kovalchuk, Director of the Kurchatov Institute, announced that an initiative aimed at developing a fast paced joint research programme in nuclear fusion research was strongly supported by the Governments of Russia and Italy.\nThe original proposal had been initiated earlier by Evgeny Velikhov (President of the Kurchatov Institute) and Bruno Coppi (Head of the High Energy Plasmas Undertaking, MIT) during the early developments of the Alcator C-Mod programme at MIT, where well known scientists of the Kurchatov Institute made key contributions to experiments that identified the unique confinement and purity properties of the high density plasmas produced by the high field Alcator machine. In effects this investigated, for the first time, physical processes leading to attain self-sustained fusion burning plasmas.\nThe collaboration with the Kurchatov Institute is directed at the construction of the Ignitor machine, the first experiment proposed to achieve ignition conditions by nuclear fusion reactions on the basis of existing knowledge of plasma physics and available technologies. Ignitor is part of the line of research on high magnetic field, experiments producing high density plasmas that began with the Alcator and the Frascati Torus programs at MIT and in Italy, respectively. It remains, at the world level, the only experiment capable of reaching ignition by the magnetic field confinement approach. However, several fusion scientists have contested the claim made for IGNITOR that it is a bigger step towards fusion power than the international ITER project.\nAccording to existing plans, Ignitor will be installed at the Triniti site at Troitsk near Moscow that has facilities which can be upgraded to house and operate the machine. This site will become open and made to be easily accessible to scientists of all nations. The management of the relevant research programme will involve Italy and Russia only to facilitate the success of the enterprise. The proponents have suggested that the US become an Associate Member of this effort with a similar arrangement to that made with CERN for its participation in the LHC (Large Hadron Collider) Programme.\nThe goal to produce meaningful fusion reactors in a reasonable time leads to pursuing the achievement of ignition conditions in the near term in order to understand the plasma physical regimes needed for a net power producing reactor. In addition, an objective other than ignition that can be envisioned for the relatively near term is that of high flux neutron sources for material testing involving compact, high density fusion machines. This has been one of the incentives that have led the Ignitor Project to adopt magnesium diboride (MgB2) superconducting cables in the machine design, a first in fusion research. Accordingly, the largest coils (about 5 m diameter) of the machine will be made entirely of MgB2 cables.\nIn the context of the Italy-Russia summit meeting held in Milan on 26 April 2010 the agreement to proceed with the proposed joint Ignitor program has been signed. The participants, from the Russian side, have included the Prime Minister Vladimir Putin, the Deputy Prime Minister Igor Sechin, the Energy Minister Sergei Shmatko, and the Vice Minister of Education and Research Sergey Mazurenko. Participants from the Italian side have included Prime Minister Silvio Berlusconi, the Foreign Affairs Advisor to the Prime Minister Valentino Valentini (who had a key role in forging the agreement on the Ignitor program), and the Minister of Education and Research Mariastella Gelmini who, together with Sergey Mazurenko, signed the agreement in the presence of the two Prime Ministers.\n\n\n== Progress on construction ==\nSome components have been built in Italy.\n\n\n== See also ==\nList of plasma (physics) articles\n\n\n== External links ==\nIGNITOR website\nFact sheet says \"Construction on the reactor is projected to be complete in 2014\"\n\n(English) IGNITOR technical specs on ENEA Laboratories in Frascati\n(Italian) Paolo Detragiache, Technical presentation of the project\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/IGNITOR", 
                "title": "IGNITOR"
            }, 
            {
                "snippet": "cross-section. Pioneering experiments followed, including for example the ITER-relevant tests of magnetic field correction with saddle coils for RMPs experiment", 
                "pageCategories": "All NPOV disputes\nCommons category without a link on Wikidata\nFusion power\nNPOV disputes from October 2016\nTokamaks", 
                "pageContent": "Tokamak COMPASS (COMPact ASSembly)[1][2] is the main experimental facility of Tokamak department of Institute of Plasma Physics[3] of the Academy of Sciences of the Czech Republic since 2006. It was designed in the 1980s in the British Culham Science Centre as a flexible research facility dedicated mostly to plasma physics studies in circular and D shaped plasmas.\nThe first plasma in COMPASS \"broke down\" in 1989 in a C-shaped vacuum vessel, i.e., in a simpler vessel with a circular cross-section. Pioneering experiments followed, including for example the ITER-relevant tests of magnetic field correction with saddle coils for RMPs experiment (Resonant magnetic perturbations) or experiments with non-inductive current drive in plasma.\nThe operation of tokamak restarted with a D-shaped vacuum vessel in 1992. The operation mode with high plasma confinement (H-mode) was achieved, which represents a reference operation (\"standard scenario\") for the ITER tokamak. The COMPASS tokamak with its size (major radius 0.6 m and height of the vessel approx. 0.7 m) ranks to smaller tokamaks capable of the H-mode operation. Importantly, due to its size and shape the COMPASS plasmas correspond to one tenth (in the linear scale) of the ITER plasmas. At present, besides COMPASS there are only two operational tokamaks in Europe with ITER-like configuration capable of regime with the high plasma confinement. It is the Joint European Torus (JET) and the German tokamak ASDEX Upgrade (Institut f\u00fcr Plasmaphysik, Garching, Germany). JET is the biggest experimental device of this type in the world.\nIn 2002, British scientists started alternative research on larger, spherical tokamak MAST. Operation of COMPASS was discontinued due to insufficient resources for operation of both tokamaks, however, the research program foreseen for the latter tokamak was not concluded. Due to its important and not completely realised opportunities - and, in particular, due to its direct relevance to the ITER project - the facility was offered for free by the European Commission and UKAEA to the Institute of Plasma Physics AS CR in Prague in autumn 2004.\nThe Prague institute has been coordinating research in thermonuclear fusion in the Czech Republic in the framework of EURATOM since 1999. Team of physicists from the institute has a long-time experience in this field of research including operation of a small tokamak CASTOR. The European Commission has declared that the institute is fully competent to operate the tokamak COMPASS. \n\n\n== Parameters of the tokamak COMPASS ==\nMovie: COMPASS discharge using fast - visible camera: [4]\n\n\n== References ==\n\n\n== See also ==\nList of fusion experiments\nELM (Edge Localized Mode)\nBall-pen probe\nLangmuir probe\nThomson scattering\nResonant magnetic perturbations\n\n\n== External links ==\nMagnetic fusion in the Czech Republic\nDiagnostic system on COMPASS", 
                "titleUrl": "https://en.wikipedia.org/wiki/COMPASS_tokamak", 
                "title": "COMPASS tokamak"
            }, 
            {
                "snippet": "Britain and Roman roads in Britain      The British section is known as the Iter Britanniarum, and can be described as the 'road map' of Roman Britain. There", 
                "pageCategories": "2nd-century Latin books\nArticles containing Latin-language text\nGeography of England\nGeography of Wales\nLatin prose texts\nMaps\nNerva\u2013Antonine dynasty\nRoman Britain\nRoman itineraries", 
                "pageContent": "The Antonine Itinerary (Latin: Itinerarium Antonini Augusti, lit. \"The Itinerary of the Emperor Antoninus\") is a famous itinerarium, a register of the stations and distances along various roads. Seemingly based on official documents, possibly from a survey carried out under Augustus, it describes the roads of the Roman Empire. Owing to the scarcity of other extant records of this type, it is a valuable historical record. Almost nothing is known of its date or author. Scholars consider it likely that the original edition was prepared at the beginning of the 3rd century: although it is traditionally ascribed to the patronage of the 2nd-century Antoninus Pius, the oldest extant copy has been assigned to the time of Diocletian and the most likely imperial patron\u2014if the work had one\u2014would have been Caracalla.\n\n\n== Iter Britanniarum ==\n\nThe British section is known as the Iter Britanniarum, and can be described as the 'road map' of Roman Britain. There are 15 such itineraries in the document applying to different geographic areas.\nThe itinerary measures distances in Roman miles, where 1,000 Roman paces equals one Roman mile. A Roman pace was two steps, left plus right. Roman paces were not everywhere the same, and conversion to modern units is imprecise, but 1 Roman mile approximately equals 4,690 feet, or 1430 m.\n\n\n=== Examples ===\nBelow is the original Latin ablative forms for sites along route 13, followed by a translation with a possible (but not necessarily authoritative) name for the modern sites. A transcriber omitted an entry, so that the total number of paces does not equal the sum of paces between locations.\nBelow is the original Latin for route 14 followed by a translation with a possible (but not necessarily authoritative) name for the modern site.\n\n\n=== A confounding factor ===\nDe Situ Britanniae (made available c.\u20091749, published 1757) was a forgery that provided much spurious information on Roman Britain, including \"itineraries\" that overlapped the legitimate Antonine Itineraries, sometimes with contradicting information. Its authenticity was not seriously challenged until 1845, and it was still cited as an authoritative source until the late nineteenth century. By then, its false data had infected almost every account of ancient British history, and been adopted into the Ordnance Survey maps, as General Roy and his successors believed it to be a legitimate source of information, on a par with the Antonine Itineraries. While the document is no longer cited, since its authenticity became indefensible, its data has not been systematically removed from past and present works.\nSome authors, such as Thomas Reynolds, without challenging the authenticity of the forgery, took care to note its discrepancies and challenge the quality of its information. This was not always so, even after the forgery was debunked.\nGonzalo Arias (died 2008) proposed that some of the distance anomalies in the British section of the Antonine Itinerary resulted from the loss of Latin grammatical endings, as these had marked junctions heading towards places, as distinct from the places themselves. However, Arias may not have taken account of earlier work indicating that distances were measured between the edges of administrative areas of named settlements as opposed to centre-to-centre, thereby explaining supposed distance shortfalls and providing additional useful data on the approximate sizes of such areas.\n\n\n== Hispania ==\n\n\n== Citations ==\n\n\n== Bibliography ==\n\n\n== External links ==\nThe Antonine Itinerary: Iter Britanniarum - The British Section\nAnalysis of the Itinerary\nItinerarium Antonini Augusti (the Balkanic roads) at SOLTDM.COM\nRoman Roads in Britain", 
                "titleUrl": "https://en.wikipedia.org/wiki/Antonine_Itinerary", 
                "title": "Antonine Itinerary"
            }
        ], 
        "phraseCharStart": "65"
    }, 
    {
        "phraseCharEnd": "127", 
        "phraseIndex": "T3", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "vessel", 
        "wikiSearchResults": [
            {
                "snippet": "Vessel or vessels may refer to:   Blood vessel, a part of the circulatory system and function to transport blood throughout the body Lymph vessel, a thin", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Vessel", 
                "title": "Vessel"
            }, 
            {
                "snippet": "Ocarinas on display            A vessel flute is a type of flute with an enclosed rather than cylindrical body (vessel rather than tube). The best-known", 
                "pageCategories": "Vessel flutes", 
                "pageContent": "A vessel flute is a type of flute with an enclosed rather than cylindrical body (vessel rather than tube). The best-known example is probably the ocarina.\nGemshorn\nPifana\n\nOcarina\nMolinukai\n\nXun\nHun\n\n\n== See also ==\nEnd-blown flute\nFipple flute\nNose flute\nOvertone flute\nSide-blown flute", 
                "titleUrl": "https://en.wikipedia.org/wiki/Vessel_flute", 
                "title": "Vessel flute"
            }, 
            {
                "snippet": "(disambiguation). A research vessel (RV or R/V) is a ship or boat designed and/or equipped to carry out research at sea. Research vessels carry out a number of", 
                "pageCategories": "Commons category with local link same as on Wikidata\nFisheries science\nHydrography\nOceanographic instrumentation\nResearch vessels\nShip types\nSurvey ships\nWikipedia articles with GND identifiers", 
                "pageContent": "A research vessel (RV or R/V) is a ship or boat designed and/or equipped to carry out research at sea. Research vessels carry out a number of roles. Some of these roles can be combined into a single vessel but others require a dedicated vessel. Due to the demanding nature of the work, research vessels are often constructed around an icebreaker hull, allowing them to operate in polar waters.\n\n\n== History ==\n\nThe research ship had origins in the early voyages of exploration. By the time of James Cook's Endeavour, the essentials of what today we would call a research ship are clearly apparent. In 1766, the Royal Society hired Cook to travel to the Pacific Ocean to observe and record the transit of Venus across the Sun. The Endeavour was a sturdy boat, well designed and equipped for the ordeals she would face, and fitted out with facilities for her \"research personnel\", Joseph Banks. And, as is common with contemporary research vessels, Endeavour carried out more than one kind of research, including comprehensive hydrographic survey work.\nSome other notable early research vessels were HMS Beagle, RV Calypso, HMS Challenger, USFC Albatross, and the Endurance and Terra Nova.\nThe names of early research vessels have been used to name later research vessels, as well as Space Shuttles.\n\n\n== Modern types ==\n\n\n=== Hydrographic survey ===\nA hydrographic survey ship is a vessel designed to conduct hydrographic research and survey. Nautical charts are produced from this information to ensure safe navigation by military and civilian shipping.\nHydrographic survey vessels also conduct seismic surveys of the seabed and the underlying geology. Apart from producing the charts, this information is useful for detecting geological features which are likely to bear oil or gas. These vessels usually mount equipment on a towed structure, for example, air cannons, used to generate a high pressure shock wave to sound the strata beneath the seabed, or mounted on the keel, for example, a depth sounder.\nIn practice, hydrographic survey vessels are often equipped to perform multiple roles. Some function also as oceanographic research ships. Naval hydrographic survey vessels often do naval research, for example, on submarine detection.\nAn example of a hydrographic survey vessel is CCGS Frederick G. Creed. For an example of the employment of a survey ship see HMS Hydra.\n\n\n=== Oceanographic research ===\nOceanographic research vessels carry out research on the physical, chemical and biological characteristics of water, the atmosphere and climate, and to these ends carry equipment for collecting water samples from a range of depths, including the deep seas, as well as equipment for the hydrographic sounding of the seabed, along with numerous other environmental sensors. These vessels often also carry scientific divers and unmanned underwater vehicles. Since the requirements of both oceanographic and hydrographic research are very different from those of fisheries research, these boats often fulfill dual roles.\nExamples of an oceanographic research vessel include the NOAAS Ronald H. Brown and the Chilean Navy Cabo de Hornos.\n\n\n=== Fisheries research ===\nA fisheries research vessel (FRV) requires platforms which are capable of towing different types of fishing nets, collecting plankton or water samples from a range of depths, and carrying acoustic fish-finding equipment. Fisheries research vessels are often designed and built along the same lines as a large fishing vessel, but with space given over to laboratories and equipment storage, as opposed to storage of the catch.\nAn example of a fisheries research vessel is FRV Scotia.\n\n\n=== Naval research ===\nNaval research vessels investigate naval concerns, such as submarine and mine detection, sonar and weapon trialling.\nAn example of a naval research vessel is the Planet of the German Navy.\n\n\n=== Polar research ===\nPolar research vessels are constructed around an icebreaker hull, allowing them to operate in polar waters. These boats usually have dual roles, particularly in the Antarctic where they function also as polar replenishment and supply vessels to the Antarctic research bases.\nAn example of a polar research vessel is USCGC Polar Star.\n\n\n=== Oil exploration ===\nOil exploration is performed in a number of ways, one of the most common being mobile drilling platforms or ships that are moved from area to area as needed to drill into the seabed to find out what deposits may or may not lie beneath it.\n\n\n== See also ==\nEuropean and American voyages of scientific exploration\nList of research vessels by country\nMarine research vessels\nOceanography\nTechnical research ship\nWeather ship\n\n\n== References ==\n\nOCEANIC International Research Vessels Database\nUnofficial (English Language) Homepage of the research icebreaker \"ARA Almirante Irizar\nAustralian research vessel facilities\nCanadian research fleet\nAlfred Wegener Institute for Polar and Marine Research - home of the \"Polarstern\"\nIfremer Fleet\nNational Institute of Oceanography and Experimental Geophysics - OGS Trieste ITALY\nNOAA Marine Operations\nScripps Institution of Oceanography\nWoods Hole Oceanographic Institution (WHOI)\nUniversity-National Oceanographic Laboratory System (UNOLS) research vessels (US academic fleet)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Research_vessel", 
                "title": "Research vessel"
            }, 
            {
                "snippet": "A mine countermeasures vessel or MCMV is a type of naval ship designed for the location of and destruction of naval mines which combines the role of a", 
                "pageCategories": "All stub articles\nCommons category with local link same as on Wikidata\nMinehunters\nMinesweepers\nNavy stubs", 
                "pageContent": "A mine countermeasures vessel or MCMV is a type of naval ship designed for the location of and destruction of naval mines which combines the role of a minesweeper and minehunter in one hull. The term MCMV is also applied collectively to minehunters and minesweepers.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Mine_countermeasures_vessel", 
                "title": "Mine countermeasures vessel"
            }, 
            {
                "snippet": "A wind turbine installation vessel is a vessel specifically designed for the installation of offshore wind turbines. Similar to a jackup rig it is self-elevating", 
                "pageCategories": "Wind turbine installation vessels", 
                "pageContent": "A wind turbine installation vessel is a vessel specifically designed for the installation of offshore wind turbines. Similar to a jackup rig it is self-elevating. To enable quick relocation in the wind farm it is self-propelled. It also has a slender ship shaped hull to achieve a quick turnaround time with the vessel carrying several foundations or wind turbines each time. Azimuth thrusters are used to position the vessel during jack-up operations.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Wind_turbine_installation_vessel", 
                "title": "Wind turbine installation vessel"
            }, 
            {
                "snippet": "A survey vessel is any type of ship or boat that is used for mapping. It is a type of research vessel.  The task of survey vessels is to map the bottom", 
                "pageCategories": "All articles lacking sources\nAll stub articles\nArticles lacking sources from November 2008\nShip type stubs\nShip types", 
                "pageContent": "A survey vessel is any type of ship or boat that is used for mapping. It is a type of research vessel.\n\n\n== Role ==\nThe task of survey vessels is to map the bottom, benthic zone, full water column, and surface for the purpose of:\nhydrography\ngeneral oceanography\nmarine habitats\nsalvage\ndredging\nmarine archaeology\n\n\n== Survey equipment ==\nTypically, modern survey vessels are equipped with one or more of the following equipment:\nGPS positioning and logging\nsingle beam sonar\nmultibeam sonar\nSide-scan sonar\ntowed magnetometer\nsubsurface profiler\ngrab sampler\nbottom coring device\nDCHP\nCTD\nInertial Measurement Unit\n\n\n== See also ==\nResearch vessel", 
                "titleUrl": "https://en.wikipedia.org/wiki/Survey_vessel", 
                "title": "Survey vessel"
            }, 
            {
                "snippet": "updated online on a continuous basis. When a ship is removed from the Naval Vessel Register in the United States, or from a Naval List of any other country", 
                "pageCategories": "All articles lacking in-text citations\nArticles lacking in-text citations from March 2013\nArticles with limited geographic scope from May 2009\nDirectories\nRoyal Navy\nUnited States Navy\nWikipedia articles needing clarification from June 2016", 
                "pageContent": "A Navy List or Naval Register is an official list of naval officers, their ranks and seniority, the ships which they command or to which they are appointed, etc., that is published by the government or naval authorities of a country.\n\n\n== Background ==\nThe Navy List fulfills an important function in international law in that warships are required by article 29 of the United Nations Convention on the Law of the Sea to be commanded by a commissioned officer whose name appears in the appropriate service list.\nPast copies of the Navy List are also important sources of information for historians and genealogists.\nThe Navy List for the Royal Navy is no longer published in hard-copy.\nThe Royal Navy (United Kingdom) publishes annual lists of active and reserve officers, and biennial lists of retired officers. The equivalent in the United States Navy is the Naval Register, which is updated online on a continuous basis. When a ship is removed from the Naval Vessel Register in the United States, or from a Naval List of any other country, the ship is said to be \"stricken\".\n\n\n== Resources ==\nGood sources of historical data on UK's Navy Lists are\nThe Naval Historical Branch, Portsmouth Naval Base.\nThe Central Library Portsmouth, Guildhall Square.\nThe National Archives, Kew, that has an almost complete set including unpublished editions produced during the Second World War for internal use by the Admiralty.\nThe Caird Library of the National Maritime Museum has in its collection bound monthly lists published by the Admiralty, and the concurrently published Steel's lists\nThe current editor of the Navy List is Cliona Willis\n\n\n== Bibliography ==\nThe 1766 Navy List, Edited by E. C. Coleman, Published by Ancholme Publishing, ISBN 0-9541443-0-9\n\n\n== See also ==\nArmy List\nNaval Vessel Register\n\n\n== References ==\n\n\n== External links ==\nNavy List 2013\nUS Naval Register (US Navy)\nNavy List Research (Royal Navy)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Navy_List", 
                "title": "Navy List"
            }, 
            {
                "snippet": "blood vessels are the part of the circulatory system that transports blood throughout the human body. There are three major types of blood vessels: the", 
                "pageCategories": "All articles needing additional references\nAngiology\nArticles needing additional references from August 2008\nCommons category with local link same as on Wikidata\nSoft tissue", 
                "pageContent": "The blood vessels are the part of the circulatory system that transports blood throughout the human body. There are three major types of blood vessels: the arteries, which carry the blood away from the heart; the capillaries, which enable the actual exchange of water and chemicals between the blood and the tissues; and the veins, which carry blood from the capillaries back toward the heart. The word vascular, meaning relating to the blood vessels, is derived from the Latin vas, meaning vessel. A few structures (such as cartilage and the lens of the eye) do not contain blood vessels and are labeled avascular.\n\n\n== StructureEdit ==\nThe arteries and veins have three layers, but the middle layer is thicker in the arteries than it is in the veins:\nTunica intima (the thinnest layer): a single layer of simple squamous endothelial cells glued by a polysaccharide intercellular matrix, surrounded by a thin layer of subendothelial connective tissue interlaced with a number of circularly arranged elastic bands called the internal elastic lamina.\nTunica media (the thickest layer in arteries): circularly arranged elastic fiber, connective tissue, polysaccharide substances, the second and third layer are separated by another thick elastic band called external elastic lamina. The tunica media may (especially in arteries) be rich in vascular smooth muscle, which controls the caliber of the vessel.\nTunica adventitia: (the thickest layer in veins) entirely made of connective tissue. It also contains nerves that supply the vessel as well as nutrient capillaries (vasa vasorum) in the larger blood vessels.\nCapillaries consist of little more than a layer of endothelium and occasional connective tissue.\nWhen blood vessels connect to form a region of diffuse vascular supply it is called an anastomosis (pl. anastomoses). Anastomoses provide critical alternative routes for blood to flow in case of blockages.\nThere is a layer of muscle surrounding the arteries and the veins which help contract and expand the vessels. This creates enough pressure for blood to be pumped around the body. Blood vessels are part of the circulatory system, together with the heart and the blood.\n\n\n=== TypesEdit ===\n\nThere are various kinds of blood vessels:\nArteries\nElastic arteries\nDistributing arteries\nArterioles\nCapillaries (the smallest blood vessels)\nVenules\nVeins\nLarge collecting vessels, such as the subclavian vein, the jugular vein, the renal vein and the iliac vein.\nVenae cavae (the two largest veins, carry blood into the heart).\n\nThey are roughly grouped as arterial and venous, determined by whether the blood in it is flowing away from (arterial) or toward (venous) the heart. The term \"arterial blood\" is nevertheless used to indicate blood high in oxygen, although the pulmonary artery carries \"venous blood\" and blood flowing in the pulmonary vein is rich in oxygen. This is because they are carrying the blood to and from the lungs, respectively, to be oxygenated.\n\n\n== PhysiologyEdit ==\nBlood vessels do not actively engage in the transport of blood (they have no appreciable peristalsis), but arteries\u2014and veins to a degree\u2014can regulate their inner diameter by contraction of the muscular layer. This changes the blood flow to downstream organs, and is determined by the autonomic nervous system. Vasodilation and vasoconstriction are also used antagonistically as methods of thermoregulation.\nOxygen (bound to hemoglobin in red blood cells) is the most critical nutrient carried by the blood. In all arteries apart from the pulmonary artery, hemoglobin is highly saturated (95-100%) with oxygen. In all veins apart from the pulmonary vein, the hemoglobin is desaturated at about 75%. (The values are reversed in the pulmonary circulation.)\nThe blood pressure in blood vessels is traditionally expressed in millimetres of mercury (1 mmHg = 133 Pa). In the arterial system, this is usually around 120 mmHg systolic (high pressure wave due to contraction of the heart) and 80 mmHg diastolic (low pressure wave). In contrast, pressures in the venous system are constant and rarely exceed 10 mmHg.\nVasoconstriction is the constriction of blood vessels (narrowing, becoming smaller in cross-sectional area) by contracting the vascular smooth muscle in the vessel walls. It is regulated by vasoconstrictors (agents that cause vasoconstriction). These include paracrine factors (e.g. prostaglandins), a number of hormones (e.g. vasopressin and angiotensin) and neurotransmitters (e.g. epinephrine) from the nervous system.\nVasodilation is a similar process mediated by antagonistically acting mediators. The most prominent vasodilator is nitric oxide (termed endothelium-derived relaxing factor for this reason).\nPermeability of the endothelium is pivotal in the release of nutrients to the tissue. It is also increased in inflammation in response to histamine, prostaglandins and interleukins, which leads to most of the symptoms of inflammation (swelling, redness, warmth and pain).\n\n\n=== Factors affecting blood flow resistanceEdit ===\nResistance occurs where the vessels away from the heart oppose the flow of blood. Resistance is an accumulation of three different factors: blood viscosity, blood vessel length, and vessel radius.\nBlood viscosity is the thickness of the blood and its resistance to flow as a result of the different components of the blood. Blood is 92% water by weight and the rest of blood is composed of protein, nutrients, electrolytes, wastes, and dissolved gases. Depending on the health of an individual, the blood viscosity can vary (i.e. anemia causing relatively lower concentrations of protein, high blood pressure an increase in dissolved salts or lipids, etc.).\nVessel length is the total length of the vessel measured as the distance away from the heart. As the total length of the vessel increases, the total resistance as a result of friction will increase.\nVessel radius also affects the total resistance as a result of contact with the vessel wall. As the radius of the wall gets smaller, the proportion of the blood making contact with the wall will increase. The greater amount of contact with the wall will increase the total resistance against the blood flow.\n\n\n== DiseaseEdit ==\n\nBlood vessels play a huge role in virtually every medical condition. Cancer, for example, cannot progress unless the tumor causes angiogenesis (formation of new blood vessels) to supply the malignant cells' metabolic demand. Atherosclerosis, the formation of lipid lumps (atheromas) in the blood vessel wall, is the most common cardiovascular disease, the main cause of death in the Western world.\nBlood vessel permeability is increased in inflammation. Damage, due to trauma or spontaneously, may lead to hemorrhage due to mechanical damage to the vessel endothelium. In contrast, occlusion of the blood vessel by atherosclerotic plaque, by an embolised blood clot or a foreign body leads to downstream ischemia (insufficient blood supply) and possibly necrosis. Vessel occlusion tends to be a positive feedback system; an occluded vessel creates eddies in the normally laminar flow or plug flow blood currents. These eddies create abnormal fluid velocity gradients which push blood elements such as cholesterol or chylomicron bodies to the endothelium. These deposit onto the arterial walls which are already partially occluded and build upon the blockage.\nVasculitis is inflammation of the vessel wall, due to autoimmune disease or infection.\n\n\n== See alsoEdit ==\nAvascular necrosis\n\n\n== ReferencesEdit ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Blood_vessel", 
                "title": "Blood vessel"
            }, 
            {
                "snippet": "about the naval vessel. For the Australian TV series, see Patrol Boat (TV series).           A patrol boat is a relatively small naval vessel generally designed", 
                "pageCategories": "All articles needing additional references\nArticles needing additional references from January 2013\nPatrol vessels\nShip types", 
                "pageContent": "A patrol boat is a relatively small naval vessel generally designed for coastal defence duties. There have been many designs for patrol boats. They may be operated by a nation's navy, coast guard, police force or customs and may be intended for marine (blue water) and/or estuarine or river (\"brown water\") environments. They are commonly found engaged in various border protection roles, including anti-smuggling, anti-piracy, fisheries patrols, and immigration law enforcement. They are also often called upon to participate in rescue operations. Vessels of this type include the original yacht (from Dutch/Low German jacht meaning hunting or hunt), a light, fast-sailing vessel used by the Dutch navy to pursue pirates and other transgressors around and into shallow waters.\n\n\n== Classification ==\nThey may be broadly classified as inshore patrol vessels (IPVs) and offshore patrol vessels (OPVs). They are warships typically smaller in size than a corvette and can include fast attack craft, torpedo boats and missile boats, although some are as large as a frigate. The offshore patrol vessels are usually the smallest ship in a navy's fleet that are large and seaworthy enough to patrol off-shore in the open ocean. In larger militaries, such as in the United States military, offshore patrol vessels usually serve in the coast guard, but many smaller nations navies operate these type of ships.\n\n\n== History ==\nDuring both World Wars in order to rapidly build up numbers, all sides created auxiliary patrol boats by arming motorboats and seagoing fishing trawlers with machine guns and obsolescent naval weapons. Some modern patrol vessels are still based on fishing and leisure boats. Seagoing patrol boats are typically around 30 m (100 ft) in length and usually carry a single medium caliber artillery gun as main armament, and a variety of lighter secondary armament such as machine guns or a close-in weapon system. Depending on role, vessels in this class may also have more sophisticated sensors and fire control systems that would enable them to carry torpedoes, anti-ship and surface-to-air missiles.\nMost modern designs are powered by gas turbine arrangements such as CODAG, and speeds are generally in the 25\u201330 knots (46\u201356 km/h; 29\u201335 mph) range. They are primarily used for patrol in a country's Exclusive Economic Zone. Common tasks are fisheries inspection, anti-smuggling (usually anti-narcotics) duties, illegal immigration patrols, anti-piracy patrols and search and rescue (law enforcement-type of work). The largest OPVs might also have a flight deck and helicopter embarked. In times of crisis or war, these vessels are expected to support the larger vessels in the navy.\n\nTheir small size and relatively low cost make them one of the most common type of warship in the world. Almost all navies operate at least a few offshore patrol vessels, especially those with only \"green water\" capabilities. They are useful in smaller seas such as the North Sea as well as in open oceans. Similar vessels for exclusively military duties include torpedo boats and missile boats. The United States Navy operated the Pegasus class of armed hydrofoils for years, in a patrol boat role. The River Patrol Boat (PBR, sometimes called \"Riverine\" and \"Pibber\") is a U.S. design of small patrol boat type designed to patrol waters of large rivers.\n\n\n== Specific nations ==\n\n\n=== Albania ===\nIliria (Albanian Navy Brigade)\n\n\n=== Argentina ===\n\nMantilla-class patrol vessel, Argentine Naval Prefecture\nZ-28-class patrol vessel, Argentine Naval Prefecture\n\n\n=== Australia ===\n\nAttack-class patrol boat ,(Royal Australian Navy) - 1967 to 1985\nFremantle-class patrol boat, (Royal Australian Navy) - 1979 to 2007\nArmidale-class patrol boat, (Royal Australian Navy) - 2005 to present\nBay-class patrol boat, (Customs Marine Unit) - 1999 to present\nAustralian Customs Vessel Triton, (Customs Marine Unit) - 2000 to present\nCape-class patrol boat, (Customs Marine Unit) - Bay class replacement from 2013\n\n\n=== Bahamas ===\nBahamas-class 60m patrol vessel, (Royal Bahamas Defence Force)\nProtector-class patrol boat\n\n\n=== Bangladesh ===\n\nDurjoy-class LPC\nIsland-class patrol vessel\nSea Dragon-class offshore patrol vessel\nPadma-class offshore patrol vessel (Bangladesh Navy)\nMeghna-class large patrol boats (Bangladesh Navy)\nHainan-class submarine chaser\nKraljevica-class patrol boat\nBarkat ('Haizhui' class/Type 062) small patrol boat\nAjay-class patrol boat\nShaheed ('Shanghai-II') class small patrol boat\n\n\n=== Brazil ===\nGraja\u00fa class offshore patrol vessel\nBracu\u00ed class patrol vessel (ex-River-class minesweeper)\nImperial Marinheiro-class offshore patrol vessel\nPiratini-class patrol vessel\nJ-class patrol vessel\nRoraima-class river patrol vessel\nPedro Teixeira-class river patrol vessel\nMaca\u00e9-class-offshore patrol vessel\nAmazonas-class corvette \"BAE Offshore Patrol Vessel\"\n\n\n=== Belgium ===\nP901 Castor (2014)\nP902 Pollux (2015)\n\n\n=== Canada ===\n\nKingston class, (Royal Canadian Navy)\nOrca class, (Royal Canadian Navy)\nHero class, (Canadian Coast Guard)\n\n\n=== China ===\nHarbour security boat (PBI) - 4 newly built 80 ton class harbour security / patrol boats, and more are planned in order to take over the port security / patrol duties currently performed by the obsolete Shantou, Beihai, Huangpu, and Yulin classes gunboats, which are increasingly being converted to inshore surveying boats and range support boats.\nShanghai III (Type 062-I) class gunboats - 2\nShanghai II class gunboats\nShanghai I (Type 062) classes gunboats - 150+ active and at least 100 in reserve\nHuludao (Type 206) class gunboat - 8+\nShantou class gunboats - less than 25 (in reserve, subordinated to naval militia)\nBeihai class gunboats - less than 30 (in reserve, subordinated to naval militia)\nHuangpu class gunboats - less than 15 (in reserve, subordinated to naval militia)\nYulin class gunboats - less than 40 (being transferred to logistic duties)\nHaixun class cutter(Type 718), China Coast Guard\n\n\n=== Chile ===\nPatrulleros de Zona Mar\u00edtima FASSMER OPV-80 class. 3 units built under license by ASMAR:\nOPV-81 \"Piloto Pardo\"\nOPV-82 \"Comandante Toro\"\nOPV-83 \"Marinero Fuentealba\". This unit has reinforced hull for Antarctic operations.\nOPV-84 \"Cabo Odger\" under construction in ASMAR Shipyard Talcahuano Chile.\n06 MICALVI Class Patrol vessels built in ASMAR.\n18 Protector Class Patrol crafts built under license by ASMAR.\n04 DABUR Class Patrol crafts built in Israel.\n\n\n=== Colombia ===\n\nDiligente-class patrol boat, (Colombian Navy)\nNodriza-class patrol boat, (Colombian Navy)\nPAF-I-class patrol boat, (Colombian Navy)\nPAF-II-class patrol boat, (Colombian Navy)\nPAF-III-class patrol boat, (Colombian Navy)\nPAF-IV-class patrol boat, (Colombian Navy)\nPatrullera Fluvial Ligera-class patrol boat, (Colombian Navy)\nRiohacha-class gunboat, (Colombian Navy)\nFassmer-80 class, (Colombian Navy) (Built in Colombia by COTECMAR)\n\n\n=== Denmark ===\nKnud Rasmussen-class OPV - 2 vessels\nDiana-class IPV - 6 vessels *Danish Wikipedia\nFlyvefisken-class patrol vessel\nThetis class OPV - 4 ships (classed as ocean patrol frigates)\nBeskytteren ocean patrol frigate OPV. (classed as ocean patrol frigates). Later sold to Estonia and renamed EML Admiral Pitka (A230)\nAgdlek-class IPV - 3 vessels\nBars\u00f8 class IPV - 9 vessels *Danish Wikipedia\nHvidbj\u00f8rnen class OPV - 4 ships, classed as ocean patrol frigates. (Link in Danish)\n\n\n=== Eritrea ===\nEritrea-class 60m patrol vessel, (Eritrean Navy)\nProtector-class patrol boat\n\n\n=== France ===\n\nP400 class, (French Navy)\nFlor\u00e9al class, (French Navy)\nFlamant class, (French Navy)\nEspadon 50 class, (French Navy)\nTrident-class patrol boat, (Maritime Gendarmerie)\nG\u00e9ranium-class patrol boat, (Maritime Gendarmerie)\nJonquille-class patrol boat, (Maritime Gendarmerie)\nVedette-class patrol boat, (Maritime Gendarmerie)\nP\u00e9tulante-class patrol craft, (Maritime Gendarmerie)\nPavois-class patrol craft, (Maritime Gendarmerie)\n\n\n=== Finland ===\nKiisla class (Formerly Finnish Border Guard, now Finnish Navy)\nUVL10, an offshore patrol vessel built at STX Finland Rauma shipyard in 2014\nVMV class (Finnish Navy)\n\n\n=== Germany ===\nBad Bramstedt class - 2002 to present\nPA class - 1943 to 1945\nR boats - 1929 to 1945\nType 139 patrol trawler - 1956 to mid-1970s\n\n\n=== Greece ===\n\n\n==== Hellenic Navy ====\nOsprey 55-class gunboats and derivatives HSY-55 and HSY-56A\nAsheville-class gunboats\nNasty-class Coastal Patrol Vessels, formerly torpedo boats\nEsterel-class Coastal Patrol Vessels\n\n\n==== Hellenic Coast Guard ====\nSaar 4 acting as Offshore Patrol Vessels (OPV)\nStan Patrol 5509 OPV\nVosper Europatrol 250 Mk1 OPV\nAbeking & Rasmussen Patrol Vessels, class Dilos\nPOB-24G Patrol Vessels, class Faiakas\nCB90-HCG\nLambro 57 and derivatives, all being boats for coastal patrols\n\n\n=== Hong Kong ===\n\n\n==== Hong Kong Police Force ====\nSea Panther class large command boat\n\n\n=== Iceland ===\nICGV T\u00fdr (Icelandic Coast Guard)\nICGV \u00d3\u00f0inn (Icelandic Coast Guard)\nICGV \u00de\u00f3r (Icelandic Coast Guard)\nICGV \u00c6gir (Icelandic Coast Guard)\n\n\n=== India ===\n\nCar Nicobar class fast attack craft, Indian Navy\nSaryu class patrol vessel, Indian Navy\nBangaram class patrol vessel, Indian Navy\nSukanya class patrol vessel, Indian Navy\nSamar class, Indian Coast Guard\nVishwast Class, Indian Coast Guard\nSarojini Naidu Class, Indian Coast Guard\nTara Bai class, Indian Coast Guard\nPriyadarshini class, Indian Coast Guard\nJija Bai class, Indian Coast Guard\nVikram class, Indian Coast Guard\nAadesh Class, Indian Coast Guard\n\n\n=== Indonesia ===\nFPB 28, Indonesian Police and Indonesian Customs, 28 meter long patrol boat made by local shipyard PT.PAL\nFPB 38, Indonesian Customs, 38 meter long aluminium patrol boat made by local shipyard PT.PAL\nFPB 57, Indonesian Navy, 57 meter long patrol boat designed by Luerssen and made by PT.PAL, ASM and heli deck equipped for some version.\nPC-40, Indonesian Navy, 40 meter long FRP/Aluminum patrol boat, locally made by in house Navy's workshop.\nPC-60 trimaran, Indonesian Navy, 63-meter-long composite material, is armed with 120 km range of anti-ship missile, made by PT Lundin industry\n\n\n=== Ireland ===\n\nIrish Naval Service Offshore Patrol VesselsL\u00c9 Aisling (P23)\n\nIrish Naval Service Helicopter Patrol Vessels\nL\u00c9 Eithne (P31)\n\nIrish Naval Service Coastal Patrol Vessels\nL\u00c9 Orla (P41)\nL\u00c9 Ciara (P42)\n\nIrish Naval Service Large Patrol Vessels\nL\u00c9 R\u00f3is\u00edn (P51)\nL\u00c9 Niamh (P52)\n\nSamuel Beckett-class OPV\nL\u00c9 Samuel Beckett (P61)\nL\u00c9 James Joyce (P62)\n\n\n=== Israel ===\nDabur class patrol boats - Israeli Navy\nDvora class fast patrol boat - Israeli Navy\nSuper Dvora Mk II - Israeli Navy\nSuper Dvora Mk III - Israeli Navy\nShaldag class fast patrol boats\nShaldag Mk II - Israeli Navy\n\nNachshol class patrol boats (Stingray Interceptor-2000) - Israeli Navy\n\n\n=== Italy ===\n\nZara class, (Italian Guardia di Finanza)\nSaettia class, (Italian Coast Guard)\nDiciotti - CP 902 class, (Italian Coast Guard)\nCassiopea class, (Italian Marina Militare)\nCassiopea II class, (Italian Marina Militare)\nEsploratore class, (Italian Marina Militare)\nComandanti class, (Italian Marina Militare)\n\n\n=== Japan ===\n\nShikishima (Japan Coast Guard), the largest patrol boat.\nMizuho class (Japan Coast Guard),Large patrol Vessel with helicopter deck and hangar.\nTsugaru class (Japan Coast Guard),Large patrol Vessel with helicopter deck and hangar.\nHida class (Japan Coast Guard), high-speed Large patrol Vessel with helicopter deck.\nAso class (Japan Coast Guard), high-speed Large patrol Vessel.\nAmami class (Japan Coast Guard), medium-sized patrol Vessel\nHayabusa-class (JMSDF,Japanese Navy), Corvette class patrol vessel by JMSDF(Navy) Fleet.\n\"S\u014dya\"\uff0c(Japan Coast Guard), icebreaker\n\n\n=== Latvia ===\nSkrunda class, world's first SWATH patrol boat (Latvian Naval Forces)\n\n\n=== Malaysia ===\nKedah class offshore patrol vessel, (Royal Malaysian Navy)\nGagah Class Ship\uff0cMalaysian Maritime Enforcement Agency\nRamunia Class Ship\uff0cMalaysian Maritime Enforcement Agency\nNusa Class Ship\uff0cMalaysian Maritime Enforcement Agency\nSipadan Class Ship\uff0cMalaysian Maritime Enforcement Agency\nRhu Class Ship\uff0cMalaysian Maritime Enforcement Agency\nPengawal Class Ship\uff0cMalaysian Maritime Enforcement Agency\nPeninjau Class Ship\uff0cMalaysian Maritime Enforcement Agency\nPelindung Class Ship\uff0cMalaysian Maritime Enforcement Agency\nSemilang Class Ship\uff0cMalaysian Maritime Enforcement Agency\nPenggalang Class Ship\uff0cMalaysian Maritime Enforcement Agency\nPenyelamat Class Ship\uff0cMalaysian Maritime Enforcement Agency\nPengaman Class Ship\uff0cMalaysian Maritime Enforcement Agency\nKilat Class Ship\uff0cMalaysian Maritime Enforcement Agency\nMalawali Class Ship\uff0cMalaysian Maritime Enforcement Agency\nLangkawi Class Patrol Ship\uff0cMalaysian Maritime Enforcement Agency\n\n\n=== Malta ===\n\nProtector class offshore patrol vessel (Maritime Squadron of the Armed Forces of Malta) - 2002\u2013present\nDiciotti class offshore patrol vessel (Maritime Squadron of the Armed Forces of Malta) - 2005\u2013present\nP21 class inshore patrol vessel (Maritime Squadron of the Armed Forces of Malta) - 2010\u2013present\nEmer class offshore patrol vessel (Maritime Squadron of the Armed Forces of Malta) - 2015\u2013present\n\n\n=== Mexico ===\n\nUribe class, offshore patrol vessel (Mexican Navy)\nHolzinger class, offshore patrol vessel (Mexican Navy)\nSierra class, offshore patrol vessel (Mexican Navy)\nDurango class, offshore patrol vessel (Mexican Navy)\nOaxaca class, offshore patrol vessel (Mexican Navy)\nAzteca class, coastal patrol boat (Mexican Navy)\n\n\n=== Morocco ===\n\nOPV-70 class, offshore patrol vessel (Royal Moroccan Navy)\nOPV-64 class, offshore patrol vessel (Royal Moroccan Navy)\n\n\n=== Netherlands ===\nHolland class offshore patrol vessels (Koninklijke Marine)\n\n\n=== New Zealand ===\nProtector-class OPV (Royal New Zealand Navy) (2008)\nProtector-class IPV (Royal New Zealand Navy) (2008)\nMoa-class patrol boat (Royal New Zealand Navy)(1983\u20132008)\n\n\n=== Norway ===\nRoyal Norwegian Navy\nRapp-class\nTjeld-class\nStorm-class\nSn\u00f8gg-class\nHauk-class\nSkjold-class\n\nNorwegian Coast Guard\nBarentshav class OPV\nHarstad class OPV\nNordkapp class OPV\nNornen class\nSvalbard class icebreaker\n\n\n=== Philippines ===\nPhilippine Navy\nMariano Alvarez class\nGen. Emilio Aguinaldo class\nKagitingan class\nTomas Batillo class\nConrado Yap class\nJose Andrada class\n\n\n=== Peru ===\nR\u00edo Zarumilla class, Peruvian Coast Guard\nRio Ca\u00f1ete class, Peruvian Coast Guard\n\n\n=== Portugal ===\n\nPortuguese Navy\nCentauro class\nViana do Castelo class\nAlbatroz class\nCacine class\nArgos class\nNational Republican Guard (GNR)\nRibamar class\n\n\n=== Romanian ===\nSNR-17 class patrol boats, Romanian Border Police\nStefan Cel Mare patrol vessel, Romanian Border Police\n\n\n=== Russia ===\n\nStenka class patrol boat(Type 02059), Russian Navy\nBogomol Class patrol boat(Type 02065), Russian Navy\nMirage class patrol boat (Type 14310), Russian Navy\nSvetlyak class patrol boat(Type 10410)\uff0c Russian Coast Guard\nOgonek class patrol boat(Type 12130), Russian Coast Guard\nMangust Class patrol boat(Type 12150\uff09, Russian Navy and Russian Coast Guard\nSobol class patrol boat(Type 12200\uff09, Russian Coast Guard\nTerrier class patrol boat (Type 14170), Russian Navy and Russian Coast Guard\nRubin class patrol boat(Type 22460), Russian Coast Guard\n\n\n=== Singapore ===\n\nFearless class, Republic of Singapore Navy\nPK class Interceptor Craft, Police Coast Guard\n1st Generation PT class patrol Craft, Police Coast Guard (decommissioned)\n2nd Generation PT class patrol Craft, Police Coast Guard (decommissioned)\n3rd Generation PT class patrol Craft, Police Coast Guard\n4th Generation PT class patrol Craft, Police Coast Guard\nPC class patrol Craft, Police Coast Guard\nSwift-class coastal patrol craft\n\n\n=== Slovenia ===\nSlovenian patrol boat Triglav\n\n\n=== Sri Lanka ===\nJayasagara class (Sri Lanka Navy)\nColombo class (Sri Lanka Navy)\n\n\n=== South Africa ===\nWarrior class (modified Saar 4 Open Sea Patrol Vessels)\nNamacurra class\n\n\n=== South Korea ===\nChamsuri-class (Republic of Korea Navy)\nGumdoksuri class patrol vessel\n\n\n=== Spain ===\n\nMeteoro class\nDescubierta class\nServiola class\nAnaga class\nBarcel\u00f3 class\nToralla class\nConejera class\nChilreu class\nP111 class patrol boat\nCabo Fradera class\n\n\n=== Suriname ===\nOcea Type FPB 98 class fast patrol boat\nOcea Type FPB 72 class fast patrol boat\n\n\n=== Sweden ===\nHugin-class (based on the Norwegian Storm-class) - 16 ships\nKaparen-class (Hugin-class modified with better subhunting capacity) - 8 ships\nAdditionally, the Swedish Navy also operates a smaller and less capable type of patrol boat (Bevakningsb\u00e5t = \"guard boat\")\nTyp 60 class (Decommissioned) - 17 ships\nTapper class - 12 ships\nThe Swedish Coast Guard operate an additional 22 patrol vessels of various sizes, go to article Coast Guard (Sweden)\n\n\n=== Thailand ===\nPattani class (Royal Thai Navy)\nRiver class (Royal Thai Navy)\nT.991 class (Royal Thai Navy)\n\n\n=== Turkey ===\nK\u0131l\u0131\u00e7 II class, Turkish Navy\nK\u0131l\u0131\u00e7 I class\uff0cTurkish Navy\nY\u0131ld\u0131z class\uff0cTurkish Navy\nR\u00fczgar class\uff0cTurkish Navy\nDo\u011fan class\uff0cTurkish Navy\nKartal class\uff0cTurkish Navy\nT\u00fcrk class\uff0cTurkish Navy\nTuzla class\uff0cTurkish Navy\nKAAN 15 class\uff0cTurkish Coast Guard Command\nKAAN 19 class\uff0cTurkish Coast Guard Command\nKAAN 29 class\uff0cTurkish Coast Guard Command\nKAAN 33 class\uff0cTurkish Coast Guard Command\nSAR 33 class\uff0cTurkish Coast Guard Command\nSAR 35 class\uff0cTurkish Coast Guard Command\n80 class\uff0cTurkish Coast Guard Command\n\n\n=== United Kingdom ===\n\nMotor Launch of World War II\nHarbour Defence Motor Launch of World War II\nRiver class patrol vessel\nCastle class patrol vessel\nArcher class patrol vessel\nIsland class patrol vessel\n\n\n=== United States ===\n\n\n==== United States Navy ====\n\nEagle class patrol craft - US Navy (1918-1947)\nCyclone class patrol ship - US Navy (1993- )\n\n\n==== United States Coast Guard ====\nCutters of the US Coast Guard\n87-foot Marine Protector class coastal patrol boat - USCG\n110-foot Island class patrol boat - USCG\n154-foot Sentinel class cutter- USCG\n210-foot Reliance class cutter - USCG\n270-foot Famous class cutter - USCG\n378-foot Hamilton class cutter - USCG\n418-foot Legend Class cutter - USCG\n399-foot Polar class icebreaker - USCG\n\n\n=== Vietnam ===\nType TT-120 patrol boat, Vietnam Coast Guard\nType TT-200 patrol boat, Vietnam Coast Guard\nType TT-400 patrol boat, Vietnam Coast Guard\nDN 2000(Damen 9014 class) offshore patrol vessels, Vietnam Coast Guard\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Patrol_boat", 
                "title": "Patrol boat"
            }, 
            {
                "snippet": "The Naval Vessel Register (NVR) is the official inventory of ships and service craft in custody of or titled by the United States Navy. It contains information", 
                "pageCategories": "Ship registration\nUnited States Department of Defense publications\nUnited States Navy\nWikipedia articles incorporating text from public domain works of the United States Government", 
                "pageContent": "The Naval Vessel Register (NVR) is the official inventory of ships and service craft in custody of or titled by the United States Navy. It contains information on ships and service craft that make up the official inventory of the Navy from the time a vessel is authorized through its life cycle and disposal. It also includes ships that have been removed from the register (often termed stricken or struck), but not disposed of by sale, transfer to another government, or other means. Ships and service craft disposed of prior to 1987 are currently not included, but are gradually being added along with other updates.\nThe NVR traces its origin back to the 1880s, having evolved from several previous publications. In 1911, the Bureau of Construction and Repair published \"Ships Data US Naval Vessels\", which subsequently became the \"Ships Data Book\" in 1952 under the Bureau of Ships. The Bureau of Ordnance's \"Vessel Register\", first published in 1942 and retitled \"Naval Vessel Register\", was combined with the \"Ships Data Book\" under the Bureau of Ships in 1959.\nSince 1962, the NVR has been maintained and published by the NAVSEA Shipbuilding Support Office (NAVSHIPSO) of the Naval Sea Systems Command. Referred to by Congress in the statutes of 10 U.S.C. \u00a7\u00a7 7304\u20137308, the NVR is maintained as directed by U.S. Navy Regulations, Article 0406, of September 14, 1990.\nThe vessels are listed in the NVR when the classification and hull number(s) are assigned to ships and service craft authorized to be built by the President of the United States, or when the Chief of Naval Operations requests instatement or reinstatement of vessels as approved by the Secretary of the Navy. Once listed, the ship or service craft remains in the NVR throughout its life as a Navy asset. Afterwards, its final disposition is recorded. Many vessels struck from the NVR are transferred to the Navy Inactive Fleet or to the United States Maritime Administration (MARAD) to become part of the National Defense Reserve Fleet. Some continue limited operation in the Ready Reserve Fleet.\nThe NVR is updated weekly and now available only in electronic form and on-line. Over 6,500 separate record transactions are processed annually with each being supported by official documentation. The NVR includes a current list of ships and service craft on hand, under construction, converted, loaned/leased, or to be loaned, and those assigned to the Military Sealift Command. Ship class, fleet assignment, name, age, home port, planning yard, custodian, hull and machinery characteristics, builder, key construction dates, battle forces, local defense and miscellaneous support forces, and status conditions are some of the data elements provided.\n\n\n== See also ==\n\nDictionary of American Naval Fighting Ships\n\n\n== References ==\n This article incorporates public domain material from the United States Government document \"Naval Vessel Register\".\n\n\n== External links ==\nNaval Vessel Register\nHistory of the Naval Vessel Register\nNAVSHIPSO web site", 
                "titleUrl": "https://en.wikipedia.org/wiki/Naval_Vessel_Register", 
                "title": "Naval Vessel Register"
            }
        ], 
        "phraseCharStart": "121"
    }, 
    {
        "phraseCharEnd": "182", 
        "phraseIndex": "T4", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "helium", 
        "wikiSearchResults": [
            {
                "snippet": "article is about the chemical element. For other uses, see Helium (disambiguation). Helium is a chemical element with symbol He and atomic number 2. It", 
                "pageCategories": "Airship technology\nArticles with hAudio microformats\nCS1 German-language sources (de)\nCS1 Japanese-language sources (ja)\nCS1 maint: Multiple names: authors list\nChemical elements\nCondensed matter physics\nCoolants\nDiving equipment\nFeatured articles", 
                "pageContent": "Helium is a chemical element with symbol He and atomic number 2. It is a colorless, odorless, tasteless, non-toxic, inert, monatomic gas, the first in the noble gas group in the periodic table. Its boiling point is the lowest among all the elements.\nAfter hydrogen, helium is the second lightest and second most abundant element in the observable universe, being present at about 24% of the total elemental mass, which is more than 12 times the mass of all the heavier elements combined. Its abundance is similar to this figure in the Sun and in Jupiter. This is due to the very high nuclear binding energy (per nucleon) of helium-4 with respect to the next three elements after helium. This helium-4 binding energy also accounts for why it is a product of both nuclear fusion and radioactive decay. Most helium in the universe is helium-4, and is believed to have been formed during the Big Bang. Large amounts of new helium are being created by nuclear fusion of hydrogen in stars.\nHelium is named for the Greek god of the Sun, Helios. It was first detected as an unknown yellow spectral line signature in sunlight during a solar eclipse in 1868 by French astronomer Jules Janssen. Janssen is jointly credited with detecting the element along with Norman Lockyer. Janssen observed during the solar eclipse of 1868 while Lockyer observed from Britain. Lockyer was the first to propose that the line was due to a new element, which he named. The formal discovery of the element was made in 1895 by two Swedish chemists, Per Teodor Cleve and Nils Abraham Langlet, who found helium emanating from the uranium ore cleveite. In 1903, large reserves of helium were found in natural gas fields in parts of the United States, which is by far the largest supplier of the gas today.\nLiquid helium is used in cryogenics (its largest single use, absorbing about a quarter of production), particularly in the cooling of superconducting magnets, with the main commercial application being in MRI scanners. Helium's other industrial uses\u2014as a pressurizing and purge gas, as a protective atmosphere for arc welding and in processes such as growing crystals to make silicon wafers\u2014account for half of the gas produced. A well-known but minor use is as a lifting gas in balloons and airships. As with any gas whose density differs from that of air, inhaling a small volume of helium temporarily changes the timbre and quality of the human voice. In scientific research, the behavior of the two fluid phases of helium-4 (helium I and helium II) is important to researchers studying quantum mechanics (in particular the property of superfluidity) and to those looking at the phenomena, such as superconductivity, produced in matter near absolute zero.\nOn Earth it is relatively rare\u20145.2 ppm by volume in the atmosphere. Most terrestrial helium present today is created by the natural radioactive decay of heavy radioactive elements (thorium and uranium, although there are other examples), as the alpha particles emitted by such decays consist of helium-4 nuclei. This radiogenic helium is trapped with natural gas in concentrations as great as 7% by volume, from which it is extracted commercially by a low-temperature separation process called fractional distillation. Previously, terrestrial helium\u2014a non-renewable resource, because once released into the atmosphere it readily escapes into space\u2014was thought to be in increasingly short supply. However, recent studies suggest that helium produced deep in the earth by radioactive decay can collect in natural gas reserves in larger than expected quantities, in some cases having been released by volcanic activity.\n\n\n== History ==\n\n\n=== Scientific discoveries ===\nThe first evidence of helium was observed on August 18, 1868, as a bright yellow line with a wavelength of 587.49 nanometers in the spectrum of the chromosphere of the Sun. The line was detected by French astronomer Jules Janssen during a total solar eclipse in Guntur, India. This line was initially assumed to be sodium. On October 20 of the same year, English astronomer Norman Lockyer observed a yellow line in the solar spectrum, which he named the D3 Fraunhofer line because it was near the known D1 and D2 lines of sodium. He concluded that it was caused by an element in the Sun unknown on Earth. Lockyer and English chemist Edward Frankland named the element with the Greek word for the Sun, \u1f25\u03bb\u03b9\u03bf\u03c2 (helios).\n\nIn 1882, Italian physicist Luigi Palmieri detected helium on Earth for the first time through its D3 spectral line, when he analyzed the lava of Mount Vesuvius.\n\nOn March 26, 1895, Scottish chemist Sir William Ramsay isolated helium on Earth by treating the mineral cleveite (a variety of uraninite with at least 10% rare earth elements) with mineral acids. Ramsay was looking for argon but, after separating nitrogen and oxygen from the gas liberated by sulfuric acid, he noticed a bright yellow line that matched the D3 line observed in the spectrum of the Sun. These samples were identified as helium by Lockyer and British physicist William Crookes. It was independently isolated from cleveite in the same year by chemists Per Teodor Cleve and Abraham Langlet in Uppsala, Sweden, who collected enough of the gas to accurately determine its atomic weight. Helium was also isolated by the American geochemist William Francis Hillebrand prior to Ramsay's discovery when he noticed unusual spectral lines while testing a sample of the mineral uraninite. Hillebrand, however, attributed the lines to nitrogen. His letter of congratulations to Ramsay offers an interesting case of discovery and near-discovery in science.\nIn 1907, Ernest Rutherford and Thomas Royds demonstrated that alpha particles are helium nuclei by allowing the particles to penetrate the thin glass wall of an evacuated tube, then creating a discharge in the tube to study the spectra of the new gas inside. In 1908, helium was first liquefied by Dutch physicist Heike Kamerlingh Onnes by cooling the gas to less than one kelvin. He tried to solidify it by further reducing the temperature but failed because helium does not solidify at atmospheric pressure. Onnes' student Willem Hendrik Keesom was eventually able to solidify 1 cm3 of helium in 1926 by applying additional external pressure.\nIn 1938, Russian physicist Pyotr Leonidovich Kapitsa discovered that helium-4 has almost no viscosity at temperatures near absolute zero, a phenomenon now called superfluidity. This phenomenon is related to Bose\u2013Einstein condensation. In 1972, the same phenomenon was observed in helium-3, but at temperatures much closer to absolute zero, by American physicists Douglas D. Osheroff, David M. Lee, and Robert C. Richardson. The phenomenon in helium-3 is thought to be related to pairing of helium-3 fermions to make bosons, in analogy to Cooper pairs of electrons producing superconductivity.\n\n\n=== Extraction and use ===\nAfter an oil drilling operation in 1903 in Dexter, Kansas, produced a gas geyser that would not burn, Kansas state geologist Erasmus Haworth collected samples of the escaping gas and took them back to the University of Kansas at Lawrence where, with the help of chemists Hamilton Cady and David McFarland, he discovered that the gas consisted of, by volume, 72% nitrogen, 15% methane (a combustible percentage only with sufficient oxygen), 1% hydrogen, and 12% an unidentifiable gas. With further analysis, Cady and McFarland discovered that 1.84% of the gas sample was helium. This showed that despite its overall rarity on Earth, helium was concentrated in large quantities under the American Great Plains, available for extraction as a byproduct of natural gas.\nThis enabled the United States to become the world's leading supplier of helium. Following a suggestion by Sir Richard Threlfall, the United States Navy sponsored three small experimental helium plants during World War I. The goal was to supply barrage balloons with the non-flammable, lighter-than-air gas. A total of 5,700 m3 (200,000 cu ft) of 92% helium was produced in the program even though less than a cubic meter of the gas had previously been obtained. Some of this gas was used in the world's first helium-filled airship, the U.S. Navy's C-7, which flew its maiden voyage from Hampton Roads, Virginia, to Bolling Field in Washington, D.C., on December 1, 1921, nearly two years before the Navy's first rigid helium-filled airship, the Naval Aircraft Factory-built USS Shenandoah, flew in September 1923.\nAlthough the extraction process, using low-temperature gas liquefaction, was not developed in time to be significant during World War I, production continued. Helium was primarily used as a lifting gas in lighter-than-air craft. During World War II, the demand increased for helium for lifting gas and for shielded arc welding. The helium mass spectrometer was also vital in the atomic bomb Manhattan Project.\nThe government of the United States set up the National Helium Reserve in 1925 at Amarillo, Texas, with the goal of supplying military airships in time of war and commercial airships in peacetime. Because of the Helium Control Act (1927), which banned the export of scarce helium on which the US then had a production monopoly, together with the prohibitive cost of the gas, the Hindenburg, like all German Zeppelins, was forced to use hydrogen as the lift gas. The helium market after World War II was depressed but the reserve was expanded in the 1950s to ensure a supply of liquid helium as a coolant to create oxygen/hydrogen rocket fuel (among other uses) during the Space Race and Cold War. Helium use in the United States in 1965 was more than eight times the peak wartime consumption.\nAfter the \"Helium Acts Amendments of 1960\" (Public Law 86\u2013777), the U.S. Bureau of Mines arranged for five private plants to recover helium from natural gas. For this helium conservation program, the Bureau built a 425-mile (684 km) pipeline from Bushton, Kansas, to connect those plants with the government's partially depleted Cliffside gas field near Amarillo, Texas. This helium-nitrogen mixture was injected and stored in the Cliffside gas field until needed, at which time it was further purified.\nBy 1995, a billion cubic meters of the gas had been collected and the reserve was US$1.4 billion in debt, prompting the Congress of the United States in 1996 to phase out the reserve. The resulting \"Helium Privatization Act of 1996\" (Public Law 104\u2013273) directed the United States Department of the Interior to empty the reserve, with sales starting by 2005.\nHelium produced between 1930 and 1945 was about 98.3% pure (2% nitrogen), which was adequate for airships. In 1945, a small amount of 99.9% helium was produced for welding use. By 1949, commercial quantities of Grade A 99.95% helium were available.\nFor many years, the United States produced more than 90% of commercially usable helium in the world, while extraction plants in Canada, Poland, Russia, and other nations produced the remainder. In the mid-1990s, a new plant in Arzew, Algeria, producing 17 million cubic meters (600 million cubic feet) began operation, with enough production to cover all of Europe's demand. Meanwhile, by 2000, the consumption of helium within the U.S. had risen to more than 15 million kg per year. In 2004\u20132006, additional plants in Ras Laffan, Qatar, and Skikda, Algeria were built. Algeria quickly became the second leading producer of helium. Through this time, both helium consumption and the costs of producing helium increased. From 2002 to 2007 helium prices doubled.\nAs of 2012, the United States National Helium Reserve accounted for 30 percent of the world's helium. The reserve was expected to run out of helium in 2018. Despite that, a proposed bill in the United States Senate would allow the reserve to continue to sell the gas. Other large reserves were in the Hugoton in Kansas, United States, and nearby gas fields of Kansas and the panhandles of Texas and Oklahoma. New helium plants were scheduled to open in 2012 in Qatar, Russia, and the United States state of Wyoming, but they were not expected to ease the shortage.\nIn 2013, Qatar started up the world's largest helium unit. 2014 was widely acknowledged to be a year of over-supply in the helium business, following years of renowned shortages. Nasdaq reported (2015) that for Air Products, an international corporation that sells gases for industrial use, helium volumes remain under pressure due to feedstock supply constraints.\n\n\n== Characteristics ==\n\n\n=== The helium atom ===\n\n\n==== Helium in quantum mechanics ====\nIn the perspective of quantum mechanics, helium is the second simplest atom to model, following the hydrogen atom. Helium is composed of two electrons in atomic orbitals surrounding a nucleus containing two protons and (usually) two neutrons. As in Newtonian mechanics, no system that consists of more than two particles can be solved with an exact analytical mathematical approach (see 3-body problem) and helium is no exception. Thus, numerical mathematical methods are required, even to solve the system of one nucleus and two electrons. Such computational chemistry methods have been used to create a quantum mechanical picture of helium electron binding which is accurate to within < 2% of the correct value, in a few computational steps. Such models show that each electron in helium partly screens the nucleus from the other, so that the effective nuclear charge Z which each electron sees, is about 1.69 units, not the 2 charges of a classic \"bare\" helium nucleus.\n\n\n==== The related stability of the helium-4 nucleus and electron shell ====\nThe nucleus of the helium-4 atom is identical with an alpha particle. High-energy electron-scattering experiments show its charge to decrease exponentially from a maximum at a central point, exactly as does the charge density of helium's own electron cloud. This symmetry reflects similar underlying physics: the pair of neutrons and the pair of protons in helium's nucleus obey the same quantum mechanical rules as do helium's pair of electrons (although the nuclear particles are subject to a different nuclear binding potential), so that all these fermions fully occupy 1s orbitals in pairs, none of them possessing orbital angular momentum, and each cancelling the other's intrinsic spin. Adding another of any of these particles would require angular momentum and would release substantially less energy (in fact, no nucleus with five nucleons is stable). This arrangement is thus energetically extremely stable for all these particles, and this stability accounts for many crucial facts regarding helium in nature.\nFor example, the stability and low energy of the electron cloud state in helium accounts for the element's chemical inertness, and also the lack of interaction of helium atoms with each other, producing the lowest melting and boiling points of all the elements.\nIn a similar way, the particular energetic stability of the helium-4 nucleus, produced by similar effects, accounts for the ease of helium-4 production in atomic reactions that involve either heavy-particle emission or fusion. Some stable helium-3 (2 protons and 1 neutron) is produced in fusion reactions from hydrogen, but it is a very small fraction compared to the highly favorable helium-4.\n\nThe unusual stability of the helium-4 nucleus is also important cosmologically: it explains the fact that in the first few minutes after the Big Bang, as the \"soup\" of free protons and neutrons which had initially been created in about 6:1 ratio cooled to the point that nuclear binding was possible, almost all first compound atomic nuclei to form were helium-4 nuclei. So tight was helium-4 binding that helium-4 production consumed nearly all of the free neutrons in a few minutes, before they could beta-decay, and also leaving few to form heavier atoms such as lithium, beryllium, or boron. Helium-4 nuclear binding per nucleon is stronger than in any of these elements (see nucleogenesis and binding energy) and thus, once helium had been formed, no energetic drive was available to make elements 3, 4 and 5. It was barely energetically favorable for helium to fuse into the next element with a lower energy per nucleon, carbon. However, due to lack of intermediate elements, this process requires three helium nuclei striking each other nearly simultaneously (see triple alpha process). There was thus no time for significant carbon to be formed in the few minutes after the Big Bang, before the early expanding universe cooled to the temperature and pressure point where helium fusion to carbon was no longer possible. This left the early universe with a very similar ratio of hydrogen/helium as is observed today (3 parts hydrogen to 1 part helium-4 by mass), with nearly all the neutrons in the universe trapped in helium-4.\nAll heavier elements (including those necessary for rocky planets like the Earth, and for carbon-based or other life) have thus been created since the Big Bang in stars which were hot enough to fuse helium itself. All elements other than hydrogen and helium today account for only 2% of the mass of atomic matter in the universe. Helium-4, by contrast, makes up about 23% of the universe's ordinary matter\u2014nearly all the ordinary matter that is not hydrogen.\n\n\n=== Gas and plasma phases ===\n\nHelium is the second least reactive noble gas after neon, and thus the second least reactive of all elements. It is inert and monatomic in all standard conditions. Because of helium's relatively low molar (atomic) mass, its thermal conductivity, specific heat, and sound speed in the gas phase are all greater than any other gas except hydrogen. For these reasons and the small size of helium monatomic molecules, helium diffuses through solids at a rate three times that of air and around 65% that of hydrogen.\nHelium is the least water-soluble monatomic gas, and one of the least water-soluble of any gas (CF4, SF6, and C4F8 have lower mole fraction solubilities: 0.3802, 0.4394, and 0.2372 x2/10\u22125, respectively, versus helium's 0.70797 x2/10\u22125), and helium's index of refraction is closer to unity than that of any other gas. Helium has a negative Joule-Thomson coefficient at normal ambient temperatures, meaning it heats up when allowed to freely expand. Only below its Joule-Thomson inversion temperature (of about 32 to 50 K at 1 atmosphere) does it cool upon free expansion. Once precooled below this temperature, helium can be liquefied through expansion cooling.\nMost extraterrestrial helium is found in a plasma state, with properties quite different from those of atomic helium. In a plasma, helium's electrons are not bound to its nucleus, resulting in very high electrical conductivity, even when the gas is only partially ionized. The charged particles are highly influenced by magnetic and electric fields. For example, in the solar wind together with ionized hydrogen, the particles interact with the Earth's magnetosphere, giving rise to Birkeland currents and the aurora.\n\n\n=== Liquid helium ===\n\nUnlike any other element, helium will remain liquid down to absolute zero at normal pressures. This is a direct effect of quantum mechanics: specifically, the zero point energy of the system is too high to allow freezing. Solid helium requires a temperature of 1\u20131.5 K (about \u2212272 \u00b0C or \u2212457 \u00b0F) and about 25 bar (2.5 MPa) of pressure. It is often hard to distinguish solid from liquid helium since the refractive index of the two phases are nearly the same. The solid has a sharp melting point and has a crystalline structure, but it is highly compressible; applying pressure in a laboratory can decrease its volume by more than 30%. With a bulk modulus of about 27 MPa it is ~100 times more compressible than water. Solid helium has a density of 0.214 \u00b1 0.006 g/cm3 at 1.15 K and 66 atm; the projected density at 0 K and 25 bar (2.5 MPa) is 0.187 \u00b1 0.009 g/cm3.\n\n\n==== Helium I state ====\nBelow its boiling point of 4.22 kelvins and above the lambda point of 2.1768 kelvins, the isotope helium-4 exists in a normal colorless liquid state, called helium I. Like other cryogenic liquids, helium I boils when it is heated and contracts when its temperature is lowered. Below the lambda point, however, helium does not boil, and it expands as the temperature is lowered further.\nHelium I has a gas-like index of refraction of 1.026 which makes its surface so hard to see that floats of Styrofoam are often used to show where the surface is. This colorless liquid has a very low viscosity and a density of 0.145\u20130.125 g/mL (between about 0 and 4 K), which is only one-fourth the value expected from classical physics. Quantum mechanics is needed to explain this property and thus both states of liquid helium (helium I and helium II) are called quantum fluids, meaning they display atomic properties on a macroscopic scale. This may be an effect of its boiling point being so close to absolute zero, preventing random molecular motion (thermal energy) from masking the atomic properties.\n\n\n==== Helium II state ====\n\nLiquid helium below its lambda point (called helium II) exhibits very unusual characteristics. Due to its high thermal conductivity, when it boils, it does not bubble but rather evaporates directly from its surface. Helium-3 also has a superfluid phase, but only at much lower temperatures; as a result, less is known about the properties of the isotope.\n\nHelium II is a superfluid, a quantum mechanical state (see: macroscopic quantum phenomena) of matter with strange properties. For example, when it flows through capillaries as thin as 10\u22127 to 10\u22128 m it has no measurable viscosity. However, when measurements were done between two moving discs, a viscosity comparable to that of gaseous helium was observed. Current theory explains this using the two-fluid model for helium II. In this model, liquid helium below the lambda point is viewed as containing a proportion of helium atoms in a ground state, which are superfluid and flow with exactly zero viscosity, and a proportion of helium atoms in an excited state, which behave more like an ordinary fluid.\nIn the fountain effect, a chamber is constructed which is connected to a reservoir of helium II by a sintered disc through which superfluid helium leaks easily but through which non-superfluid helium cannot pass. If the interior of the container is heated, the superfluid helium changes to non-superfluid helium. In order to maintain the equilibrium fraction of superfluid helium, superfluid helium leaks through and increases the pressure, causing liquid to fountain out of the container.\nThe thermal conductivity of helium II is greater than that of any other known substance, a million times that of helium I and several hundred times that of copper. This is because heat conduction occurs by an exceptional quantum mechanism. Most materials that conduct heat well have a valence band of free electrons which serve to transfer the heat. Helium II has no such valence band but nevertheless conducts heat well. The flow of heat is governed by equations that are similar to the wave equation used to characterize sound propagation in air. When heat is introduced, it moves at 20 meters per second at 1.8 K through helium II as waves in a phenomenon known as second sound.\nHelium II also exhibits a creeping effect. When a surface extends past the level of helium II, the helium II moves along the surface, against the force of gravity. Helium II will escape from a vessel that is not sealed by creeping along the sides until it reaches a warmer region where it evaporates. It moves in a 30 nm-thick film regardless of surface material. This film is called a Rollin film and is named after the man who first characterized this trait, Bernard V. Rollin. As a result of this creeping behavior and helium II's ability to leak rapidly through tiny openings, it is very difficult to confine liquid helium. Unless the container is carefully constructed, the helium II will creep along the surfaces and through valves until it reaches somewhere warmer, where it will evaporate. Waves propagating across a Rollin film are governed by the same equation as gravity waves in shallow water, but rather than gravity, the restoring force is the van der Waals force. These waves are known as third sound.\n\n\n== Isotopes ==\n\nThere are nine known isotopes of helium, but only helium-3 and helium-4 are stable. In the Earth's atmosphere, one atom is 3He for every million that are 4He. Unlike most elements, helium's isotopic abundance varies greatly by origin, due to the different formation processes. The most common isotope, helium-4, is produced on Earth by alpha decay of heavier radioactive elements; the alpha particles that emerge are fully ionized helium-4 nuclei. Helium-4 is an unusually stable nucleus because its nucleons are arranged into complete shells. It was also formed in enormous quantities during Big Bang nucleosynthesis.\nHelium-3 is present on Earth only in trace amounts; most of it since Earth's formation, though some falls to Earth trapped in cosmic dust. Trace amounts are also produced by the beta decay of tritium. Rocks from the Earth's crust have isotope ratios varying by as much as a factor of ten, and these ratios can be used to investigate the origin of rocks and the composition of the Earth's mantle. 3He is much more abundant in stars as a product of nuclear fusion. Thus in the interstellar medium, the proportion of 3He to 4He is about 100 times higher than on Earth. Extraplanetary material, such as lunar and asteroid regolith, have trace amounts of helium-3 from being bombarded by solar winds. The Moon's surface contains helium-3 at concentrations on the order of 10 ppb, much higher than the approximately 5 ppt found in the Earth's atmosphere. A number of people, starting with Gerald Kulcinski in 1986, have proposed to explore the moon, mine lunar regolith, and use the helium-3 for fusion.\nLiquid helium-4 can be cooled to about 1 kelvin using evaporative cooling in a 1-K pot. Similar cooling of helium-3, which has a lower boiling point, can achieve about 6999200000000000000\u26600.2 kelvin in a helium-3 refrigerator. Equal mixtures of liquid 3He and 4He below 6999800000000000000\u26600.8 K separate into two immiscible phases due to their dissimilarity (they follow different quantum statistics: helium-4 atoms are bosons while helium-3 atoms are fermions). Dilution refrigerators use this immiscibility to achieve temperatures of a few millikelvins.\nIt is possible to produce exotic helium isotopes, which rapidly decay into other substances. The shortest-lived heavy helium isotope is helium-5 with a half-life of 6978760000000000000\u26607.6\u00d710\u221222 s. Helium-6 decays by emitting a beta particle and has a half-life of 0.8 second. Helium-7 also emits a beta particle as well as a gamma ray. Helium-7 and helium-8 are created in certain nuclear reactions. Helium-6 and helium-8 are known to exhibit a nuclear halo.\n\n\n== Compounds ==\n\nHelium has a valence of zero and is chemically unreactive under all normal conditions. It is an electrical insulator unless ionized. As with the other noble gases, helium has metastable energy levels that allow it to remain ionized in an electrical discharge with a voltage below its ionization potential. Helium can form unstable compounds, known as excimers, with tungsten, iodine, fluorine, sulfur, and phosphorus when it is subjected to a glow discharge, to electron bombardment, or reduced to plasma by other means. The molecular compounds HeNe, HgHe10, and WHe2, and the molecular ions He+\n2, He2+\n2, HeH+, and HeD+ have been created this way. HeH+ is also stable in its ground state, but is extremely reactive\u2014it is the strongest Br\u00f8nsted acid known, and therefore can exist only in isolation, as it will protonate any molecule or counteranion it contacts. This technique has also produced the neutral molecule He2, which has a large number of band systems, and HgHe, which is apparently held together only by polarization forces.\nVan der Waals compounds of helium can also be formed with cryogenic helium gas and atoms of some other substance, such as LiHe and He2.\nTheoretically, other true compounds may be possible, such as helium fluorohydride (HHeF) which would be analogous to HArF, discovered in 2000. Calculations show that two new compounds containing a helium-oxygen bond could be stable. Two new molecular species, predicted using theory, CsFHeO and N(CH3)4FHeO, are derivatives of a metastable [F\u2013 HeO] anion first theorized in 2005 by a group from Taiwan. If confirmed by experiment, the only remaining element with no known stable compounds would be neon.\nHelium atoms have been inserted into the hollow carbon cage molecules (the fullerenes) by heating under high pressure. The endohedral fullerene molecules formed are stable at high temperatures. When chemical derivatives of these fullerenes are formed, the helium stays inside. If helium-3 is used, it can be readily observed by helium nuclear magnetic resonance spectroscopy. Many fullerenes containing helium-3 have been reported. Although the helium atoms are not attached by covalent or ionic bonds, these substances have distinct properties and a definite composition, like all stoichiometric chemical compounds.\nUnder high pressures helium can form compounds with various other elements. Helium-nitrogen clathrate (He(N2)11) crystals have been grown at room temperature at pressures ca. 10 GPa in a diamond anvil cell. At 130 GPa Na2He is thermodynamically stable with a fluorite structure.\n\n\n== Occurrence and production ==\n\n\n=== Natural abundance ===\nAlthough it is rare on Earth, helium is the second most abundant element in the known Universe (after hydrogen), constituting 23% of its baryonic mass. The vast majority of helium was formed by Big Bang nucleosynthesis one to three minutes after the Big Bang. As such, measurements of its abundance contribute to cosmological models. In stars, it is formed by the nuclear fusion of hydrogen in proton-proton chain reactions and the CNO cycle, part of stellar nucleosynthesis.\nIn the Earth's atmosphere, the concentration of helium by volume is only 5.2 parts per million. The concentration is low and fairly constant despite the continuous production of new helium because most helium in the Earth's atmosphere escapes into space by several processes. In the Earth's heterosphere, a part of the upper atmosphere, helium and other lighter gases are the most abundant elements.\nMost helium on Earth is a result of radioactive decay. Helium is found in large amounts in minerals of uranium and thorium, including cleveite, pitchblende, carnotite and monazite, because they emit alpha particles (helium nuclei, He2+) to which electrons immediately combine as soon as the particle is stopped by the rock. In this way an estimated 3000 metric tons of helium are generated per year throughout the lithosphere. In the Earth's crust, the concentration of helium is 8 parts per billion. In seawater, the concentration is only 4 parts per trillion. There are also small amounts in mineral springs, volcanic gas, and meteoric iron. Because helium is trapped in the subsurface under conditions that also trap natural gas, the greatest natural concentrations of helium on the planet are found in natural gas, from which most commercial helium is extracted. The concentration varies in a broad range from a few ppm to more than 7% in a small gas field in San Juan County, New Mexico.\nAs of 2011 the world's helium reserves were estimated at 40 billion cubic meters, with a quarter of that being in the South Pars / North Dome Gas-Condensate field owned jointly by Qatar and Iran.\n\n\n=== Modern extraction and distribution ===\nFor large-scale use, helium is extracted by fractional distillation from natural gas, which can contain as much as 7% helium. Since helium has a lower boiling point than any other element, low temperature and high pressure are used to liquefy nearly all the other gases (mostly nitrogen and methane). The resulting crude helium gas is purified by successive exposures to lowering temperatures, in which almost all of the remaining nitrogen and other gases are precipitated out of the gaseous mixture. Activated charcoal is used as a final purification step, usually resulting in 99.995% pure Grade-A helium. The principal impurity in Grade-A helium is neon. In a final production step, most of the helium that is produced is liquefied via a cryogenic process. This is necessary for applications requiring liquid helium and also allows helium suppliers to reduce the cost of long distance transportation, as the largest liquid helium containers have more than five times the capacity of the largest gaseous helium tube trailers.\nIn 2008, approximately 169 million standard cubic meters (SCM) of helium were extracted from natural gas or withdrawn from helium reserves with approximately 78% from the United States, 10% from Algeria, and most of the remainder from Russia, Poland and Qatar. By 2013, increases in helium production in Qatar (under the company RasGas managed by Air Liquide) had increased Qatar's fraction of world helium production to 25%, and made it the second largest exporter after the United States. An estimated 54 billion cubic feet (1.5\u00d7109 m3) of helium was detected in Tanzania in 2016.\nIn the United States, most helium is extracted from natural gas of the Hugoton and nearby gas fields in Kansas, Oklahoma, and the Panhandle Field in Texas. Much of this gas was once sent by pipeline to the National Helium Reserve, but since 2005 this reserve is being depleted and sold off, and is expected under present law to be largely depleted by 2021.\nDiffusion of crude natural gas through special semipermeable membranes and other barriers is another method to recover and purify helium. In 1996, the U.S. had proven helium reserves, in such gas well complexes, of about 147 billion standard cubic feet (4.2 billion SCM). At rates of use at that time (72 million SCM per year in the U.S.; see pie chart below) this would have been enough helium for about 58 years of U.S. use, and less than this (perhaps 80% of the time) at world use rates, although factors in saving and processing impact effective reserve numbers.\nHelium must be extracted from natural gas because it is present in air at only a fraction of that of neon, yet the demand for it is far higher. It is estimated that if all neon production were retooled to save helium, that 0.1% of the world's helium demands would be satisfied. Similarly, only 1% of the world's helium demands could be satisfied by re-tooling all air distillation plants. Helium can be synthesized by bombardment of lithium or boron with high-velocity protons, but this process is a completely uneconomic method of production.\nHelium is commercially available in either liquid or gaseous form. As a liquid, it can be supplied in small insulated containers called dewars which hold as much as 1,000 liters of helium, or in large ISO containers which have nominal capacities as large as 42 m3 (around 11,000 U.S. gallons). In gaseous form, small quantities of helium are supplied in high-pressure cylinders holding as much as 8 m3 (approx. 282 standard cubic feet), while large quantities of high-pressure gas are supplied in tube trailers which have capacities of as much as 4,860 m3 (approx. 172,000 standard cubic feet).\n\n\n=== Conservation advocates ===\nAccording to helium conservationists like Nobel laureate physicist Robert Coleman Richardson, the free market price of helium has contributed to \"wasteful\" usage (e.g. for helium balloons). Prices in the 2000s have been lowered by U.S. Congress' decision to sell off the country's large helium stockpile by 2015. According to Richardson, the current price needs to be multiplied by 20 to eliminate the excessive wasting of helium. In their book, the Future of helium as a natural resource (Routledge, 2012), Nuttall, Clarke & Glowacki (2012) also proposed to create an International Helium Agency (IHA) to build a sustainable market for this precious commodity.\n\n\n== Applications ==\n\nWhile balloons are perhaps the best known use of helium, they are a minor part of all helium use. Helium is used for many purposes that require some of its unique properties, such as its low boiling point, low density, low solubility, high thermal conductivity, or inertness. Of the 2014 world helium total production of about 32 million kg (180 million standard cubic meters) helium per year, the largest use (about 32% of the total in 2014) is in cryogenic applications, most of which involves cooling the superconducting magnets in medical MRI scanners and NMR spectrometers. Other major uses were pressurizing and purging systems, welding, maintenance of controlled atmospheres, and leak detection. Other uses by category were relatively minor fractions.\n\n\n=== Controlled atmospheres ===\nHelium is used as a protective gas in growing silicon and germanium crystals, in titanium and zirconium production, and in gas chromatography, because it is inert. Because of its inertness, thermally and calorically perfect nature, high speed of sound, and high value of the heat capacity ratio, it is also useful in supersonic wind tunnels and impulse facilities.\n\n\n=== Gas tungsten arc welding ===\n\nHelium is used as a shielding gas in arc welding processes on materials that at welding temperatures are contaminated and weakened by air or nitrogen. A number of inert shielding gases are used in gas tungsten arc welding, but helium is used instead of cheaper argon especially for welding materials that have higher heat conductivity, like aluminium or copper.\n\n\n=== Minor uses ===\n\n\n==== Industrial leak detection ====\n\nOne industrial application for helium is leak detection. Because helium diffuses through solids three times faster than air, it is used as a tracer gas to detect leaks in high-vacuum equipment (such as cryogenic tanks) and high-pressure containers. The tested object is placed in a chamber, which is then evacuated and filled with helium. The helium that escapes through the leaks is detected by a sensitive device (helium mass spectrometer), even at the leak rates as small as 10\u22129 mbar\u00b7L/s (10\u221210 Pa\u00b7m3/s). The measurement procedure is normally automatic and is called helium integral test. A simpler procedure is to fill the tested object with helium and to manually search for leaks with a hand-held device.\nHelium leaks through cracks should not be confused with gas permeation through a bulk material. While helium has documented permeation constants (thus a calculable permeation rate) through glasses, ceramics, and synthetic materials, inert gases such as helium will not permeate most bulk metals.\n\n\n==== Flight ====\n\nBecause it is lighter than air, airships and balloons are inflated with helium for lift. While hydrogen gas is more buoyant, and escapes permeating through a membrane at a lower rate, helium has the advantage of being non-flammable, and indeed fire-retardant. Another minor use is in rocketry, where helium is used as an ullage medium to displace fuel and oxidizers in storage tanks and to condense hydrogen and oxygen to make rocket fuel. It is also used to purge fuel and oxidizer from ground support equipment prior to launch and to pre-cool liquid hydrogen in space vehicles. For example, the Saturn V rocket used in the Apollo program needed about 370,000 m3 (13 million cubic feet) of helium to launch.\n\n\n==== Minor commercial and recreational uses ====\nHelium as a breathing gas has no narcotic properties, so helium mixtures such as trimix, heliox and heliair are used for deep diving to reduce the effects of narcosis, which worsen with increasing depth. As pressure increases with depth, the density of the breathing gas also increases, and the low molecular weight of helium is found to considerably reduce the effort of breathing by lowering the density of the mixture. This reduces the Reynolds number of flow, leading to a reduction of turbulent flow and an increase in laminar flow, which requires less work of breathing. At depths below 150 metres (490 ft) divers breathing helium\u2013oxygen mixtures begin to experience tremors and a decrease in psychomotor function, symptoms of high-pressure nervous syndrome. This effect may be countered to some extent by adding an amount of narcotic gas such as hydrogen or nitrogen to a helium\u2013oxygen mixture.\nHelium\u2013neon lasers, a type of low-powered gas laser producing a red beam, had various practical applications which included barcode readers and laser pointers, before they were almost universally replaced by cheaper diode lasers.\nFor its inertness and high thermal conductivity, neutron transparency, and because it does not form radioactive isotopes under reactor conditions, helium is used as a heat-transfer medium in some gas-cooled nuclear reactors.\nHelium, mixed with a heavier gas such as xenon, is useful for thermoacoustic refrigeration due to the resulting high heat capacity ratio and low Prandtl number. The inertness of helium has environmental advantages over conventional refrigeration systems which contribute to ozone depletion or global warming.\nHelium is also used in some hard disk drives.\n\n\n==== Scientific uses ====\nThe use of helium reduces the distorting effects of temperature variations in the space between lenses in some telescopes, due to its extremely low index of refraction. This method is especially used in solar telescopes where a vacuum tight telescope tube would be too heavy.\nHelium is a commonly used carrier gas for gas chromatography.\nThe age of rocks and minerals that contain uranium and thorium can be estimated by measuring the level of helium with a process known as helium dating.\nHelium at low temperatures is used in cryogenics, and in certain cryogenics applications. As examples of applications, liquid helium is used to cool certain metals to the extremely low temperatures required for superconductivity, such as in superconducting magnets for magnetic resonance imaging. The Large Hadron Collider at CERN uses 96 metric tons of liquid helium to maintain the temperature at 1.9 kelvin.\n\n\n== Inhalation and safety ==\n\n\n=== Effects ===\nNeutral helium at standard conditions is non-toxic, plays no biological role and is found in trace amounts in human blood.\nThe speed of sound in helium is nearly three times the speed of sound in air. Because the fundamental frequency of a gas-filled cavity is proportional to the speed of sound in the gas, when helium is inhaled there is a corresponding increase in the resonant frequencies of the vocal tract. The fundamental frequency (sometimes called pitch) does not change, since this is produced by direct vibration of the vocal folds, which is unchanged. However, the higher resonant frequencies cause a change in timbre, resulting in a reedy, duck-like vocal quality. The opposite effect, lowering resonant frequencies, can be obtained by inhaling a dense gas such as sulfur hexafluoride or xenon.\n\n\n=== Hazards ===\nInhaling helium can be dangerous if done to excess, since helium is a simple asphyxiant and so displaces oxygen needed for normal respiration. Fatalities have been recorded, including a youth who suffocated in Vancouver in 2003 and two adults who suffocated in South Florida in 2006. In 1998, an Australian girl (her age is not known) from Victoria fell unconscious and temporarily turned blue after inhaling the entire contents of a party balloon. Inhaling helium directly from pressurized cylinders or even balloon filling valves is extremely dangerous, as high flow rate and pressure can result in barotrauma, fatally rupturing lung tissue.\nDeath caused by helium is rare. The first media-recorded case was that of a 15-year-old girl from Texas who died in 1998 from helium inhalation at a friend's party; the exact type of helium death is unidentified.\nIn the United States only two fatalities were reported between 2000 and 2004, including a man who died in North Carolina of barotrauma in 2002. A youth asphyxiated in Vancouver during 2003, and a 27-year-old man in Australia had an embolism after breathing from a cylinder in 2000. Since then two adults asphyxiated in South Florida in 2006, and there were cases in 2009 and 2010, one a Californian youth who was found with a bag over his head, attached to a helium tank, and another teenager in Northern Ireland died of asphyxiation. At Eagle Point, Oregon a teenage girl died in 2012 from barotrauma at a party. A girl from Michigan died from hypoxia later in the year.\nOn February 4, 2015 it was revealed that during the recording of their main TV show on January 28, a 12-year-old member (name withheld) of Japanese all-girl singing group 3B Junior suffered from air embolism, losing consciousness and falling in a coma as a result of air bubbles blocking the flow of blood to the brain, after inhaling huge quantities of helium as part of a game. The incident was not made public until a week later. The staff of TV Asahi held an emergency press conference to communicate that the member had been taken to the hospital and is showing signs of rehabilitation such as moving eyes and limbs, but her consciousness has not been sufficiently recovered as of yet. Police have launched an investigation due to a neglect of safety measures.\nThe safety issues for cryogenic helium are similar to those of liquid nitrogen; its extremely low temperatures can result in cold burns, and the liquid-to-gas expansion ratio can cause explosions if no pressure-relief devices are installed. Containers of helium gas at 5 to 10 K should be handled as if they contain liquid helium due to the rapid and significant thermal expansion that occurs when helium gas at less than 10 K is warmed to room temperature.\nAt high pressures (more than about 20 atm or two MPa), a mixture of helium and oxygen (heliox) can lead to high-pressure nervous syndrome, a sort of reverse-anesthetic effect; adding a small amount of nitrogen to the mixture can alleviate the problem.\n\n\n== Additional images ==\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\n\n\n== External links ==\n\nGeneral\nU.S. Government's Bureau of Land Management: Sources, Refinement, and Shortage. With some history of helium.\nU.S. Geological Survey publications on helium beginning 1996: Helium\nWhere is all the helium? Aga website\nIt's Elemental \u2013 Helium\nChemistry in its element podcast (MP3) from the Royal Society of Chemistry's Chemistry World: Helium\nInternational Chemical Safety Cards \u2013 Helium; includes health and safety information regarding accidental exposures to helium\nMore detail\nHelium at The Periodic Table of Videos (University of Nottingham)\nHelium at the Helsinki University of Technology; includes pressure-temperature phase diagrams for helium-3 and helium-4\nLancaster University, Ultra Low Temperature Physics \u2013 includes a summary of some low temperature techniques\nMiscellaneous\nPhysics in Speech with audio samples that demonstrate the unchanged voice pitch\nArticle about helium and other noble gases\nHelium shortage\nAmerica\u2019s Helium Supply: Options for Producing More Helium from Federal Land: Oversight Hearing before the Subcommittee on Energy and Mineral Resources of the Committee on Natural Resources, U.S. House Of Representatives, One Hundred Thirteenth Congress, First Session, Thursday, July 11, 2013\nHelium Program: Urgent Issues Facing BLM's Storage and Sale of Helium Reserves: Testimony before the Committee on Natural Resources, House of Representatives Government Accountability Office\nKramer, David (May 22, 2012). \"Senate bill would preserve US helium reserve: Measure would give scientists first dibs on helium should a shortage develop. Physics Today web site\". \n[2]", 
                "titleUrl": "https://en.wikipedia.org/wiki/Helium", 
                "title": "Helium"
            }, 
            {
                "snippet": "very low chemical reactivity. The six noble gases that occur naturally are helium (He), neon (Ne), argon (Ar), krypton (Kr), xenon (Xe), and the radioactive", 
                "pageCategories": "Articles containing German-language text\nArticles containing Greek-language text\nCS1 Russian-language sources (ru)\nCS1 maint: Explicit use of et al.\nCommons category with local link same as on Wikidata\nFeatured articles\nGroups in the periodic table\nNoble gases\nWikipedia articles with GND identifiers\nWikipedia articles with LCCN identifiers", 
                "pageContent": "The noble gases make a group of chemical elements with similar properties; under standard conditions, they are all odorless, colorless, monatomic gases with very low chemical reactivity. The six noble gases that occur naturally are helium (He), neon (Ne), argon (Ar), krypton (Kr), xenon (Xe), and the radioactive radon (Rn). Ununoctium (Uuo) is predicted to be a noble gas as well, but its chemistry has not yet been investigated.\nFor the first six periods of the periodic table, the noble gases are exactly the members of group 18 of the periodic table. Noble gases are typically highly unreactive except when under particular extreme conditions. The inertness of noble gases makes them very suitable in applications where reactions are not wanted. For example, argon is used in light bulbs to prevent the hot tungsten filament from oxidizing; also, helium is used in breathing gas by deep-sea divers to prevent oxygen and nitrogen toxicity.\nThe properties of the noble gases can be well explained by modern theories of atomic structure: their outer shell of valence electrons is considered to be \"full\", giving them little tendency to participate in chemical reactions, and it has been possible to prepare only a few hundred noble gas compounds. The melting and boiling points for a given noble gas are close together, differing by less than 10 \u00b0C (18 \u00b0F); that is, they are liquids over only a small temperature range.\nNeon, argon, krypton, and xenon are obtained from air in an air separation unit using the methods of liquefaction of gases and fractional distillation. Helium is sourced from natural gas fields which have high concentrations of helium in the natural gas, using cryogenic gas separation techniques, and radon is usually isolated from the radioactive decay of dissolved radium, thorium, or uranium compounds (since those compounds give off alpha particles). Noble gases have several important applications in industries such as lighting, welding, and space exploration. A helium-oxygen breathing gas is often used by deep-sea divers at depths of seawater over 55 m (180 ft) to keep the diver from experiencing oxygen toxemia, the lethal effect of high-pressure oxygen, and nitrogen narcosis, the distracting narcotic effect of the nitrogen in air beyond this partial-pressure threshold. After the risks caused by the flammability of hydrogen became apparent, it was replaced with helium in blimps and balloons.\n\n\n== History ==\nNoble gas is translated from the German noun Edelgas, first used in 1898 by Hugo Erdmann to indicate their extremely low level of reactivity. The name makes an analogy to the term \"noble metals\", which also have low reactivity. The noble gases have also been referred to as inert gases, but this label is deprecated as many noble gas compounds are now known. Rare gases is another term that was used, but this is also inaccurate because argon forms a fairly considerable part (0.94% by volume, 1.3% by mass) of the Earth's atmosphere due to decay of radioactive potassium-40.\n\nPierre Janssen and Joseph Norman Lockyer discovered a new element on August 18, 1868 while looking at the chromosphere of the Sun, and named it helium after the Greek word for the Sun, \u03ae\u03bb\u03b9\u03bf\u03c2 (\u00edlios or helios). No chemical analysis was possible at the time, but helium was later found to be a noble gas. Before them, in 1784, the English chemist and physicist Henry Cavendish had discovered that air contains a small proportion of a substance less reactive than nitrogen. A century later, in 1895, Lord Rayleigh discovered that samples of nitrogen from the air were of a different density than nitrogen resulting from chemical reactions. Along with Scottish scientist William Ramsay at University College, London, Lord Rayleigh theorized that the nitrogen extracted from air was mixed with another gas, leading to an experiment that successfully isolated a new element, argon, from the Greek word \u03b1\u03c1\u03b3\u03cc\u03c2 (arg\u00f3s, \"inactive\"). With this discovery, they realized an entire class of gases was missing from the periodic table. During his search for argon, Ramsay also managed to isolate helium for the first time while heating cleveite, a mineral. In 1902, having accepted the evidence for the elements helium and argon, Dmitri Mendeleev included these noble gases as group 0 in his arrangement of the elements, which would later become the periodic table.\nRamsay continued to search for these gases using the method of fractional distillation to separate liquid air into several components. In 1898, he discovered the elements krypton, neon, and xenon, and named them after the Greek words \u03ba\u03c1\u03c5\u03c0\u03c4\u03cc\u03c2 (krypt\u00f3s, \"hidden\"), \u03bd\u03ad\u03bf\u03c2 (n\u00e9os, \"new\"), and \u03be\u03ad\u03bd\u03bf\u03c2 (x\u00e9nos, \"stranger\"), respectively. Radon was first identified in 1898 by Friedrich Ernst Dorn, and was named radium emanation, but was not considered a noble gas until 1904 when its characteristics were found to be similar to those of other noble gases. Rayleigh and Ramsay received the 1904 Nobel Prizes in Physics and in Chemistry, respectively, for their discovery of the noble gases; in the words of J. E. Cederblom, then president of the Royal Swedish Academy of Sciences, \"the discovery of an entirely new group of elements, of which no single representative had been known with any certainty, is something utterly unique in the history of chemistry, being intrinsically an advance in science of peculiar significance\".\nThe discovery of the noble gases aided in the development of a general understanding of atomic structure. In 1895, French chemist Henri Moissan attempted to form a reaction between fluorine, the most electronegative element, and argon, one of the noble gases, but failed. Scientists were unable to prepare compounds of argon until the end of the 20th century, but these attempts helped to develop new theories of atomic structure. Learning from these experiments, Danish physicist Niels Bohr proposed in 1913 that the electrons in atoms are arranged in shells surrounding the nucleus, and that for all noble gases except helium the outermost shell always contains eight electrons. In 1916, Gilbert N. Lewis formulated the octet rule, which concluded an octet of electrons in the outer shell was the most stable arrangement for any atom; this arrangement caused them to be unreactive with other elements since they did not require any more electrons to complete their outer shell.\nIn 1962, Neil Bartlett discovered the first chemical compound of a noble gas, xenon hexafluoroplatinate. Compounds of other noble gases were discovered soon after: in 1962 for radon, radon difluoride , which was identified by radiotracer techniques and in 1963 for krypton, krypton difluoride (KrF\n2). The first stable compound of argon was reported in 2000 when argon fluorohydride (HArF) was formed at a temperature of 40 K (\u2212233.2 \u00b0C; \u2212387.7 \u00b0F).\nIn December 1998, scientists at the Joint Institute for Nuclear Research working in Dubna, Russia bombarded plutonium (Pu) with calcium (Ca) to produce a single atom of element 114, flerovium (Fl). Preliminary chemistry experiments have indicated this element may be the first superheavy element to show abnormal noble-gas-like properties, even though it is a member of group 14 on the periodic table. In October 2006, scientists from the Joint Institute for Nuclear Research and Lawrence Livermore National Laboratory successfully created synthetically ununoctium (Uuo), the seventh element in group 18, by bombarding californium (Cf) with calcium (Ca).\n\n\n== Physical and atomic properties ==\n\nThe noble gases have weak interatomic force, and consequently have very low melting and boiling points. They are all monatomic gases under standard conditions, including the elements with larger atomic masses than many normally solid elements. Helium has several unique qualities when compared with other elements: its boiling and melting points are lower than those of any other known substance; it is the only element known to exhibit superfluidity; it is the only element that cannot be solidified by cooling under standard conditions\u2014a pressure of 25 standard atmospheres (2,500 kPa; 370 psi) must be applied at a temperature of 0.95 K (\u2212272.200 \u00b0C; \u2212457.960 \u00b0F) to convert it to a solid. The noble gases up to xenon have multiple stable isotopes. Radon has no stable isotopes; its longest-lived isotope, 222Rn, has a half-life of 3.8 days and decays to form helium and polonium, which ultimately decays to lead. Melting and boiling points generally increase going down the group.\n\nThe noble gas atoms, like atoms in most groups, increase steadily in atomic radius from one period to the next due to the increasing number of electrons. The size of the atom is related to several properties. For example, the ionization potential decreases with an increasing radius because the valence electrons in the larger noble gases are farther away from the nucleus and are therefore not held as tightly together by the atom. Noble gases have the largest ionization potential among the elements of each period, which reflects the stability of their electron configuration and is related to their relative lack of chemical reactivity. Some of the heavier noble gases, however, have ionization potentials small enough to be comparable to those of other elements and molecules. It was the insight that xenon has an ionization potential similar to that of the oxygen molecule that led Bartlett to attempt oxidizing xenon using platinum hexafluoride, an oxidizing agent known to be strong enough to react with oxygen. Noble gases cannot accept an electron to form stable anions; that is, they have a negative electron affinity.\nThe macroscopic physical properties of the noble gases are dominated by the weak van der Waals forces between the atoms. The attractive force increases with the size of the atom as a result of the increase in polarizability and the decrease in ionization potential. This results in systematic group trends: as one goes down group 18, the atomic radius, and with it the interatomic forces, increases, resulting in an increasing melting point, boiling point, enthalpy of vaporization, and solubility. The increase in density is due to the increase in atomic mass.\nThe noble gases are nearly ideal gases under standard conditions, but their deviations from the ideal gas law provided important clues for the study of intermolecular interactions. The Lennard-Jones potential, often used to model intermolecular interactions, was deduced in 1924 by John Lennard-Jones from experimental data on argon before the development of quantum mechanics provided the tools for understanding intermolecular forces from first principles. The theoretical analysis of these interactions became tractable because the noble gases are monatomic and the atoms spherical, which means that the interaction between the atoms is independent of direction, or isotropic.\n\n\n== Chemical properties ==\n\nThe noble gases are colorless, odorless, tasteless, and nonflammable under standard conditions. They were once labeled group 0 in the periodic table because it was believed they had a valence of zero, meaning their atoms cannot combine with those of other elements to form compounds. However, it was later discovered some do indeed form compounds, causing this label to fall into disuse.\n\n\n=== Configuration ===\n\nLike other groups, the members of this family show patterns in its electron configuration, especially the outermost shells resulting in trends in chemical behavior:\nThe noble gases have full valence electron shells. Valence electrons are the outermost electrons of an atom and are normally the only electrons that participate in chemical bonding. Atoms with full valence electron shells are extremely stable and therefore do not tend to form chemical bonds and have little tendency to gain or lose electrons. However, heavier noble gases such as radon are held less firmly together by electromagnetic force than lighter noble gases such as helium, making it easier to remove outer electrons from heavy noble gases.\nAs a result of a full shell, the noble gases can be used in conjunction with the electron configuration notation to form the noble gas notation. To do this, the nearest noble gas that precedes the element in question is written first, and then the electron configuration is continued from that point forward. For example, the electron notation of phosphorus is 1s2 2s2 2p6 3s2 3p3, while the noble gas notation is [Ne] 3s2 3p3. This more compact notation makes it easier to identify elements, and is shorter than writing out the full notation of atomic orbitals.\n\n\n=== Compounds ===\n\nThe noble gases show extremely low chemical reactivity; consequently, only a few hundred noble gas compounds have been formed. Neutral compounds in which helium and neon are involved in chemical bonds have not been formed (although there is some theoretical evidence for a few helium compounds), while xenon, krypton, and argon have shown only minor reactivity. The reactivity follows the order Ne < He < Ar < Kr < Xe < Rn.\nIn 1933, Linus Pauling predicted that the heavier noble gases could form compounds with fluorine and oxygen. He predicted the existence of krypton hexafluoride (KrF\n6) and xenon hexafluoride (XeF\n6), speculated that XeF\n8 might exist as an unstable compound, and suggested xenic acid could form perxenate salts. These predictions were shown to be generally accurate, except that XeF\n8 is now thought to be both thermodynamically and kinetically unstable.\nXenon compounds are the most numerous of the noble gas compounds that have been formed. Most of them have the xenon atom in the oxidation state of +2, +4, +6, or +8 bonded to highly electronegative atoms such as fluorine or oxygen, as in xenon difluoride (XeF\n2), xenon tetrafluoride (XeF\n4), xenon hexafluoride (XeF\n6), xenon tetroxide (XeO\n4), and sodium perxenate (Na\n4XeO\n6). Xenon reacts with fluorine to form numerous xenon fluorides according to the following equations:\n\nXe + F2 \u2192 XeF2\nXe + 2F2 \u2192 XeF4\nXe + 3F2 \u2192 XeF6\n\nSome of these compounds have found use in chemical synthesis as oxidizing agents; XeF\n2, in particular, is commercially available and can be used as a fluorinating agent. As of 2007, about five hundred compounds of xenon bonded to other elements have been identified, including organoxenon compounds (containing xenon bonded to carbon), and xenon bonded to nitrogen, chlorine, gold, mercury, and xenon itself. Compounds of xenon bound to boron, hydrogen, bromine, iodine, beryllium, sulphur, titanium, copper, and silver have also been observed but only at low temperatures in noble gas matrices, or in supersonic noble gas jets.\nIn theory, radon is more reactive than xenon, and therefore should form chemical bonds more easily than xenon does. However, due to the high radioactivity and short half-life of radon isotopes, only a few fluorides and oxides of radon have been formed in practice.\nKrypton is less reactive than xenon, but several compounds have been reported with krypton in the oxidation state of +2. Krypton difluoride is the most notable and easily characterized. Under extreme conditions, krypton reacts with fluorine to form KrF2 according to the following equation:\n\nKr + F2 \u2192 KrF2\n\nCompounds in which krypton forms a single bond to nitrogen and oxygen have also been characterized, but are only stable below \u221260 \u00b0C (\u221276 \u00b0F) and \u221290 \u00b0C (\u2212130 \u00b0F) respectively.\nKrypton atoms chemically bound to other nonmetals (hydrogen, chlorine, carbon) as well as some late transition metals (copper, silver, gold) have also been observed, but only either at low temperatures in noble gas matrices, or in supersonic noble gas jets. Similar conditions were used to obtain the first few compounds of argon in 2000, such as argon fluorohydride (HArF), and some bound to the late transition metals copper, silver, and gold. As of 2007, no stable neutral molecules involving covalently bound helium or neon are known.\nThe noble gases\u2014including helium\u2014can form stable molecular ions in the gas phase. The simplest is the helium hydride molecular ion, HeH+, discovered in 1925. Because it is composed of the two most abundant elements in the universe, hydrogen and helium, it is believed to occur naturally in the interstellar medium, although it has not been detected yet. In addition to these ions, there are many known neutral excimers of the noble gases. These are compounds such as ArF and KrF that are stable only when in an excited electronic state; some of them find application in excimer lasers.\nIn addition to the compounds where a noble gas atom is involved in a covalent bond, noble gases also form non-covalent compounds. The clathrates, first described in 1949, consist of a noble gas atom trapped within cavities of crystal lattices of certain organic and inorganic substances. The essential condition for their formation is that the guest (noble gas) atoms must be of appropriate size to fit in the cavities of the host crystal lattice. For instance, argon, krypton, and xenon form clathrates with hydroquinone, but helium and neon do not because they are too small or insufficiently polarizable to be retained. Neon, argon, krypton, and xenon also form clathrate hydrates, where the noble gas is trapped in ice.\n\nNoble gases can form endohedral fullerene compounds, in which the noble gas atom is trapped inside a fullerene molecule. In 1993, it was discovered that when C\n60, a spherical molecule consisting of 60 carbon atoms, is exposed to noble gases at high pressure, complexes such as He@C\n60 can be formed (the @ notation indicates He is contained inside C\n60 but not covalently bound to it). As of 2008, endohedral complexes with helium, neon, argon, krypton, and xenon have been obtained. These compounds have found use in the study of the structure and reactivity of fullerenes by means of the nuclear magnetic resonance of the noble gas atom.\n\nNoble gas compounds such as xenon difluoride (XeF\n2) are considered to be hypervalent because they violate the octet rule. Bonding in such compounds can be explained using a three-center four-electron bond model. This model, first proposed in 1951, considers bonding of three collinear atoms. For example, bonding in XeF\n2 is described by a set of three molecular orbitals (MOs) derived from p-orbitals on each atom. Bonding results from the combination of a filled p-orbital from Xe with one half-filled p-orbital from each F atom, resulting in a filled bonding orbital, a filled non-bonding orbital, and an empty antibonding orbital. The highest occupied molecular orbital is localized on the two terminal atoms. This represents a localization of charge which is facilitated by the high electronegativity of fluorine.\nThe chemistry of heavier noble gases, krypton and xenon, are well established. The chemistry of the lighter ones, argon and helium, is still at an early stage, while a neon compound is yet to be identified.\n\n\n== Occurrence and production ==\nThe abundances of the noble gases in the universe decrease as their atomic numbers increase. Helium is the most common element in the universe after hydrogen, with a mass fraction of about 24%. Most of the helium in the universe was formed during Big Bang nucleosynthesis, but the amount of helium is steadily increasing due to the fusion of hydrogen in stellar nucleosynthesis (and, to a very slight degree, the alpha decay of heavy elements). Abundances on Earth follow different trends; for example, helium is only the third most abundant noble gas in the atmosphere. The reason is that there is no primordial helium in the atmosphere; due to the small mass of the atom, helium cannot be retained by the Earth's gravitational field. Helium on Earth comes from the alpha decay of heavy elements such as uranium and thorium found in the Earth's crust, and tends to accumulate in natural gas deposits. The abundance of argon, on the other hand, is increased as a result of the beta decay of potassium-40, also found in the Earth's crust, to form argon-40, which is the most abundant isotope of argon on Earth despite being relatively rare in the Solar System. This process is the basis for the potassium-argon dating method. Xenon has an unexpectedly low abundance in the atmosphere, in what has been called the missing xenon problem; one theory is that the missing xenon may be trapped in minerals inside the Earth's crust. After the discovery of xenon dioxide, a research showed that Xe can substitute for Si in the quartz. Radon is formed in the lithosphere as from the alpha decay of radium. It can seep into buildings through cracks in their foundation and accumulate in areas that are not well ventilated. Due to its high radioactivity, radon presents a significant health hazard; it is implicated in an estimated 21,000 lung cancer deaths per year in the United States alone.\n\nFor large-scale use, helium is extracted by fractional distillation from natural gas, which can contain up to 7% helium.\nNeon, argon, krypton, and xenon are obtained from air using the methods of liquefaction of gases, to convert elements to a liquid state, and fractional distillation, to separate mixtures into component parts. Helium is typically produced by separating it from natural gas, and radon is isolated from the radioactive decay of radium compounds. The prices of the noble gases are influenced by their natural abundance, with argon being the cheapest and xenon the most expensive. As an example, the table to the right lists the 2004 prices in the United States for laboratory quantities of each gas.\n\n\n== Applications ==\n\nNoble gases have very low boiling and melting points, which makes them useful as cryogenic refrigerants. In particular, liquid helium, which boils at 4.2 K (\u2212268.95 \u00b0C; \u2212452.11 \u00b0F), is used for superconducting magnets, such as those needed in nuclear magnetic resonance imaging and nuclear magnetic resonance. Liquid neon, although it does not reach temperatures as low as liquid helium, also finds use in cryogenics because it has over 40 times more refrigerating capacity than liquid helium and over three times more than liquid hydrogen.\nHelium is used as a component of breathing gases to replace nitrogen, due its low solubility in fluids, especially in lipids. Gases are absorbed by the blood and body tissues when under pressure like in scuba diving, which causes an anesthetic effect known as nitrogen narcosis. Due to its reduced solubility, little helium is taken into cell membranes, and when helium is used to replace part of the breathing mixtures, such as in trimix or heliox, a decrease in the narcotic effect of the gas at depth is obtained. Helium's reduced solubility offers further advantages for the condition known as decompression sickness, or the bends. The reduced amount of dissolved gas in the body means that fewer gas bubbles form during the decrease in pressure of the ascent. Another noble gas, argon, is considered the best option for use as a drysuit inflation gas for scuba diving. Helium is also used as filling gas in nuclear fuel rods for nuclear reactors.\n\nSince the Hindenburg disaster in 1937, helium has replaced hydrogen as a lifting gas in blimps and balloons due to its lightness and incombustibility, despite an 8.6% decrease in buoyancy.\nIn many applications, the noble gases are used to provide an inert atmosphere. Argon is used in the synthesis of air-sensitive compounds that are sensitive to nitrogen. Solid argon is also used for the study of very unstable compounds, such as reactive intermediates, by trapping them in an inert matrix at very low temperatures. Helium is used as the carrier medium in gas chromatography, as a filler gas for thermometers, and in devices for measuring radiation, such as the Geiger counter and the bubble chamber. Helium and argon are both commonly used to shield welding arcs and the surrounding base metal from the atmosphere during welding and cutting, as well as in other metallurgical processes and in the production of silicon for the semiconductor industry.\n\nNoble gases are commonly used in lighting because of their lack of chemical reactivity. Argon, mixed with nitrogen, is used as a filler gas for incandescent light bulbs. Krypton is used in high-performance light bulbs, which have higher color temperatures and greater efficiency, because it reduces the rate of evaporation of the filament more than argon; halogen lamps, in particular, use krypton mixed with small amounts of compounds of iodine or bromine. The noble gases glow in distinctive colors when used inside gas-discharge lamps, such as \"neon lights\". These lights are called after neon but often contain other gases and phosphors, which add various hues to the orange-red color of neon. Xenon is commonly used in xenon arc lamps which, due to their nearly continuous spectrum that resembles daylight, find application in film projectors and as automobile headlamps.\nThe noble gases are used in excimer lasers, which are based on short-lived electronically excited molecules known as excimers. The excimers used for lasers may be noble gas dimers such as Ar2, Kr2 or Xe2, or more commonly, the noble gas is combined with a halogen in excimers such as ArF, KrF, XeF, or XeCl. These lasers produce ultraviolet light which, due to its short wavelength (193 nm for ArF and 248 nm for KrF), allows for high-precision imaging. Excimer lasers have many industrial, medical, and scientific applications. They are used for microlithography and microfabrication, which are essential for integrated circuit manufacture, and for laser surgery, including laser angioplasty and eye surgery.\nSome noble gases have direct application in medicine. Helium is sometimes used to improve the ease of breathing of asthma sufferers. Xenon is used as an anesthetic because of its high solubility in lipids, which makes it more potent than the usual nitrous oxide, and because it is readily eliminated from the body, resulting in faster recovery. Xenon finds application in medical imaging of the lungs through hyperpolarized MRI. Radon, which is highly radioactive and is only available in minute amounts, is used in radiotherapy.\n\n\n== Discharge color ==\nThe color of gas discharge emission depends on several factors, including the following:\ndischarge parameters (local value of current density and electric field, temperature, etc. \u2013 note the color variation along the discharge in the top row);\ngas purity (even small fraction of certain gases can affect color);\nmaterial of the discharge tube envelope \u2013 note suppression of the UV and blue components in the bottom-row tubes made of thick household glass.\n\n\n== See also ==\nNoble gas (data page), for extended tables of physical properties.\nNoble metal, for metals that are resistant to corrosion or oxidation.\nInert gas, for any gas that is not reactive under normal circumstances.\nIndustrial gas\nNeutronium\nNoble gas configuration\n\n\n== Notes ==\n\n\n== References ==\nBennett, Peter B.; Elliott, David H. (1998). The Physiology and Medicine of Diving. SPCK Publishing. ISBN 0-7020-2410-4. \nBobrow Test Preparation Services (2007-12-05). CliffsAP Chemistry. CliffsNotes. ISBN 0-470-13500-X. \nGreenwood, N. N.; Earnshaw, A. (1997). Chemistry of the Elements (2nd ed.). Oxford:Butterworth-Heinemann. ISBN 0-7506-3365-4. \nHarding, Charlie J.; Janes, Rob (2002). Elements of the P Block. Royal Society of Chemistry. ISBN 0-85404-690-9. \nHolloway, John H. (1968). Noble-Gas Chemistry. London: Methuen Publishing. ISBN 0-412-21100-9. \nMendeleev, D. (1902\u20131903). Osnovy Khimii (The Principles of Chemistry) (in Russian) (7th ed.). \nOjima, Minoru; Podosek, Frank A. (2002). Noble Gas Geochemistry. Cambridge University Press. ISBN 0-521-80366-7. \nWeinhold, F.; Landis, C. (2005). Valency and bonding. Cambridge University Press. ISBN 0-521-83128-8.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Noble_gas", 
                "title": "Noble gas"
            }, 
            {
                "snippet": "isotope of helium is present: the common isotope helium-4 or the rare isotope helium-3. These are the only two stable isotopes of helium. See the table", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from January 2016\nCoolants\nCryogenics\nHelium\nIndustrial gases\nLiquid helium\nNoble gases\nScience and technology in the Netherlands", 
                "pageContent": "At standard pressure, the chemical element helium exists in a liquid form only at the extremely low temperature of \u2212269 \u00b0C (about 4 K or \u2212452.2 \u00b0F). Its boiling point and critical point depend on which isotope of helium is present: the common isotope helium-4 or the rare isotope helium-3. These are the only two stable isotopes of helium. See the table below for the values of these physical quantities. The density of liquid helium-4 at its boiling point and a pressure of one atmosphere (101.3 kilopascals) is about 0.125 grams per cm3, or about 1/8th the density of liquid water.\n\n\n== Liquefaction ==\nHelium was first liquefied on July 10, 1908, by the Dutch physicist Heike Kamerlingh Onnes at the University of Leiden in the Netherlands. At that time, helium-3 was unknown because the mass spectrometer had not yet been invented. In more recent decades, liquid helium has been used as a cryogenic refrigerant, and liquid helium is produced commercially for use in superconducting magnets such as those used in magnetic resonance imaging (MRI), nuclear magnetic resonance (NMR), Magnetoencephalography (MEG), and experiments in physics, such as low temperature M\u00f6ssbauer spectroscopy.\n\n\n== Characteristics ==\nThe temperature required to produce liquid helium is low because of the weakness of the attractions between the helium atoms. These interatomic forces in helium are weak to begin with because helium is a noble gas, but the interatomic attractions are reduced even more by the effects of quantum mechanics. These are significant in helium because of its low atomic mass of about four atomic mass units. The zero point energy of liquid helium is less if its atoms are less confined by their neighbors. Hence in liquid helium, its ground state energy can decrease by a naturally-occurring increase in its average interatomic distance. However at greater distances, the effects of the interatomic forces in helium are even weaker.\nBecause of the very weak interatomic forces in helium, this element would remain a liquid at atmospheric pressure all the way from its liquefaction point down to absolute zero. Liquid helium solidifies only under very low temperatures and great pressures. At temperatures below their liquefaction points, both helium-4 and helium-3 undergo transitions to superfluids. (See the table below.)\nLiquid helium-4 and the rare helium-3 are not completely miscible. Below 0.9 kelvin at their saturated vapor pressure, a mixture of the two isotopes undergoes a phase separation into a normal fluid (mostly helium-3) that floats on a denser superfluid consisting mostly of helium-4. This phase separation happens because the overall mass of liquid helium can reduce its thermodynamic enthalpy by separating.\nAt extremely low temperatures, the superfluid phase, rich in helium-4, can contain up to 6% of helium-3 in solution. This makes possible the small-scale use of the dilution refrigerator, which is capable of reaching temperatures of a few millikelvins.\nSuperfluid helium-4 has substantially different properties from ordinary liquid helium.\n\n\n== Data ==\n\n\n== Gallery ==\n\n\n== See also ==\n\n\n== References ==\n\nGeneral\n\n\n== External links ==\nHe-3 and He-4 phase diagrams, etc.\nHelium-3 phase diagram, etc.\nOnnes's liquifaction of helium", 
                "titleUrl": "https://en.wikipedia.org/wiki/Liquid_helium", 
                "title": "Liquid helium"
            }, 
            {
                "snippet": "there are nine known isotopes of helium (He) (relative atomic mass: 4.002602(2)), only helium-3 (3 He ) and helium-4 (4 He ) are stable. All radioisotopes", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from December 2011\nArticles with unsourced statements from January 2012\nCS1 errors: dates\nHelium\nIsotopes of helium\nLists of isotopes by element\nPages with login required references or sources", 
                "pageContent": "Although there are nine known isotopes of helium (He) (relative atomic mass: 4.002602(2)), only helium-3 (3He) and helium-4 (4He) are stable. All radioisotopes are short-lived, the longest-lived being 6He with a half-life of 7002806700000000000\u2660806.7 milliseconds. The least stable is 5He, with a half-life of 6978760000000000000\u26607.6\u00d710\u221222 s, although it is possible that 2He has an even shorter half-life.\nIn the Earth's atmosphere, there is one 3He atom for every million 4He atoms. However, helium is unusual in that its isotopic abundance varies greatly depending on its origin. In the interstellar medium, the proportion of 3He is around a hundred times higher. Rocks from the Earth's crust have isotope ratios varying by as much as a factor of ten; this is used in geology to investigate the origin of rocks and the composition of the Earth's mantle. The different formation processes of the two stable isotopes of helium produce the differing isotope abundances.\nEqual mixtures of liquid 3He and 4He below 6999800000000000000\u26600.8 K will separate into two immiscible phases due to their dissimilarity (they follow different quantum statistics: 4He atoms are bosons while 3He atoms are fermions). Dilution refrigerators take advantage of the immiscibility of these two isotopes to achieve temperatures of a few millikelvins.\n\n\n== Helium-2 (diproton) ==\nHelium-2 or 2He, also known as a diproton, is an extremely unstable isotope of helium that consists of two protons with no neutrons. According to theoretical calculations it would have been much more stable (although still beta decaying to deuterium) if the strong force had been 2% greater. Its instability is due to spin\u2013spin interactions in the nuclear force, and the Pauli exclusion principle, which forces the two protons to have anti-aligned spins and gives the diproton a negative binding energy.\nThere may have been observations of 2He. In 2000, physicists first observed a new type of radioactive decay in which a nucleus emits two protons at once\u2014perhaps a 2He nucleus. The team led by Alfredo Galindo-Uribarri of the Oak Ridge National Laboratory announced that the discovery will help scientists understand the strong nuclear force and provide fresh insights into the creation of elements inside stars. Galindo-Uribarri and co-workers chose an isotope of neon with an energy structure that prevents it from emitting protons one at a time. This means that the two protons are ejected simultaneously. The team fired a beam of fluorine ions at a proton-rich target to produce 18Ne, which then decayed into oxygen and two protons. Any protons ejected from the target itself were identified by their characteristic energies. There are two ways in which the two-proton emission may proceed. The neon nucleus might eject a 'diproton'\u2014a pair of protons bound together as a 2He nucleus\u2014which then decays into separate protons. Alternatively, the protons may be emitted separately but at the same time\u2014so-called 'democratic decay'. The experiment was not sensitive enough to establish which of these two processes was taking place.\nMore evidence of 2He was found in 2008 at the Istituto Nazionale di Fisica Nucleare, in Italy. A beam of 20Ne ions was directed at a target of beryllium foil. This collision converted some of the heavier neon nuclei in the beam into 18Ne nuclei. These nuclei then collided with a foil of lead. The second collision had the effect of exciting the 18Ne nucleus into a highly unstable condition. As in the earlier experiment at Oak Ridge, the 18Ne nucleus decayed into an 16O nucleus, plus two protons detected exiting from the same direction. The new experiment showed that the two protons were initially ejected together, correlated in a quasibound 1S configuration, before decaying into separate protons much less than a nanosecond later.\nFurther evidence comes from RIKEN in Japan and JINR in Dubna, Russia, where beams of 6He nuclei were directed at a cryogenic hydrogen target to produce 5He. It was discovered that the 6He nucleus can donate all four of its neutrons to the hydrogen. The two remaining protons could be simultaneously ejected from the target as a 2He nucleus, which quickly decayed into two protons. A similar reaction has also been observed from 8He nuclei colliding with hydrogen.\n2He is an intermediate in the first step of the proton-proton chain reaction. The first step of the proton-proton chain reaction is a two-stage process; first, two protons fuse to form a diproton:\n\nfollowed by the immediate beta-plus decay of the diproton to deuterium:\n\nwith the overall formula:\n\nR. A. W. Bradford has considered the hypothetical effect of the binding of the diproton on Big Bang and stellar nucleosynthesis.\n\n\n== Helium-3 ==\n\n3He is stable. There is only a trace amount (0.000137%) of 3He on Earth, primarily present since the formation of the Earth, although some falls to Earth trapped in cosmic dust. Trace amounts are also produced by the beta decay of tritium. In stars, however, 3He is more abundant, a product of nuclear fusion. Extraplanetary material, such as lunar and asteroid regolith, has trace amounts of 3He from bombardment with solar wind.\nFor helium-3 to form a superfluid, it must be cooled to a temperature of 0.0025 K, or almost a thousand times lower than helium-4 (2.17 K). This difference is explained by quantum statistics, since helium-3 atoms are fermions while helium-4 atoms are bosons, which condense to a superfluid more easily.\n\n\n== Helium-4 ==\n\nThe most common isotope, 4He, is produced on Earth by alpha decay of heavier radioactive elements; the alpha particles that emerge are fully ionized 4He nuclei. 4He is an unusually stable nucleus because its nucleons are arranged into complete shells. It was also formed in enormous quantities during Big Bang nucleosynthesis.\nTerrestrial helium consists almost exclusively (99.99986%) of this isotope. Its boiling point of 4.2 K is the lowest of any known substance. When cooled further to 2.17 K, it transforms to a unique superfluid state of zero viscosity. It solidifies only at pressures above 25 atmospheres, where its melting point is 0.95 K.\n\n\n== Heavier helium isotopes ==\nAlthough all heavier helium isotopes decay with a half-life of less than one second, researchers have created new isotopes through particle accelerator collisions to create unusual atomic nuclei for elements such as helium, lithium and nitrogen. The unusual nuclear structures of such isotopes may offer insight into the isolated properties of neutrons.\nThe shortest-lived isotope is helium-5 with a half-life of 7.6\u00d710\u221222 seconds. Helium-6 decays by emitting a beta particle and has a half-life of 0.8 seconds. Helium-7 also emits a beta particle as well as a gamma ray. The most widely studied heavy helium isotope is helium-8. This isotope, as well as helium-6, are thought to consist of a normal helium-4 nucleus surrounded by a neutron \"halo\" (containing two neutrons in 6He and four neutrons in 8He). Halo nuclei have become an area of intense research. Isotopes up to helium-10, with two protons and eight neutrons, have been confirmed. Helium-7 and helium-8 are hyperfragments that are created in certain nuclear reactions. 10He, despite being a doubly magic isotope, has a very short half-life.\n\n\n== Table ==\n\n\n=== Notes ===\nThe isotopic composition refers to that in air.\nThe precision of the isotope abundances and atomic mass is limited through variations. The given ranges should be applicable to any normal terrestrial material.\nGeologically exceptional samples are known in which the isotopic composition lies outside the reported range. The uncertainty in the atomic mass may exceed the stated value for such specimens.\nValues marked # are not purely derived from experimental data, but at least partly from systematic trends. Spins with weak assignment arguments are enclosed in parentheses.\nUncertainties are given in concise form in parentheses after the corresponding last digits. Uncertainty values denote one standard deviation, except isotopic composition and standard atomic mass from IUPAC, which use expanded uncertainties.\nNuclide masses are given by IUPAP Commission on Symbols, Units, Nomenclature, Atomic Masses and Fundamental Constants (SUNAMCO)\nIsotope abundances are given by IUPAC Commission on Isotopic Abundances and Atomic Weights\n\n\n== Decay chains ==\nAlthough some helium isotopes, such as 6He and 8He, decay mostly to isotopes of lithium, the major tendency among known isotopes seems to be decay into lighter helium isotopes. Fission, seen only in even-numbered isotopes, is also unusually common.\nDecay chains for isotopes with multiple decay modes listed in order of probability.\n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              2\n            \n          \n          H\n          e\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                U\n                n\n                k\n                n\n                o\n                w\n                n\n              \n            \n          \n        \n         \n        \n          2\n          \n            \n\n            \n            \n              1\n            \n            \n              1\n            \n          \n          H\n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{2}^{2}He} \\ {\\xrightarrow {\\ \\mathrm {Unknown} }}\\ \\mathrm {2{}_{1}^{1}H} }\n  \n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              2\n            \n          \n          H\n          e\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                U\n                n\n                k\n                n\n                o\n                w\n                n\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              1\n            \n            \n              2\n            \n          \n          H\n        \n        +\n        \n          e\n          \n            \n\n            \n            \n\n            \n            \n              +\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{2}^{2}He} \\ {\\xrightarrow {\\ \\mathrm {Unknown} }}\\ \\mathrm {{}_{1}^{2}H} +\\mathrm {e{}_{}^{+}} }\n  \n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              5\n            \n          \n          H\n          e\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                700\n                y\n                s\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              4\n            \n          \n          H\n          e\n        \n        +\n        \n          \n            \n\n            \n            \n              0\n            \n            \n              1\n            \n          \n          n\n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{2}^{5}He} \\ {\\xrightarrow {\\ \\mathrm {700ys} }}\\ \\mathrm {{}_{2}^{4}He} +\\mathrm {{}_{0}^{1}n} }\n  \n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              6\n            \n          \n          H\n          e\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                806.7\n                m\n                s\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              3\n            \n            \n              6\n            \n          \n          L\n          i\n        \n        +\n        \n          e\n          \n            \n\n            \n            \n\n            \n            \n              \u2212\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{2}^{6}He} \\ {\\xrightarrow {\\ \\mathrm {806.7ms} }}\\ \\mathrm {{}_{3}^{6}Li} +\\mathrm {e{}_{}^{-}} }\n  \n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              6\n            \n          \n          H\n          e\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                806.7\n                m\n                s\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              4\n            \n          \n          H\n          e\n        \n        +\n        \n          \n            \n\n            \n            \n              1\n            \n            \n              2\n            \n          \n          H\n        \n        +\n        \n          e\n          \n            \n\n            \n            \n\n            \n            \n              \u2212\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{2}^{6}He} \\ {\\xrightarrow {\\ \\mathrm {806.7ms} }}\\ \\mathrm {{}_{2}^{4}He} +\\mathrm {{}_{1}^{2}H} +\\mathrm {e{}_{}^{-}} }\n  \n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              7\n            \n          \n          H\n          e\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                2.9\n                z\n                s\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              6\n            \n          \n          H\n          e\n        \n        +\n        \n          \n            \n\n            \n            \n              0\n            \n            \n              1\n            \n          \n          n\n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{2}^{7}He} \\ {\\xrightarrow {\\ \\mathrm {2.9zs} }}\\ \\mathrm {{}_{2}^{6}He} +\\mathrm {{}_{0}^{1}n} }\n  \n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              8\n            \n          \n          H\n          e\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                119\n                m\n                s\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              3\n            \n            \n              8\n            \n          \n          L\n          i\n        \n        +\n        \n          e\n          \n            \n\n            \n            \n\n            \n            \n              \u2212\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{2}^{8}He} \\ {\\xrightarrow {\\ \\mathrm {119ms} }}\\ \\mathrm {{}_{3}^{8}Li} +\\mathrm {e{}_{}^{-}} }\n  \n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              8\n            \n          \n          H\n          e\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                119\n                m\n                s\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              3\n            \n            \n              7\n            \n          \n          L\n          i\n        \n        +\n        \n          \n            \n\n            \n            \n              0\n            \n            \n              1\n            \n          \n          n\n        \n        +\n        \n          e\n          \n            \n\n            \n            \n\n            \n            \n              \u2212\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{2}^{8}He} \\ {\\xrightarrow {\\ \\mathrm {119ms} }}\\ \\mathrm {{}_{3}^{7}Li} +\\mathrm {{}_{0}^{1}n} +\\mathrm {e{}_{}^{-}} }\n  \n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              8\n            \n          \n          H\n          e\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                119\n                m\n                s\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              5\n            \n          \n          H\n          e\n        \n        +\n        \n          \n            \n\n            \n            \n              1\n            \n            \n              3\n            \n          \n          H\n        \n        +\n        \n          e\n          \n            \n\n            \n            \n\n            \n            \n              \u2212\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{2}^{8}He} \\ {\\xrightarrow {\\ \\mathrm {119ms} }}\\ \\mathrm {{}_{2}^{5}He} +\\mathrm {{}_{1}^{3}H} +\\mathrm {e{}_{}^{-}} }\n  \n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              9\n            \n          \n          H\n          e\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                7\n                z\n                s\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              8\n            \n          \n          H\n          e\n        \n        +\n        \n          \n            \n\n            \n            \n              0\n            \n            \n              1\n            \n          \n          n\n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{2}^{9}He} \\ {\\xrightarrow {\\ \\mathrm {7zs} }}\\ \\mathrm {{}_{2}^{8}He} +\\mathrm {{}_{0}^{1}n} }\n  \n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              10\n            \n          \n          H\n          e\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                2.7\n                z\n                s\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              8\n            \n          \n          H\n          e\n        \n        +\n        \n          2\n          \n            \n\n            \n            \n              0\n            \n            \n              1\n            \n          \n          n\n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{2}^{10}He} \\ {\\xrightarrow {\\ \\mathrm {2.7zs} }}\\ \\mathrm {{}_{2}^{8}He} +\\mathrm {2{}_{0}^{1}n} }\n  \n\n\n== References ==\n\nIsotope masses from:\nG. Audi; A. H. Wapstra; C. Thibault; J. Blachot; O. Bersillon (2003). \"The NUBASE evaluation of nuclear and decay properties\" (PDF). Nuclear Physics A. 729: 3\u2013128. Bibcode:2003NuPhA.729....3A. doi:10.1016/j.nuclphysa.2003.11.001. \n\nIsotopic compositions and standard atomic masses from:\nJ. R. de Laeter; J. K. B\u00f6hlke; P. De Bi\u00e8vre; H. Hidaka; H. S. Peiser; K. J. R. Rosman; P. D. P. Taylor (2003). \"Atomic weights of the elements. Review 2000 (IUPAC Technical Report)\". Pure and Applied Chemistry. 75 (6): 683\u2013800. doi:10.1351/pac200375060683. \nM. E. Wieser (2006). \"Atomic weights of the elements 2005 (IUPAC Technical Report)\". Pure and Applied Chemistry. 78 (11): 2051\u20132066. doi:10.1351/pac200678112051. Lay summary. \n\nHalf-life, spin, and isomer data selected from the following sources. See editing notes on this article's talk page.\nG. Audi; A. H. Wapstra; C. Thibault; J. Blachot; O. Bersillon (2003). \"The NUBASE evaluation of nuclear and decay properties\" (PDF). Nuclear Physics A. 729: 3\u2013128. Bibcode:2003NuPhA.729....3A. doi:10.1016/j.nuclphysa.2003.11.001. \nNational Nuclear Data Center. \"NuDat 2.1 database\". Brookhaven National Laboratory. Retrieved September 2005.  \nN. E. Holden (2004). \"Table of the Isotopes\". In D. R. Lide. CRC Handbook of Chemistry and Physics (85th ed.). CRC Press. Section 11. ISBN 978-0-8493-0485-9. \n\n\n== External links ==\nGeneral Tables \u2014 abstracts for helium and other exotic light nuclei", 
                "titleUrl": "https://en.wikipedia.org/wiki/Isotopes_of_helium", 
                "title": "Isotopes of helium"
            }, 
            {
                "snippet": "A helium planet is a planet with a helium-dominated atmosphere. This is in contrast to ordinary gas giants such as Jupiter and Saturn, whose atmospheres", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from August 2015\nHypothetical planet types\nWikipedia articles needing clarification from August 2015", 
                "pageContent": "A helium planet is a planet with a helium-dominated atmosphere. This is in contrast to ordinary gas giants such as Jupiter and Saturn, whose atmospheres consist primarily of hydrogen, with helium as a secondary component only. Helium planets might form in a variety of ways. The Gliese 436 b exoplanet is a candidate helium planet.\n\n\n== Theoretical ideas ==\nThere are several theoretical ideas for how a helium planet might form.\n\n\n=== Hydrogen evaporation from giant planets ===\n\nA helium planet might form via hydrogen evaporation from a gaseous planet orbiting close to a star. The star will drive off lighter gases more effectively through evaporation than heavier gasses, and over time deplete the hydrogen, leaving a greater proportion of helium behind.\nHelium planets are predicted to have roughly the same diameter as hydrogen\u2013helium planets of the same mass.\n\n\n==== Origins ====\nA scenario for forming helium planets from regular giant planets involves an ice giant, in an orbit so close to its host star that the hydrogen effectively boils out of the atmosphere, evaporating from and escaping the gravitational hold of the planet. The planet's atmosphere will experience a large energy input and because light gases are more readily evaporated than heavier gases, the proportion of helium will steadily increase in the remaining atmosphere. Such a process will take some time to stabilize and completely drive out all the hydrogen, perhaps on the order of 10 billion years, depending on the precise physical conditions and the nature of the planet and the star. Hot Neptunes are candidates for such a scenario.\nThe loss of hydrogen also leads to a depletion of methane in the atmosphere. On ice giants, methane naturally forms a cycle of melting, evaporation, breakdown and subsequent recombination and condensation. But as hydrogen gets depleted, a fraction of the carbon atoms will not be able to recombine with free hydrogen in the atmosphere and over time this will lead to an overall loss of methane. With time, the methane in the atmospheres of hot ice giants will also get depleted.\n\n\n=== Evaporating white dwarfs ===\nA helium planet may also form via mass loss from a low-mass white dwarf depleted of hydrogen. Such a white dwarf can form from a star where all the hydrogen has been processed to helium or other heavier elements by nuclear fusion. Mass-loss from the white dwarf will transform it to a giant planet over time, and this planet will automatically be depleted of hydrogen.\n\n\n==== Origins ====\nOne scenario involves an AM CVn type of symbiotic binary star composed of two helium-core white dwarfs surrounded by a circumbinary helium accretion disk formed during mass transfer from the less massive to the more massive white dwarf. After it loses most of its mass, the less massive white dwarf may approach planetary mass.\n\n\n== Helium planet characteristics ==\nHelium planets are expected to be distinguishable from regular hydrogen-dominated planets by strong evidence of carbon monoxide and carbon dioxide in the atmosphere. Due to hydrogen-depletion, the expected methane in the atmosphere cannot form because there is no hydrogen for the carbon to combine with, and hence carbon combines with oxygen instead, forming CO and CO2. Due to the atmospheric composition, helium planets are expected to be white or grey in appearance. Such a signature can be found in Gliese 436 b, which has a predominance of carbon monoxide.\n\n\n== See also ==\nCarbon planet \u2013 another form of mass-losing white dwarf becoming a planet\nExoplanet\nFormer stars\n\n\n== References ==\n\n\n== External links ==\n\"All Planets Possible\". Astrobiology Magazine. 2007-09-30.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Helium_planet", 
                "title": "Helium planet"
            }, 
            {
                "snippet": "A helium flash is a very brief thermal runaway nuclear fusion of large quantities of helium into carbon through the triple-alpha process in the core of", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from February 2016\nAstrophysics\nStellar evolution", 
                "pageContent": "A helium flash is a very brief thermal runaway nuclear fusion of large quantities of helium into carbon through the triple-alpha process in the core of low mass stars (between 0.8 solar masses (M\u2609) and 2.0 M\u2609) during their red giant phase (the Sun is predicted to experience a flash 1.2 billion years after it leaves the main sequence). A much rarer runaway helium fusion process can also occur on the surface of accreting white dwarf stars.\nLow mass stars do not produce enough gravitational pressure to initiate normal helium fusion. As the hydrogen in the core is exhausted, some of the helium left behind is instead compacted into degenerate matter, supported against gravitational collapse by quantum mechanical pressure rather than thermal pressure. This increases the density and temperature of the core until it reaches approximately 100 million kelvin, which is hot enough to cause helium fusion (or \"helium burning\") in the core.\nHowever, a fundamental quality of degenerate matter is that changes in temperature do not produce a change of volume of the matter until the thermal pressure becomes so incredibly high that it exceeds degeneracy pressure. In main sequence stars, thermal expansion regulates the core temperature, but in degenerate cores this does not occur. Helium fusion increases the temperature, which increases the fusion rate, which further increases the temperature in a runaway reaction. This produces a flash of very intense helium fusion lasts only a few minutes, but briefly emits energy at a rate comparable to the entire Milky Way galaxy.\nIn the case of normal low mass stars, the vast energy release is absorbed by the star's upper layers, and thus is mostly undetectable to observation, described solely by astrophysical models. The process ends when the core material is heated to the point where thermal pressure again becomes dominant, and the material then expands and cools. It is estimated that the electron-degenerate helium core weighs about 40% of the star mass and that 6% of the core is converted into carbon.\n\n\n== Red giants ==\n\nDuring the red giant phase of stellar evolution in stars with less than 2.0 M\u2609 the nuclear fusion of hydrogen ceases in the core as it is depleted, leaving a helium-rich core. While fusion of hydrogen continues in the star\u2019s shell causing a continuation of the accumulation of helium ash in the core, making the core denser, the temperature still is unable to reach the level required for helium fusion, as happens in more massive stars. Thus the thermal pressure from fusion is no longer sufficient to counter the gravitational collapse and create the hydrostatic equilibrium found in most stars. This causes the star to start contracting and increasing in temperature until it eventually becomes compressed enough for the helium core to become degenerate matter. This degeneracy pressure is finally sufficient to stop further collapse of the most central material but the rest of the core continues to contract and the temperature continues to rise until it reaches a point (7008100000000000000\u2660\u22481\u00d7108 K) at which the helium can ignite and start to fuse.\nThe explosive nature of the helium flash arises from its taking place in degenerate matter. Once the temperature reaches 100 million\u2013200 million kelvin and helium fusion begins using the triple-alpha process, the temperature rapidly increases, further raising the helium fusion rate and, because degenerate matter is a good conductor of heat, widening the reaction region.\nHowever, since degeneracy pressure (which is purely a function of density) is dominating thermal pressure (proportional to the product of density and temperature), the total pressure is only weakly dependent on temperature. Thus, the dramatic increase in temperature only causes a slight increase in pressure, so there is no stabilizing cooling expansion of the core.\nThis runaway reaction quickly climbs to about 100 billion times the star's normal energy production (for a few seconds) until the temperature increases to the point that thermal pressure again becomes dominant, eliminating the degeneracy. The core can then expand and cool down and a stable burning of helium will continue.\nA star with mass greater than about 2.25 M\u2609 starts to burn helium without its core becoming degenerate, and so does not exhibit this type of helium flash. In a very low-mass star (less than about 0.5 M\u2609), the core is never hot enough to ignite helium. The degenerate helium core will keep on contracting, and finally becomes a helium white dwarf.\nThe helium flash is not directly observable on the surface by electromagnetic radiation. The flash occurs in the core deep inside the star, and the net effect will be that all released energy is absorbed by the entire core, leaving the degenerate state to become nondegenerate. Earlier computations indicated that a nondisruptive mass loss would be possible in some cases, but later star modeling taking neutrino energy loss into account indicates no such mass loss.\n\n\n== Binary white dwarfs ==\nWhen hydrogen gas is accreted onto a white dwarf from a binary companion star, the hydrogen can fuse to form helium for a narrow range of accretion rates, but most systems develop a layer of hydrogen over the degenerate white dwarf interior. This hydrogen can build up to form a shell near the surface of the star. When the mass of hydrogen becomes sufficiently large, runaway fusion causes a nova. In a few binary systems where the hydrogen fuses on the surface, the mass of helium built up can burn in an unstable helium flash. In certain binary systems the companion star may have lost most of its hydrogen and donate helium-rich material to the compact star. Note that similar flashes occur on neutron stars.\n\n\n== Shell helium flash ==\nShell helium flashes are a somewhat analogous but much less violent, nonrunaway helium ignition event, taking place in the absence of degenerate matter. They occur periodically in asymptotic giant branch stars in a shell outside the core. This is late in the life of a star in its giant phase. The star has burnt most of the helium available in the core, which is now composed of carbon and oxygen. Helium fusion continues in a thin shell around this core, but then turns off as helium becomes depleted. This allows hydrogen fusion to start in a layer above the helium layer. After enough additional helium accumulates, helium fusion is reignited, leading to a thermal pulse which eventually causes the star to expand and brighten temporarily (the pulse in luminosity is delayed because it takes a number of years for the energy from restarted helium fusion to reach the surface). Such pulses may last a few hundred years, and are thought to occur periodically every 10,000 to 100,000 years. After the flash, helium fusion continues at an exponentially decaying rate for about 40% of the cycle as the helium shell is consumed. Thermal pulses may cause a star to shed circumstellar shells of gas and dust.\n\n\n== See also ==\nCarbon detonation\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Helium_flash", 
                "title": "Helium flash"
            }, 
            {
                "snippet": "known as the helium hydride ion or helium-hydride molecular ion, is a positively charged ion formed by the reaction of a proton with a helium atom in the", 
                "pageCategories": "Acids\nArticles containing unverified chemical infoboxes\nArticles without KEGG source\nArticles without UNII source\nCS1 maint: Multiple names: authors list\nCations\nChemboxes which contain changes to watched fields\nChemical articles using a fixed chemical formula\nChemical articles without CAS Registry Number\nHelium", 
                "pageContent": "The hydrohelium(1+) cation, HeH+, also known as the helium hydride ion or helium-hydride molecular ion, is a positively charged ion formed by the reaction of a proton with a helium atom in the gas phase, first produced in the laboratory in 1925. It is isoelectronic with molecular hydrogen. It is the strongest known acid, with a proton affinity of 177.8 kJ/mol. It has been suggested that HeH+ should occur naturally in the interstellar medium, but it has not yet been detected. It is the simplest heteronuclear ion, and is comparable with the hydrogen molecular ion, H+\n2. Unlike H+\n2, however, it has a permanent dipole moment, which makes its spectroscopic characterization easier.\n\n\n== Properties ==\nHHe+ cannot be prepared in a condensed phase, as it would protonate any anion, molecule or atom with which it were associated. However it is possible to estimate a hypothetical aqueous acidity using Hess's law:\nA free energy change of dissociation of \u2212360 kJ/mol is equivalent to a pKa of \u221263.\nThe length of the covalent bond in HeH+ is 0.772 \u00c5.\nOther helium hydride ions are known or have been studied theoretically. HeH+\n2, which has been observed using microwave spectroscopy, has a calculated binding energy of 25.1 kJ/mol, while HeH+\n3 has a calculated binding energy of 0.42 kJ/mol.\nThe dihelium hydride cation is formed by the reaction of dihelium cation with molecular hydrogen:\nHe2+ + H2 \u2192 He2H+ + H\u2022.\nHe2H+ is a linear ion with hydrogen in the centre.\n\n\n== Reactions ==\nThe helium hydride cation reacts with most substances. It has been shown to add a proton to O2, NH3, SO2, H2O, and CO2, giving O2H+, NH+\n4, HSO+\n2, H3O+, and HCO+\n2. Other molecules such as nitric oxide, nitrogen dioxide, nitrous oxide, hydrogen sulfide, methane, acetylene, ethylene, ethane, methanol and acetonitrile react but break up due to the large amount of energy produced. One technique used to study reactions between organic substances and HeH+ is to make a tritium derivative of the organic compound. Decay of tritium to 3He+ followed by its extraction of a hydrogen atom yields 3HeH+ which is then surrounded by the organic material and will in turn react.\nExtra helium atoms can attach to HeH+ to form larger clusters such as He2H+, He3H+, He4H+, He5H+ and He6H+ which is particularly stable.\n\n\n== Natural occurrence ==\nThe helium hydride ion is formed during the decay of tritium in the molecule HT or tritium molecule T2. Although excited by the recoil from the beta decay the molecule remains bound together.\nHeH+ is thought to exist in the interstellar medium, although it has not yet been unambiguously detected. It is believed to be the first compound to have formed in the universe, and is of fundamental importance in understanding the chemistry of the early universe. This is because hydrogen and helium were almost the only types of atoms formed in Big Bang nucleosynthesis. Stars formed from the primordial material should contain HeH+, which could influence their formation and subsequent evolution. In particular, its strong dipole moment makes it relevant to the opacity of zero-metallicity stars. HeH+ is also thought to be an important constituent of the atmospheres of helium-rich white dwarfs, where it increases the opacity of the gas and causes the star to cool more slowly.\nSeveral locations have been suggested as possible places HeH+ might be detected. These include cool helium stars, H II regions, and dense planetary nebulae (in particular NGC 7027). Detecting HeH+ spectroscopically is complicated by the fact that one of its most prominent spectral lines, at 149.14 \u03bcm, coincides with a doublet of spectral lines belonging to the methylidyne radical \u2af6CH.\nHeH+ could be formed in the cooling gas behind dissociative shocks in dense interstellar clouds, such as the shocks caused by stellar winds, supernovae and outflowing material from young stars. If the speed of the shock is greater than about 90 km/s, quantities large enough to detect might be formed. If detected, the emissions from HeH+ would then be useful tracers of the shock.\n\n\n== Neutral molecule ==\nUnlike the helium hydride ion, the neutral helium hydride molecule is not stable in the ground state. However, it does exist in an excited state as an excimer, and its spectrum was first observed in the mid 1980s.\n\n\n== References and notes ==\nUnless otherwise stated, numerical data are taken from Weast, R. C. (Ed.) (1981). CRC Handbook of Chemistry and Physics (62nd Edn.). Boca Raton, FL: CRC Press. ISBN 0-8493-0462-8.\n\nJ. Chem. Phys. 41, 1646 (1964)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Helium_hydride_ion", 
                "title": "Helium hydride ion"
            }, 
            {
                "snippet": "Helium-4 (4  2He  or 4 He ) is a non-radioactive isotope of the element helium. It is by far the most abundant of the two naturally occurring isotopes", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from April 2015\nIsotopes of helium", 
                "pageContent": "Helium-4 (4\n2He or 4He) is a non-radioactive isotope of the element helium. It is by far the most abundant of the two naturally occurring isotopes of helium, making up about 99.99986% of the helium on Earth. Its nucleus is identical to an alpha particle, and consists of two protons and two neutrons.\nAlpha decay of heavy elements in the Earth's crust is the source of most naturally occurring helium-4 on Earth. While it is also produced by nuclear fusion in stars, most helium-4 in the Sun and in the universe is thought to have been produced by the Big Bang, and is referred to as \"primordial helium\". However, primordial helium-4 is largely absent from the Earth, having escaped during the high-temperature phase of Earth's formation. Radioactive decay from other elements is the source of most of the helium-4 found on Earth, produced after the planet cooled and solidified.\nHelium-4 makes up about one quarter of the ordinary matter in the universe by mass, with almost all of the rest being hydrogen.\nWhen liquid helium-4 is cooled to below 2.17 kelvins (\u2013271.17 \u00b0C) it becomes a superfluid, with properties that are very unlike those of an ordinary liquid. For example, if superfluid helium-4 is kept in an open vessel, a thin film will climb up the sides of the vessel and overflow. In this state and situation, it is called a \"Rollin film\". This strange behavior is a result of the Clausius\u2013Clapeyron relation, and cannot be explained by the current model of classical mechanics, nor by nuclear or electrical models \u2013 it can only be understood as a quantum mechanical phenomenon. The total spin of the helium-4 nucleus is an integer (zero), and therefore it is a boson (as are neutral atoms of helium-4). The superfluid behavior is now understood to be a manifestation of Bose\u2013Einstein condensation, which occurs only with collections of bosons.\nIt is theorized that, at 0.2 K and 50 Atm, solid helium-4 may be a superglass (an amorphous solid exhibiting superfluidity).\nHelium-4 also exists on the Moon and\u2014as on Earth\u2014it is the most abundant helium isotope.\n\n\n== The helium-4 atom ==\n\nThe helium atom is the second simplest atom (hydrogen is the simplest), but the extra electron introduces a third \"body\", so the solution to its wave equation becomes a \"three-body problem\", which has no analytic solution. However, numerical approximations of the equations of quantum mechanics have given a good estimate of the key atomic properties of helium-4, such as its size and ionization energy.\n\n\n== Stability of the He-4 nucleus and electron shell ==\nThe nucleus of the helium-4 atom is identical with an alpha particle. High-energy electron-scattering experiments show its charge to decrease exponentially from a maximum at a central point, exactly as does the charge density of helium's own electron cloud. This symmetry reflects similar underlying physics: the pair of neutrons and the pair of protons in helium's nucleus obey the same quantum mechanical rules as do helium's pair of electrons (although the nuclear particles are subject to a different nuclear binding potential), so that all these fermions fully occupy 1s1s orbitals in pairs, none of them possessing orbital angular momentum, and each canceling the other's intrinsic spin. Adding another of any of these particles would require angular momentum, and would release substantially less energy (in fact, no nucleus with five nucleons is stable). This arrangement is thus energetically extremely stable for all these particles, and this stability accounts for many crucial facts regarding helium in nature.\nFor example, the stability and low energy of the electron cloud of helium causes helium's chemical inertness (the most extreme of all the elements), and also the lack of interaction of helium atoms with each other (producing the lowest melting and boiling points of all the elements).\nIn a similar way, the particular energetic stability of the helium-4 nucleus, produced by similar effects, accounts for the ease of helium-4 production in atomic reactions involving both heavy-particle emission and fusion. Some stable helium-3 is produced in fusion reactions from hydrogen, but it is a very small fraction, compared with the highly energetically favorable production of helium-4. The stability of helium-4 is the reason that hydrogen is converted to helium-4, and not deuterium (hydrogen-2) or helium-3 or other heavier elements during fusion reactions in the Sun. It is also partly responsible for the fact that the alpha particle is by far the most common type of baryonic particle to be ejected from an atomic nucleus - in other words, alpha decay is far more common than is cluster decay.\n\nThe unusual stability of the helium-4 nucleus is also important cosmologically. It explains the fact that, in the first few minutes after the Big Bang, as the \"soup\" of free protons and neutrons which had initially been created in about a 6:1 ratio cooled to the point where nuclear binding was possible, almost all atomic nuclei to form were helium-4 nuclei. So tight was the binding of the nucleons in helium-4, its production consumed nearly all of the free neutrons in just a few minutes, before they could beta-decay, and left very few to form heavier atoms (i.e. lithium, beryllium, and boron). The energy of helium-4 nuclear binding per nucleon is stronger than in any of these elements (see nucleogenesis and binding energy), and thus no energetic \"drive\" was available to make elements 3, 4, and 5 once helium had been formed. It was barely energetically favorable for helium to fuse into the next element with a lower energy per nucleon (carbon). However, due to the lack of intermediate elements, this process requires three helium nuclei striking each other nearly simultaneously (see triple alpha process). There was thus no time for significant carbon to be formed in the few minutes after the Big Bang, before the early expanding universe cooled to the temperature and pressure where helium fusion to carbon was no longer possible. This left the early universe with a very similar ratio of hydrogen/helium as is observed today (3 parts hydrogen to 1 part helium-4 by mass), with nearly all the neutrons in the universe trapped in helium-4.\nAll heavier elements - including those necessary for rocky planets like the Earth, and for carbon-based or other life - have thus had to be produced, since the Big Bang, in stars which were hot enough to fuse not just hydrogen (for this produces only more helium), but to fuse helium itself. All elements other than hydrogen and helium today account for only 2% of the mass of atomic matter in the universe. Helium-4, by contrast, makes up about 23% of the universe's ordinary matter \u2014 nearly all the ordinary matter that isn't hydrogen.\n\n\n== References ==\n\nTur, Clarisse (2009), \"DEPENDENCE OF s-PROCESS NUCLEOSYNTHESIS IN MASSIVE STARS ON TRIPLE-ALPHA AND 12C(\u03b1, \u03b3)16O REACTION RATE UNCERTAINTIES\", The Astrophysical Journal, 702, doi:10.1088/0004-637x/702/2/1068", 
                "titleUrl": "https://en.wikipedia.org/wiki/Helium-4", 
                "title": "Helium-4"
            }, 
            {
                "snippet": "Late B.P. Helium is the solo recording project and, at times, stage name of Elephant Six musician Bryan Poole, who also goes by Bryan Helium. Poole has", 
                "pageCategories": "All articles lacking sources\nAll stub articles\nAmerican indie rock groups\nAmerican rock musician stubs\nArticles lacking sources from November 2012\nOf Montreal\nThe Elephant 6 Recording Company artists\nWikipedia articles with MusicBrainz identifiers", 
                "pageContent": "The Late B.P. Helium is the solo recording project and, at times, stage name of Elephant Six musician Bryan Poole, who also goes by Bryan Helium.\nPoole has spent much of his musical career playing guitar on and off with of Montreal and Elf Power since the mid-1990s. He was also a member of the XTC cover band Helium Kids (a.k.a. The Mummers) from 1994 to 1995. In 2001, he released a few singles and submitted tracks to various compilations before offering his first EP, Kumquat Mae, on his 2002 national tour. It was later re-released on Orange Twin Records in 2003, and allowed him to work on his solo debut album, Amok, which was also released on Orange Twin in 2004. Bryan Poole is also recognized as one of the best scrabble players in all of the music industry.\n\n\n== Discography ==\n\n\n=== Albums ===\nAmok (CD) - Orange Twin - 2004\n\n\n=== Singles and EPs ===\nHappy Happy Birthday to Me Singles Club: August (7\") - HHBTM - 2001\nSplit single with Of Montreal (7\") - Jonathan Whiskey - 2001\nKumquat Mae (CD) - Hype City Records/Orange Twin - 2002\n\n\n== External links ==\nOfficial Late B.P. Helium Site\nThe Late BP Helium at AllMusic", 
                "titleUrl": "https://en.wikipedia.org/wiki/The_Late_B.P._Helium", 
                "title": "The Late B.P. Helium"
            }, 
            {
                "snippet": "The Gas Turbine Modular Helium Reactor (GT-MHR) is a nuclear fission power reactor design under development by a group of Russian enterprises (OKBM Afrikantov", 
                "pageCategories": "All stub articles\nArticles with Wayback Machine links\nGas turbines\nNuclear power reactor types\nNuclear power stubs", 
                "pageContent": "The Gas Turbine Modular Helium Reactor (GT-MHR) is a nuclear fission power reactor design under development by a group of Russian enterprises (OKBM Afrikantov, Kurchatov Institute, VNIINM and others), an American group headed by General Atomics, French Framatome and Japanese Fuji Electric. It is a helium cooled, graphite moderated reactor and uses TRISO fuel compacts in a prismatic core design.\n\n\n== Construction ==\nThe core consists of a graphite cylinder with a radius of 4 m and a height of 10 m which includes 1 m axial reflectors at top and bottom. The cylinder allocates three or four concentric rings, each of 36 hexagonal blocks with an interstitial gap of 0.2 cm. Each hexagonal block contains 108 helium coolant channels and 216 fuel pins. Each fuel pin contains a random lattice of TRISO particles dispersed into a graphite matrix. The reactor exhibits a thermal spectrum with a peak located at about 0.2 eV. The QUADRISO fuel  concept conceived at Argonne National Laboratory has been used to better manage the excess of reactivity. The reactor and containment structure are located below grade and in contact with the ground, which serves as a passive safety measure to conduct heat away from the reactor in the event of a coolant failure.\n\n\n== Advantages ==\nThe Gas Turbine Modular Helium Reactor utilizes the Brayton cycle turbine arrangement, which gives it an efficiency of up to 48% - higher than any other reactor, as of 1995. Commercial light water reactors (LWRs) generally use the Rankine cycle, which is what coal-fired power plants use. Commercial LWRs average 32% efficiency, again as of 1995.\n\n\n== Energy Multiplier Module (EM2) ==\nGeneral Atomics presented a new version of the GT-MHR, the Energy Multiplier Module (EM2), in 2010. The EM2 uses fast neutrons and is a gas-cooled fast reactor, enabling it to reduce nuclear waste considerably by transmutation.\n\n\n== See also ==\nEnergy multiplier module\nGas-cooled fast reactor\nPebble bed reactor\nVery high temperature reactor\n\n\n== References ==\n\n[1]\n\n\n== External links ==\nGeneral Atomics GT-MHR Page at the Wayback Machine (archived December 11, 2013)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Gas_turbine_modular_helium_reactor", 
                "title": "Gas turbine modular helium reactor"
            }
        ], 
        "phraseCharStart": "176"
    }, 
    {
        "phraseCharEnd": "219", 
        "phraseIndex": "T5", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "cooling system", 
        "wikiSearchResults": [
            {
                "snippet": "technology Cooling systems for vehicles, such as internal combustion engine cooling Cooling systems for nuclear powerplants, using nuclear reactor coolant", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Cooling_system", 
                "title": "Cooling system"
            }, 
            {
                "snippet": "coolant Liquid Cooling and Ventilation Garment, a garment worn by astronauts Liquid metal cooled reactor Radiator (engine cooling) Liquid cooling for computers", 
                "pageCategories": "All stub articles\nBroad-concept articles\nCooling technology\nTechnology stubs", 
                "pageContent": "Liquid cooling refers to cooling by means of the convection or circulation of a liquid. Examples include:\nWater cooling\nCooling by convection or circulation of coolant\nLiquid Cooling and Ventilation Garment, a garment worn by astronauts\nLiquid metal cooled reactor\nRadiator (engine cooling)\nLiquid cooling for computers", 
                "titleUrl": "https://en.wikipedia.org/wiki/Liquid_cooling", 
                "title": "Liquid cooling"
            }, 
            {
                "snippet": "was a Hungarian professor credited with inventing the Heller\u2013Forg\u00f3 cooling system for power stations.  Born in Nagyv\u00e1rad, Heller took a degree in mechanical", 
                "pageCategories": "1907 births\n1980 deaths\nAll articles needing additional references\nAll stub articles\nArticles needing additional references from November 2009\nCommons category with local link same as on Wikidata\nEuropean engineer stubs\nHungarian engineers\nHungarian inventors\nHungarian scientist stubs", 
                "pageContent": "Heller L\u00e1szl\u00f3 (1907\u20131980) was a Hungarian professor credited with inventing the Heller\u2013Forg\u00f3 cooling system for power stations.\n\n\n== Biography ==\nBorn in Nagyv\u00e1rad, Heller took a degree in mechanical engineering in 1931 at the Eidgen\u00f6ssische Technische Hochschule in Z\u00fcrich. In the 1940s the first high-pressure industrial power station was built according to his plans. It was around this time that he invented the Heller - Forg\u00f3 system. In 1951 he was awarded the Kossuth Prize. He organized the Department of Energetics at the Budapest University of Technology and Economics, where he worked as a professor. He was a large contributor to the domain of statics, and helped establish the concept of entropy for engineering practices. In 1962 he became a full member of the Hungarian Academy of Sciences. Heller's nephew graduated from USC, obtaining a degree in Mechanical engineering.\n\n\n== The Heller\u2013Forg\u00f3 System, or Indirect Dry Cooling System ==\nThe system is named after Heller and Forg\u00f3 L\u00e1szl\u00f3, the active collaborator in the industrial implementation of the system.\nThe Heller\u2013Forg\u00f3 system solved an important problem at power stations by utilizing cooling water more efficiently. The main point of their invention was to condense the vacuum steam using an injection of cool water. The still-warm water enters into the fine-gilled heat exchanger, cools down and becomes usable again for when the cycle is repeated. The system is known and used all over the world.\n\n\n== External links ==\nBiography of Heller\nNews on an implementation of the Heller - Forg\u00f3 system", 
                "titleUrl": "https://en.wikipedia.org/wiki/L\u00e1szl\u00f3_Heller", 
                "title": "L\u00e1szl\u00f3 Heller"
            }, 
            {
                "snippet": "includes cooling the systems that remove decay heat from both the primary system and the spent fuel rod cooling ponds, the ESWS is a safety-critical system. Since", 
                "pageCategories": "All articles needing additional references\nAll articles with unsourced statements\nArticles needing additional references from January 2011\nArticles with unsourced statements from February 2013\nNuclear power plant components\nNuclear safety", 
                "pageContent": "This article covers the technical aspects of active nuclear safety systems in the United States. For a general approach to nuclear safety, see nuclear safety.\nThe three primary objectives of nuclear reactor safety systems as defined by the U.S. Nuclear Regulatory Commission are to shut down the reactor, maintain it in a shutdown condition and prevent the release of radioactive material.\n\n\n== Reactor protection system (RPS) ==\nA reactor protection system is designed to immediately terminate the nuclear reaction. By breaking the chain reaction, the source of heat is eliminated. Other systems can then be used to remove decay heat from the core. All nuclear plants have some form of reactor protection system.\n\n\n=== Control rods ===\nControl rods are a series of rods that can be quickly inserted into the reactor core to absorb neutrons and rapidly terminate the nuclear reaction. They are typically composed of actinides, lanthanides, transition metals, and boron, in various alloys with structural backing such as steel. In addition to being neutron absorbent, the alloys used also have to have at least a low coefficient of thermal expansion so that they do not jam under high temperatures, and they have to be self-lubricating metal on metal, because at the temperatures experienced by nuclear reactor cores oil lubrication would foul too quickly.\n\n\n=== Safety injection / standby liquid control ===\nBoiling water reactors are able to scram the reactor completely with the help of their control rods. In the case of a Loss of coolant accident (LOCA), the water-loss of the primary cooling system can be compensated with normal water pumped into the cooling circuit. On the other hand, the standby liquid control system (SLC) consists of a solution containing boric acid, which acts as a neutron poison and rapidly floods the core in case of problems with the stopping of the chain reaction.\nPressurized water reactors on the other hand have to use boron solution in addition to the control rods to shut down the reactor. In the case of problems with the control rods, they are able to increase the normal concentration of boron in the coolant water rapidly, with the help of emergency boric acid tanks (SLC). In the case of LOCA, PWRs flood the core with water that auxiliarily contains further boron.\n\n\n== Essential service water system ==\n\nThe essential service water system (ESWS) circulates the water that cools the plant\u2019s heat exchangers and other components before dissipating the heat into the environment. Because this includes cooling the systems that remove decay heat from both the primary system and the spent fuel rod cooling ponds, the ESWS is a safety-critical system. Since the water is frequently drawn from an adjacent river, the sea, or other large body of water, the system can be fouled by seaweed, marine organisms, oil pollution, ice and debris. In locations without a large body of water in which to dissipate the heat, water is recirculated via a cooling tower.\nThe failure of half of the ESWS pumps was one of the factors that endangered safety in the 1999 Blayais Nuclear Power Plant flood, while a total loss occurred during the Fukushima I and Fukushima II nuclear accidents in 2011.\n\n\n== Emergency core cooling system ==\n\nEmergency core cooling systems (ECCS) are designed to safely shut down a nuclear reactor during accident conditions. The ECCS allows the plant to respond to a variety of accident conditions (e.g. LOCAs) and additionally introduce redundancy so that the plant can be shut down even with one or more subsystem failures. In most plants, ECCS is composed of the following systems:\n\n\n=== High pressure coolant injection system ===\nHPCI consists of a pump or pumps that have sufficient pressure to inject coolant into the reactor vessel while it is pressurized. It is designed to monitor the level of coolant in the reactor vessel and automatically inject coolant when the level drops below a threshold. This system is normally the first line of defense for a reactor since it can be used while the reactor vessel is still highly pressurized.\n\n\n=== Automatic Depressurization system ===\n\nADS consists in the case of Boiling water reactors of a series of valves which open to vent steam several feet under the surface of a large pool of liquid water (known as the wetwell or torus) in pressure suppression type containments, or directly into the primary containment structure in other types of containments, such as large-dry or ice-condenser containments. The actuation of these valves depressurizes the reactor vessel and allows lower pressure coolant injection systems to function, which have very large capacities in comparison to high pressure systems. Some depressurization systems are automatic in function but can be inhibited, some are manual and operators may activate if necessary. In Pressurized water reactors with large dry or ice condenser containments, the valves of the system are called Pilot operated release valves.\n\n\n=== Low pressure coolant injection system ===\nLPCI consists of a pump or pumps that inject coolant into the reactor vessel once it has been depressurized. In some nuclear power plants, LPCI is a mode of operation of a residual heat removal system (RHR or RHS). LPCI is generally not a stand-alone system.\n\n\n=== Corespray system (only in BWRs) ===\nThis system uses spargers (special spray nozzles) within the reactor pressure vessel to spray water directly onto the fuel rods, suppressing the generation of steam. Reactor designs can include corespray in high-pressure and low-pressure modes.\n\n\n=== Containment spray system ===\nThis system consists of a series of pumps and spargers that spray coolant into the primary containment structure. It is designed to condense the steam into liquid within the primary containment structure to prevent overpressure, which could lead to leakage, followed by involuntary depressurization.\n\n\n=== Isolation cooling system ===\nThis system is often driven by a steam turbine to provide enough water to safely cool the reactor if the reactor building is isolated from the control and turbine buildings. Steam turbine driven cooling pumps with pneumatic controls can run at mechanically controlled adjustable speeds, without battery power, emergency generator, or off-site electrical power. The Isolation cooling system is a defensive system against a condition known as station blackout. It should be noted this system in not part of the ECCS and does not have a low coolant accident function. For Pressurized water reactors, this system acts in the secondary cooling circuit and is called Turbine driven auxiliary feedwater system.\n\n\n== Emergency electrical systems ==\nUnder normal conditions, nuclear power plants receive power from off-site. However, during an accident a plant may lose access to this power supply and thus may be required to generate its own power to supply its emergency systems. These electrical systems usually consist of diesel generators and batteries.\n\n\n=== Diesel generators ===\nDiesel generators are employed to power the site during emergency situations. They are usually sized such that a single one can provide all the required power for a facility to shut down during an emergency. Facilities have multiple generators for redundancy. Additionally, systems that are required to shut down the reactor have separate electrical sources (often separate generators) so that they do not affect shutdown capability.\n\n\n=== Motor generator flywheels ===\nLoss of electrical power can occur suddenly and can damage or undermine equipment. To prevent damage, motor-generators can be tied to flywheels that can provide uninterrupted electrical power to equipment for a brief period. Often they are used to provide electrical power until the plant electrical supply can be switched to the batteries and/or diesel generators.\n\n\n=== Batteries ===\nBatteries often form the final redundant backup electrical system and are also capable of providing sufficient electrical power to shut down a plant.\n\n\n== Containment systems ==\nContainment systems are designed to prevent the release of radioactive material into the environment.\n\n\n=== Fuel cladding ===\nThe fuel cladding is the first layer of protection around the nuclear fuel and is designed to protect the fuel from corrosion that would spread fuel material throughout the reactor coolant circuit. In most reactors it takes the form of a sealed metallic or ceramic layer. It also serves to trap fission products, especially those that are gaseous at the reactor's operating temperature, such as krypton, xenon and iodine. Cladding does not constitute shielding, and must be developed such that it absorbs as little radiation as possible. For this reason, materials such as magnesium and zirconium are used for their low neutron capture cross sections.\n\n\n=== Reactor vessel ===\nThe reactor vessel is the first layer of shielding around the nuclear fuel and usually is designed to trap most of the radiation released during a nuclear reaction. The reactor vessel is also designed to withstand high pressures.\n\n\n=== Primary containment ===\nThe primary containment system usually consists of a large metal and concrete structure (often cylindrical or bulb shaped) that contains the reactor vessel. In most reactors it also contains the radioactively contaminated systems. The primary containment system is designed to withstand strong internal pressures resulting from a leak or intentional depressurization of the reactor vessel.\n\n\n=== Secondary containment ===\nSome plants have a secondary containment system that encompasses the primary system. This is very common in BWRs because most of the steam systems, including the turbine, contain radioactive materials.\n\n\n=== Core catching ===\nIn case of a full melt-down, the fuel would most likely end up on the concrete floor of the primary containment building. Concrete can withstand a great deal of heat, so the thick flat concrete floor in the primary containment will often be sufficient protection against the so-called China Syndrome. The Chernobyl plant didn't have a containment building, but the core was eventually stopped by the concrete foundation. Due to concerns that the core would melt its way through the concrete, a \"core catching device\" was invented, and a mine was quickly dug under the plant with the intention to install such a device. The device contains a quantity of metal designed to melt, diluting the corium and increasing its heat conductivity; the diluted metallic mass could then be cooled by water circulating in the floor. Today, all new Russian-designed reactors are equipped with core-catchers in the bottom of the containment building.\nThe AREVA EPR, SNR-300, SWR1000, ESBWR, and Atmea I reactors have core catchers.\n\n\n== Standby gas treatment ==\nA standby gas treatment (SBGT) system is part of the secondary containment system. The SBGT system filters and pumps air from secondary containment to the environment and maintains a negative pressure within the secondary containment to limit the release of radioactive material.\nEach SBGT train generally consists of a mist eliminator/roughing filter; an electric heater; a prefilter; two absolute (HEPA) filters; an activated charcoal filter; an exhaust fan; and associated valves, ductwork, dampers, instrumentation and controls. The signals that trip the SBGT system are plant-specific; however, automatic trips are generally associated with the electric heaters and a high temperature condition in the charcoal filters.\n\n\n== Ventilation and radiation protection ==\nIn case of a radioactive release, most plants have a system designed to remove radioactivity from the air to reduce the effects of the radioactivity release on the employees and public. This system usually consists of containment ventilation that removes radioactivity and steam from primary containment. Control room ventilation ensures that plant operators are protected. This system often consists of activated charcoal filters that remove radioactive isotopes from the air.\n\n\n== See also ==\nBoiling water reactor safety systems\nNuclear accidents in the United States\nNuclear safety in the U.S.\nPassive nuclear safety\nWorld Association of Nuclear Operators\n\n\n== References ==\n\n\n=== Standards ===\nAmerican National Standard, ANSI N18.2, \u201cNuclear Safety Criteria for the Design of Stationary Pressurized Water Reactor Plants,\u201d August 1973.\nIEEE 279, \u201cCriteria for Protection Systems for Nuclear Power Generating Stations.\u201d", 
                "titleUrl": "https://en.wikipedia.org/wiki/Nuclear_reactor_safety_system", 
                "title": "Nuclear reactor safety system"
            }, 
            {
                "snippet": "Deep water source cooling (DWSC) or deep water air cooling is a form of air cooling for process and comfort space cooling which uses a large body of naturally", 
                "pageCategories": "CS1 errors: dates\nHeating, ventilating, and air conditioning\nMarine energy\nRenewable energy", 
                "pageContent": "Deep water source cooling (DWSC) or deep water air cooling is a form of air cooling for process and comfort space cooling which uses a large body of naturally cold water as a heat sink. It uses water at 4 to 10 degrees Celsius drawn from deep areas within lakes, oceans, aquifers or rivers, which is pumped through the one side of a heat exchanger. On the other side of the heat exchanger, cooled water is produced.\n\n\n== Basic concept ==\nWater is most dense at 3.98 \u00b0C (39.16 \u00b0F) at standard atmospheric pressure. Thus as water cools below 3.98 \u00b0C it decreases in density and will rise. As the temperature climbs above 3.98 \u00b0C, water density also decreases and causes the water to rise, which is why lakes are warmer on the surface during the summer. The combination of these two effects means that the bottom of most deep bodies of water located well away from the equatorial regions is at a constant 3.98 \u00b0C.\nAir conditioners are heat pumps. During the summer, when outside air temperatures are higher than the temperature inside a building, air conditioners use electricity to transfer heat from the cooler interior of the building to the warmer exterior ambient. This process uses electrical energy.\nUnlike residential air conditioners, most modern commercial air conditioning systems do not transfer heat directly into the exterior air. The thermodynamic efficiency of the overall system can be improved by utilizing evaporative cooling, where the temperature of the cooling water is lowered close to the wet-bulb temperature by evaporation in a cooling tower. This cooled water then acts as the heat sink for the heat pump.\nDeep lake water cooling uses cold water pumped from the bottom of a lake as a heat sink for climate control systems. Because heat pump efficiency improves as the heat sink gets colder, deep lake water cooling can reduce the electrical demands of large cooling systems where it is available. It is similar in concept to modern geothermal sinks, but generally simpler to construct given a suitable water source.\nDeep lake water cooling allows higher thermodynamic efficiency by using cold deep lake water, which is colder than the ambient wet bulb temperature. The higher efficiency results in less electricity used. For many buildings, the lake water is sufficiently cold that the refrigeration portion of the air conditioning systems can be shut down during some environmental conditions and the building interior heat can be transferred directly to the lake water heat sink. This is referred to as \"free cooling\", but is not actually free, since pumps and fans run to circulate the lake water and building air.\nOne added attraction of deep lake water cooling is that it saves energy during peak load times, such as summer afternoons, when a sizable amount of the total electrical grid load is air conditioning.\n\n\n== Advantages ==\nDeep water source cooling is very energy efficient, requiring only 1/10 of the average energy required by conventional cooler systems. Consequently, its running costs can also be expected to be much lower.\nThe energy source is very local and fully renewable, provided that the water and heat rejected into the environment (often the same lake or a nearby river) does not disturb the natural cycles. It does not use any ozone depleting refrigerant.\nDepending on the needs and on the water temperature, couple heating and cooling can be considered. For example, heat could first be extracted from the water (making it colder); and, secondly, that same water could cycle to a refrigerating unit to be used for even more effective cold production.\n\n\n== Disadvantages ==\nDeep water source cooling requires a large and deep water quantity in the surroundings. To obtain water in the 3 to 6 \u00b0C (37 to 43 \u00b0F) range, a depth of 50 m (160 ft) to 70 m (230 ft) is generally required, depending on the local conditions.\nThe set-up of a system is expensive and labour-intensive. The system also requires a great amount of source material for its construction and placement.\n\n\n== First major system in the United States ==\nCornell University's Lake Source Cooling System uses Cayuga Lake as a heat sink to operate the central chilled water system for its campus and to also provide cooling to the Ithaca City School District. The system has operated since the summer of 2000 and was built at a cost of $55\u201360 million. It cools a 14,500 ton (51 megawatt) load.\n\n\n== First system in Canada ==\nSince August 2004, a deep lake water cooling system has been operated by the Enwave Energy Corporation in Toronto, Ontario. It draws water from Lake Ontario through tubes extending 5 kilometres (3.1 mi) into the lake, reaching to a depth of 83 metres (272 ft). The deep lake water cooling system is part of an integrated district cooling system that covers Toronto's financial district, and has a cooling power of 59,000 tons (207 MW). The system currently has enough capacity to cool 3,200,000 square metres (34,000,000 sq ft) of office space.\nThe cold water drawn from Lake Ontario's deep layer in the Enwave system is not returned directly to the lake, once it has been run through the heat exchange system. The Enwave system only uses water that is destined to meet the city's domestic water needs. Therefore, the Enwave system does not pollute the lake with a plume of waste heat.\n\n\n== Seawater Air Conditioning ==\nAlso known as Ocean Water Cooling. The InterContinental Resort and Thalasso-Spa on the island of Bora Bora uses a seawater air conditioning (SWAC) system to air-condition its buildings. The system accomplishes this by passing cold seawater through a heat exchanger where it cools freshwater in a closed loop system. This cool freshwater is then pumped to buildings and is used for cooling directly (no conversion to electricity takes place). Similar systems are also in place in The Excelsior hotel and The Hong Kong and Shanghai Banking Corporation main building in Victoria, Hong Kong, and at the Natural Energy Laboratory of Hawaii Authority. The InterContinental Resort is the largest seawater air conditioning system to date, though there are several other, larger systems being planned.\n\n\n== See also ==\n\nDistrict heating\nFree cooling\nSeasonal thermal energy storage (STES)\nSolar pond\nOcean energy\nOcean thermal energy conversion\n\n\n== Notes ==\n\n\n== References ==\nSudick, Jennifer (Vol. 13, Issue 15 - Tuesday, January 15, 2008). \"New seawater cooling plant in the works\". Honolulu Star-Bulletin. Retrieved 2008-04-26.  \nLong Beach Press-Telegram , April 7, 2005, USING COLD SEAWATER FOR AIR-CONDITIONING\n\n\n== External links ==\nFrom Lake Depths, a Blast of Cool for Consumers\nCornell University Lake Source Cooling overview and details of how it works\nGeocean has performed the design and installation of a SWAC system for the Brando Hotel in French Polynesia.\nMakai Ocean Engineering has designed SWAC systems in Bora Bora (installed), Kona (installed) and Honolulu, Hawaii, la Reunion, Cura\u00e7ao, Bahamas, and DWSC systems at Cornell University and Toronto (both installed).", 
                "titleUrl": "https://en.wikipedia.org/wiki/Deep_water_source_cooling", 
                "title": "Deep water source cooling"
            }, 
            {
                "snippet": "engine cooling uses either air or a liquid to remove the waste heat from an internal combustion engine. For small or special purpose engines, air cooling makes", 
                "pageCategories": "All articles needing additional references\nAll articles with specifically marked weasel-worded phrases\nAll articles with unsourced statements\nArticles needing additional references from July 2015\nArticles with specifically marked weasel-worded phrases from February 2014\nArticles with unsourced statements from February 2014\nArticles with unsourced statements from July 2015\nCommons category with local link same as on Wikidata\nCooling technology\nEngine components", 
                "pageContent": "Internal combustion engine cooling uses either air or a liquid to remove the waste heat from an internal combustion engine. For small or special purpose engines, air cooling makes for a lightweight and relatively simple system. The more complex circulating liquid-cooled engines also ultimately reject waste heat to the air, but circulating liquid improves heat transfer from internal parts of the engine. Engines for watercraft may use open-loop cooling, but air and surface vehicles must recirculate a fixed volume of liquid.\n\n\n== Overview ==\nHeat engines generate mechanical power by extracting energy from heat flows, much as a water wheel extracts mechanical power from a flow of mass falling through a distance. Engines are inefficient, so more heat energy enters the engine than comes out as mechanical power; the difference is waste heat which must be removed. Internal combustion engines remove waste heat through cool intake air, hot exhaust gases, and explicit engine cooling.\nEngines with higher efficiency have more energy leave as mechanical motion and less as waste heat. Some waste heat is essential: it guides heat through the engine, much as a water wheel works only if there is some exit velocity (energy) in the waste water to carry it away and make room for more water. Thus, all heat engines need cooling to operate.\nCooling is also needed because high temperatures damage engine materials and lubricants. Cooling becomes more important in when the climate becomes very hot. Internal-combustion engines burn fuel hotter than the melting temperature of engine materials, and hot enough to set fire to lubricants. Engine cooling removes energy fast enough to keep temperatures low so the engine can survive.\nSome high-efficiency engines run without explicit cooling and with only incidental heat loss, a design called adiabatic. Such engines can achieve high efficiency but compromise power output, duty cycle, engine weight, durability, and emissions.\n\n\n== Basic principles ==\nMost internal combustion engines are fluid cooled using either air (a gaseous fluid) or a liquid coolant run through a heat exchanger (radiator) cooled by air. Marine engines and some stationary engines have ready access to a large volume of water at a suitable temperature. The water may be used directly to cool the engine, but often has sediment, which can clog coolant passages, or chemicals, such as salt, that can chemically damage the engine. Thus, engine coolant may be run through a heat exchanger that is cooled by the body of water.\nMost liquid-cooled engines use a mixture of water and chemicals such as antifreeze and rust inhibitors. The industry term for the antifreeze mixture is engine coolant. Some antifreezes use no water at all, instead using a liquid with different properties, such as propylene glycol or a combination of propylene glycol and ethylene glycol. Most \"air-cooled\" engines use some liquid oil cooling, to maintain acceptable temperatures for both critical engine parts and the oil itself. Most \"liquid-cooled\" engines use some air cooling, with the intake stroke of air cooling the combustion chamber. An exception is Wankel engines, where some parts of the combustion chamber are never cooled by intake, requiring extra effort for successful operation.\nThere are many demands on a cooling system. One key requirement is to adequately serve the entire engine, as the whole engine fails if just one part overheats. Therefore, it is vital that the cooling system keep all parts at suitably low temperatures. Liquid-cooled engines are able to vary the size of their passageways through the engine block so that coolant flow may be tailored to the needs of each area. Locations with either high peak temperatures (narrow islands around the combustion chamber) or high heat flow (around exhaust ports) may require generous cooling. This reduces the occurrence of hot spots, which are more difficult to avoid with air cooling. Air-cooled engines may also vary their cooling capacity by using more closely spaced cooling fins in that area, but this can make their manufacture difficult and expensive.\nOnly the fixed parts of the engine, such as the block and head, are cooled directly by the main coolant system. Moving parts such as the pistons, and to a lesser extent the crank and rods, must rely on the lubrication oil as a coolant, or to a very limited amount of conduction into the block and thence the main coolant. High performance engines frequently have additional oil, beyond the amount needed for lubrication, sprayed upwards onto the bottom of the piston just for extra cooling. Air-cooled motorcycles often rely heavily on oil-cooling in addition to air-cooling of the cylinder barrels.\nLiquid-cooled engines usually have a circulation pump. The first engines relied on thermo-syphon cooling alone, where hot coolant left the top of the engine block and passed to the radiator, where it was cooled before returning to the bottom of the engine. Circulation was powered by convection alone.\nOther demands include cost, weight, reliability, and durability of the cooling system itself.\nConductive heat transfer is proportional to the temperature difference between materials. If engine metal is at 250 \u00b0C and the air is at 20 \u00b0C, then there is a 230 \u00b0C temperature difference for cooling. An air-cooled engine uses all of this difference. In contrast, a liquid-cooled engine might dump heat from the engine to a liquid, heating the liquid to 135 \u00b0C (Water's standard boiling point of 100 \u00b0C can be exceeded as the cooling system is both pressurised, and uses a mixture with antifreeze) which is then cooled with 20 \u00b0C air. In each step, the liquid-cooled engine has half the temperature difference and so at first appears to need twice the cooling area.\nHowever, properties of the coolant (water, oil, or air) also affect cooling. As example, comparing water and oil as coolants, one gram of oil can absorb about 55% of the heat for the same rise in temperature (called the specific heat capacity). Oil has about 90% the density of water, so a given volume of oil can absorb only about 50% of the energy of the same volume of water. The thermal conductivity of water is about 4 times that of oil, which can aid heat transfer. The viscosity of oil can be ten times greater than water, increasing the energy required to pump oil for cooling, and reducing the net power output of the engine.\nComparing air and water, air has vastly lower heat capacity per gram and per volume (4000) and less than a tenth the conductivity, but also much lower viscosity (about 200 times lower: 17.4 \u00d7 10\u22126 Pa\u00b7s for air vs 8.94 \u00d7 10\u22124 Pa\u00b7s for water). Continuing the calculation from two paragraphs above, air cooling needs ten times of the surface area, therefore the fins, and air needs 2000 times the flow velocity and thus a recirculating air fan needs ten times the power of a recirculating water pump. Moving heat from the cylinder to a large surface area for air cooling can present problems such as difficulties manufacturing the shapes needed for good heat transfer and the space needed for free flow of a large volume of air. Water boils at about the same temperature desired for engine cooling. This has the advantage that it absorbs a great deal of energy with very little rise in temperature (called heat of vaporization), which is good for keeping things cool, especially for passing one stream of coolant over several hot objects and achieving uniform temperature. In contrast, passing air over several hot objects in series warms the air at each step, so the first may be over-cooled and the last under-cooled. However, once water boils, it is an insulator, leading to a sudden loss of cooling where steam bubbles form (for more, see heat transfer). Steam may return to water as it mixes with other coolant, so an engine temperature gauge can indicate an acceptable temperature even though local temperatures are high enough that damage is being done.\nAn engine needs different temperatures. The inlet including the compressor of a turbo and in the inlet trumpets and the inlet valves need to be as cold as possible. A countercurrent heat exchange with forced cooling air does the job. The cylinder-walls should not heat up the air before compression, but also not cool down the gas at the combustion. A compromise is a wall temperature of 90 \u00b0C. The viscosity of the oil is optimized for just this temperature. Any cooling of the exhaust and the turbine of the turbocharger reduces the amount of power available to the turbine, so the exhaust system is often insulated between engine and turbocharger to keep the exhaust gases as hot as possible.\nThe temperature of the cooling air may range from well below freezing to 50 \u00b0C. Further, while engines in long-haul boat or rail service may operate at a steady load, road vehicles often see widely varying and quickly varying load. Thus, the cooling system is designed to vary cooling so the engine is neither too hot nor too cold. Cooling system regulation includes adjustable baffles in the air flow (sometimes called 'shutters' and commonly run by a pneumatic 'shutterstat); a fan which operates either independently of the engine, such as an electric fan, or which has an adjustable clutch; a thermostatic valve or just 'thermostat' that can block the coolant flow when too cool. In addition, the motor, coolant, and heat exchanger have some heat capacity which smooths out temperature increase in short sprints. Some engine controls shut down an engine or limit it to half throttle if it overheats. Modern electronic engine controls adjust cooling based on throttle to anticipate a temperature rise, and limit engine power output to compensate for finite cooling.\nFinally, other concerns may dominate cooling system design. As example, air is a relatively poor coolant, but air cooling systems are simple, and failure rates typically rise as the square of the number of failure points. Also, cooling capacity is reduced only slightly by small air coolant leaks. Where reliability is of utmost importance, as in aircraft, it may be a good trade-off to give up efficiency, longevity (interval between engine rebuilds), and quietness in order to achieve slightly higher reliability; the consequences of a broken airplane engine are so severe, even a slight increase in reliability is worth giving up other good properties to achieve it.\nAir-cooled and liquid-cooled engines are both used commonly. Each principle has advantages and disadvantages, and particular applications may favor one over the other. For example, most cars and trucks use liquid-cooled engines, while many small airplane and low-cost engines are air-cooled.\n\n\n== Generalization difficulties ==\nIt is difficult to make generalizations about air-cooled and liquid-cooled engines. Air-cooled Deutz diesel engines are known for reliability even in extreme heat, and are often used in situations where the engine runs unattended for months at a time.\nSimilarly, it is usually desirable to minimize the number of heat transfer stages in order to maximize the temperature difference at each stage. However, Detroit Diesel 2-stroke cycle engines commonly use oil cooled by water, with the water in turn cooled by air.\nThe coolant used in many liquid-cooled engines must be renewed periodically, and can freeze at ordinary temperatures thus causing permanent engine damage. Air-cooled engines do not require coolant service, and do not suffer engine damage from freezing, two commonly cited advantages for air-cooled engines. However, coolant based on propylene glycol is liquid to -55 \u00b0C, colder than is encountered by many engines; shrinks slightly when it crystallizes, thus avoiding engine damage; and has a service life over 10,000 hours, essentially the lifetime of many engines.\nIt is usually more difficult to achieve either low emissions or low noise from an air-cooled engine, two more reasons most road vehicles use liquid-cooled engines. It is also often difficult to build large air-cooled engines, so nearly all air-cooled engines are under 500 kW (670 hp), whereas large liquid-cooled engines exceed 80 MW (107000 hp) (W\u00e4rtsil\u00e4-Sulzer RTA96-C 14-cylinder diesel).\n\n\n== Air-cooling ==\nCars and trucks using direct air cooling (without an intermediate liquid) were built over a long period from the very beginning and ending with a small and generally unrecognized technical change. Before World War II, water-cooled cars and trucks routinely overheated while climbing mountain roads, creating geysers of boiling cooling water. This was considered normal, and at the time, most noted mountain roads had auto repair shops to minister to overheating engines.\nACS (Auto Club Suisse) maintains historical monuments to that era on the Susten Pass where two radiator refill stations remain. These have instructions on a cast metal plaque and a spherical bottom watering can hanging next to a water spigot. The spherical bottom was intended to keep it from being set down and, therefore, be useless around the house, in spite of which it was stolen, as the picture shows.\nDuring that period, European firms such as Magirus-Deutz built air-cooled diesel trucks, Porsche built air-cooled farm tractors, and Volkswagen became famous with air-cooled passenger cars. In the United States, Franklin built air-cooled engines.\nFor many years air cooling was favored for military applications as liquid cooling systems are more vulnerable to damage by shrapnel.\nThe Czechoslovakia based company Tatra is known for their large displacement air-cooled V8 car engines; Tatra engineer Julius Mackerle published a book on it. Air-cooled engines are better adapted to extremely cold and hot environmental weather temperatures: you can see air-cooled engines starting and running in freezing conditions that seized water-cooled engines and continue working when water-cooled ones start producing steam jets. Air-cooled engines have may be an advantage from a thermodynamic point of view due to higher operating temperature. The worst problem met in air-cooled aircraft engines was the so-called \"Shock cooling\", when the airplane entered in a dive after climbing or level flight with throttle open, with the engine under no load while the airplane dives generating less heat, and the flow of air that cools the engine is increased, a catastrophic engine failure may result as different parts of engine have different temperatures, and thus different thermal expansions. In such conditions, the engine may seize, and any sudden change or imbalance in the relation between heat produced by the engine and heat dissipated by cooling may result in an increased wear of engine, as a consequence also of thermal expansion differences between parts of engine, liquid-cooled engines having more stable and uniform working temperatures.\n\n\n== Liquid cooling ==\n\nToday, most automotive and larger IC engines are liquid-cooled.\n\nLiquid cooling is also employed in maritime vehicles (vessels, ...). For vessels, the seawater itself is mostly used for cooling. In some cases, chemical coolants are also employed (in closed systems) or they are mixed with seawater cooling.\n\n\n== Transition from air cooling ==\nThe change of air cooling to liquid cooling occurred at the start of World War II when the US military needed reliable vehicles. The subject of boiling engines was addressed, researched, and a solution found. Previous radiators and engine blocks were properly designed and survived durability tests, but used water pumps with a leaky graphite-lubricated \"rope\" seal (gland) on the pump shaft. The seal was inherited from steam engines, where water loss is accepted, since steam engines already expend large volumes of water. Because the pump seal leaked mainly when the pump was running and the engine was hot, the water loss evaporated inconspicuously, leaving at best a small rusty trace when the engine stopped and cooled, thereby not revealing significant water loss. Automobile radiators (or heat exchangers) have an outlet that feeds cooled water to the engine and the engine has an outlet that feeds heated water to the top of the radiator. Water circulation is aided by a rotary pump that has only a slight effect, having to work over such a wide range of speeds that its impeller has only a minimal effect as a pump. While running, the leaking pump seal drained cooling water to a level where the pump could no longer return water to the top of the radiator, so water circulation ceased and water in the engine boiled. However, since water loss led to overheat and further water loss from boil-over, the original water loss was hidden.\nAfter isolating the pump problem, cars and trucks built for the war effort (no civilian cars were built during that time) were equipped with carbon-seal water pumps that did not leak and caused no more geysers. Meanwhile, air cooling advanced in memory of boiling engines... even though boil-over was no longer a common problem. Air-cooled engines became popular throughout Europe. After the war, Volkswagen advertised in the USA as not boiling over, even though new water-cooled cars no longer boiled over, but these cars sold well. But as air quality awareness rose in the 1960s, and laws governing exhaust emissions were passed, unleaded gas replaced leaded gas and leaner fuel mixtures became the norm. Subaru chose liquid-cooling for their EA series (flat) engine when it was introduced in 1966.\n\n\n== Low heat rejection engines ==\nA special class of experimental prototype internal combustion piston engines have been developed over several decades with the goal of improving efficiency by reducing heat loss. These engines are variously called adiabatic engines, due to better approximation of adiabatic expansion, low heat rejection engines, or high temperature engines. They are generally diesel engines with combustion chamber parts lined with ceramic thermal barrier coatings. Some make use of titanium pistons and other titanium parts due to its low thermal conductivity and mass. Some designs are able to eliminate the use of a cooling system and associated parasitic losses altogether. Developing lubricants able to withstand the higher temperatures involved has been a major barrier to commercialization.\n\n\n== See also ==\nRadiator\nHeater core\n\n\n== References ==\n\nBiermann, Arnold E.; Ellerbrock, Herman H., Jr (1939). The design of fins for air-cooled cylinders (pdf). NACA. Report N\u00ba. 726. \nP V Lamarque: \"The Design of Cooling Fins for Motor-Cycle Engines\". Report of the Automobile Research Committee, Institution of Automobile Engineers Magazine, March 1943 issue, and also in \"The Institution of Automobile Engineers Proceedings, XXXVII, Session 1942-43, pp 99-134 and 309-312.\n\"Air-cooled Automotive Engines\", Julius Mackerle, M. E.; Charles Griffin & Company Ltd., London, 1972.\n\n\n== External links ==\nengineeringtoolbox.com for physical properties of air, oil and water.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Internal_combustion_engine_cooling", 
                "title": "Internal combustion engine cooling"
            }, 
            {
                "snippet": "The Suzuki Advanced Cooling System (SACS) was developed by Suzuki engineer Etsuo Yokouchi in the early 1980s. The system was used extensively on GSXR model", 
                "pageCategories": "All articles lacking in-text citations\nAll stub articles\nArticles lacking in-text citations from May 2009\nCS1 errors: external links\nMotorcycle engines\nMotorcycle stubs\nMotorcycle technology", 
                "pageContent": "The Suzuki Advanced Cooling System (SACS) was developed by Suzuki engineer Etsuo Yokouchi in the early 1980s. The system was used extensively on GSXR model bikes from 1985 through 1992. Suzuki continued to use the system in its GSF (Bandit) and GSX (GSX-F, GSX1400, Inazuma) lines until the 2006 model-year and DR650 from 1990 to present. Engines using the SACS system were generally regarded as being very durable.\n\n\n== Development ==\nWhile addressing reliability issues in Suzuki's only turbo charged bike, the XN85, the SACS system was first conceived by Mr. Estuo Yokouchi, who looked to World War II\u2013era aircraft for inspiration. Like air-cooled motorcycles, radial engines used in many early aircraft suffered from heat and reliability issues. To overcome these problems, aircraft engineers often used oil jets aimed at the bottom of an engine's pistons to carry away excess heat. Following their example, Yokouchi decided to apply the approach to motorcycles. The result was a success.\nWhen the GSXR entered development, Suzuki set a goal of 100 horsepower (75 kW) for a 750 engine and, due to known heat-related problems in high-power air-cooled engines, determined that air cooling alone would not be sufficient. Therefore, the SACS system was applied to the bike's design and was eventually carried over to all larger GSXRs. The final GSX-R SACS engine appeared on the Suzuki GSX-R1100 in 1992, later bikes featured water cooling.\n\n\n== Mechanics ==\nThe SACS system uses high volumes of engine oil aimed at strategic points of the engine, like the top of the combustion chamber, which are not typically well served by air cooling alone. In order to provide enough oil for both cooling and lubrication, the system uses a double-chamber oil pump, using the high-pressure side for lubrication of the parts (crankshaft, connecting rods, valvetrain), while the low-pressure, high-volume side provides oil to the cooling and filtering circuit. The oil removes heat from hot engine parts through direct contact, is pumped away and subsequently routed through the oil filter, followed by routing through an oil cooler before being returned to the main sump.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Suzuki_Advanced_Cooling_System", 
                "title": "Suzuki Advanced Cooling System"
            }, 
            {
                "snippet": "was prone to fires, since the cooling system didn't work sufficiently. This was solved through rebuilding the cooling system in the 1970s.  J\u00e4rnv\u00e4g.net on", 
                "pageCategories": "All articles lacking sources\nArticles lacking sources from January 2016\nArticles that mention track gauge 1435 mm\nArticles with Swedish-language external links\nDouble-decker rail vehicles\nMultiple units of Sweden\nSJ multiple units\nTrain-related introductions in 1966", 
                "pageContent": "Y3 was a series of diesel railcars operated by Statens J\u00e4rnv\u00e4gar (SJ) of Sweden. Six units were delivered by Linke-Hofmann of Germany in 1966\u201367. They remained in service until 1990, serving first on the unelectrified services Stockholm \u2013 Mora and Malm\u00f6 \u2013 Karlskrona, later on Ystadbanan.\nThe Y3 served the same purpose as the electric X9-series. A variation of configurations was used, with the production consisting of six motor cars, two cab cars and eleven trailers. With motor cars on each end the Y3 could have six trailers between. The Y3 was the first double decker used in Sweden. The series was prone to fires, since the cooling system didn't work sufficiently. This was solved through rebuilding the cooling system in the 1970s.\n\n\n== External links ==\nJ\u00e4rnv\u00e4g.net on Y3 (Swedish)", 
                "titleUrl": "https://en.wikipedia.org/wiki/SJ_Y3", 
                "title": "SJ Y3"
            }, 
            {
                "snippet": "conventional cooling system using ethylene glycol coolant and a retractable radiator. Flight trials started but soon showed that the evaporative cooling system was", 
                "pageCategories": "Abandoned military aircraft projects of the Soviet Union\nIlyushin aircraft\nSoviet fighter aircraft 1930\u20131939", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Ilyushin_I-21", 
                "title": "Ilyushin I-21"
            }, 
            {
                "snippet": "Water cooling is a method of heat removal from components and industrial equipment. As opposed to air cooling, water is used as the heat conductor. Water", 
                "pageCategories": "All articles needing additional references\nAll articles with unsourced statements\nArticles needing additional references from December 2010\nArticles with unsourced statements from February 2011\nArticles with unsourced statements from June 2013\nArticles with unsourced statements from May 2011\nArticles with unsourced statements from November 2009\nComputer hardware cooling\nCooling technology\nEngine cooling systems", 
                "pageContent": "Water cooling is a method of heat removal from components and industrial equipment. As opposed to air cooling, water is used as the heat conductor. Water cooling is commonly used for cooling automobile internal combustion engines and large industrial facilities such as steam electric power plants, hydroelectric generators, petroleum refineries and chemical plants. Other uses include cooling the barrels of machine guns, cooling of lubricant oil in pumps; for cooling purposes in heat exchangers; cooling products from tanks or columns, and recently, cooling of various major components inside high-end personal computers. The main mechanism for water cooling is convective heat transfer.\n\n\n== Nomenclature ==\nCooling water is the water removing heat from a machine or system. Cooling water may be recycled through a recirculating system or used in a single pass once-through cooling (OTC) system. Recirculating systems may be open if they rely upon cooling towers or cooling ponds to remove heat or closed if heat removal is accomplished with negligible evaporative loss of cooling water. A heat exchanger or condenser may separate non-contact cooling water from a fluid being cooled, or contact cooling water may directly impinge on items like saw blades where phase difference allows easy separation. Environmental regulations emphasize the reduced concentrations of waste products in non-contact cooling water.\n\n\n== Advantages ==\nWater is inexpensive and non-toxic. The advantages of using water cooling over air cooling include water's higher specific heat capacity, density, and thermal conductivity. This allows water to transmit heat over greater distances with much less volumetric flow and reduced temperature difference.\nFor cooling CPU cores in computing equipment, the primary advantage of water cooling is that its tremendously increased ability to transport heat away from source to a secondary cooling surface allows for large, more optimally designed radiators rather than small, inefficient fins mounted directly on the heat source.\nThe water jacket around an engine is also very effective at deadening mechanical noises, which makes the engine quieter. However, the primary disadvantage is that it costs significantly more than an air-cooled engine system.\n\n\n== Disadvantages ==\nWater accelerates corrosion of metal parts and is a favorable medium for biological growth. Dissolved minerals in natural water supplies are concentrated by evaporation to leave deposits called scale. Cooling water often requires addition of chemicals to minimize corrosion and insulating deposits of scale and biofouling. In water cooling systems for electronic devices, the use of deionized water is required, which must be carefully controlled in order to avoid contamination, which would cause a decrease in resistance of the water and subsequently increase risk of short circuits.\n\n\n== Open method ==\n\nAn open water cooling system makes use of evaporative cooling, lowering the temperature of the remaining (unevaporated) water. This method was common in early internal combustion engines, until scale buildup was observed from dissolved salts and minerals in the water. Modern open cooling systems continuously waste a fraction of recirculating water as blowdown to remove dissolved solids at concentrations low enough to prevent scale formation. Some open systems use inexpensive tap water, but this requires higher blowdown rates than deionized or distilled water. Purified water systems still require blowdown to remove accumulation of byproducts of chemical treatment to prevent corrosion and biofouling.\n\n\n== Automotive usage ==\n\n\n=== Pressurization ===\nModern automotive cooling systems are slightly pressurized, often to 15 psi (103 kPa). This raises the boiling-point of the coolant and reduces evaporation.\n\n\n=== Antifreeze ===\nThe use of water cooling carries the risk of damage from freezing. Automotive and many other engine cooling applications require the use of a water and antifreeze mixture to lower the freezing point to a temperature unlikely to be experienced. Antifreeze also inhibits corrosion from dissimilar metals and can increase the boiling point, allowing a wider range of water cooling temperatures. Its distinctive odor also alerts operators to cooling system leaks and problems that would go unnoticed in a water-only cooling system. The heated coolant mixture can also be used to warm the air conditioning system inside the car by means of the heater core.\n\n\n=== Other additives ===\nOther less common chemical additives are products to reduce surface tension. These additives are meant to increase the efficiency of automotive cooling systems. Such products are used to enhance the cooling of underperforming or undersized cooling systems or in racing where the weight of a larger cooling system could be a disadvantage.\n\n\n== Power electronics and transmitters ==\nSince approximately 1930 it is common to use water cooling for tubes of powerful transmitters. As these devices uses high operation voltages ( around 10 kV), the use of deionized water is required and it has to be carefully controlled. Modern solid-state transmitters can be built so, that even high power transmitters do not require water cooling. Water cooling is however also sometimes used for thyristors of HVDC valves, for which also the use of deionized water is required.\n\n\n== Computer usage ==\n\nCooling hot computer components with various fluids has been in use since at least as far back as the development of Cray-2 in 1982, using Fluorinert. Through the 1990s, water cooling for home PCs slowly gained recognition amongst enthusiasts, but it started to become noticeably more prevalent after the introduction of AMD's hot-running Athlon processor in mid-2000. As of 2011, there are several manufacturers of water cooling components and kits, and some custom computer retailers include various setups of water cooling for their high-performance systems.\nWater cooling can be used to cool many computer components, but especially the CPU. Water cooling usually uses a CPU water block, a water pump, and a water-to-air heat exchanger. By transferring device heat to a separate heat exchanger which can variously be made large and use larger, lower-speed fans, Water cooling can allow quieter operation, improved processor speeds (overclocking), or a balance of both. Less commonly, GPUs, Northbridges, Southbridges, hard disk drives, memory, voltage regulator modules (VRMs), and even power supplies can be water-cooled.\nRadiator size may vary: from 40mm dual fan (80mm) to 140 quad fan (560mm) and thickness from 30mm to 80mm\nWater coolers for desktop computers were, until the end of the 1990s, homemade. They were made from car radiators (or more commonly, a car's heater core), aquarium pumps and home-made water blocks, laboratory-grade PVC and silicone tubing and various reservoirs (homemade using plastic bottles, or constructed using cylindrical acrylic or sheets of acrylic, usually clear) and or a T-Line. More recently a growing number of companies are manufacturing water-cooling components compact enough to fit inside a computer case. This, and the trend to CPUs of higher power dissipation, has greatly increased the popularity of water cooling, although only a very small minority of computers are water-cooled.\nDedicated overclockers occasionally use vapor-compression refrigeration or thermoelectric coolers in place of more common standard heat exchangers. Water cooling systems in which water is cooled directly by the evaporator coil of a phase change system are able to chill the circulating coolant below the ambient air temperature (impossible with a standard heat exchanger) and, as a result, generally provide superior cooling of the computer's heat-generating components. The downside of phase-change or thermoelectric cooling is that it uses much more electricity, and antifreeze must be added due to the low temperature. Additionally, insulation, usually in the form of lagging around water pipes and neoprene pads around the components to be cooled, must be used in order to prevent damage caused by condensation of water vapour from the air on chilled surfaces. Common places from which to borrow the required phase transition systems are a household dehumidifier or air conditioner.\n\nAn alternative cooling system, which enables components to be cooled below the ambient temperature, but which obviates the requirement for antifreeze and lagged pipes, is to place a thermoelectric device (commonly referred to as a 'Peltier junction' or 'pelt' after Jean Peltier, who documented the effect) between the heat-generating component and the water block. Because the only sub-ambient temperature zone now is at the interface with the heat-generating component itself, insulation is required only in that localized area. The disadvantage of such a system is a higher power dissipation.\nTo avoid damage from condensation around the Peltier junction, a proper installation requires it to be \"potted\" with silicone epoxy. The epoxy is applied around the edges of the device, preventing air from entering or leaving the interior.\n\nApple's Power Mac G5 was the first mainstream desktop computer to have water cooling as standard. Dell followed suit by shipping their XPS computers with liquid cooling, using thermoelectric cooling to help cool the liquid. Currently, Dell's only computers to offer liquid cooling are their Alienware desktops.\n\n\n== Liquid cooling maintenance ==\n\nLiquid cooling techniques are increasingly being used for the thermal management of electronic components. This type of cooling is a solution to ensure the optimisation of energy efficiency while simultaneously minimising noise and space requirements. Especially useful in supercomputers or Data Centers as maintenance of the racks is quick and easy. After disassembly of the rack, advanced technology quick release couplings eliminate spillage for the safety of operators and protects the integrity of fluids (no impurities in the circuits). These couplings are also capable of being locked (Panel mounted?) to allow blind connection in difficult to access areas. It is important in electronics technology to analyse the connection systems to ensure:\nNon-spill sealing (clean break, flush face couplings)\nCompact and lightweight (materials in special aluminum alloys)\nOperator safety (disconnection without spillage)\nQuick-release couplings sized for optimized flow\nConnection guiding system and compensation of misalignment during connection on rack systems\nExcellent resistance to vibration and corrosion\nDesigned to withstand a large number of connections even on refrigerant circuits under residual pressure\n\n\n== Industrial usage ==\n\nIndustrial cooling towers may use river water, coastal water (seawater), or well water as their source of fresh cooling water. The large mechanical induced-draft or forced-draft cooling towers in industrial plants continuously circulate cooling water through heat exchangers and other equipment where the water absorbs heat. That heat is then rejected to the atmosphere by the partial evaporation of the water in cooling towers where upflowing air is contacted with the circulating downflow of water. The loss of evaporated water into the air exhausted to the atmosphere is replaced by \"make-up\" fresh river water or fresh cooling water. Since the evaporation of pure water is replaced by make-up water containing carbonates and other dissolved salts, a portion of the circulating water is also continuously discarded as \"blowdown\" water to prevent the excessive build-up of salts in the circulating water.\n\nOn very large rivers, but more often at coastal and estuarine sites, \"direct cooled\" systems are often used, instead. These industrial plants do not use cooling towers and the atmosphere as a heat sink, but put the waste heat to the river or coastal water instead. These OTC systems thus rely upon a good supply of river water or seawater for their cooling needs. Many facilities, particularly electric power plants, use millions of gallons of water per day for cooling. Such facilities are built with intake structures designed to pump in large volumes of water at a high rate of flow. These structures tend to also pull in large numbers of fish and other aquatic organisms, which are killed or injured on the intake screens.\nThe warmed water is returned directly to the aquatic environment, often at temperatures significantly (to aquatic life) above the ambient receiving water. Thermal pollution of rivers, estuaries and coastal waters is a consideration when siting such plants.\nHigh-grade industrial water (produced by reverse osmosis) and potable water are sometimes used in industrial plants requiring high-purity cooling water.\nSome nuclear reactors use heavy water as cooling. Heavy water is employed in nuclear reactors because it is a weaker neutron absorber. This allows for the use of less enriched fuel. For the main cooling system, normal water is preferably employed through the use of a heat exchanger, as heavy water is much more expensive. Reactors that use other materials for moderation (graphite) may also use normal water for cooling.\n\n\n== Marine vessel use ==\nWater is an ideal cooling medium for vessels as they are constantly surrounded by water that generally remains at a low temperature throughout the year. This does however pose new challenges as cooling systems operating with sea water need to be manufactured from materials that are suitable for the environment. A heat exchanger for example will need to be manufactured from materials such as Cupronickel, Bronze or Titanium to protect it from erosion or corrosion. The velocity will also need to be far more restricted compared with a fresh water cooling system. If the velocity is too low; then sand and other sediments can block the tubes. If the velocity is too high then the tubes can be eroded by the sediments in the water.\n\n\n== Environmental considerations ==\nWater is a favorable environment for many life forms. Water cooling may alter natural water environments and create new environments. Flow characteristics of recirculating cooling water systems encourage colonization by sessile organisms to use the circulating supply of food, oxygen and nutrients. Volumes of water lost during evaporative cooling may decrease natural habitat for aquatic organisms. Water temperature increases modify aquatic habitat by increasing biochemical reaction rates and decreasing oxygen saturation capacity of the habitat. Temperature increases initially favor a population shift from those requiring the high-oxygen concentration of cold water to those enjoying advantages of increased metabolic rates in warm water. Temperatures may become high enough to support thermophilic populations.\nBiofouling of heat exchange surfaces can reduce heat transfer rates of the cooling system; and biofouling of cooling towers can alter flow distribution to reduce evaporative cooling rates. Biofouling may also create differential oxygen concentrations increasing corrosion rates. OTC and open recirculating systems are most susceptible to biofouling. Biofouling may be inhibited by temporary habitat modifications. Temperature differences may discourage establishment of thermophilic populations in intermittently operated facilities; and intentional short term temperature spikes may periodically kill less tolerant populations. Biocides have been commonly used to control biofouling where sustained facility operation is required.\nLarge OTC flow rates may immobilize slow-swimming organisms including fish and shrimp on screens protecting the small bore tubes of the heat exchangers from blockage. High temperatures or pump turbulence and shear may kill or disable smaller organisms passing the screens entrained with the cooling water. In the U.S., cooling water intake structures kill billions of fish and other organisms each year. More agile aquatic predators consume organisms impinged on the screens; and warm water predators and scavengers colonize the cooling water discharge to feed on entrained organisms.\nManufactured metals tend to revert to ores via electrochemical reactions of corrosion. Water can accelerate corrosion as both an electrical conductor and solvent for metal ions and oxygen. Corrosion reactions proceed more rapidly as temperature increases. Preservation of machinery in the presence of hot water has been improved by addition of chemicals including zinc, chromates and phosphates. The first two have toxicity concerns; and the last has been associated with eutrophication. Residual concentrations of biocides and corrosion inhibitors are of potential concern for OTC and blowdown from open recirculating systems. With the exception of machines with short design life, closed recirculating systems require periodic cooling water treatment or replacement raising similar concern about ultimate disposal of cooling water containing chemicals used with environmental safety assumptions of a closed system.\nIndustrial cooling water regulations\nThe U.S. Clean Water Act requires the Environmental Protection Agency (EPA) to issue regulations on industrial cooling water intake structures. EPA issued final regulations for new facilities in 2001 (amended 2003), and for existing facilities in 2014.\n\n\n== Cooling water chemistry ==\nWater contains varying amounts of impurities from contact with the atmosphere, soil, and containers. Cooling water treatments add other chemicals attempting to maintain satisfactory heat exchange.\n\n\n=== Solids ===\nTotal dissolved solids or TDS (sometimes called filtrable residue) is measured as the mass of residue remaining when a measured volume of filtered water is evaporated. Salinity measures water density or conductivity changes caused by dissolved materials. Probability of scale formation increases with increasing total dissolved solids. Solids commonly associated with scale formation are calcium and magnesium carbonate and sulfate. Corrosion rates initially increase with salinity in response to increasing electrical conductivity, but then decrease after reaching a peak as higher levels of salinity decrease dissolved oxygen levels.\n\n\n=== Hydrogen ===\nWater ionizes into hydronium (H3O+) cations and hydroxide (OH\u2212) anions. The concentration of ionized hydrogen (as protonated water) is expressed as pH. Low pH values increase rate of corrosion while high pH values encourage scale formation. Amphoterism is uncommon among metals used in water cooling systems, but aluminum corrosion rates increase with pH values above 9. Galvanic corrosion may be severe in water systems with copper and aluminum components. Acid may be added to cooling water systems to prevent scale formation if the pH decrease will offset increased salinity and dissolved solids.\n\n\n=== Phosphorus and chromium ===\nConcentrations of polyphosphates or phosphonates with zinc and chromates or similar compounds have been maintained in cooling systems to keep heat exchange surfaces clean so a film of gamma iron oxide and zinc phosphate may inhibit corrosion by passivating anodic and cathodic reaction points. These increase salinity and total dissolved solids, and phosphorus compounds may provide the limiting essential nutrient for algal growth contributing to biofouling of the cooling system or to eutrophication of natural aquatic environments receiving blowdown or OTC water. Chromates reduce biofouling in addition to effective corrosion inhibition, but residual toxicity in blowdown or OTC water has encouraged reduced chromate concentrations and use of less flexible corrosion inhibitors. Blowdown may also contain chromium leached from cooling towers constructed of wood preserved with chromated copper arsenate.\n\n\n=== Oxygen ===\nSome groundwater contains very little oxygen when pumped from wells, but most natural water supplies include dissolved oxygen. Corrosion increases with increasing oxygen concentrations. Dissolved oxygen approaches saturation levels in cooling towers. Dissolved oxygen is desirable in blowdown or OTC water being returned to natural aquatic environments.\n\n\n=== Biocides ===\nChlorine may be added in the form of hypochlorite to decrease biofouling, but is later reduced to chloride to minimize toxicity of blowdown or OTC water returned to natural aquatic environments. Hypochlorite is increasingly destructive to wooden cooling towers as pH increases. Chlorinated phenols have been used as biocides or leached from preserved wood in cooling towers. Both hypochlorite and pentachlorophenol have reduced effectiveness at pH values greater than 8. Non-oxidizing biocides may be more difficult to detoxify prior to release of blowdown or OTC water to natural aquatic environments.\n\n\n== See also ==\nCooling pond\nDeep lake water cooling\nFree cooling\nFull immersion cooling\nHeat pipe cooling\nHopper cooling\nOil cooling\nPeltier cooling\nThermosiphon (passive heat exchange)\n\n\n== References ==\n\n\n== Bibliography ==\n\n\n== External links ==\nBasic Theory and Practice of Cooling Towers\nHowstuffworks \"How Liquid-cooled PCs Work\"", 
                "titleUrl": "https://en.wikipedia.org/wiki/Water_cooling", 
                "title": "Water cooling"
            }
        ], 
        "phraseCharStart": "205"
    }, 
    {
        "phraseCharEnd": "276", 
        "phraseIndex": "T6", 
        "phraseGoldStandardTag": "Task", 
        "phrase": "reconditioning", 
        "wikiSearchResults": [
            {
                "snippet": "This squadron was responsible for reconditioning airframes of RNZAF aircraft. Formally the Airframe Reconditioning Squadron RNZAF, the unit was commercialised", 
                "pageCategories": "All Wikipedia articles written in New Zealand English\nAll stub articles\nBuildings and structures in the Marlborough Region\nCoordinates on Wikidata\nFortification stubs\nMarlborough Region geography stubs\nRoyal New Zealand Air Force bases\nUse New Zealand English from May 2013\nUse dmy dates from May 2013", 
                "pageContent": "RNZAF Base Woodbourne is a base of the Royal New Zealand Air Force, located 8 km west of Blenheim.\nWoodbourne was established in 1939 as the base for No. 2 Service Flying Training School (No.2 SFTS). Also located nearby during WWII were the ground training camps of the Delta. In 1942-43, Nos 16 and 18 Squadrons flying Curtiss Kittyhawks used the satellite Fairhall field. In 1945 No.2 SFTS was closed and the Royal New Zealand Air Force Central Flying School and some ground training units, including the Officers' School of Instruction were relocated to Woodbourne. In 1949, The Aircraft Repair Depot RNZAF was relocated from Ohakea, and in 1951 the Boy Entrant School was established at Woodbourne.\nToday, Woodbourne is the Air Force's only support base and has no operational squadrons based there. It shares its runways with the Blenheim civil airport, Woodbourne Airport.\nThe Ground Training Wing was created in 1995 from existing units at Woodbourne and those relocated from Wigram and Hobsonville, and is responsible for the training of recruits (General Service Training School), initial officer training (Command Training School), trade training (except aircrew, medical and photography training) and command training.\nAlso at Woodbourne is the Air Force's only heavy maintenance facility for the repair of aircraft airframes, engines and avionics systems. This squadron was responsible for reconditioning airframes of RNZAF aircraft. Formally the Airframe Reconditioning Squadron RNZAF, the unit was commercialised in 1998 and is now managed by SAFE Air Ltd. No. 1 Repair Depot was also active at the base for a number of years, though during the war it appears to have been stationed at Hamilton.\nPersonnel stationed at the base number around 1,250, of which 450 are SAFE Air Ltd employees.\n\n\n== Units based at Woodbourne ==\nNew Zealand Defence Force Military Police\nRNZAF Rescue Fire\nNZDF Physical Training Instructors School\nRNZAF Security Forces\nRNZAF Logistics\nBase Medical\nAirbus Group\n\n\n== References ==\n\n\n== See also ==\nList of New Zealand military bases\n\n\n== External links ==\nRNZAF Base Woodbourne", 
                "titleUrl": "https://en.wikipedia.org/wiki/RNZAF_Base_Woodbourne", 
                "title": "RNZAF Base Woodbourne"
            }, 
            {
                "snippet": "Bravia was a Portuguese vehicle manufacturer. It started by reconditioning military vehicles and by 1964 it had become a manufacturer in its own right", 
                "pageCategories": "Car manufacturers of Portugal\nDefunct companies of Portugal\nDefunct truck manufacturers", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Bravia_(automobile)", 
                "title": "Bravia (automobile)"
            }, 
            {
                "snippet": "made a grant of about N12 million to the Anambra State Polytechnic for reconditioning and renovation of the facilities. He awarded a contract for N650 million", 
                "pageCategories": "All stub articles\nGovernors of Anambra State\nLiving people\nNigerian Army officers\nNigerian military personnel stubs\nNigerian politician stubs", 
                "pageContent": "Colonel Mike E. Attah was the Military Administrator of Anambra State in Nigeria from 9 December 1993 to 21 August 1996 during the military regime of General Sani Abacha.\nOn October 25, 1995, Mike Attah set up a Commission of Inquiry to investigate violent disturbances that had erupted on 30 September 1995 between the Aguleri and Umuleri communities. The commission found that the attack by the Aguleri had been carefully planned, including use of hired mercenaries, and that the local authorities had done little to avert the crisis.\nHe dismissed six government-employed journalists for failing to join his entourage because their car was out of fuel.\nIn 1995 he made a grant of about N12 million to the Anambra State Polytechnic for reconditioning and renovation of the facilities. He awarded a contract for N650 million to Chief Christian Uba, a businessman, to build the new Government House and the governor\u2019s lodge, known as Zik\u2019s Place. In June 2006, the work was still not complete, and the contractor was suing for payment to cover costs to date.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Mike_Attah", 
                "title": "Mike Attah"
            }, 
            {
                "snippet": "body repair, dealership management systems, dent repair and automotive reconditioning, and automotive re-marketing at each location.    In 1945, Benjamin", 
                "pageCategories": "1945 establishments in Pennsylvania\nAll Wikipedia articles in need of updating\nAll articles needing additional references\nAll articles with a promotional tone\nAll articles with topics of unclear notability\nAmerican auction houses\nAmerican companies established in 1945\nArticles needing additional references from November 2011\nArticles with a promotional tone from January 2013\nArticles with topics of unclear notability from January 2013", 
                "pageContent": "Manheim, Inc. is an automobile auction company and the world's largest wholesale auto auction based on trade volume with 145 auctions located in North America, Europe, Asia and Australia. As a subsidiary of privately owned Cox Enterprises, Inc. based in Atlanta, Georgia, Manheim's primary business is wholesaling vehicles via a bidding process that can done in traditional or online formats. Manheim also provides other vital dealership and wholesale services, such as financing, title work, transportation (auto hauling), recovery, auto body repair, dealership management systems, dent repair and automotive reconditioning, and automotive re-marketing at each location.\n\n\n== History ==\n\n\n=== The Beginning ===\nIn 1945, Benjamin Z. Mellinger, a Ford dealer in New Holland, Pennsylvania, and Arthur F. Walters, a Firestone tire dealer in Manheim, met to discuss the idea of auctioning cars after watching a farm equipment auction. They with Jacob \"Jack\" H. Ruhl, Paul H. Stern, and Robert Schreiber formed a partnership, each contributing $5000, and then, purchased a decrepit building along with its seven acres just south of Manheim to form the Manheim Auto Auction, Inc.. Their first sale ran three cars and sold just one to the general public. By 1947, the new building with four lanes to auction vehicles opened and soon the partners realized the general public was purchasing their vehicles at the auction instead of their dealership franchises. This action prompted the partners to make Manheim Auto Auction a dealer-only enterprise, making Manheim the largest auto exchange in 1959.\nBy 1966, Manheim Auto Auction established itself as the world's volume leader, selling off 45 vehicles per hour or 700 cars/trucks on a given Friday night at the 16-laned auction. Adding closed-circuit television, the dealers and wholesalers watched the auction from the new cafeteria in its expanded building. In 1965, Manheim Auto Auction purchased the National Auto Dealers Exchange in Bordentown, New Jersey, and then in 1967, it purchased the Fredericksburg Auto Exchange in Fredericksburg, Virginia. Thus, the partners made Manheim attractive for a potential buyer.\n\n\n=== Manheim joins Cox Enterprises ===\nCox Enterprises entered the auto auction business in 1968, when it purchased Manheim Auto Auction in Manheim, Pennsylvania. Under Cox, Manheim continued to expand by providing vital services, such as reconditioning, recovery, and auto hauling, to both dealership and wholesalers. By the end of the 20th century, the Manheim Auto Auctions had advanced its sales by adding information technology (online sales). Today, Cox Enterprises owns 98% of Auto Trader, one of the world\u2019s leading providers of online and print automotive consumer information.\n\n\n=== Notable Acquisitions ===\nManheim acquired Dent Wizard, the paintless dent removal company (also known as \"PDR\") in 1998. After turning it into the largest PDR company in the world, Manheim sold Dent Wizard's United States and Canada markets to H.I.G. Capital, a leading global private equity firm, in November 2010.\nIn 2006, Manheim became interested in Akinvest Inc.[1] This Canadian company started off as a business that speculated on world automobile values in 1991. Owners, Andrei Kouznetsov and Dwight Grovum, developed a new concept that allowed a foreign dealer to have Akinvest perform all of the necessary procedures to have vehicles delivered to the port nearest to that buyer. By 2005 Akinvest, doing business as Exporttrader, was completely developed the service aspects of the business that assisted non-North American dealers. Company became known as the leader in the export of used cars field from North America. All of the major remarketing companies such as Copart , IAA, ADESA, Carfax and Manheim had foreign transactions that were completed by Akinvest/Exporttrader.\nThe strength of the procedures used were enhanced by superior programming and automated Web Sites, retaining enormous banks of data. Export Trader/Akinvest negotiated for new export programs and procedures with the governments of Canada and United States and was consulted numerous times by International law enforcement on global irregularities . Exporttrader was consulted by marketing firms and journals like the Wall Street Journal [2] to give opinions on global automotive trade and were asked to speak at international conferences. In 2008 Akinvest/Export Trader had 70 employees with offices in Toronto, Moscow and Finland. \"Akinvest is the parent company of two of the leading players in the export business: Freightmar International and ExportTrader.com. Freightmar, a registered Non-Vessel Operating Common Carrier (NVOCC)\"\nThe agreement was signed in January 2009 for a staged buyout of Akinvest/ Export Trader transforming the entire organization under name WES Exporttrader . The final stage of the transfer to Manheim occurred in late 2014. Subsequently in April 2015, Manheim has closed the Canadian offices of WES Exporttrader and moved the operation to Atlanta. They now run \"all new \"Global Trader\" out of Atlanta. In February Manheim Global Export was sold, End of March 2016 Manheim Global Export was sold, but it was too many debts and company was declared as the bankrupt.\n\n\n== References ==\n\n\n== External links ==\nOfficial website\nAuto Mechanic Jobs with Manheim\nManheim in export venture by Autonews\nhttp://press.manheim.com/2009-04-02-Manheim-enters-joint-venture-with-Leading-vehicle-exporter-akinvest", 
                "titleUrl": "https://en.wikipedia.org/wiki/Manheim_Auctions", 
                "title": "Manheim Auctions"
            }, 
            {
                "snippet": "\"R110A\" redirects here. For the road, see Route 110. The R110A (contract R130) was a prototype class of experimental new technology New York City Subway", 
                "pageCategories": "Articles that mention track gauge 1435 mm\nCommons category with local link same as on Wikidata\nKawasaki rolling stock\nNew York City Subway passenger equipment\nTrain-related introductions in 1992", 
                "pageContent": "The R110A (contract R130) was a prototype class of experimental new technology New York City Subway cars delivered in 1992. The R110A was designed to test out new technology features that would be incorporated into future New Technology Trains, including the R142 car order, and it was not intended for long-term production use. Built by Kawasaki Heavy Industries, there are ten cars, unit numbered #8001\u20138010, and they were permanently linked in five-car sets. Between 2013 and 2014, all the B-cars (#8002-8004 and #8007-8009) were converted into flood pump cars.\n\n\n== Description ==\n\n\n=== Consists ===\nThe R110As are a ten-car ordered numbered #8001-8010, and the order is split into two five-car sets that are permanently coupled together. Each car is 51 feet 4 inches (15.65 m) like other A Division subway cars.\nAt each end of the five-car set, there is a full-width cab. The cab cars are powered by four traction motors each. The center car of each five-car set is an unpowered trailer, and the other two cars are powered by two traction motors each.\nThe cab is computerized, with a control stand consisting of a single lever for traction and braking control, a reversing key, a small numeric and symbol keypad, and an LCD flat panel display. The display is used in conjunction with the keypad to control doors, reset alarms of various sorts including the passenger alert system, display train speed and braking information, and do much more.\n\n\n=== Innovations ===\nThe R110A cars are similar to R62s, but they have squarer ends and wider 64-inch passenger entry doors (over a foot wider than the R62 doors, which were 50 inches) that are staggered for better passenger flow, because passengers would stand in the niche instead of in front of each door. All car ends have clear lexan glass, allowing passengers to see through to the next car, except on cab ends. The famous Italian designer Massimo Vignelli was hired to design the car interior with the Metropolitan Transportation Authority Arts for Transit program. The R110A has very bright colors with speckled black floors and with walls that are speckled gray. Unnecessary edges were removed from stanchions, boles, and bars to create a smoother and cleaner appearance. The United States Department of Transportation National Endowment for the Arts gave the 1995 national award for transportation design as a result of these efforts.\nSeating is improved by eliminating the bucket seats in favor of comfortable benches in bright colors. The interior has longitudinal seats on one side and transverse seating on the other, unlike previous IRT cars, which since 1910 have always featured all-longitudinal seating. One side is shifted from the other, making part of the bench on one side of the car face a door on the other side. Some seating space is removed to allow for wider doors. Interior surfaces are fiberglass, which is resistant to graffiti. As a result, there was a significant reduction in seats, from a total of 440 in a train of R62As, to 264 in a train of R110As. However, the number of standees went up from 1,332 to 1,684. The seating capacity is 24 in the A cars, and 28 in the non-cab B cars. As a result of the loss of seats, there were complaints from the riding public, and as a result, most of the seats were restored on the first New Technology Train orders, the R142s and R142As.\nThere are LED exterior line indicator signs on all cars, LCD destination signs in windows, and LED interior next stop/variable message signs inside the cars. The LED display on the front of the car could either be red, for Broadway\u2013Seventh Avenue Line service, or green, for Lexington Avenue Line service.\n\n\n== History ==\nDuring the 1970s and 1980s, the Metropolitan Transportation Authority had made several large orders for subway cars, such as the R46, which had new components added to them. However, because there was not a prototype built first for testing, many expensive retrofits were required. The Metropolitan Transportation Authority was in the process of creating the first technologically-advanced subway car since the R44 in the early 1970s. In 1989, the MTA awarded contracts for two prototype test trains, one of which was the R110A (contract R130) for the A Division built by Kawasaki Heavy Industries, and the R110B (contract R131) for the B Division built by Bombardier Transportation. In order to avoid the aforementioned problem, in 1989, the MTA awarded contracts for two prototype test trains, one of which was the R110A (contract R130) for the A Division built by Kawasaki Heavy Industries, and the R110B (contract R131) for the B Division built by Bombardier Transportation. The cost for each R110A car was $2,209,000. The R110As were built in 1992.\nThese two fleets, were called the New Technology test trains (NTTTs), and would test features that would be implemented on future mass-production orders, specifically the New Technology Trains. The R110A tested new technology including AC propulsion with regeneration, microprocessor-controlled doors and brakes, roof-mounted hermetic air-conditioning units, and fabricated trucks with air bags suspension. Passenger emergency intercoms for contacting train crews, passenger alarm strips to press in case of an emergency, improved lighting, glass to see into the next cars and the platform, and computerized announcements were all implemented.\nIt was proposed by MTA New York City Transit, to include an articulated train under the R110A contract, but because of the impact it would have had on the project's budget and schedule it was rejected.\nThe R110A cars entered service on June 15, 1993, on the 2 service. In 1999, they were pulled out of service due to brake problems and fire damage, and were transported back and forth between IRT line yards and stored until 2013.\n\n\n=== Reconditioning and current status ===\nIn 2013, it was decided to convert the cars to pump cars as the car bodies had many years of service left on them. Cars 8002\u20138004 were converted to pump cars in 2013 until summer 2014, while 8007\u20138009 were converted in the fall 2014. 8005 was completely stripped of parts to become a pump train as well; however, the conversion process was halted sometime in 2014 as it was decided to use only the B-cars for pump train service. The B-cars were renumbered to P8002-P8004 and P8007-P8009 after conversion. The conversion of six cars for pump train service helped increase the number of available pump trains; this will shorten the amount of time it takes to pump water out of the subway system.\nA more detailed list of the known statuses of the non-converted cars is shown below:\n8001 and 8006 \u2013 Missing various components, and currently stored at 207th Street Yard. Future plans for these cars are unknown.\n8005 \u2013 Completely stripped of parts, remains in 207th Street Yard. Future plans for this car are unknown.\n8010 \u2013 Remains in its original condition in 207th Street Yard. Future plans for this car are unknown, though recently it has been located on the same track as other subway cars slated for preservation.\n\n\n== References ==\n\n\n== External links ==\nnycsubway.org: R110A\nNew York City Transit Authority brochure Tommorow's Train Today 1992", 
                "titleUrl": "https://en.wikipedia.org/wiki/R110A_(New_York_City_Subway_car)", 
                "title": "R110A (New York City Subway car)"
            }, 
            {
                "snippet": "Remanufacture may refer to: Remanufacturing, the process of reconditioning products to sound working condition Remanufacture - Cloning Technology, an album", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Remanufacture", 
                "title": "Remanufacture"
            }, 
            {
                "snippet": "first seven rounds of the competition to participate in the All Blacks reconditioning programme. Following the 2007 Super 14 he was selected for the All Blacks", 
                "pageCategories": "1985 births\nAll Wikipedia articles written in New Zealand English\nAll stub articles\nChiefs (rugby union) players\nExpatriate rugby union players in Italy\nLiving people\nNew Zealand expatriate rugby union players\nNew Zealand expatriates in Italy\nNew Zealand international rugby union players\nNew Zealand rugby union biography stubs", 
                "pageContent": "Brendon Leonard (born 16 April 1985) is a New Zealand rugby union footballer.\nLeonard plays half-back and made his provincial debut for Waikato in the 2005 National Provincial Championship. He was in the Chiefs wider training group for the 2006 Super 14. After playing for Waikato in their championship winning 2006 Air New Zealand Cup side, Leonard was selected for the Chiefs in the 2007 Super 14. Leonard had a successful Super 14 where he played for Byron Kelleher who missed the first seven rounds of the competition to participate in the All Blacks reconditioning programme. Following the 2007 Super 14 he was selected for the All Blacks squad for the 2007 mid-year Tests and 2007 Tri-Nations. He was one of only two uncapped players named in the squad, and All Blacks coach Graham Henry said of Leonard's selection \"Brendon [Leonard] has been the best attacking scrum-half in the Super 14 this season and has forced his way into the side based on form.\" He made his Test debut against France at Eden Park on 2 June 2007. He came on as a substitute for Piri Weepu in the 64th minute of the All Blacks 42-11 victory. He then scored his first Test try against South Africa on 14 July 2007. On 12 August 2013, Leonard left New Zealand as he signed for new Italy franchise Zebre in the Pro 12 from the 2013-14 season. On 22 January it was announced Leonard had signed for the Ospreys, mainly to provide experienced cover for Welsh international Rhys Webb during the 2015 Rugby World Cup.\n\n\n== References ==\n\n\n== External links ==\n\"Brendon Leonard\". chiefs.co.nz. Retrieved 2007-05-26. \n\"Brendon Leonard\". Waikato Rugby Union. Retrieved 2007-05-26. \nBrendon Leonard at AllBlacks.com", 
                "titleUrl": "https://en.wikipedia.org/wiki/Brendon_Leonard", 
                "title": "Brendon Leonard"
            }, 
            {
                "snippet": "efficient and sought alternative methods such as reconditioning or remanufacturing.  Reconditioning is a type of precycling that requires the rebuilding", 
                "pageCategories": "Pages using citations with accessdate and no URL\nPages using web citations with no URL\nRecycling\nWaste management concepts\nWaste reduction", 
                "pageContent": "Precycling is the practice of reducing waste by attempting to avoid bringing items which will generate waste into home or business. The U.S. Environmental Protection Agency (EPA) also cites that precycling is the preferred method of integrated solid waste management because it cuts waste at its source and therefore trash is eliminated before it is created. According to the EPA, precycling is also characterized as a decision-making process on the behalf of the consumer because it involves making informed judgments regarding a product\u2019s waste implications. The implications that are taken into consideration by the consumer include: whether a product is reusable, durable, or repairable; made from renewable or non-renewable resources; over-packaged; and whether or not the container is reusable.\n\n\n== About ==\nPrecycling has the ability to build industrial, social, environmental, and economic circumstances that allow for old products to be converted into new resources \nIndustrial: increasing the independence from accumulative substances, such as heavy metals, fossil fuels, synthetics, etc.\nEconomic: create a circular economy\nEcological/environmental: allowance for more extensive and diverse natural habitats where the resources are returned to nature\nSocietal: extend the capacity of precycling to meet everyone\u2019s needs\nThe concept of \u2018precycling\u2019 was coined in 1988 by social marketing executive Maureen O\u2019Rorke in a public waste education campaign for the City of Berkeley.  The application of precycling is not limited to large corporations, but can be administered on smaller scales in local communities. The reason precycling is effective on large scales and on small scales stems from the idea that it shares a common language between experts and non-experts, buyers and sellers, economists and environmentalists. However, it is important to consider that waste prevention systems, such as precycling, require the collaborative effort from several working parts. These parts include prevention targets, producer responsibility, householder charging, funding for pilot projects, public involvement, engagement of private and third sectors, and public campaigns that spread awareness.\n\n\n== Integration of waste management ==\nThe original three-pronged push for trash management is \"Reduce, Reuse, Recycle.\" Precycling emphasizes \"reducing and reusing\", while harnessing and questioning the momentum and popularity of the term \"recycle.\" In addition to this three-pronged waste management strategy of \u201cReduce, Reuse, Recycle\u201d, precycling incorporates four supplementary R\u2019s: Repair, Recondition, Remanufacture and Refuse. Waste is a resource that can be reused, recycled, recovered, or treated. Precycling differs from other singular forms of waste prevention because it encompasses not one behavior, but many.\n\n\n=== Reduce ===\nReduce is a form of precycling that allows for the preservation of natural resources and also saves money on behalf of the manufacturer, the consumer, and the waste manager. Moreover, effective source reduction slows the depletion of environmental resources, prolongs the life of waste management facilities, and makes combustion and landfills safer by removing toxic waste components.\n\n\n=== Reuse ===\nReuse is a form of precycling that reinvents items after their initial life and avoids creating additional waste.\n\n\n=== Recycle ===\nAlthough precycling harnesses the familiarity of the term recycling, it is still important to note the difference between recycling and prevention. Since precycling focuses on the prevention of waste production, this entails that measures are taken before a substance, material, or product has become waste. Whereas recycling is a type of precycling that involves taking action before existing waste is abandoned to nature. Recycling is a process where discarded materials are collected, sorted, processed, and used in the production of new products. Every time a person engages in the act of recycling, they help increase the market and bring the cost down. However, current research from the American Plastics Council states that only 25% of the nation\u2019s recycling capabilities are being utilized.\nTraditionally recycling requires large amounts of energy to \"melt down\" and then re-manufacture items. While this may cut down on the amount of trash that is going into landfills, it is not sustainable unless the underlying energy supply is sustainable. In addition, recycling often means downcycling and always involves at least some loss of the original material, so primary extraction is still required to make up the difference. Precycling reduces these problems by using less material in the first place, so less has to be recycled.\n\n\n=== Repairing ===\nRepair is a type of precycling that corrects specified faults in a product, however the quality of a repaired product is inferior to reconditioned or remanufactured items. One survey found that 68% of the respondents believed repairing was not cost efficient and sought alternative methods such as reconditioning or remanufacturing.\n\n\n=== Reconditioning ===\nReconditioning is a type of precycling that requires the rebuilding of major components to restore a product\u2019s working condition, which is expected to be inferior to the original product.\n\n\n=== Remanufacturing ===\nRemanufacturing is another type of precycling that involves the greatest degree of work content, which results in superior quality of the product. In order to remanufacture a product, it requires a total dismantling of the product and the subsequent restoration and replacement of its parts. Remanufacturing is a preferred method of waste reduction compared to repairing and reconditioning because it preserves the embodied energy that has been used to shape the components of a product for their first life and it only requires 20-25% of the initial energy used in formation.\n\n\n=== Refuse ===\nRefusal to buy certain products due to detrimental impacts on the environment or wasteful packaging is another type of precycling because the rejection of such items paves the way for products that can be reduced, reused, or recycled.\n\n\n== Zero-waste strategy ==\nA zero waste approach aims to prevent rather than just reduce accumulated waste. Zero-waste goes beyond recycling to include the whole system, which includes the flow of resources and waste through human society. This \u201cdesign principle\u201d works to maximize recycling, minimize waste, reduce consumption and ensures that products are reused, repaired or recycled back into nature or the market. This preventative approach is more manageable and effective than incremental approaches that focus on gradually reducing the amount of impact because it is less complex and contains less information, which permits wider public participation.\n\n\n=== Sustainability ===\nIn regards to sustainability, the term itself is often associated with resource constraints and maintenance of the status quo rather than growth and prosperity. However, with the implementation of a zero-waste management strategy, sustainable practices can push the status quo in order to create a society that is capable of development, technically and culturally advanced, dynamic in population and production, thoughtful with the use of non-renewable resources, and diverse, democratic, and challenging.\n\n\n=== Economic effects ===\nIncreased waste production is often negatively associated with increased economic growth. However, a zero-waste management strategy allows for economic growth that works cohesively with sustainability rather than against it. The implementation of a zero-waste strategy is part of an economic goal-set that aims to create a circular economy. A circular economy refers to a closed-loop socio-economic system that focuses on minimizing wastes while simultaneously maximizing stocks of resources for the economy. This closed-loop design diverts linear (open-loop) waste disposal streams into new raw material streams.\nIn a circular economy, one way to minimize waste is through the employment of precycling insurance, which allows for a full range of financed waste prevention opportunities. This type of insurance would set premiums related to the risk of a product ending up as waste, and these premiums would serve to fund actions concerning waste prevention. When establishing a premium for precycling insurance several factors need to calculated: recyclability or biodegradability; provision of infrastructure, habitat or collaborations for the generation of the product from new resources; the ecosystem concentrations of product components above natural levels. The idea of precycling insurance is plausible considering the aim of insurance industries is to avoid losses rather than paying for losses. However, in order for this idea to work, private and third sectors need to be involved and engaged in the issue. In this instance, a third sector refers to small charities and a handful of societal enterprises that coordinate with charity shops.\n\n\n=== Environmental effects ===\nAccording to the \u201cExtended Producer Responsibility\u201d principle, impacts are substantially determined at the point of design where key decisions are made on materials, production process, and how products are used and disposed of at the end of life-cycle, which falls on the producer. However, in a circular economy there is the recognition that nature\u2019s capacity needs to be maximized through the reprocess of biodegradable wastes produced by industries and human activity. This task is accomplished through the procurement and funding of precycling insurance premiums that invest in systematic preservation of endangered habitats, careful harvesting of biological resources and expansions of productive ecosystems. Additionally, in terms of climate change, precycling insurance offers a flexible alternative to the binding limits on greenhouse gas emissions and international taxation on mineral fuels. In terms of waste management systems, the environment benefits from the reparation of products to the greatest degree because less energy is required and the majority of the original material is kept intact.\n\n\n=== Societal effects ===\nThe social structure operating under a circular economy is referred to as a circular society. The aim of a circular society is to create a cooperative culture by means of problem-prevention, resource-availability and fuller participation, with reference to precycling. One critique of this approach, in terms of waste management, is that It is difficult to maintain a cooperative culture within a society because they it is constantly evolving and changing.\n\n\n== Raising awareness ==\nThere is an increasing public awareness on the need for sustainable production and consumption. One campaign that aimed at raising awareness of precycling focused on whether people\u2019s self-reported behaviors were affected by exposure to precycling advertisements on the radio, television, or in-store flyers. The researchers concluded that the most effective results stemmed from the inclusion of social rewards that invoke an intrinsic motivation to engage in precycling behaviors.\nAnother way to raise awareness is through statistics that highlight the potential impacts that can be achieved through waste prevention. For instance, if 70 million Americans bought a half-gallon container of milk each week (instead of two quarts), then 41.6 million pounds of paper discards and 5.7 pounds of plastic discards would be reduced annually. This transition from two quarts to a half-gallon would save $145.6 million on packaging each year.\n\n\n== Implementation ==\nIn order to effectively implement precycling practices and behaviors, the public needs to feel \"enabled\", \"engaged\", \"encouraged\", and \"exemplified\" in their efforts to partake in precycling.\nNot only can the average consumer practice precycling, but industries can also participate. Purchasing from parts suppliers, reuse of chemicals, and reduction of unnecessary packaging are some methods. There are some companies and countries that have taken it upon themselves to implement more sustainable practices that align with precycling principles. For instance, Fonterra reduced its packaging through the implementation of bulking, reuse and redesign. Further, Waste Management New Zealand created Recycle New Zealand, which provided a subsidiary focusing on the collection of materials that could be diverted and sorted prior to the operations of reducing, recycling, or recovering. Moving forward, free-trade organizations can further implement precycling practices by exploring this strategy as a new way to reduce regulations and to promote greater industrial freedom of choice.\nMoreover, the individual consumer can develop precycling habits by engaging in the following practices and behaviors:\n\n\n=== \"Enviro-shopping\" ===\nEnviro-shopping is considered shopping with the environment and implements a precycling strategy:\nBringing one's own grocery bag or bring old ones back to the store\nBuying packages with the least amount of packaging\nBuying in bulk, but not buying more than one will use\nLooking for products with reusable dishes\n\n\n=== Product selection ===\nProducts to choose from in accordance with precycling principles:\nPlastic milk jugs or glass milk containers (no cartons)\nFresh fruit and vegetables\nConcentrated products that involves less packaging\nRecycled products\nRechargeable batteries\n\n\n=== Behaviors ===\nIn addition to shopping practices that implement precycling principles, there are also behaviors that can be undertaken to prevent waste:\nHome composting\nAvoid junk mail\nBuy second-hand\nOne way to participate in precycling is to carry a \"precycling kit\". Include a Tupperware or non-disposable container, silverware set, a cloth napkin or handkerchief, and a thermos or water-bottle within a cloth bag that can double as a grocery/shopping bag.\n\n\n== References ==\n\"Precycle: Reducing Waste Right From the Start\". Archived from the original on March 13, 2006. Retrieved October 20, 2006.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Precycling", 
                "title": "Precycling"
            }, 
            {
                "snippet": "widened to accommodate two cantilever footpaths. Further widening and reconditioning of the bridge took place in 1933, including the addition of four new", 
                "pageCategories": "1884 establishments in Australia\nAll stub articles\nAustralian bridge (structure) stubs\nBridges in Melbourne\nCoordinates on Wikidata\nCrossings of the Yarra River\nVictoria (Australia) building and structure stubs\nVictorian Heritage Register", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Victoria_Bridge,_Melbourne", 
                "title": "Victoria Bridge, Melbourne"
            }, 
            {
                "snippet": "construction of Crossrail, by Vinci, the contractor responsible for the reconditioning of the Connaught tunnel. Despite talk of constructing a replacement", 
                "pageCategories": "All articles needing additional references\nArticles needing additional references from July 2011\nCoordinates on Wikidata\nDisused railway stations in Newham\nFormer Great Eastern Railway stations\nRailway stations closed in 2006\nRailway stations opened in 1863", 
                "pageContent": "Silvertown railway station was on the North London Line (NLL) serving the Silvertown area of east London, until the station and the eastern section of the line it was on were closed in 2006. It was situated between Custom House (now a Docklands Light Railway station) and North Woolwich, the eastern terminus of the line.\nSilvertown was opened in 1863 by the Great Eastern Railway, on the route of the former Eastern Counties and Thames Junction Railway, with two tracks and platforms. A decline in use of the line led to the removal of one of the two tracks in 1980, leaving just one platform to serve trains in both directions. North Woolwich was also reduced to one platform. After third rail electrification of the line in 1986 by British Rail, services were increased and the single-track section became a bottleneck. Prior to closure, the typical Monday to Saturday service frequency westbound towards Stratford and eastbound towards North Woolwich was one train every 30 minutes during the daytime, increasing to one every 20 minutes in the evening; one train called every 30 minutes all day on Sundays.\nAfter London City Airport opened nearby there was an attempt to offer the station for connection to the airport, the station being renamed Silvertown and London City Airport on 4 October 1987, but the walk through adjacent side streets, and the relatively infrequent service, which was peripheral rather than into central London, led to little usage of the airport interchange.\nThe Docklands Light Railway Woolwich Arsenal branch now provides a substitute service for much of this part of the NLL; the nearest DLR station to the disused Silvertown station is London City Airport.\nThe remaining station buildings and platforms were demolished in 2012 as part of the construction of Crossrail, by Vinci, the contractor responsible for the reconditioning of the Connaught tunnel. Despite talk of constructing a replacement station nearby, this has not been provided for in the Crossrail Act 2008. Nevertheless, passive provision will be made for a station shortly to the east in the event of development of nearby properties.\nCurrently, there are proposals from London City Airport to fund the construction of a \u00a350 million Crossrail station so as to serve London City Airport. However, Transport for London require a formal feasibility study from City Airport to be completed. This is so that progress of the Crossrail project would not be affected by station's construction.\n\n\n== Gallery ==\n\n\n== References ==\n\n\n== External links ==\nDisused stations in the UK - Silvertown", 
                "titleUrl": "https://en.wikipedia.org/wiki/Silvertown_railway_station", 
                "title": "Silvertown railway station"
            }
        ], 
        "phraseCharStart": "262"
    }, 
    {
        "phraseCharEnd": "313", 
        "phraseIndex": "T7", 
        "phraseGoldStandardTag": "Task", 
        "phrase": "plasma operation", 
        "wikiSearchResults": [
            {
                "snippet": "current drive in plasma. The operation of tokamak restarted with a D-shaped vacuum vessel in 1992. The operation mode with high plasma confinement (H-mode)", 
                "pageCategories": "All NPOV disputes\nCommons category without a link on Wikidata\nFusion power\nNPOV disputes from October 2016\nTokamaks", 
                "pageContent": "Tokamak COMPASS (COMPact ASSembly)[1][2] is the main experimental facility of Tokamak department of Institute of Plasma Physics[3] of the Academy of Sciences of the Czech Republic since 2006. It was designed in the 1980s in the British Culham Science Centre as a flexible research facility dedicated mostly to plasma physics studies in circular and D shaped plasmas.\nThe first plasma in COMPASS \"broke down\" in 1989 in a C-shaped vacuum vessel, i.e., in a simpler vessel with a circular cross-section. Pioneering experiments followed, including for example the ITER-relevant tests of magnetic field correction with saddle coils for RMPs experiment (Resonant magnetic perturbations) or experiments with non-inductive current drive in plasma.\nThe operation of tokamak restarted with a D-shaped vacuum vessel in 1992. The operation mode with high plasma confinement (H-mode) was achieved, which represents a reference operation (\"standard scenario\") for the ITER tokamak. The COMPASS tokamak with its size (major radius 0.6 m and height of the vessel approx. 0.7 m) ranks to smaller tokamaks capable of the H-mode operation. Importantly, due to its size and shape the COMPASS plasmas correspond to one tenth (in the linear scale) of the ITER plasmas. At present, besides COMPASS there are only two operational tokamaks in Europe with ITER-like configuration capable of regime with the high plasma confinement. It is the Joint European Torus (JET) and the German tokamak ASDEX Upgrade (Institut f\u00fcr Plasmaphysik, Garching, Germany). JET is the biggest experimental device of this type in the world.\nIn 2002, British scientists started alternative research on larger, spherical tokamak MAST. Operation of COMPASS was discontinued due to insufficient resources for operation of both tokamaks, however, the research program foreseen for the latter tokamak was not concluded. Due to its important and not completely realised opportunities - and, in particular, due to its direct relevance to the ITER project - the facility was offered for free by the European Commission and UKAEA to the Institute of Plasma Physics AS CR in Prague in autumn 2004.\nThe Prague institute has been coordinating research in thermonuclear fusion in the Czech Republic in the framework of EURATOM since 1999. Team of physicists from the institute has a long-time experience in this field of research including operation of a small tokamak CASTOR. The European Commission has declared that the institute is fully competent to operate the tokamak COMPASS. \n\n\n== Parameters of the tokamak COMPASS ==\nMovie: COMPASS discharge using fast - visible camera: [4]\n\n\n== References ==\n\n\n== See also ==\nList of fusion experiments\nELM (Edge Localized Mode)\nBall-pen probe\nLangmuir probe\nThomson scattering\nResonant magnetic perturbations\n\n\n== External links ==\nMagnetic fusion in the Czech Republic\nDiagnostic system on COMPASS", 
                "titleUrl": "https://en.wikipedia.org/wiki/COMPASS_tokamak", 
                "title": "COMPASS tokamak"
            }, 
            {
                "snippet": "Funds. Operations included a research unit focused on metabolic diseases, a process development unit for protein drugs and a plasma product operation. In", 
                "pageCategories": "2001 establishments in Sweden\nCompanies based in Stockholm\nCompanies established in 2001\nCompanies listed on the Stockholm Stock Exchange\nCompanies related to the Wallenberg family\nPharmaceutical companies of Sweden", 
                "pageContent": "Swedish Orphan Biovitrum AB (publ) (Sobi) is an international specialty healthcare company dedicated to rare diseases, based in Stockholm, Sweden.\n\n\n== History ==\nSobi has been involved in the process development and manufacturing of recombinant protein drugs since the technology was first developed around 30 years ago, then as part of KabiVitrum.\nBiovitrum was formed in 2001 through the merger of several units of Pharmacia (now Pfizer) and spun off to a consortium of investors led by Nordic Capital and MPM Capital Funds. Operations included a research unit focused on metabolic diseases, a process development unit for protein drugs and a plasma product operation. In 2002 Sobi sold its plasma operation to Octapharma as part of efforts to concentrate operations on protein-based and small molecular drugs.\nIn 2004, Biovitrum started to manufacture the active protein component for Wyeth\u2019s (now Pfizer\u2019s) ReFacto\u00ae and ReFacto/Xynta\u00ae drugs for treatment of hemophilia, and marketing of specialty pharmaceuticals (ReFacto, Mimpara and Kineret\u00ae) was initiated in the Nordic region. In 2005, the research and development portfolio was expanded through the acquisition of Arexis, a Swedish biotech company, and the following year a partnership was formed with Syntonix (subsequently Biogen Idec) to jointly develop a drug for hemophilia B, a long-lasting recombinant factor IX Fc fusion protein candidate, rFIXFc. This partnership was extended the following year to also include the development of a long-lasting recombinant factor VIII Fc fusion protein candidate, rFVIIIFc, for the treatment of hemophilia A.\nIn 2008, an agreement with Amgen regarding the acquisition of the products Kepivance\u00ae and Stemgen\u00ae as well as a global license for Kineret was signed\nIn 2009, Sobi and partner Biogen Idec took the decision to enter final registration studies for the recombinant factor FIXFc. The company also received positive data regarding its Kiobrina\u00ae phase II program, an investigational enzyme replacement therapy to improve growth in preterm infants who receive pasteurized breast milk or infant formula.\nIn 2010, Biovitrum acquired Swedish Orphan International Holding AB, a pioneer in orphan drugs, and Swedish Orphan Biovitrum AB (publ) was formed. In addition, the decisions to advance both hemophilia projects as well as Kiobrina into phase III were taken. The following year the first patient was enrolled in the phase 3 study for Kiobrina and data from the rFVIIIFc hemophilia phase I/II study were presented showing an approximately 1.7-fold increase in half-life compared with Advate\u00ae. The company also established a US subsidiary.\nIn 2012 the supply agreement with Pfizer for ReFacto/Xyntha was extended until 2020 in addition to the agreement to return the co-promotion rights for the Nordic region for ReFacto to Pfizer for a payment of USD 47.4 M. The same year, the Sobi and partner Biogen Idec initiated global pediatric clinical trials of their long-lasting hemophilia A and B product candidates.\n\n\n== Company ==\nSobi is an international specialty healthcare company dedicated to rare diseases. The company's mission is to develop and deliver innovative therapies and services to improve the lives of patients. The product portfolio is primarily focused on Inflammation and Genetic diseases, with three late stage biological development projects within Haemophilia and Neonatology. The company also market a portfolio of specialty and rare disease products for partner companies. Sobi is a pioneer in biotechnology with world-class capabilities in protein biochemistry and biologics manufacturing. In 2013, Sobi had total revenues of SEK 2.2 billion (\u20ac253 M) and about 550 employees. The share (STO: SOBI) is listed on NASDAQ OMX Stockholm.\n\n\n== External links ==\nSobi website\nSobi Partner Products website\n\n\n== References ==\nComment on Biovitrum IPO, 2006-09-04", 
                "titleUrl": "https://en.wikipedia.org/wiki/Swedish_Orphan_Biovitrum", 
                "title": "Swedish Orphan Biovitrum"
            }, 
            {
                "snippet": "with Plasma globe. Plasma lamps are a type of gas discharge lamp energized by radio frequency (RF) power. They are distinct from the novelty plasma lamps", 
                "pageCategories": "Gas discharge lamps\nPlasma physics\nTypes of lamp", 
                "pageContent": "Plasma lamps are a type of gas discharge lamp energized by radio frequency (RF) power. They are distinct from the novelty plasma lamps that were popular in the 1980s.\nThe internal-electrodeless lamp was invented by Tesla after his experimentation with high-frequency currents in evacuated glass tubes for the purposes of lighting and the study of high voltage phenomena. The first practical plasma lamps were the sulfur lamps manufactured by Fusion Lighting. This lamp suffered a number of practical problems and did not prosper commercially. These problems have gradually been overcome by manufacturers such as Ceravision and Luxim, and high-efficiency plasma (HEP) lamps have been introduced to the general lighting market. Plasma lamps with an internal phosphor coating are called external electrode fluorescent lamps (EEFL); these external electrodes or terminal conductors provide the radio frequency electric field.\n\n\n== Description ==\nModern plasma lamps are a family of light sources that generate light by exciting plasma inside a closed transparent burner or bulb using radio frequency (RF) power. Typically, such lamps use a noble gas or a mixture of these gases and additional materials such as metal halides, sodium, mercury or sulfur. In modern plasma lamps, a waveguide is used to constrain and focus the electrical field into the plasma. In operation, the gas is ionized, and free electrons, accelerated by the electrical field, collide with gas and metal atoms. Some atomic electrons circling around the gas and metal atoms are excited by these collisions, bringing them to a higher energy state. When the electron falls back to its original state, it emits a photon, resulting in visible light or ultraviolet radiation, depending on the fill materials.\nThe first commercial plasma lamp was an ultraviolet curing lamp with a bulb filled with argon and mercury vapor developed by Fusion UV. That lamp led Fusion Lighting to the development of the sulfur lamp, a bulb filled with argon and sulfur that is bombarded with microwaves through a hollow waveguide. The bulb had to be spun rapidly to prevent it burning through. Fusion Lighting did not prosper commercially, but other manufacturers, such as LG Group, continue to pursue sulfur lamps. Sulfur lamps, though relatively efficient, have had a number of problems, chiefly:\nLimited life \u2013 Magnetrons had limited lives.\nLarge size\nHeat \u2013 The sulfur burnt through the bulb wall unless they were rotated rapidly.\nHigh power demand \u2013 They could not sustain a plasma in powers under 1000W.\n\n\n=== Limited life ===\nIn the past, the life of the plasma lamps was limited by the magnetron used to generate the microwaves. Solid state RF chips can be used and give long lives. However, using solid-state chips to generate RF is currently an order of magnitude more expensive than using a magnetron and so only appropriate for high-value lighting niches. It has recently been shown by Dipolar [1] of Sweden to be possible to extend the life of magnetrons to over 40,000 hours, making low-cost plasma lamps possible.\n\n\n=== Size ===\nIn the year 2000, a system was developed that concentrated radio frequency waves into a dielectric waveguide made of ceramic, which energized light-emitting plasma in a bulb positioned inside. This system, for the first time, permitted an extremely compact yet bright electrode-less lamp. The invention has been a matter of dispute. Claimed by Frederick Espiau, Chandrashekhar Joshi and Yian Chang, these claims were disputed by Ceravision Limited. Recently, a number of the core patents have been assigned to Ceravision.\n\n\n=== Heat and power ===\nThe use of a high-dielectric waveguide allowed the sustaining of plasmas at much lower powers\u2014down to 100 W in some instances. It also allowed the use of conventional gas-discharge lamp fill materials which removed the need to spin the bulb. The only issue with the ceramic waveguide was that much of the light generated by the plasma was trapped inside the opaque ceramic waveguide. In 2009, Ceravision introduced an optically clear quartz waveguide that appears to resolve this issue.\n\n\n== High-efficiency plasma (HEP) ==\nHigh-efficiency plasma lighting is the class of plasma lamps that have system efficiencies of 90 lumens per watt or more. Lamps in this class are potentially the most energy-efficient light source for outdoor, commercial and industrial lighting. This is due not only to their high system efficiency but also to the small light source they present enabling very high luminaire efficiency.\nLuminaire Efficacy Rating (LER) is the single figure of merit the National Electrical Manufacturers Association has defined to help address problems with lighting manufacturers' efficiency claims  and is designed to allow robust comparison between lighting types. It is given by the product of luminaire efficiency (EFF) times total rated lamp output in lumens (TLL) times ballast factor (BF), divided by the input power in watts (IP):\nLER = EFF \u00d7 TLL \u00d7 BF / IP\nThe \"system efficiency\" for a High Efficiency Plasma lamp is given by the last three variables, that is, it excludes the luminaire efficiency. Though plasma lamps do not have a ballast, they have an RF power supply that fulfills the equivalent function. In electrodeless lamps, the inclusion of the electrical losses, or \"ballast factor\", in lumens per watt claimed can be particularly significant as conversion of electrical power to radio frequency (RF) power can be a highly inefficient process.\nMany modern plasma lamps, such as those manufactured by Ceravision and Luxim, have very small light sources\u2014far smaller than HID bulbs or fluorescent tubes\u2014leading to much higher luminaire efficiencies also. High intensity discharge lamps have typical luminaire efficiencies of 55%, and fluorescent lamps of 70%. Plasma lamps typically have luminaire efficiencies exceeding 90%.\n\n\n== Producers ==\nCompanies producing or developing plasma lamps include Chameleon Plasma Grow Lighting, Hive Lighting, Ceravision, Luxim, Plasma International, i-Giant, Topanga USA, Solaronix and Lumartix[2].\nLuxim's Li-Fi lamp, claims 120 lumens per RF watt (i.e., before taking into account electrical losses). The lamp is used by Stray Light Optical Technologies in their commercial lighting fixtures. The lamp has been used in Robe lighting's ROBIN 300 Plasma Spot moving head light. It was also used in a line of, now discontinued, Panasonic rear projection TV.\nCeravision has introduced a combined lamp and luminaire under the trade name Alvara for use in high bay and street lighting applications. It uses an optically clear quartz waveguide with an integral burner allowing all the light from the plasma to be collected. The small source also allows the luminaire to utilize more than 90% of the available light, compared with 55% for typical high-intensity discharge fittings. Ceravision claims the highest luminaire efficacy rating of any light fitting on the market and to have created the first HEP lamp. Ceravision uses a magnetron to generate the required RF power and claims a life of 20,000 hours.\n\n\n== See also ==\nList of plasma (physics) articles\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Plasma_lamp", 
                "title": "Plasma lamp"
            }, 
            {
                "snippet": "the ITER complex began in 2007, and it is projected to begin plasma-generating operations in the 2020s. Cadarache also plays host to a number of research", 
                "pageCategories": "All Wikipedia articles in need of updating\nArticles with French-language external links\nBuildings and structures in Bouches-du-Rh\u00f4ne\nCoordinates on Wikidata\nFusion power\nMilitary nuclear reactors\nNuclear energy in France\nNuclear history of France\nNuclear reprocessing sites\nNuclear research institutes", 
                "pageContent": "The Cadarache facility is a French scientific research centre which specialises in nuclear power research. It is located in the commune of Saint-Paul-l\u00e8s-Durance, Bouches-du-Rh\u00f4ne, in the southern region of Provence-Alpes-C\u00f4te-d'Azur. Located approximately 60 kilometres (37 mi) north-east of the city of Marseille, Cadarache has been a nuclear research centre since President Charles de Gaulle launched France's atomic energy program in 1959. The centre is operated by the Commissariat \u00e0 l'\u00c9nergie Atomique (CEA, en: Atomic Energy Commission).\nIn 2005, Cadarache was selected to be the site of the International Thermonuclear Experimental Reactor (ITER), the world's largest nuclear fusion reactor. Construction of the ITER complex began in 2007, and it is projected to begin plasma-generating operations in the 2020s. Cadarache also plays host to a number of research reactors, such as the Jules Horowitz Reactor, which is expected to enter operation around 2016.\n\n\n== Facilities ==\n\nThe Cadarache facility is one of the largest nuclear research sites in Europe, hosting 21 fixed nuclear installations, including reactors, waste stockpiling and recycling facilities, and research centers. It employs over 4,500 people, and approximately 350 students and foreign collaborators carry out research in the facility\u2019s laboratories.\nOne of the most notable nuclear installations at Cadarache is the ITER experimental nuclear fusion tokamak, which is expected to be completed by 2020. When it becomes operational in the mid-to-late 2020s, ITER is hoped to be the first large-scale fusion reactor to produce more energy than is used to initiate its fusion reactions. Other nuclear installations at Cadarache include the Tore Supra tokamak \u2013 a predecessor to ITER \u2013 and the Jules Horowitz Reactor, a 100-megawatt research reactor which is planned to begin operation in 2016.\n\n\n== Activities ==\nNumerous nuclear research activities are conducted at Cadarache, including mixed-oxide fuel (MOX) production, nuclear propulsion and fission reactor prototyping, nuclear fusion research and research into new forms of fission fuel. Nuclear waste is also treated and recycled at the site.\n\n\n== Notable incidents ==\nA number of accidents, of varying severity, have occurred at Cadarache since its inception. Some of these incidents are listed below.\n31 March 1994: A sodium explosion took place while the Rapsodie experimental reactor was being dismantled. The explosion was classified as a Class 2 incident by the ASN.\n25 September 1998: A sodium fire occurred in a non-nuclear test facility, but caused no significant damage.\n2 November 2004: A fire broke out, but caused no radioactive contamination.\n6 November 2006: A fault in the equipment used to weigh MOX led to a grinder being loaded with more than the authorized amount 8 kg (18 lb) of fissile material, presenting the possible threat of a spontaneous nuclear reaction. Initially classified as a low-priority Class-1 level incident, it was subsequently revised to Class 2 by the ASN.\n1 October 2008 : A fire broke out in a non-nuclear installation.\n6 October 2009: A higher quantity of plutonium than authorized was uncovered in the Plutonium Technology Workshop. The ASN classified the incident as Class 2, and suspended dismantling work on the workshop. After investigation, it was revealed that the cause of the incident was the accumulation between 1966 and 2004 of fine plutonium dust in the 450 glove boxes in the workshop. Following the incident, further inspections revealed that another installation, STAR, also showed quantities of plutonium in excess of the authorized amounts. The incident was classified level 1 by the ASN.\n\n\n== Seismological risk ==\nCadarache is situated on the Aix-en-Provence-Durance seismological fault, and lies close to another fault, Tr\u00e9varesse. The Aix-Durance fault caused France's worst recorded earthquake in 1909. In a 2000 report, the ASN mandated the closure of six installations at Cadarache that did not meet aseismic construction standards; a similar report was issued by a French nuclear safety organisation in 1994. By 2010, three of these had been shut down, with the remaining three to be shut down by 2015.\n\n\n== See also ==\nNational Ignition Facility\nNuclear energy in France\n\n\n== References ==\n\n\n== External links ==\nCadarache ITER website (in French)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Cadarache", 
                "title": "Cadarache"
            }, 
            {
                "snippet": "new fundamental mode of plasma confinement -- enhanced reversed shear, to reduce plasma turbulence. 1997: end of plasma operations. In September 2002, staff", 
                "pageCategories": "All articles with unsourced statements\nAll stub articles\nArticles with unsourced statements from December 2015\nInterlanguage link template link number\nNuclear and atomic physics stubs\nTokamaks", 
                "pageContent": "The Tokamak Fusion Test Reactor (TFTR) was an experimental tokamak built at Princeton Plasma Physics Laboratory (in Princeton, New Jersey) circa 1980. Following on from the PDX (Poloidal Diverter Experiment) and PLT (Princeton Large Torus) devices, it was hoped that TFTR would finally achieve fusion energy break-even. Unfortunately, the TFTR never achieved this goal. However it did produce major advances in confinement time and energy density, which ultimately contributed to the knowledge base necessary to build ITER. TFTR operated from 1982 to 1997.\nTFTR was the world's first magnetic fusion device to perform extensive scientific experiments with plasmas composed of 50/50 deuterium/tritium (D-T), the fuel mix required for practical fusion power production, and also the first to produce more than 10 million watts of fusion power.\n\n\n== General ==\nIn nuclear fusion, there are two known types of reactors stable enough to conduct fusion: magnetic confinement reactors and inertial confinement reactors. The former method of fusion seeks to lengthen the time that ions spend close together in order to fuse them together, while the latter aims to fuse the ions so fast that they do not have time to move apart. Inertial confinement reactors, unlike magnetic confinement reactors, use laser fusion and ion-beam fusion in order to conduct fusion. However, with magnetic confinement reactors you avoid the problem of having to find a material that can withstand the high temperatures of nuclear fusion reactions.The heating current is induced by the changing magnetic fields in central induction coils and exceeds a million amperes. Magnetic fusion devices keep the hot plasma out of contact with the walls of its container by keeping it moving in circular or helical paths by means of the magnetic force on charged particles and by a centripetal force acting on the moving particles.\n\n\n== Results - History ==\nIn 1986 it produced the first 'supershots' which produced many more fusion neutrons.\nIn experiments conducted during July 1986, the TFTR achieved a plasma temperature of 200 million kelvin (200 MK). This temperature was the highest ever reached in a laboratory. The temperature is 10 times greater than the center of the sun, but more important, it is more than enough for breakeven, which is the point where fusion produce as much energy needed to be expended to ignite them. Besides temperature, break-even requires another criterion: the product of plasma density and confinement time, usually called the triple product.\nIn April 1986, TFTR experiments at lower temperatures produced a triple product of 1.5 x 7014100000000000000\u26601014 seconds per cubic centimeter, which is close to the goal for a practical reactor and five to seven times what is needed for break-even. However, the 200-MK experiments had a triple product of 7013100000000000000\u26601013, two or three times too small for break-even. The next step for the physicists working at TFTR was to put the high values together and get break-even. Donald Grove, TFTR project manager, said they expected to achieve that in 1987 using the hydrogen isotope deuterium, with which they had been working with so far. Then they intended to introduce another hydrogen isotope, tritium. Deuterium-tritium fusion, which most controlled fusion experiments today are trying to achieve, produces energetic neutrons, from which energy can easily be harvested and converted to useful things like steam or electric power. They hoped to achieve deuterium-tritium break-even in 1989.\nIn December, 1993, TFTR became the world's first magnetic fusion device to perform extensive experiments with plasmas composed of 50/50 deuterium/tritium. In 1994 it produced a then world-record of 10.7 megawatts of fusion power from a plasma composed of equal parts of deuterium and tritium (exceeded at JET in the UK, which generated 16MW for 22MW input in 1997, which is the current record). The two experiments had emphasized the alpha particles produced in the deuterium-tritium reactions. It was followed by the NSTX spherical tokamak.\nIn 1995, TFTR attained a world-record temperature of 510 million \u00b0C - more than 25 times that at the center of the sun. Also In 1995, TFTR scientists explored a new fundamental mode of plasma confinement -- enhanced reversed shear, to reduce plasma turbulence.\n1997: end of plasma operations.\nIn September 2002, staff completed the dismantling and removal of the Tokamak Fusion Test Reactor, which shut down in 1997 following 15 years of operation.\n\n\n== See also ==\n\nList of fusion experiments\n\n\n== References ==\n\n3. http://www.pppl.gov/Tokamak%20Fusion%20Test%20Reactor (\"In addition to meeting its physics objectives, TFTR achieved all of its hardware design goals\")\n\n\n== External links ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Tokamak_Fusion_Test_Reactor", 
                "title": "Tokamak Fusion Test Reactor"
            }, 
            {
                "snippet": "waveguide is used to constrain and focus the electrical field into the plasma. In operation the gas is ionized and free electrons, accelerated by the electrical", 
                "pageCategories": "All articles with dead external links\nAll articles with unsourced statements\nArticles with Wayback Machine links\nArticles with dead external links from June 2016\nArticles with unsourced statements from July 2012\nArticles with unsourced statements from May 2014\nArticles with unsourced statements from October 2015\nGas discharge lamps", 
                "pageContent": "The internal electrodeless lamp or induction light is a gas discharge lamp in which the power required to generate light is transferred from outside the lamp envelope to the gas inside via an electric or magnetic field, in contrast with a typical gas discharge lamp that uses internal electrodes connected to the power supply by conductors that pass through the lamp envelope. There are two advantages to elimination of the internal electrodes:\nExtended lamp life, because the internal electrodes are usually the limiting factor in lamp life.\nThe ability to use light-generating substances of higher efficiency that would react with internal metal electrodes in normal lamps.\nTwo systems are described below \u2013 plasma lamps, which use electrostatic induction to energize a bulb filled with sulfur vapor or metal halides, and fluorescent induction lamps, based upon a conventional fluorescent lamp bulb in which current is induced by an external coil of wire via electrodynamic induction.\n\n\n== History ==\n\nIn 1705, the scientist Francis Hauksbee demonstrated that in a rotating glass globe with internal vacuum like in a barometer, filled with mercury, and statical charged by holding a hand against the rotating globe, a light phenomenon occurred, so bright that one could read a paper.\nNikola Tesla demonstrated wireless transfer of power to electrodeless lamps in his lectures and articles in the 1890s, and subsequently patented a system of light and power distribution on those principles.\n\nIn 1967 and 1968, John Anderson of General Electric applied for patents for electrodeless lamps. In 1971, Fusion UV Systems installed a 300-watt electrodeless microwave plasma UV lamp on a Coors can production line. Philips introduced their QL induction lighting systems, operating at 2.65 MHz, in 1990 in Europe and in 1992 in the US. Matsushita had induction light systems available in 1992. Intersource Technologies also announced one in 1992, called the E-lamp. Operating at 13.6 MHz, it was to be available on the US market in 1993.\nIn 1990, Michael Ury, Charles Wood and colleagues formulated the concept of the sulphur lamp. With support from the United States Department of Energy, it was further developed in 1994 by Fusion Lighting of Rockville, Maryland, a spinoff of the Fusion UV division of Fusion Systems Corporation. Its origins are in microwave discharge light sources used for ultraviolet curing in the semiconductor and printing industries.\nSince 1994, General Electric has produced its induction lamp Genura with an integrated ballast, operating at 2.65 MHz. In 1996, Osram started selling their Endura induction light system, operating at 250 kHz. It is available in the US as the Sylvania Icetron. In 1997, PQL Lighting introduced in the US the Superior Life Brand induction lighting systems. Most induction lighting systems are rated for 100,000 hours of use before requiring absolute component replacements.\nIn 2005, Amko Solara in Taiwan introduced induction lamps that can dim and use IP (Internet Protocol) based controls. Their lamps have a range from 12 to 400 watts and operate at 250 kHz.\nFrom 1995, the former distributors of Fusion, Jenton / Jenact, expanded on the fact that energised UV-emitting plasmas act as lossy conductors to create a number of patents regarding electrodeless UV lamps for sterilising and germicidal uses.\nAround 2000, a system was developed that concentrated radio frequency waves into a solid dielectric waveguide made of ceramic which energized a light-emitting plasma in a bulb positioned inside. This system, for the first time, permitted an extremely bright and compact electrodeless lamp. The invention has been a matter of dispute. Claimed by Frederick Espiau (then of Luxim, now of Topanga Technologies), Chandrashekhar Joshi and Yian Chang, these claims were disputed by Ceravision Limited. A number of the core patents were assigned to Ceravision.\nIn 2006, Luxim introduced a projector lamp product trade-named LIFI. The company further extended the technology with light source products in instrument, entertainment, street, area and architectural lighting applications among others throughout 2007 and 2008.\nIn 2009, Ceravision Limited introduced the first High Efficiency Plasma (HEP) lamp under the trade name Alvara. This lamp replaces the opaque ceramic waveguide used in earlier lamps with an optically clear quartz waveguide giving greatly increased efficiency. In previous lamps, though the burner, or bulb, was very efficient, the opaque ceramic waveguide severely obstructed the collection of light. A quartz waveguide allows all of the light from the plasma to be collected.\nIn 2012, Topanga Technologies introduced a line of advanced plasma lamps (APL), driven by a solid state radio frequency (RF) driver, thereby circumventing the limited life of magnetron-based drivers, with system power of 127 and 230 watts and system efficacies of 96 and 87 lumen/watt, with a CRI of about 70.\n\n\n== Plasma lamps ==\n\nPlasma lamps are a family of light sources that generate light by exciting a plasma inside a closed transparent burner or bulb using radio frequency (RF) power. Typically, such lamps use a noble gas or a mixture of these gases and additional materials such as metal halides, sodium, mercury or sulfur. A waveguide is used to constrain and focus the electrical field into the plasma. In operation the gas is ionized and free electrons, accelerated by the electrical field, collide with gas and metal atoms. Some electrons circling around the gas and metal atoms are excited by these collisions, bringing them to a higher energy state. When the electron falls back to its original state, it emits a photon, resulting in visible light or ultraviolet radiation depending on the fill materials.\nThe first plasma lamp was an ultraviolet curing lamp with a bulb filled with argon and mercury vapor developed by Fusion UV. That lamp led Fusion Systems to the development of the sulfur lamp, a bulb filled with argon and sulfur which is bombarded with microwaves through a hollow waveguide.\nIn the past, the reliability of the technology was limited by the magnetron used to generate the microwaves. Solid state RF generation can be used and gives long life. However, using solid state chips to generate RF is approximately fifty times more expensive currently than using a magnetron and so only appropriate for high value lighting niches. It has recently been shown by Dipolar [1] of Sweden to be possible to greatly extend the life of magnetrons to over 40,000 hours making low cost plasma lamps possible. Plasma lamps are currently produced by Ceravision and Luxim and in development by Topanga Technologies.\nCeravision has introduced a combined lamp and luminaire under the trade name Alvara for use in high bay and street lighting applications. It uses an optically clear quartz waveguide with an integral burner allowing all the light from the plasma to be collected. The small source also allows the luminaire to utilize more than 90% of the available light compared with 55% for typical HID fittings. Ceravision claims the highest Luminaire Efficacy Rating (LER) of any light fitting on the market and to have created the first High Efficiency Plasma (HEP) lamp. Ceravision uses a magnetron to generate the required RF power and claim a life of 20,000 hours.\nLuxim's Li-Fi lamp, claims 120 lumens per RF watt (i.e. before taking into account electrical losses). The lamp has been used in Robe lighting's ROBIN 300 Plasma Spot moving headlight. It was also used in a line of, now discontinued, Panasonic rear projection TVs.\n\n\n== Magnetic induction lamps ==\n\nAside from the method of coupling energy into the mercury vapor, these lamps are very similar to conventional fluorescent lamps. Mercury vapor in the discharge vessel is electrically excited to produce short-wave ultraviolet light, which then excites internal phosphors to produce visible light. While still relatively unknown to the public, these lamps have been available since 1990. Unlike an incandescent lamp or conventional fluorescent lamps, there is no electrical connection going inside the glass bulb; the energy is transferred through the glass envelope solely by electromagnetic induction.\n\nThere are two main types of magnetic induction lamps: external core lamps and internal core lamps. The first commercially available and still widely used form of induction lamp is the internal core type. The external core type, which was commercialized later, has a wider range of applications and is available in round, rectangular and \"olive\" shaped form factors.\nExternal core lamps are basically fluorescent lamps with magnetic cores wrapped around a part of the discharge tube. The core is usually made of ferrite, a ceramic material containing iron oxide and other metals. In external core lamps, high frequency energy from a special power supply called an electronic ballast is sent through wires that are wrapped in a coil around a toroidal ferrite core placed around the outside of a portion of the glass tube, creating a high frequency magnetic field within the ferrite core. Since the magnetic permeability of the ferrite is hundreds or thousands of times higher than that of the surrounding air or glass, and the ferrite core provides a closed path for the magnetic field, virtually all of the magnetic field is contained inside the ferrite core. As shown in Faraday's law of induction, the time varying magnetic field in the core will generate a time varying electric voltage in any closed path that encloses the time varying magnetic field. The discharge tube forms one such closed path around the ferrite core, and in that manner the time varying magnetic field in the core generates a time varying electric field in the discharge tube, There is no need for the magnetic field to penetrate the discharge tube. The electric field generated by the time varying magnetic field drives the mercury-rare gas discharge in the same way the discharge is driven by the electric field in a conventional fluorescent lamp. The primary winding on the ferrite core, the core, and the discharge form a transformer, with the discharge being a one-turn secondary on that transformer.\nThe discharge tube contains a low pressure of a rare gas such as argon and mercury vapor. The mercury atoms are provided by a drop of liquid mercury or by a semi-solid amalgam of mercury and other metals such as bismuth, lead or tin. Some of the liquid mercury or the mercury in the amalgam vaporizes to provide the mercury vapor. The electric field ionizes some of the mercury atoms to produce free electrons, and then accelerates those free electrons. When the free electrons collide with mercury atoms, some of those atoms absorb energy from the electrons and are \u201cexcited\u201d to higher energy levels. After a short delay, the excited mercury atoms spontaneously relax to their original lower energy state and emit a UV photon with the excess energy. As in a conventional fluorescent tube, the UV photon diffuses through the gas to the inside of the outer bulb, and is absorbed by the phosphor coating that surface, transferring its energy to the phosphor. When the phosphor then relaxes to its original, lower energy state, it emits visible light. In this way the UV photon is down-converted to visible light by the phosphor coating on the inside of the tube. The glass walls of the lamp prevent the emission of the UV photons because ordinary glass blocks UV radiation at the 253.7 nm and shorter wavelengths.\nIn the internal core form (see diagram), a glass tube (B) protrudes bulb-wards from the bottom of the discharge vessel (A), forming a re-entrant cavity. This tube contains an antenna called a power coupler, which consists of a coil wound over a cylindrical ferrite core. The coil and ferrite forms the inductor which couples the energy into the lamp interior\nThe antenna coils receive electric power from the electronic ballast (C) that generates a high frequency. The exact frequency varies with lamp design, but popular examples include 13.6 MHz, 2.65 MHz and 250 kHz. A special resonant circuit in the ballast produces an initial high voltage on the coil to start a gas discharge; thereafter the voltage is reduced to normal running level.\nThe system can be seen as a type of transformer, with the power coupler (inductor) forming the primary coil and the gas discharge arc in the bulb forming the one-turn secondary coil and the load of the transformer. The ballast is connected to mains electricity, and is generally designed to operate on voltages between 100 and 277 VAC at a frequency of 50 or 60 Hz, or on a voltage between 100 and 400 VDC for battery fed emergency light systems. Many ballasts are available in low voltage models so can also be connected to DC voltage sources like batteries for emergency lighting purposes or for use with renewable energy (solar & wind) powered systems.\nIn other conventional gas discharge lamps, the electrodes are the part with the shortest life, limiting the lamp lifespan severely. Since an induction lamp has no electrodes, it can have a very long service life. For induction lamp systems with a separate ballast, the service life can be as long as 100,000 hours, which is 11.4 years continuous operation. For induction lamps with integrated ballast, the lifespan is in the 15,000 to 50,000 hours range. Extremely high-quality electronic circuits are needed for the ballast to attain such a long service life. Such lamps are typically used in commercial or industrial applications. Typically operations and maintenance costs are significantly lower with induction lighting systems due to their industry average 100,000 hour life cycle and five to ten year warranty.\n\n\n=== Advantages ===\n\nLong lifespan due to the lack of electrodes \u2013 Strictly speaking almost indefinite on the lamp itself but between 25,000 and 100,000 hours depending on lamp model and quality of electronics used, comparable to low quality LEDs of the 1970s;\nVery high energy conversion efficiency of between 62 and 90 Lumens/Watt [higher power lamps are more energy efficient];\nHigh power factor due to the low loss of the high frequency electronic ballasts which are typically between 95% and 98% efficient;\nMinimal Lumen depreciation (declining light output with age) compared to other lamp types as filament evaporation and depletion is absent;\n\"Instant-on\" and hot re-strike, unlike most HID lamps used in commercial-industrial lighting applications (such as mercury-vapor lamp, sodium-vapor lamp and metal halide lamp);\nEnvironmentally friendly as induction lamps use less energy, and use less mercury per hour of operation than conventional lighting due to their long lifespan. The mercury is in a solid form and can be easily recovered if the lamp is broken, or for recycling at end-of-life.\nThese benefits offer considerable cost savings of between 35% and 55% in energy and maintenance costs for induction lamps compared to other types of commercial and industrial lamps which they replace.\n\n\n=== Disadvantages ===\nSome models of internal inductor lamps that use high frequency ballasts can produce radio frequency interference (RFI) which interferes with radio communications in the area. Newer, external inductor type lamps use low frequency ballasts that usually have FCC or other certification, thus suggesting compliance with RFI regulations.\nSome types of inductor lamps contain mercury, which is highly toxic if released to the environment.\n\n\n== See also ==\nList of light sources\nInduction cooker\n\n\n== References ==\n\n\n== External links ==\nExamples of Electrodeless lamps", 
                "titleUrl": "https://en.wikipedia.org/wiki/Electrodeless_lamp", 
                "title": "Electrodeless lamp"
            }, 
            {
                "snippet": "jet, it does not need to be oxidized. This process differs from plasma cutting operations because in air carbon cutting, an open, or un-constricted, arc", 
                "pageCategories": "Metalworking", 
                "pageContent": "Air carbon arc cutting previously known as air arc cutting, is an arc cutting process where metal is cut and melted by the heat of a carbon arc. Molten metal is then removed by a blast of air. It employs a consumable carbon or graphite electrode to melt the material, which is then blown away by an air jet.\nThis process is useful for cutting a variety of materials, but it is most often used for cutting, and gouging aluminum, copper, iron, magnesium, and carbon and stainless steels. Because the metal is blown away by the air jet, it does not need to be oxidized. This process differs from plasma cutting operations because in air carbon cutting, an open, or un-constricted, arc is used, and the arc operates separately from the air jet.\nAir pressures for the jet usually vary from 60 to 100 psig. The carbon electrode can be worn away by oxidation from heat buildup. This can be reduced by coating the carbon electrodes with copper.\nThe sharpened carbon electrode is drawn along the metal, an arc forms and melts the metal. The air jet is then used to blow away molten material. This can be dangerous as the molten material can be blown substantial distances. The process is also very noisy.\n\n\n== See also ==\nGas cutting\nPlasma cutting\n\n\n== References ==\n\n\n== External links ==\nSweetHaven Publishing Services, \u201cFundamentals of Professional Engineering\u201d, 2001, [1] (April 17, 2008).\nhttp://www.bestplasmacutterreviews.com/wp-content/uploads/2015/12/89-250-008C.pdf", 
                "titleUrl": "https://en.wikipedia.org/wiki/Air_carbon_arc_cutting", 
                "title": "Air carbon arc cutting"
            }, 
            {
                "snippet": "GPS Attitude Determination and Micro-discharge Plasma Thruster Operation, and Amateur Radio Operations. In the first phase, the two nanosatellites will", 
                "pageCategories": "All Wikipedia articles in need of updating\nAll Wikipedia articles written in American English\nAll articles needing additional references\nArticles needing additional references from November 2010\nStudent satellites\nUse American English from January 2014\nWikipedia articles in need of updating from April 2016", 
                "pageContent": "Formation Autonomy Spacecraft with Thrust, Relnav, Attitude and Crosslink (or FASTRAC) is a pair of nanosatellites (respectively named Sara-Lily and Emma) developed and built by students at The University of Texas at Austin. The project is part of a program sponsored by the Air Force Research Laboratory (AFRL), whose goal is to lead the development of affordable space technology. The FASTRAC mission will specifically investigate technologies that facilitate the operation of multiple satellites in formation. These enabling technologies include relative navigation, cross-link communications, attitude determination, and thrust. Due to the high cost of lifting mass into orbit, there is a strong initiative to miniaturize the overall weight of spacecraft. The utilization of formations of satellites, in place of large single satellites, reduces the risk of single point failure and allows for the use of low-cost hardware.\nIn January 2005, the University of Texas won the University Nanosat-3 Program, a grant-based competition that included 12 other participating universities. As a winner, FASTRAC was given the opportunity to launch its satellites into space. The student-led team received $100,000 from AFRL for the competition portion of the project, and another $100,000 for the implementation phase. FASTRAC is the first student-developed satellite mission incorporating on-orbit real-time relative navigation, on-orbit real-time attitude determination using a single GPS antenna, and a micro-discharge plasma thruster.\nFASTRAC launched on 19 November 2010 aboard a Minotaur IV rocket from the Kodiak Launch Complex in Kodiak, Alaska. Separation of the satellites from each other and cross-link communication were successfully carried out.\nFASTRAC was developed under the US Air Force Research Laboratory University Nanosatellite Program, and was ranked number 32 in the Space Experiments Review Board's list of priortised spacecraft experiments in 2006. The spacecraft were expected to demonstrate Global Positioning System relative navigation and micro-charge thruster performance.\n\n\n== Operations ==\nThe main mission sequence is composed of six distinct phases: Launch, Launch Vehicle Separation, Initial Acquisition, GPS Onboard Relative Navigation, Onboard Single Antenna GPS Attitude Determination and Micro-discharge Plasma Thruster Operation, and Amateur Radio Operations. In the first phase, the two nanosatellites will be launched on the Department of Defense Space Test Program STP-S26 Mission from Kodiak Launch Complex (KLC) in Kodiak, Alaska. They will be transported to a 72 degree inclination circular low Earth orbit with an altitude of 650 km by a Minotaur IV rocket. Initially, the two nanosatellites will be in a stacked configuration. Once the rocket reaches the desired orbit, the satellites will be powered on by the launch vehicle before finally separating from the launch vehicle.\nThe third phase will begin once the two nanosatellites are ejected from the rocket. During this phase, there will be a 30-minute period where the satellites will go through a check out and initialization process. After this period, the satellites will begin transmitting beacon messages containing telemetry information that will help determine each satellite's status. During this phase the ground station will attempt to establish first contact with the satellites and perform a check out procedure to make sure all the subsystems on board are working correctly. It is expected that this checkout procedure will take several hours or even a few days depending on the duration of the communication passes with the ground station. Once the operators are satisfied with the status of the satellites, the satellites will be commanded from the ground to separate, finalizing the third phase of the mission.\nWhen the satellites have successfully separated, the primary mission will begin, signaling the start of fourth phase. First, the satellites will autonomously establish a cross-link, or in other words, they will communicate with each other through UHF/VHF bands. The satellites will then exchange GPS data through this cross-link in order to calculate on-orbit real time relative navigation solutions.\nThe fifth phase will activate a micro-discharge plasma thruster with a command from the ground that will autonomously operate when the thrusting vector is within a 15 degree cone of the anti-velocity vector. The thruster operation will be dependent on the on-orbit real-time single antenna GPS attitude determination solution. After this phase is over, a command from the ground station will disable the thruster on FASTRAC 1.\nThe final phase of the mission will start once the communication architecture of the satellites is reconfigured from the ground to work with the Automatic Packet Reporting System (APRS) network. This will make the satellites available to amateur radio users all around the world. Once the ground station loses all communication with the satellites, the mission will be terminated and the satellites will passively de-orbit, burning up in the atmosphere. The FASTRAC team has estimated that it will take six months to successfully achieve its mission objectives.\n\n\n== Subsystems ==\n\n\n=== Structure ===\nThe structure of the FASTRAC satellites is a hexagonal iso-grid design that is composed of two titanium adapter plates, aluminum 6061 T-6 side panels, six hollow outer columns with inserts and six inner columns. The mass of the two nanosatellites is approximately 127 lbs with all of the components included.\n\n\n=== Communication Architecture ===\nThe communications architecture is based on a system flown on PCSat2. The FASTRAC implementation consists of two receivers, one transmitter, a terminal node controller (TNC), a transmitter relay board, and a receiver relay board. On FASTRAC 1 \u201cSara Lily\u201d, two R-100 VHF receivers and one TA-451 UHF transmitter from Hamtronics are used. On FASTRAC 2 \u201cEmma\u201d, two R-451 UHF receivers and one TA-51 VHF transmitter from Hamtronics are used. The TNC used is a KPC-9612+ from Kantronics. Both the transmitter and receiver relay boards were designed and manufactured in house.\n\n\n=== Command & Data Handling ===\nThe command & data handling (C&DH) system is composed by four distributed AVRs which were developed by Santa Clara University. Each AVR has an Atmega 128 microcontroller and controls an individual subsystem on the satellite (i.e.: COM, EPS, GPS, and THR or IMU). The AVRs communicate with each other through the I2C bus.\n\n\n=== GPS Subsystem ===\nThe GPS position and attitude determination system was designed and built by student researchers at The University of Texas' GPS Research Lab. The system utilizes GPS code measurements, as well as antenna signal-to-noise ratio (SNR) and 3-axis magnetometer measurements to provide estimates of position, velocity, and attitude. Each satellite will have redundant ORION GPS receivers, dual cross-strapped antennas with RF switching and splitting hardware.\n\n\n=== Power System ===\nThe power system for each satellite is composed of eight solar panels, a VREG box, and a battery box. The battey box is made from black anodized aluminum and holds 10 Sanyo N4000-DRL D-cells provided to the team by AFRL. Both the solar panels and the VREG board were designed and made in-house. On each satellite, the VREG board distributes power from three VICOR VI-J00 voltage regulators, and also charges the batteries with the power collected from the solar panels.\n\n\n=== Separation System ===\nThere are two separation systems for the FASTRAC satellites, both designed and manufactured by Planetary Systems Corporation (PSC), which will be used to separate the satellites in their stacked configuration from the Launch Vehicle and then to separate the two satellites while they are in orbit. The PSC Lightband Separation System is composed of two spring-loaded rings and a motorized release mechanism.\n\n\n=== Micro-Discharge Plasma Thruster ===\nThe micro-discharge plasma thruster was designed and built at UT-Austin. The thruster channels and superheats an inert gas through a micro-channel nozzle producing a micro-Newton level of thrust. It uses a custom made composite tank from CTD. The operation of the thruster will be automated by the spacecraft C&DH using the attitude measurements provided by the GPS attitude determination system. After enabling the operation of the thruster from the ground, it will be only be active when one of the two nozzles is within a 15 degree cone of the anti-velocity vector. The thruster subsystem is only present on FASTRAC 1 \"Sara Lily\".\n\n\n=== Inertial Measurement Unit (IMU) ===\nOn FASTRAC 2 \u201cEmma\u201d, instead of using a thruster, an Inertial Measurement Unit (IMU) MASIMU01 from Micro Aerospace Solutions is used to measure the separation of the two satellites.\n\n\n== Amateur Radio Participation ==\nThe FASTRAC satellites transmit and receive data (GPS, Health, etc.) on amateur radio frequencies. All amateur radio operators are encouraged to downlink data from either satellite and upload the data to the radio operator section on the FASTRAC Website.\n\n\n=== Operation Frequencies ===\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/FASTRAC", 
                "title": "FASTRAC"
            }, 
            {
                "snippet": "thermal plasma technology, spurred by the needs of aerospace programs. Among the various methods of thermal plasma generation, induction plasma (or inductively", 
                "pageCategories": "Plasma physics", 
                "pageContent": "The 1960s were the incipient period of thermal plasma technology, spurred by the needs of aerospace programs. Among the various methods of thermal plasma generation, induction plasma (or inductively coupled plasma) takes up an important role.\nEarly attempts to maintain inductively coupled plasma on a stream of gas date back to Babat in 1947 and Reed in 1961. Effort was concentrated on the fundamental studies of energy coupling mechanism and the characteristics of the flow, temperature and concentration fields in plasma discharge. In 1980s, there was increasing interest in high-performance materials and other scientific issues, and in induction plasma for industrial-scale applications such as waste treatment. Numerous research and development were devoted to bridge the gap between the laboratory gadget and the industry integration. After decades\u2019 effort, induction plasma technology has gained a firm foothold in modern advanced industry.\n\n\n== The generation of induction plasma ==\nInduction heating is a mature technology with centuries of history. A conductive metallic piece, inside a coil of high frequency, will be \u201cinduced\u201d, and heated to the red-hot state. There is no difference in cardinal principle for either induction heating or \u201cinductively coupled plasma\u201d, only that the medium to induce, in the latter case, is replaced by the flowing gas, and the temperature obtained is extremely high, as it arrives the \"fourth state of the matter\u201d\u2014plasma.\n\nAn inductively coupled plasma (ICP) torch is essentially a copper coil of several turns, through which cooling water is running in order to dissipate the heat produced in operation. The coil wraps a confinement tube, inside which the induction plasma is generated. One end of the confinement tube is open; the plasma is actually maintained on a continuum gas flow. During induction plasma operation, the generator supplies an alternating current (ac) of radio frequency (r.f.) to the torch coil; this ac induces an alternating magnetic field inside the coil, after Amp\u00e8re\u2019s law (for a solenoid coil):\n\n  \n    \n      \n        \n          \u03d5\n          \n            B\n          \n        \n        =\n        (\n        \n          \u03bc\n          \n            0\n          \n        \n        \n        \n          I\n          \n            c\n          \n        \n        \n        N\n        )\n        (\n        \u03c0\n        \n        \n          r\n          \n            0\n          \n          \n            2\n          \n        \n        )\n        \n        (\n        1\n        )\n      \n    \n    {\\displaystyle \\phi _{B}=(\\mu _{0}\\,I_{c}\\,N)(\\pi \\,r_{0}^{2})\\quad (1)}\n  \nwhere, \n  \n    \n      \n        \n          \u03d5\n          \n            B\n          \n        \n      \n    \n    {\\displaystyle \\phi _{B}}\n   is the flux of magnetic field, \n  \n    \n      \n        \n          \u03bc\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\mu _{0}}\n   is permeability constant \n  \n    \n      \n        4\n        \u03c0\n        \n        \n          10\n          \n            \u2212\n            7\n          \n        \n        \n        \n          \n            Wb/A.m\n          \n        \n      \n    \n    {\\displaystyle 4\\pi \\,10^{-7}\\,{\\textrm {Wb/A.m}}}\n  , \n  \n    \n      \n        \n          I\n          \n            c\n          \n        \n      \n    \n    {\\displaystyle I_{c}}\n   is the coil current, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   is the number of coil turns per unit length, and \n  \n    \n      \n        \n          r\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle r_{0}}\n   is the mean radius of the coil turns.\nAccording to Faraday\u2019s Law, a variation in magnetic field flux will induce a voltage, or electromagnetic force:\n\n  \n    \n      \n        E\n        =\n        \u2212\n        N\n        (\n        \u0394\n        \n          \u03d5\n          \n            B\n          \n        \n        \n          /\n        \n        \u0394\n        t\n        )\n        \n        (\n        2\n        )\n      \n    \n    {\\displaystyle E=-N(\\Delta \\phi _{B}/\\Delta t)\\quad (2)}\n  \nwhere, \n  \n    \n      \n        N\n      \n    \n    {\\displaystyle N}\n   is the number of coil turns, and the item in parenthesis is the rate at which the flux is changing. The plasma is conductive (assuming a plasma already exists in the torch). This electromagnetic force, E, will in turn drive a current of density j in closed loops. The situation is much similar to heating a metal rod in the induction coil: energy transferred to the plasma is dissipated via Joule heating, j2R, from Ohm\u2019s Law, where R is the resistance of plasma.\nSince the plasma has a relatively high electrical conductivity, it is difficult for the alternating magnetic field to penetrate it, especially at very high frequencies. This phenomenon is usually described as the \u201cskin effect\u201d. The intuitive scenario is that the induced currents surrounding each magnetic line counteract each other, so that a net induced current is concentrated only near the periphery of plasma. It means the hottest part of plasma is off-axis. Therefore, the induction plasma is something like an \u201cannular shell\u201d. Observing on the axis of plasma, it looks like a bright \u201cbagel\u201d.\n\nIn practice, the ignition of plasma under low pressure conditions (<300 torr) is almost spontaneous, once the r.f. power imposed on the coil achieves a certain threshold value (depending on the torch configuration, gas flow rate etc.). The state of plasma gas (usually argon) will swiftly transit from glow-discharge to arc-break and create a stable induction plasma. For the case of atmospheric ambient pressure conditions, ignition is often accomplished with the aid of a Tesla coil, which produces high-frequency, high-voltage electric sparks that induce local arc-break inside the torch and stimulate a cascade of ionization of plasma gas, ultimately resulting in a stable plasma.\n\n\n== Induction plasma torch ==\nInduction plasma torch is the core of the induction plasma technology. Despite the existence of hundreds of different designs, an induction plasma torch consists of essentially three components:\n\ncoil The induction coil consists of several spiral turns, depending on the r.f. power source characteristics. Coil parameters including the coil diameter, number of coil turns, and radius of each turn, are specified in such a way to create an electrical \"tank circuit\" with proper electrical impedance. Coils are typically hollow along their cylindrical axis, filled with internal liquid cooling (e.g., de-ionized water) to mitigate high operating temperatures of the coils that result from the high electrical currents required during operation.\nconfinement tube This tube serves to confine the plasma. Quartz tube is the common implementation. The tube is often cooled either by compressed air (<10 kW) or cooling water. While the transparency of quartz tube is demanded in many laboratory applications (such as spectrum diagnostic), its relatively poor mechanical and thermal properties pose a risk to other parts (e.g., o-ring seals) that may be damaged under the intense radiation of high-temperature plasma. These constraints limit the use of quartz tubes to low power torches only (<30 kW). For industrial, high power plasma applications (30~250 kW), tubes made of ceramic materials are typically used. The ideal candidate material will possess good thermal conductivity and excellent thermal shock resistance. For the time being, silicon nitride (Si3N4) is the first choice. Torches of even greater power employ a metal wall cage for the plasma confinement tube, with engineering tradeoffs of lower power coupling efficiencies and increased risk of chemical interactions with the plasma gases.\ngas distributor Often called a torch head, this part is responsible for the introduction of different gas streams into the discharge zone. Generally, there are three gas lines passing to the torch head. According to their distance to the center of circle, these three gas streams are also arbitrarily named as Q1, Q2, and Q3.\nQ1 is the carrier gas that is usually introduced into the plasma torch through an injector at the center of the torch head. As the name indicates it, the function of Q1 is to convey the precursor (powders or liquid) into plasma. Argon is the usual carrier gas, however, many other reactive gases (i.e., oxygen, NH3, CH4, etc.) are often involved in the carrier gas, depending on the processing requirement.\nQ2 is the plasma forming gas, commonly called as the \u201cCentral Gas\u201d. In today\u2019s induction plasma torch design, it is almost unexceptional that the central gas is introduced into the torch chamber by tangentially swirling. The swirling gas stream is maintained by an internal tube that hoops the swirl till to the level of the first turn of induction coil. All these engineering concepts are aiming to create the proper flow pattern necessary to insure the stability of the gas discharge in the center of the coil region.\nQ3 is commonly referred to as \u201cSheath Gas\u201d that is introduced outside the internal tube mentioned above. The flow pattern of Q3 can be either vortex or straight. The function of sheath gas is twofold. It helps to stabilize the plasma discharge; most importantly, it protects the confinement tube, as a cooling medium.\nPlasma gases and plasma performance The minimum power to sustain an induction plasma depends on pressure, frequency and gas composition. The lower sustaining power setting is achieved with high r.f. frequency, low pressure, and monatomic gas, such as argon. Once diatomic gas is introduced into the plasma, the sustaining power would be drastically increased, because extra dissociation energy is required to break gaseous molecular bonds first, so then further excitation to plasma state is possible. The major reasons to use diatomic gases in plasma processing are (1) to get a plasma of high energy content and good thermal conductivity (see Table below), and (2) to conform the processing chemistry.\nIn practice, the selection of plasma gases in an induction plasma processing is first determined by the processing chemistry, i.e., if the processing requiring a reductive or oxidative, or other environment. Then suitable second gas may be selected and added to argon, so as to get a better heat transfer between plasma and the materials to treat. Ar-He, Ar-H2, Ar-N2, Ar-O2, Air, etc. mixture are very commonly used induction plasmas. Since the energy dissipation in the discharge takes places essentially in the outer annular shell of plasma, the second gas is usually introduced along with the sheath gas line, rather than the central gas line.\n\n\n== The industrial application of induction plasma technology ==\nFollowing the evolution of the induction plasma technology in laboratory, the major advantages of the induction plasma have been distinguished:\nWithout the erosion and contamination concern of electrode, due to the different plasma generation mechanism compared with other plasma method, for example, direct current non-transfer arc (dc) plasma.\nThe possibility of the axial feeding of precursors, being solid powders, or suspensions, liquids. This feature overcomes the difficulty of exposing materials to the high temperature of plasma, from the high viscosity of high temperature of plasma.\nBecause of non electrode problem, a wide versatile chemistry selection is possible, i.e., the torch could work in either reductive, or, oxidative, even corrosive conditions. With this capability, induction plasma torch often works as not only a high temperature, high enthalpy heat source, but also chemical reaction vessels.\nRelatively long residence time of precursor in the plasma plume (several milliseconds up to hundreds milliseconds), compared with dc plasma.\nRelatively large plasma volume.\nThese features of induction plasma technology, has found niche applications in industrial scale operation in the last decade. The successful industrial application of induction plasma process depends largely on many fundamental engineering supports. For example, the industrial plasma torch design, which allows high power level (50 to 600 kW) and long duration (three shifts of 8 hours/day) of plasma processing. Another example is the powder feeders that convey large quantity of solid precursor (1 to 30 kg/h) with reliable and precise delivery performance.\nNowadays, we have been in a position to be able to numerate many examples of the industrial applications of induction plasma technology, such as, powder spheroidisation, nanosized powders synthesis, induction plasma spraying, waste treatments, etc., However, the most impressive success of induction plasma technology is doubtless in the fields of spheroidisation and nano-materials synthesis.\n\n\n=== Powder spheroidisation ===\n\nThe requirement of powders spheroidisation (as well as densification) comes from very different industrial fields, from powder metallurgy to the electronic packaging. Generally speaking, the pressing need for an industrial process to turn to spherical powders is to seek at least one of the following benefits which result from the spheroidisation process:\nImprove the powders flow-ability.\nIncrease the powders packing density.\nEliminate powder internal cavities and fractures.\nChange the surface morphology of the particles.\nOther unique motive, such as optical reflection, chemical purity etc.\nSpheroidisation is a process of in-flight melting. The powder precursor of angular shape is introduced into induction plasma, and melted immediately in the high temperatures of plasma. The melted powder particles are assuming the spherical shape under the action of surface tension of liquid state. These droplets will be drastically cooled down when fly out of the plasma plume, because of the big temperature gradient exciting in the plasma. The condensed spheres are thus collected as the spheroidisation products.\nA great variety of ceramics, metals and metal alloys have been successfully spheroidized/densified using induction plasma spheroidisation. Following are some typical materials spheroidized on commercial scale.\nOxide ceramics: SiO2, ZrO2, YSZ, Al2TiO5, glass\nNon-oxides: WC, WC-Co, CaF2, TiN\nMetals: Re, Ta, Mo, W\nAlloys: Cr-Fe-C, Re-Mo, Re-W\n\n\n=== Nano-materials synthesis ===\nIt is the increased demand for nanopowders that promotes the extensive research and development of various techniques for nanometric powders. The challenges for an industrial application technology are productivity, quality controllability, and affordability. Induction plasma technology implements in-flight evaporation of precursor, even those raw materials of the highest boiling-point; operating under various atmospheres, permitting synthesis of a great variety of nanopowders, and thus become much more reliable and efficient technology for synthesis of nanopowders in both laboratory and industrial scales. Induction plasma used for nanopowder synthesis has many advantages over the alternative techniques, such as high purity, high flexibility, easy to scale-up, easy to operate and process control.\nIn the nano-synthesis process, material is first heated up to evaporation in induction plasma, and the vapours are subsequently subjected to a very rapid quenching in the quench/reaction zone, The quench gas can be inert gases such as Ar and N2 or reactive gases such as CH4 and NH3, depending on the type of nanopowders to be synthesized. The nanometric powders produced are usually collected by porous filters, which are installed away from the plasma reactor section. Because of the high reactivity of metal powders, special attention should be given to powder pacification prior to the removal of the collected powder from the filtration section of the process.\nThe induction plasma system has been successfully used in the synthesis nanopowders. The typical size range of the nano-particles produced is from 20 to 100 nm, depending on the quench conditions employed. The productivity varies from few hundreds g/h to 3~4 kg/h, according to the different materials' physical properties. A typical induction plasma nano-synthsize system for industrial application is shown below. The photos of some nano-product from the same equipment are included.\n\n\n== Gallery ==\n\n\n== Summary ==\nInduction plasma technology achieves mainly the aforementioned high-added-value processes. Besides the \u201cspheroidisation\u201d and \u201cnanomaterial synthesis\u201d, the high risk waste treatment, refractory materials deposit, noble material synthesis etc. may be the next industrial fields for induction plasma technology.\n\n\n== See also ==\nPlasma propulsion engine\nList of plasma (physics) articles\n\n\n== Notes ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Induction_plasma", 
                "title": "Induction plasma"
            }, 
            {
                "snippet": "leaving the plasma. If successful, Super-X could be used in DEMO and other future fusion devices.  CCFE is responsible for the operation and safety of", 
                "pageCategories": "All stub articles\nArticles contradicting other articles\nCoordinates on Wikidata\nFusion reactors\nNuclear energy in the United Kingdom\nNuclear technology in the United Kingdom\nPhysics organization stubs\nResearch and development in the United Kingdom\nResearch institutes in Oxfordshire\nScience parks in the United Kingdom", 
                "pageContent": "The Culham Centre for Fusion Energy (CCFE) is the UK's national laboratory for fusion research. It is located at the Culham Science Centre, near Culham, Oxfordshire, and is the site of the Joint European Torus (JET), Mega Ampere Spherical Tokamak (MAST) and the now closed Small Tight Aspect Ratio Tokamak (START).\nFormerly known as UKAEA Culham, the laboratory was renamed in October 2009 as part of organisational changes at its parent body, the United Kingdom Atomic Energy Authority (UKAEA).\nSince September 2008 the director has been Professor Steven Cowley, and the centre has been engaged in work towards the final detailed design of ITER as well as preparatory work in support of DEMO.\nIn 2014 it was announced the centre would house the new RACE (Remote Applications in Challenging Environments)\n\n\n== Culham Science Centre ==\nThe centre occupies the site of the former Royal Navy airfield RNAS Culham (HMS Hornbill), which was transferred to UKAEA in 1960. The UKAEA continues to operate the site and is the major tenant.\nAs well as CCFE, the centre houses the headquarters of the UKAEA, and hosts many commercial and other organisations.\n\n\n== History ==\nUKAEA officially opened Culham Laboratory in 1965, having moved its fusion research operations from the nearby Harwell research site. Culham also amalgamated fusion activities at Aldermaston and other UK locations to form a national centre for fusion research. John Adams, who would go on to become Director-General of CERN, was appointed the first Director of the laboratory.\nCulham built almost 30 different experiments in its first two decades as a variety of fusion concepts were tried out; among them shock-waves, magnetic mirror machines, stellarators and levitrons. During the 1970s, research became focused on magnetic confinement fusion using the tokamak device, which had emerged as the most promising design for a future fusion reactor. In the late 1960s, Culham scientists had already assisted in tokamak development by using laser scattering measurement techniques to verify the highly promising results achieved by the Russian T3 device). This led to the adoption of the tokamak by the majority of fusion research establishments internationally.\nIn 1977, following protracted negotiations, Culham was chosen as the site for the Joint European Torus (JET) tokamak. Construction began in 1978 and was completed on time and on budget, with first plasma in June 1983. Since then the machine has gone on to set a series of fusion milestones, including the first demonstration of controlled deuterium-tritium fusion power (1991) and the record fusion power output of 16 megawatts (1997). Initially the JET facility was run by a multi-national team as a separate entity on the Culham site under the JET Joint Undertaking agreement. However, since 2000, UKAEA has been responsible for the operation of JET on behalf of its European research partners, through a contract with the European Commission.\nIn the 1980s, Culham Laboratory was instrumental in the development of the spherical tokamak concept \u2013 a more compact version of the tokamak in which plasma is held in a tighter magnetic field in a \u2018cored apple\u2019 shape instead of the conventional toroidal configuration. This is thought to offer potential advantages by enabling smaller, more efficient fusion devices. The START (Small Tight Aspect Ratio Tokamak) experiment at Culham (1991-1998) was the first full-sized spherical tokamak. Its impressive performance led to the construction of a larger device, MAST (Mega Amp Spherical Tokamak), which operated between 2000 and 2013.\n\n\n== Directors ==\n1960-1966: John Adams\n1966-1981: Bas Pease\n1981-1990: Mick Lomer\n1990-1996: Don Sweetman\n1996-2002: Derek Robinson\n2002-2003: Frank Briscoe (Acting Director)\n2003-2008: Christopher Llewellyn Smith\n2008-2016: Steven Cowley\n2016-present: Ian Chapman\n\n\n== Current activities ==\n\n\n=== UK fusion programme ===\nCCFE has a broad ranging programme of activities encompassing tokamak plasma physics, technology developments for the DEMO prototype fusion power plant, the development of materials suitable for a fusion environment, engineering activities, the training of students, graduates and apprentices, and public and industry outreach activities.\nIt also participates in a co-ordinated European programme, which is managed by the EUROfusion consortium of research institutes. This is focussed on delivering the European fusion roadmap, with the goal of achieving fusion electricity by 2050.\nCCFE is involved in a number of other international collaborations, notably the ITER tokamak being built at Cadarache in France. As well as contributing to scientific preparations for ITER with plasma physics experiments at Culham, CCFE is developing technology for the project \u2013 such as remote handling applications, specialist heating systems and instrumentation for plasma measurements (\u2018diagnostics\u2019).\n\n\n=== MAST Upgrade ===\nThe focus of the UK domestic fusion programme is MAST Upgrade \u2013 a more powerful, better-equipped successor to the Mega Ampere Spherical Tokamak. Currently under construction, MAST Upgrade is expected to start commissioning in late 2016.\nMAST Upgrade will be implemented in three stages. Funding has been agreed with the Engineering and Physical Sciences Research Council for the core upgrade (Stage 1a), which will be ready for plasma operations in 2016/17. Two additional phases (Stage 1b and Stage 2) will follow in later years subject to funding.\nMAST Upgrade has three main missions:\nMake the case for a fusion Component Test Facility (CTF). A CTF would test reactor systems for DEMO, and a spherical tokamak is seen as an ideal design for the facility;\nAdd to the knowledge base for ITER and help resolve key plasma physics issues to ensure its success;\nTest reactor systems. MAST Upgrade will be the first tokamak to trial the innovative Super-X divertor \u2013 a high-power exhaust system that reduces power loads from particles leaving the plasma. If successful, Super-X could be used in DEMO and other future fusion devices.\n\n\n=== Joint European Torus (JET) ===\nCCFE is responsible for the operation and safety of the JET facilities on behalf of EUROfusion. Its engineers also ensure that the JET device is maintained and upgraded to meet the demands of the research programme. Upgrades are largely carried out using a sophisticated remote handling system which avoids the need for manual entry. For example, in 2009-2011, remote handling engineers stripped out the interior of JET to fit a new 4,500-tile inner wall to enable researchers to test materials for the forthcoming ITER tokamak.\nIn addition, CCFE participates in the JET scientific programme alongside the other 28 EUROfusion research organisations throughout Europe.\n\n\n== Funding ==\nFunding for CCFE\u2019s domestic fusion programme is provided by a grant from the Engineering and Physical Sciences Research Council. The operation of JET is funded under a bilateral contract between the United Kingdom Atomic Energy Authority and the European Commission, which runs until the end of 2018.\n\n\n== References ==\n\n\n== External links ==\nCulham Centre For Fusion Energy website", 
                "titleUrl": "https://en.wikipedia.org/wiki/Culham_Centre_for_Fusion_Energy", 
                "title": "Culham Centre for Fusion Energy"
            }
        ], 
        "phraseCharStart": "297"
    }, 
    {
        "phraseCharEnd": "343", 
        "phraseIndex": "T8", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "oxygen", 
        "wikiSearchResults": [
            {
                "snippet": "this element, see Allotropes of oxygen. For other uses, see Oxygen (disambiguation) and O2 (disambiguation). Oxygen is a chemical element with symbol\u00a0O", 
                "pageCategories": "Articles containing Ancient Greek-language text\nArticles with hAudio microformats\nBiology and pharmacology of chemical elements\nBreathing gases\nCS1 maint: Uses authors parameter\nChalcogens\nChemical elements\nChemical substances for emergency medicine\nDiatomic nonmetals\nFeatured articles", 
                "pageContent": "Oxygen is a chemical element with symbol O and atomic number 8. It is a member of the chalcogen group on the periodic table and is a highly reactive nonmetal and oxidizing agent that readily forms oxides with most elements as well as other compounds. By mass, oxygen is the third-most abundant element in the universe, after hydrogen and helium. At standard temperature and pressure, two atoms of the element bind to form dioxygen, a colorless and odorless diatomic gas with the formula O\n2. This is an important part of the atmosphere and diatomic oxygen gas constitutes 20.8% of the Earth's atmosphere. Additionally, as oxides the element also makes up almost half of the Earth's crust.\nOxygen is necessary to sustain most terrestrial life. Oxygen is used in cellular respiration and many major classes of organic molecules in living organisms contain oxygen, such as proteins, nucleic acids, carbohydrates, and fats, as do the major constituent inorganic compounds of animal shells, teeth, and bone. Most of the mass of living organisms is oxygen as a component of water, the major constituent of lifeforms. Conversely, oxygen is continuously replenished by photosynthesis, which uses the energy of sunlight to produce oxygen from water and carbon dioxide. Oxygen is too chemically reactive to remain a free element in air without being continuously replenished by the photosynthetic action of living organisms. Another form (allotrope) of oxygen, ozone (O\n3), strongly absorbs ultraviolet UVB radiation and the high-altitude ozone layer helps protect the biosphere from ultraviolet radiation. But ozone is a pollutant near the surface where it is a by-product of smog. At low earth orbit altitudes, sufficient atomic oxygen is present to cause corrosion of spacecraft.\nOxygen was discovered independently by Carl Wilhelm Scheele, in Uppsala, in 1773 or earlier, and Joseph Priestley in Wiltshire, in 1774, but Priestley is often given priority because his work was published first. The name oxygen was coined in 1777 by Antoine Lavoisier, whose experiments with oxygen helped to discredit the then-popular phlogiston theory of combustion and corrosion. Its name derives from the Greek roots \u1f40\u03be\u03cd\u03c2 oxys, \"acid\", literally \"sharp\", referring to the sour taste of acids and -\u03b3\u03b5\u03bd\u03ae\u03c2 -genes, \"producer\", literally \"begetter\", because at the time of naming, it was mistakenly thought that all acids required oxygen in their composition.\nCommon use of oxygen includes residential heating, internal combustion engines, production of steel, plastics and textiles, brazing, welding and cutting of steels and other metals, rocket propellant, oxygen therapy, and life support systems in aircraft, submarines, spaceflight and diving.\n\n\n== History ==\n\n\n=== Early experiments ===\n\nOne of the first known experiments on the relationship between combustion and air was conducted by the 2nd century BCE Greek writer on mechanics, Philo of Byzantium. In his work Pneumatica, Philo observed that inverting a vessel over a burning candle and surrounding the vessel's neck with water resulted in some water rising into the neck. Philo incorrectly surmised that parts of the air in the vessel were converted into the classical element fire and thus were able to escape through pores in the glass. Many centuries later Leonardo da Vinci built on Philo's work by observing that a portion of air is consumed during combustion and respiration.\nIn the late 17th century, Robert Boyle proved that air is necessary for combustion. English chemist John Mayow (1641\u20131679) refined this work by showing that fire requires only a part of air that he called spiritus nitroaereus. In one experiment, he found that placing either a mouse or a lit candle in a closed container over water caused the water to rise and replace one-fourteenth of the air's volume before extinguishing the subjects. From this he surmised that nitroaereus is consumed in both respiration and combustion.\nMayow observed that antimony increased in weight when heated, and inferred that the nitroaereus must have combined with it. He also thought that the lungs separate nitroaereus from air and pass it into the blood and that animal heat and muscle movement result from the reaction of nitroaereus with certain substances in the body. Accounts of these and other experiments and ideas were published in 1668 in his work Tractatus duo in the tract \"De respiratione\".\n\n\n=== Phlogiston theory ===\n\nRobert Hooke, Ole Borch, Mikhail Lomonosov, and Pierre Bayen all produced oxygen in experiments in the 17th and the 18th century but none of them recognized it as a chemical element. This may have been in part due to the prevalence of the philosophy of combustion and corrosion called the phlogiston theory, which was then the favored explanation of those processes.\nEstablished in 1667 by the German alchemist J. J. Becher, and modified by the chemist Georg Ernst Stahl by 1731, phlogiston theory stated that all combustible materials were made of two parts. One part, called phlogiston, was given off when the substance containing it was burned, while the dephlogisticated part was thought to be its true form, or calx.\nHighly combustible materials that leave little residue, such as wood or coal, were thought to be made mostly of phlogiston; non-combustible substances that corrode, such as iron, contained very little. Air did not play a role in phlogiston theory, nor were any initial quantitative experiments conducted to test the idea; instead, it was based on observations of what happens when something burns, that most common objects appear to become lighter and seem to lose something in the process. The fact that a substance like wood gains overall weight in burning was hidden by the buoyancy of the gaseous combustion products.\nThis theory, while it was on the right track, was unfortunately set up backwards. Rather than combustion or corrosion occurring as a result of the decomposition of phlogiston compounds into their base elements with the phlogiston being lost to the air, it is in fact the result of oxygen from the air combining with the base elements to produce oxides. Indeed, one of the first clues that the phlogiston theory was incorrect was that metals gain weight in rusting (when they were supposedly losing phlogiston).\n\n\n=== Discovery ===\n\nOxygen was first discovered by Swedish pharmacist Carl Wilhelm Scheele. He had produced oxygen gas by heating mercuric oxide and various nitrates by about 1772. Scheele called the gas \"fire air\" because it was the only known supporter of combustion, and wrote an account of this discovery in a manuscript he titled Treatise on Air and Fire, which he sent to his publisher in 1775. That document was published in 1777.\n\nIn the meantime, on August 1, 1774, an experiment conducted by the British clergyman Joseph Priestley focused sunlight on mercuric oxide (HgO) inside a glass tube, which liberated a gas he named \"dephlogisticated air\". He noted that candles burned brighter in the gas and that a mouse was more active and lived longer while breathing it. After breathing the gas himself, he wrote: \"The feeling of it to my lungs was not sensibly different from that of common air, but I fancied that my breast felt peculiarly light and easy for some time afterwards.\" Priestley published his findings in 1775 in a paper titled \"An Account of Further Discoveries in Air\" which was included in the second volume of his book titled Experiments and Observations on Different Kinds of Air. Because he published his findings first, Priestley is usually given priority in the discovery.\nThe French chemist Antoine Laurent Lavoisier later claimed to have discovered the new substance independently. Priestley visited Lavoisier in October 1774 and told him about his experiment and how he liberated the new gas. Scheele also posted a letter to Lavoisier on September 30, 1774 that described his discovery of the previously unknown substance, but Lavoisier never acknowledged receiving it (a copy of the letter was found in Scheele's belongings after his death).\n\n\n=== Lavoisier's contribution ===\nWhat Lavoisier did (although this was disputed at the time) was to conduct the first adequate quantitative experiments on oxidation and give the first correct explanation of how combustion works. He used these and similar experiments, all started in 1774, to discredit the phlogiston theory and to prove that the substance discovered by Priestley and Scheele was a chemical element.\n\nIn one experiment, Lavoisier observed that there was no overall increase in weight when tin and air were heated in a closed container. He noted that air rushed in when he opened the container, which indicated that part of the trapped air had been consumed. He also noted that the tin had increased in weight and that increase was the same as the weight of the air that rushed back in. This and other experiments on combustion were documented in his book Sur la combustion en g\u00e9n\u00e9ral, which was published in 1777. In that work, he proved that air is a mixture of two gases; 'vital air', which is essential to combustion and respiration, and azote (Gk. \u1f04\u03b6\u03c9\u03c4\u03bf\u03bd \"lifeless\"), which did not support either. Azote later became nitrogen in English, although it has kept the name in French and several other European languages.\nLavoisier renamed 'vital air' to oxyg\u00e8ne in 1777 from the Greek roots \u1f40\u03be\u03cd\u03c2 (oxys) (acid, literally \"sharp\", from the taste of acids) and -\u03b3\u03b5\u03bd\u03ae\u03c2 (-gen\u0113s) (producer, literally begetter), because he mistakenly believed that oxygen was a constituent of all acids. Chemists (such as Sir Humphry Davy in 1812) eventually determined that Lavoisier was wrong in this regard (hydrogen forms the basis for acid chemistry), but by then the name was too well established.\nOxygen entered the English language despite opposition by English scientists and the fact that the Englishman Priestley had first isolated the gas and written about it. This is partly due to a poem praising the gas titled \"Oxygen\" in the popular book The Botanic Garden (1791) by Erasmus Darwin, grandfather of Charles Darwin.\n\n\n=== Later history ===\n\nJohn Dalton's original atomic hypothesis presumed that all elements were monatomic and that the atoms in compounds would normally have the simplest atomic ratios with respect to one another. For example, Dalton assumed that water's formula was HO, giving the atomic mass of oxygen was 8 times that of hydrogen, instead of the modern value of about 16. In 1805, Joseph Louis Gay-Lussac and Alexander von Humboldt showed that water is formed of two volumes of hydrogen and one volume of oxygen; and by 1811 Amedeo Avogadro had arrived at the correct interpretation of water's composition, based on what is now called Avogadro's law and the diatomic elemental molecules in those gases.\nBy the late 19th century scientists realized that air could be liquefied and its components isolated by compressing and cooling it. Using a cascade method, Swiss chemist and physicist Raoul Pierre Pictet evaporated liquid sulfur dioxide in order to liquefy carbon dioxide, which in turn was evaporated to cool oxygen gas enough to liquefy it. He sent a telegram on December 22, 1877 to the French Academy of Sciences in Paris announcing his discovery of liquid oxygen. Just two days later, French physicist Louis Paul Cailletet announced his own method of liquefying molecular oxygen. Only a few drops of the liquid were produced in each case and no meaningful analysis could be conducted. Oxygen was liquified in a stable state for the first time on March 29, 1883 by Polish scientists from Jagiellonian University, Zygmunt Wr\u00f3blewski and Karol Olszewski.\nIn 1891 Scottish chemist James Dewar was able to produce enough liquid oxygen for study. The first commercially viable process for producing liquid oxygen was independently developed in 1895 by German engineer Carl von Linde and British engineer William Hampson. Both men lowered the temperature of air until it liquefied and then distilled the component gases by boiling them off one at a time and capturing them. Later, in 1901, oxyacetylene welding was demonstrated for the first time by burning a mixture of acetylene and compressed O\n2. This method of welding and cutting metal later became common.\nIn 1923, the American scientist Robert H. Goddard became the first person to develop a rocket engine that burned liquid fuel; the engine used gasoline for fuel and liquid oxygen as the oxidizer. Goddard successfully flew a small liquid-fueled rocket 56 m at 97 km/h on March 16, 1926 in Auburn, Massachusetts, US.\nOxygen levels in the atmosphere are trending slightly downward globally, possibly because of fossil-fuel burning.\n\n\n== Characteristics ==\n\n\n=== Properties and molecular structure ===\n\nAt standard temperature and pressure, oxygen is a colorless, odorless, and tasteless gas with the molecular formula O\n2, referred to as dioxygen.\nAs dioxygen, two oxygen atoms are chemically bound to each other. The bond can be variously described based on level of theory, but is reasonably and simply described as a covalent double bond that results from the filling of molecular orbitals formed from the atomic orbitals of the individual oxygen atoms, the filling of which results in a bond order of two. More specifically, the double bond is the result of sequential, low-to-high energy, or Aufbau, filling of orbitals, and the resulting cancellation of contributions from the 2s electrons, after sequential filling of the low \u03c3 and \u03c3* orbitals; \u03c3 overlap of the two atomic 2p orbitals that lie along the O-O molecular axis and \u03c0 overlap of two pairs of atomic 2p orbitals perpendicular to the O-O molecular axis, and then cancellation of contributions from the remaining two of the six 2p electrons after their partial filling of the lowest \u03c0 and \u03c0* orbitals.\nThis combination of cancellations and \u03c3 and \u03c0 overlaps results in dioxygen's double bond character and reactivity, and a triplet electronic ground state. An electron configuration with two unpaired electrons, as is found in dioxygen (see the filled \u03c0* orbitals in the diagram) orbitals that are of equal energy\u2014i.e., degenerate\u2014is a configuration termed a spin triplet state. Hence, the ground state of the O\n2 molecule is referred to as triplet oxygen. The highest energy, partially filled orbitals are antibonding, and so their filling weakens the bond order from three to two. Because of its unpaired electrons, triplet oxygen reacts only slowly with most organic molecules, which have paired electron spins; this prevents spontaneous combustion.\n\nIn the triplet form, O\n2 molecules are paramagnetic. That is, they impart magnetic character to oxygen when it is in the presence of a magnetic field, because of the spin magnetic moments of the unpaired electrons in the molecule, and the negative exchange energy between neighboring O\n2 molecules. Liquid oxygen is so magnetic that, in laboratory demonstrations, a bridge of liquid oxygen may be supported against its own weight between the poles of a powerful magnet.\nSinglet oxygen is a name given to several higher-energy species of molecular O\n2 in which all the electron spins are paired. It is much more reactive with common organic molecules than is molecular oxygen per se. In nature, singlet oxygen is commonly formed from water during photosynthesis, using the energy of sunlight. It is also produced in the troposphere by the photolysis of ozone by light of short wavelength, and by the immune system as a source of active oxygen. Carotenoids in photosynthetic organisms (and possibly animals) play a major role in absorbing energy from singlet oxygen and converting it to the unexcited ground state before it can cause harm to tissues.\n\n\n=== Allotropes ===\n\nThe common allotrope of elemental oxygen on Earth is called dioxygen, O\n2, the major part of the Earth's atmospheric oxygen (see Occurrence). O2 has a bond length of 121 pm and a bond energy of 498 kJ\u00b7mol\u22121, which is smaller than the energy of other double bonds or pairs of single bonds in the biosphere and responsible for the exothermic reaction of O2 with any organic molecule. Due to its energy content, O2 is used by complex forms of life, such as animals, in cellular respiration (see Biological role). Other aspects of O\n2 are covered in the remainder of this article.\nTrioxygen (O\n3) is usually known as ozone and is a very reactive allotrope of oxygen that is damaging to lung tissue. Ozone is produced in the upper atmosphere when O\n2 combines with atomic oxygen made by the splitting of O\n2 by ultraviolet (UV) radiation. Since ozone absorbs strongly in the UV region of the spectrum, the ozone layer of the upper atmosphere functions as a protective radiation shield for the planet. Near the Earth's surface, it is a pollutant formed as a by-product of automobile exhaust. The metastable molecule tetraoxygen (O\n4) was discovered in 2001, and was assumed to exist in one of the six phases of solid oxygen. It was proven in 2006 that this phase, created by pressurizing O\n2 to 20 GPa, is in fact a rhombohedral O\n8 cluster. This cluster has the potential to be a much more powerful oxidizer than either O\n2 or O\n3 and may therefore be used in rocket fuel. A metallic phase was discovered in 1990 when solid oxygen is subjected to a pressure of above 96 GPa and it was shown in 1998 that at very low temperatures, this phase becomes superconducting.\n\n\n=== Physical properties ===\n\nOxygen dissolves more readily in water than nitrogen, and in freshwater more readily than seawater. Water in equilibrium with air contains approximately 1 molecule of dissolved O\n2 for every 2 molecules of N\n2 (1:2), compared with an atmospheric ratio of approximately 1:4. The solubility of oxygen in water is temperature-dependent, and about twice as much (14.6 mg\u00b7L\u22121) dissolves at 0 \u00b0C than at 20 \u00b0C (7.6 mg\u00b7L\u22121). At 25 \u00b0C and 1 standard atmosphere (101.3 kPa) of air, freshwater contains about 6.04 milliliters (mL) of oxygen per liter, and seawater contains about 4.95 mL per liter. At 5 \u00b0C the solubility increases to 9.0 mL (50% more than at 25 \u00b0C) per liter for water and 7.2 mL (45% more) per liter for sea water.\nOxygen condenses at 90.20 K (\u2212182.95 \u00b0C, \u2212297.31 \u00b0F), and freezes at 54.36 K (\u2212218.79 \u00b0C, \u2212361.82 \u00b0F). Both liquid and solid O\n2 are clear substances with a light sky-blue color caused by absorption in the red (in contrast with the blue color of the sky, which is due to Rayleigh scattering of blue light). High-purity liquid O\n2 is usually obtained by the fractional distillation of liquefied air. Liquid oxygen may also be condensed from air using liquid nitrogen as a coolant.\nOxygen is a highly reactive substance and must be segregated from combustible materials.\nThe spectroscopy of molecular oxygen is associated with the atmospheric processes of aurora, airglow and nightglow. The absorption in the Herzberg continuum and Schumann\u2013Runge bands in the ultraviolet produces atomic oxygen that is important in the chemistry of the middle atmosphere. Excited state singlet molecular oxygen is responsible for red chemiluminescence in solution.\n\n\n=== Isotopes and stellar origin ===\n\nNaturally occurring oxygen is composed of three stable isotopes, 16O, 17O, and 18O, with 16O being the most abundant (99.762% natural abundance).\nMost 16O is synthesized at the end of the helium fusion process in massive stars but some is made in the neon burning process. 17O is primarily made by the burning of hydrogen into helium during the CNO cycle, making it a common isotope in the hydrogen burning zones of stars. Most 18O is produced when 14N (made abundant from CNO burning) captures a 4He nucleus, making 18O common in the helium-rich zones of evolved, massive stars.\nFourteen radioisotopes have been characterized. The most stable are 15O with a half-life of 122.24 seconds and 14O with a half-life of 70.606 seconds. All of the remaining radioactive isotopes have half-lives that are less than 27 s and the majority of these have half-lives that are less than 83 milliseconds. The most common decay mode of the isotopes lighter than 16O is \u03b2+ decay to yield nitrogen, and the most common mode for the isotopes heavier than 18O is beta decay to yield fluorine.\n\n\n=== Occurrence ===\n\nOxygen is the most abundant chemical element by mass in the Earth's biosphere, air, sea and land. Oxygen is the third most abundant chemical element in the universe, after hydrogen and helium. About 0.9% of the Sun's mass is oxygen. Oxygen constitutes 49.2% of the Earth's crust by mass as part of oxide compounds such as silicon dioxide and is the most abundant element by mass in the Earth's crust. It is also the major component of the world's oceans (88.8% by mass). Oxygen gas is the second most common component of the Earth's atmosphere, taking up 20.8% of its volume and 23.1% of its mass (some 1015 tonnes). Earth is unusual among the planets of the Solar System in having such a high concentration of oxygen gas in its atmosphere: Mars (with 0.1% O\n2 by volume) and Venus have much less. The O\n2 surrounding those planets is produced solely by ultraviolet radiation on oxygen-containing molecules such as carbon dioxide.\nThe unusually high concentration of oxygen gas on Earth is the result of the oxygen cycle. This biogeochemical cycle describes the movement of oxygen within and between its three main reservoirs on Earth: the atmosphere, the biosphere, and the lithosphere. The main driving factor of the oxygen cycle is photosynthesis, which is responsible for modern Earth's atmosphere. Photosynthesis releases oxygen into the atmosphere, while respiration, decay, and combustion remove it from the atmosphere. In the present equilibrium, production and consumption occur at the same rate of roughly 1/2000th of the entire atmospheric oxygen per year.\n\nFree oxygen also occurs in solution in the world's water bodies. The increased solubility of O\n2 at lower temperatures (see Physical properties) has important implications for ocean life, as polar oceans support a much higher density of life due to their higher oxygen content. Water polluted with plant nutrients such as nitrates or phosphates may stimulate growth of algae by a process called eutrophication and the decay of these organisms and other biomaterials may reduce the O\n2 content in eutrophic water bodies. Scientists assess this aspect of water quality by measuring the water's biochemical oxygen demand, or the amount of O\n2 needed to restore it to a normal concentration.\n\n\n=== Analysis ===\n\nPaleoclimatologists measure the ratio of oxygen-18 and oxygen-16 in the shells and skeletons of marine organisms to determine the climate millions of years ago (see oxygen isotope ratio cycle). Seawater molecules that contain the lighter isotope, oxygen-16, evaporate at a slightly faster rate than water molecules containing the 12% heavier oxygen-18, and this disparity increases at lower temperatures. During periods of lower global temperatures, snow and rain from that evaporated water tends to be higher in oxygen-16, and the seawater left behind tends to be higher in oxygen-18. Marine organisms then incorporate more oxygen-18 into their skeletons and shells than they would in a warmer climate. Paleoclimatologists also directly measure this ratio in the water molecules of ice core samples as old as hundreds of thousands of years.\nPlanetary geologists have measured the relative quantities of oxygen isotopes in samples from the Earth, the Moon, Mars, and meteorites, but were long unable to obtain reference values for the isotope ratios in the Sun, believed to be the same as those of the primordial solar nebula. Analysis of a silicon wafer exposed to the solar wind in space and returned by the crashed Genesis spacecraft has shown that the Sun has a higher proportion of oxygen-16 than does the Earth. The measurement implies that an unknown process depleted oxygen-16 from the Sun's disk of protoplanetary material prior to the coalescence of dust grains that formed the Earth.\nOxygen presents two spectrophotometric absorption bands peaking at the wavelengths 687 and 760 nm. Some remote sensing scientists have proposed using the measurement of the radiance coming from vegetation canopies in those bands to characterize plant health status from a satellite platform. This approach exploits the fact that in those bands it is possible to discriminate the vegetation's reflectance from its fluorescence, which is much weaker. The measurement is technically difficult owing to the low signal-to-noise ratio and the physical structure of vegetation; but it has been proposed as a possible method of monitoring the carbon cycle from satellites on a global scale.\n\n\n== Biological role of O2 ==\n\n\n=== Photosynthesis and respiration ===\n\nIn nature, free oxygen is produced by the light-driven splitting of water during oxygenic photosynthesis. According to some estimates, green algae and cyanobacteria in marine environments provide about 70% of the free oxygen produced on Earth, and the rest is produced by terrestrial plants. Other estimates of the oceanic contribution to atmospheric oxygen are higher, while some estimates are lower, suggesting oceans produce ~45% of Earth's atmospheric oxygen each year.\nA simplified overall formula for photosynthesis is:\n\n6 CO2 + 6 H\n2O + photons \u2192 C\n6H\n12O\n6 + 6 O\n2\n\nor simply\n\ncarbon dioxide + water + sunlight \u2192 glucose + dioxygen\n\nPhotolytic oxygen evolution occurs in the thylakoid membranes of photosynthetic organisms and requires the energy of four photons. Many steps are involved, but the result is the formation of a proton gradient across the thylakoid membrane, which is used to synthesize adenosine triphosphate (ATP) via photophosphorylation. The O\n2 remaining (after production of the water molecule) is released into the atmosphere.\nMolecular dioxygen, O\n2, is essential for cellular respiration in all aerobic organisms. Oxygen is used in mitochondria to generate ATP during oxidative phosphorylation. The reaction for aerobic respiration is essentially the reverse of photosynthesis and is simplified as:\n\nC\n6H\n12O\n6 + 6 O\n2 \u2192 6 CO2 + 6 H\n2O + 2880 kJ\u00b7mol\u22121\n\nIn vertebrates, O\n2 diffuses through membranes in the lungs and into red blood cells. Hemoglobin binds O\n2, changing color from bluish red to bright red (CO\n2 is released from another part of hemoglobin through the Bohr effect). Other animals use hemocyanin (molluscs and some arthropods) or hemerythrin (spiders and lobsters). A liter of blood can dissolve 200 cm3 of O\n2.\nUntil the discovery of anaerobic metazoa, oxygen was thought to be a requirement for all complex life.\nReactive oxygen species, such as superoxide ion (O\u2212\n2) and hydrogen peroxide (H\n2O\n2), are dangerous by-products of oxygen use in organisms. Parts of the immune system of higher organisms create peroxide, superoxide, and singlet oxygen to destroy invading microbes. Reactive oxygen species also play an important role in the hypersensitive response of plants against pathogen attack. Oxygen is toxic to obligately anaerobic organisms, which were the dominant form of early life on Earth until O\n2 began to accumulate in the atmosphere about 2.5 billion years ago during the Great Oxygenation Event, about a billion years after the first appearance of these organisms.\nAn adult human at rest inhales 1.8 to 2.4 grams of oxygen per minute. This amounts to more than 6 billion tonnes of oxygen inhaled by humanity per year.\n\n\n=== Living organisms ===\n\nThe free oxygen partial pressure in the body of a living vertebrate organism is highest in the respiratory system, and decreases along any arterial system, peripheral tissues, and venous system, respectively. Partial pressure is the pressure that oxygen would have if it alone occupied the volume.\n\n\n=== Build-up in the atmosphere ===\n\nFree oxygen gas was almost nonexistent in Earth's atmosphere before photosynthetic archaea and bacteria evolved, probably about 3.5 billion years ago. Free oxygen first appeared in significant quantities during the Paleoproterozoic eon (between 3.0 and 2.3 billion years ago). For the first billion years, any free oxygen produced by these organisms combined with dissolved iron in the oceans to form banded iron formations. When such oxygen sinks became saturated, free oxygen began to outgas from the oceans 3\u20132.7 billion years ago, reaching 10% of its present level around 1.7 billion years ago.\nThe presence of large amounts of dissolved and free oxygen in the oceans and atmosphere may have driven most of the extant anaerobic organisms to extinction during the Great Oxygenation Event (oxygen catastrophe) about 2.4 billion years ago. Cellular respiration using O\n2 enables aerobic organisms to produce much more ATP than anaerobic organisms. Cellular respiration of O\n2 occurs in all eukaryotes, including all complex multicellular organisms such as plants and animals.\nSince the beginning of the Cambrian period 540 million years ago, atmospheric O\n2 levels have fluctuated between 15% and 30% by volume. Towards the end of the Carboniferous period (about 300 million years ago) atmospheric O\n2 levels reached a maximum of 35% by volume, which may have contributed to the large size of insects and amphibians at this time.\nVariations of oxygen shaped the climates of the past. When oxygen declined, atmospheric density dropped and this in turn increased surface evaporation, and led to precipitation increases and warmer temperatures.\nAt the current rate of photosynthesis it would take about 2,000 years to regenerate the entire O\n2 in the present atmosphere.\n\n\n== Industrial production ==\n\nOne hundred million tonnes of O\n2 are extracted from air for industrial uses annually by two primary methods. The most common method is fractional distillation of liquefied air, with N\n2 distilling as a vapor while O\n2 is left as a liquid.\nThe other primary method of producing O\n2 is passing a stream of clean, dry air through one bed of a pair of identical zeolite molecular sieves, which absorbs the nitrogen and delivers a gas stream that is 90% to 93% O\n2. Simultaneously, nitrogen gas is released from the other nitrogen-saturated zeolite bed, by reducing the chamber operating pressure and diverting part of the oxygen gas from the producer bed through it, in the reverse direction of flow. After a set cycle time the operation of the two beds is interchanged, thereby allowing for a continuous supply of gaseous oxygen to be pumped through a pipeline. This is known as pressure swing adsorption. Oxygen gas is increasingly obtained by these non-cryogenic technologies (see also the related vacuum swing adsorption).\nOxygen gas can also be produced through electrolysis of water into molecular oxygen and hydrogen. DC electricity must be used: if AC is used, the gases in each limb consist of hydrogen and oxygen in the explosive ratio 2:1. Contrary to popular belief, the 2:1 ratio observed in the DC electrolysis of acidified water does not prove that the empirical formula of water is H2O unless certain assumptions are made about the molecular formulae of hydrogen and oxygen themselves. A similar method is the electrocatalytic O\n2 evolution from oxides and oxoacids. Chemical catalysts can be used as well, such as in chemical oxygen generators or oxygen candles that are used as part of the life-support equipment on submarines, and are still part of standard equipment on commercial airliners in case of depressurization emergencies. Another air separation method is forcing air to dissolve through ceramic membranes based on zirconium dioxide by either high pressure or an electric current, to produce nearly pure O\n2 gas.\nIn large quantities, the price of liquid oxygen in 2001 was approximately $0.21/kg. Since the primary cost of production is the energy cost of liquefying the air, the production cost will change as energy cost varies.\n\n\n== Storage ==\nOxygen storage methods include high pressure oxygen tanks, cryogenics and chemical compounds. For reasons of economy, oxygen is often transported in bulk as a liquid in specially insulated tankers, since one liter of liquefied oxygen is equivalent to 840 liters of gaseous oxygen at atmospheric pressure and 20 \u00b0C (68 \u00b0F). Such tankers are used to refill bulk liquid oxygen storage containers, which stand outside hospitals and other institutions that need large volumes of pure oxygen gas. Liquid oxygen is passed through heat exchangers, which convert the cryogenic liquid into gas before it enters the building. Oxygen is also stored and shipped in smaller cylinders containing the compressed gas; a form that is useful in certain portable medical applications and oxy-fuel welding and cutting.\n\n\n== Applications ==\n\n\n=== Medical ===\n\nUptake of O\n2 from the air is the essential purpose of respiration, so oxygen supplementation is used in medicine. Treatment not only increases oxygen levels in the patient's blood, but has the secondary effect of decreasing resistance to blood flow in many types of diseased lungs, easing work load on the heart. Oxygen therapy is used to treat emphysema, pneumonia, some heart disorders (congestive heart failure), some disorders that cause increased pulmonary artery pressure, and any disease that impairs the body's ability to take up and use gaseous oxygen.\nTreatments are flexible enough to be used in hospitals, the patient's home, or increasingly by portable devices. Oxygen tents were once commonly used in oxygen supplementation, but have since been replaced mostly by the use of oxygen masks or nasal cannulas.\nHyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O\n2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the 'bends') are sometimes addressed with this therapy. Increased O\n2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in the blood. Increasing the pressure of O\n2 as soon as possible helps to redissolve the bubbles back into the blood so that these excess gasses can be exhaled naturally through the lungs.\nOxygen is also used medically for patients who require mechanical ventilation, often at concentrations above the 21% found in ambient air.\n\n\n=== Life support and recreational use ===\n\nAn application of O\n2 as a low-pressure breathing gas is in modern space suits, which surround their occupant's body with pressurized air. These devices use nearly pure oxygen at about one third normal pressure, resulting in a normal blood partial pressure of O\n2. This trade-off of higher oxygen concentration for lower pressure is needed to maintain suit flexibility.\nScuba divers and submariners also rely on artificially delivered O\n2, but most often use normal pressure, and/or mixtures of oxygen and air. Pure or nearly pure O\n2 use in diving at higher-than-sea-level pressures is usually limited to rebreather, decompression, or emergency treatment use at relatively shallow depths (~6 meters depth, or less). Deeper diving requires significant dilution of O\n2 with other gases, such as nitrogen or helium, to prevent oxygen toxicity.\nPeople who climb mountains or fly in non-pressurized fixed-wing aircraft sometimes have supplemental O\n2 supplies. Pressurized commercial airplanes have an emergency supply of O\n2 automatically supplied to the passengers in case of cabin depressurization. Sudden cabin pressure loss activates chemical oxygen generators above each seat, causing oxygen masks to drop. Pulling on the masks \"to start the flow of oxygen\" as cabin safety instructions dictate, forces iron filings into the sodium chlorate inside the canister. A steady stream of oxygen gas is then produced by the exothermic reaction.\nOxygen, as a supposed mild euphoric, has a history of recreational use in oxygen bars and in sports. Oxygen bars are establishments found in Japan, California, and Las Vegas, Nevada since the late 1990s that offer higher than normal O\n2 exposure for a fee. Professional athletes, especially in American football, sometimes go off-field between plays to don oxygen masks to boost performance. The pharmacological effect is doubted; a placebo effect is a more likely explanation. Available studies support a performance boost from enriched O\n2 mixtures only if it is breathed during aerobic exercise.\nOther recreational uses that do not involve breathing include pyrotechnic applications, such as George Goble's five-second ignition of barbecue grills.\n\n\n=== Industrial ===\n\nSmelting of iron ore into steel consumes 55% of commercially produced oxygen. In this process, O\n2 is injected through a high-pressure lance into molten iron, which removes sulfur impurities and excess carbon as the respective oxides, SO\n2 and CO\n2. The reactions are exothermic, so the temperature increases to 1,700 \u00b0C.\nAnother 25% of commercially produced oxygen is used by the chemical industry. Ethylene is reacted with O\n2 to create ethylene oxide, which, in turn, is converted into ethylene glycol; the primary feeder material used to manufacture a host of products, including antifreeze and polyester polymers (the precursors of many plastics and fabrics).\nMost of the remaining 20% of commercially produced oxygen is used in medical applications, metal cutting and welding, as an oxidizer in rocket fuel, and in water treatment. Oxygen is used in oxyacetylene welding burning acetylene with O\n2 to produce a very hot flame. In this process, metal up to 60 cm (24 in) thick is first heated with a small oxy-acetylene flame and then quickly cut by a large stream of O\n2.\n\n\n== Compounds ==\n\nThe oxidation state of oxygen is \u22122 in almost all known compounds of oxygen. The oxidation state \u22121 is found in a few compounds such as peroxides. Compounds containing oxygen in other oxidation states are very uncommon: \u22121/2 (superoxides), \u22121/3 (ozonides), 0 (elemental, hypofluorous acid), +1/2 (dioxygenyl), +1 (dioxygen difluoride), and +2 (oxygen difluoride).\n\n\n=== Oxides and other inorganic compounds ===\nWater (H\n2O) is an oxide of hydrogen and the most familiar oxygen compound. Hydrogen atoms are covalently bonded to oxygen in a water molecule but also have an additional attraction (about 23.3 kJ\u00b7mol\u22121 per hydrogen atom) to an adjacent oxygen atom in a separate molecule. These hydrogen bonds between water molecules hold them approximately 15% closer than what would be expected in a simple liquid with just van der Waals forces.\n\nDue to its electronegativity, oxygen forms chemical bonds with almost all other elements to give corresponding oxides. The surface of most metals, such as aluminium and titanium, are oxidized in the presence of air and become coated with a thin film of oxide that passivates the metal and slows further corrosion. Many oxides of the transition metals are non-stoichiometric compounds, with slightly less metal than the chemical formula would show. For example, the mineral FeO (w\u00fcstite) is written as Fe\n1 \u2212 xO, where x is usually around 0.05.\nOxygen is present in the atmosphere in trace quantities in the form of carbon dioxide (CO\n2). The Earth's crustal rock is composed in large part of oxides of silicon (silica SiO\n2, as found in granite and quartz), aluminium (aluminium oxide Al\n2O\n3, in bauxite and corundum), iron (iron(III) oxide Fe\n2O\n3, in hematite and rust), and calcium carbonate (in limestone). The rest of the Earth's crust is also made of oxygen compounds, in particular various complex silicates (in silicate minerals). The Earth's mantle, of much larger mass than the crust, is largely composed of silicates of magnesium and iron.\nWater-soluble silicates in the form of Na\n4SiO\n4, Na\n2SiO\n3, and Na\n2Si\n2O\n5 are used as detergents and adhesives.\nOxygen also acts as a ligand for transition metals, forming transition metal dioxygen complexes, which feature metal\u2013O\n2. This class of compounds includes the heme proteins hemoglobin and myoglobin. An exotic and unusual reaction occurs with PtF\n6, which oxidizes oxygen to give O2+PtF6\u2212.\n\n\n=== Organic compounds and biomolecules ===\n\nAmong the most important classes of organic compounds that contain oxygen are (where \"R\" is an organic group): alcohols (R-OH); ethers (R-O-R); ketones (R-CO-R); aldehydes (R-CO-H); carboxylic acids (R-COOH); esters (R-COO-R); acid anhydrides (R-CO-O-CO-R); and amides (R-C(O)-NR\n2). There are many important organic solvents that contain oxygen, including: acetone, methanol, ethanol, isopropanol, furan, THF, diethyl ether, dioxane, ethyl acetate, DMF, DMSO, acetic acid, and formic acid. Acetone ((CH\n3)\n2CO) and phenol (C\n6H\n5OH) are used as feeder materials in the synthesis of many different substances. Other important organic compounds that contain oxygen are: glycerol, formaldehyde, glutaraldehyde, citric acid, acetic anhydride, and acetamide. Epoxides are ethers in which the oxygen atom is part of a ring of three atoms.\nOxygen reacts spontaneously with many organic compounds at or below room temperature in a process called autoxidation. Most of the organic compounds that contain oxygen are not made by direct action of O\n2. Organic compounds important in industry and commerce that are made by direct oxidation of a precursor include ethylene oxide and peracetic acid.\nThe element is found in almost all biomolecules that are important to (or generated by) life. Only a few common complex biomolecules, such as squalene and the carotenes, contain no oxygen. Of the organic compounds with biological relevance, carbohydrates contain the largest proportion by mass of oxygen. All fats, fatty acids, amino acids, and proteins contain oxygen (due to the presence of carbonyl groups in these acids and their ester residues). Oxygen also occurs in phosphate (PO3\u2212\n4) groups in the biologically important energy-carrying molecules ATP and ADP, in the backbone and the purines (except adenine) and pyrimidines of RNA and DNA, and in bones as calcium phosphate and hydroxylapatite.\n\n\n== Safety and precautions ==\nThe NFPA 704 standard rates compressed oxygen gas as nonhazardous to health, nonflammable and nonreactive, but an oxidizer. Refrigerated liquid oxygen (LOX) is given a health hazard rating of 3 (for increased risk of hyperoxia from condensed vapors, and for hazards common to cryogenic liquids such as frostbite), and all other ratings are the same as the compressed gas form.\n\n\n=== Toxicity ===\n\nOxygen gas (O\n2) can be toxic at elevated partial pressures, leading to convulsions and other health problems. Oxygen toxicity usually begins to occur at partial pressures more than 50 kilopascals (kPa), equal to about 50% oxygen composition at standard pressure or 2.5 times the normal sea-level O\n2 partial pressure of about 21 kPa. This is not a problem except for patients on mechanical ventilators, since gas supplied through oxygen masks in medical applications is typically composed of only 30%\u201350% O\n2 by volume (about 30 kPa at standard pressure). (although this figure also is subject to wide variation, depending on type of mask).\nAt one time, premature babies were placed in incubators containing O\n2-rich air, but this practice was discontinued after some babies were blinded by the oxygen content being too high.\nBreathing pure O\n2 in space applications, such as in some modern space suits, or in early spacecraft such as Apollo, causes no damage due to the low total pressures used. In the case of spacesuits, the O\n2 partial pressure in the breathing gas is, in general, about 30 kPa (1.4 times normal), and the resulting O\n2 partial pressure in the astronaut's arterial blood is only marginally more than normal sea-level O\n2 partial pressure (for more information on this, see space suit and arterial blood gas).\nOxygen toxicity to the lungs and central nervous system can also occur in deep scuba diving and surface supplied diving. Prolonged breathing of an air mixture with an O\n2 partial pressure more than 60 kPa can eventually lead to permanent pulmonary fibrosis. Exposure to a O\n2 partial pressures greater than 160 kPa (about 1.6 atm) may lead to convulsions (normally fatal for divers). Acute oxygen toxicity (causing seizures, its most feared effect for divers) can occur by breathing an air mixture with 21% O\n2 at 66 m (217 ft) or more of depth; the same thing can occur by breathing 100% O\n2 at only 6 m (20 ft).\n\n\n=== Combustion and other hazards ===\n\nHighly concentrated sources of oxygen promote rapid combustion. Fire and explosion hazards exist when concentrated oxidants and fuels are brought into close proximity; an ignition event, such as heat or a spark, is needed to trigger combustion. Oxygen is the oxidant, not the fuel, but nevertheless the source of most of the chemical energy released in combustion. Combustion hazards also apply to compounds of oxygen with a high oxidative potential, such as peroxides, chlorates, nitrates, perchlorates, and dichromates because they can donate oxygen to a fire.\nConcentrated O\n2 will allow combustion to proceed rapidly and energetically. Steel pipes and storage vessels used to store and transmit both gaseous and liquid oxygen will act as a fuel; and therefore the design and manufacture of O\n2 systems requires special training to ensure that ignition sources are minimized. The fire that killed the Apollo 1 crew in a launch pad test spread so rapidly because the capsule was pressurized with pure O\n2 but at slightly more than atmospheric pressure, instead of the \u200a1\u20443 normal pressure that would be used in a mission.\nLiquid oxygen spills, if allowed to soak into organic matter, such as wood, petrochemicals, and asphalt can cause these materials to detonate unpredictably on subsequent mechanical impact. As with other cryogenic liquids, on contact with the human body it can cause frostbites to the skin and the eyes.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== Citations ==\n\n\n== References ==\nCook, Gerhard A.; Lauer, Carol M. (1968). \"Oxygen\". In Clifford A. Hampel. The Encyclopedia of the Chemical Elements. New York: Reinhold Book Corporation. pp. 499\u2013512. LCCN 68-29938. \nEmsley, John (2001). \"Oxygen\". Nature's Building Blocks: An A-Z Guide to the Elements. Oxford, England: Oxford University Press. pp. 297\u2013304. ISBN 0-19-850340-7. \nRaven, Peter H.; Evert, Ray F.; Eichhorn, Susan E. (2005). Biology of Plants (7th ed.). New York: W.H. Freeman and Company Publishers. pp. 115\u201327. ISBN 0-7167-1007-2. \n\n\n== External links ==\n\nOxygen at The Periodic Table of Videos (University of Nottingham)\nOxidizing Agents > Oxygen\nOxygen (O2) Properties, Uses, Applications\nRoald Hoffmann article on \"The Story of O\"\nWebElements.com \u2013 Oxygen\nChemistry in its element podcast (MP3) from the Royal Society of Chemistry's Chemistry World: Oxygen\n\nOxygen on In Our Time at the BBC. (listen now)\nScripps Institute: Atmospheric Oxygen has been dropping for 20 years", 
                "titleUrl": "https://en.wikipedia.org/wiki/Oxygen", 
                "title": "Oxygen"
            }, 
            {
                "snippet": "The Oxygen XML Editor (styled <oXygen/>) is a multi-platform XML editor, XSLT/XQuery debugger and profiler with Unicode support. It is a Java application", 
                "pageCategories": "All articles with a promotional tone\nArticles with a promotional tone from January 2012\nHTML editors\nWikipedia articles with possible conflicts of interest from January 2012\nXML editors", 
                "pageContent": "The Oxygen XML Editor (styled <oXygen/>) is a multi-platform XML editor, XSLT/XQuery debugger and profiler with Unicode support. It is a Java application, so it can run in Windows, Mac OS X, and Linux. It also has a version that can run as an Eclipse plugin.\n\n\n== Release cycle ==\nOxygen XML has three types of releases, not counting betas or development versions. Major releases, such as 17 as of the end of 2015, occur on average once per year. Minor releases, 17.1 as of the end of 2015, are made at least once a few months after a major release, occasionally twice a year. Incremental build releases are provided on an as needed basis, usually in response to either bugs or security issues. Build numbering is based upon the date and time (to the hour) of the build. As of the end of 2015 the current full version and build number is \"oXygen XML Editor 17.1, build 2015121117\" with a full release history available online.\n\n\n== XML editing features ==\nOxygen XML offers a number of features for editing XML documents. Documents can be checked for proper XML form. They can also be validated against a schema. For validation purposes, the documents can be validated against DTD, W3C XML Schema, RELAX NG, Schematron, NRL and NVDL schemas. The editor can also validate the XML as it is entered. For additional schema types, a validation scenario can be generated, which allows oXygen to call out to arbitrary programs to perform validation.\nAlso, the program has support for XML catalogs. An XML catalog is an XML file of a specific format that maps a schema definition string to an actual file name on the disk or web. Using catalogs allows the user to specify a web address for a schema, but allows oXygen to find a file form of the address if the catalog specifies one.\nOxygen XML comes with schemas and DTDs for popular or major XML and XSL formats including DocBook (versions 4.0 and 5.0), TEI format, XSLT (versions 1.0, 2.0 and 3.0), DITA, XHTML and HTML 5. Extending to new XML dialects or specialisations is achieved by adding the relevant framework or implementation to the software or loading the document type or schema, thus enabling an adaptable environment which is itself configured entirely by XML, which draws parallels with Emacs and its ability to edit itself while implemnting the Lisp dialect it runs in.\nThe program is aware of XInclude, and all validation and transformation services can follow the XInclude statements to their included files.\nOxygen XML offers three views designed for editing XML documents. These views are text, grid, and author.\n\n\n=== Text view ===\nThe text view is the default view for editing an XML document. As the name suggests, this view shows the XML text as text.\nFor documents that are associated with an XML schema, Oxygen XML offers tag completion. Oxygen XML can use a number of XML schema languages, including DTD, W3C XML Schema, RELAX NG (both compact and full). Both W3C XML Schema and RELAX NG schemas can include embedded Schematron rules. It also can use the NRL and NVDL routing languages, which allow multiple schemas of different types to be applied to different files.\nIn addition to tag completion, annotations in the schema will be displayed as tooltips for the elements that those annotations apply to.\nFor schema formats that do not have a standard mechanism to bind the schema to the XML file, Oxygen XML provides a processing instruction that instructs the program as to which schemas to use.\nFor documents that do not have a schema, Oxygen can analyze the structure of the document and generate a schema.\n\n\n=== Grid view ===\nThe grid view shows the XML document in a spreadsheet-like fashion. The left-most column shows the elements, including comments and processing instructions, at the root level. The next column shows attributes of root elements, and every unique first child of the root XML element. If the root element has six children all named \"section\", then the grid view will show only one section element and a notation that there are six of them. This iteration continues for the next column.\nThis view is not often useful for HTML or other document-like formats, but it can be useful for certain XML formats that resemble spreadsheets.\nWith the exception of spaces, this view shows the entire structure of the XML file. All of the textual information in the file will be presented in this view.\n\n\n=== Author view ===\nNew to Oxygen XML v9.x was an author view providing a WYSIWYM view of the XML document. This smaller version of the Editor, called oXygenXML Author, is provided as a cheaper option in the commercial options where the full feature set may not be required. Author is centered on general XML document editing.\nThis view is based on providing a CSS file for the document that specifies the data type for each element in the document's schema. Oxygen XML comes with document CSS files for formats like DITA, DocBook, and TEI.\nXML tags and attributes in this view can be completely disabled or can be shown in various combinations.\nEditing in this view is an intermediate step between true WYSIWYG and editing in the regular text view in terms of complexity for the author. The XML elements are made more human-readable and intuitive, but the nesting and semantics of the XML document are still clear. The cursor can be placed between any elements, and when the cursor's position is ambiguous, a tool-tip window will appear showing a local view of the XML tree and the cursor's position in it. A bar along the top of the view shows the list of elements from the document root to the element under the cursor.\nXML elements are never implicitly inserted into the document. However, a common action in editing document-like XML files is to create a new element of the same name following the current one. The author view will perform this operation if the user presses the enter key twice (pressing it once brings up a dialog of possible elements to add, if tag competition is available).\nInserting elements can be done through oXygen's XML refactoring commands to insert an element at the current cursor location. Even if XML tags are set to be non-visible, an indication for an empty element is always displayed using that element's name.\nAttributes on XML elements cannot directly be edited. However, Oxygen XML does have an attribute panel that, when content completion information is available, can be used to both see and set the value of attributes on the current element.\n\n\n== Editing of specialized XML formats ==\nThough Oxygen XML can edit any XML document, providing content completion for documents with a schema binding, it is able to recognize certain XML documents innately.\nOxygen XML provides schema editing features for both W3C XML Schema and RELAX NG's XML form. It offers visual editing support for both, as well as schema-less syntax highlighting and content completion.\nOxygen XML offers support for XSLT documents, both version 1.0 (with EXSLT extensions) and 2.0. XSLT elements are recognized and drawn in a different color from non-XSLT XML elements. It also provides special validation services for XSLT documents. For example, it can validate that an attribute containing an XPath string is a valid XPath. oXygen XML automatically assumes that documents with the .xsl and .xslt extensions are XSLT files, and it treats them accordingly.\nIt also offers support for editing XSL-FO documents, though it does not provide visual editing features for it.\n\n\n== Editing of non-XML files ==\nThough Oxygen XML is primarily an XML editor, it does come with the ability to edit a number of non-XML textual formats. It has syntax completion for DTD, RELAX NG's compact format, XQuery, CSS and regular HTML. It also provides basic syntax highlighting support for a number of common web scripting languages to a degree, such as Python, Perl, and JavaScript, among others.\n\n\n== Document transformation ==\nXSLT-based document transformation is a common operation on XML files, and Oxygen XML provides support for these operations. It allows the user to define a transformation scenario that specifies the application of a particular XSLT file to the current XML document. Each transformation scenario is aware of all of the parameters of its designated XSLT file and provides for editing them graphically.\nAdditionally, the results of the transformation scenario can be piped through an XSL-FO processor, whether that be the built-in FOP processor or an external one.\nThe final output filename, path and extension can be specified for a transform scenario, as can command-line parameters.\nTransform scenarios can be local to a particular Oxygen XML-project workspace or global to all projects. Oxygen XML comes with a number of standard global-transform scenarios for common tasks, e.g., from DocBook documents into PDF through XSL-FO and FOP, or into HTML. It also comes with a recent version of the DocBook XSL XSLT transformation suite. Oxygen XML comes with DITA Open Toolkit, which allows publishing (exporting) entire DITA-document structures to different output formats, including PDF, WebHelp, and EPUB.\n\n\n== XSLT debugger ==\nOxygen XML provides comprehensive debugging facilities for XSLTs. It offers features comparable to source-code debuggers like gdb, including breakpoints, the ability to look at the current context and \"memory\", and single-stepping through the XSLT. It can debug both XSLT version 1.0 and 2.0.\n\n\n== See also ==\nList of XML editors\nComparison of XML editors\nComparison of HTML editors\nOffice Open XML software\n\n\n== Licensing ==\nA choice of either \"Named User\" or server based floating licensing. The former favours small businesses or individual developers, who may install it anywhere as long as it is just the specific named user utilising it. The latter favours larger teams that can benefit by sharing licenses across a global network as timezones shift. An additional group license is available for the academic version only.\nAcademic licenses are available to academic staff, students and educational institutions, but the license limits use of the software to academic or research purposes only and cannot be used for commercial purposes.\nCommercial licenses are available in a Professional stream and an Enterprise stream for both the Author only edition and the full Editor edition. Either user or floating licenses are available for each stream with value gaining for the latter with a larger number of users. Chief differences between the Enterprise edition and the Professional edition is the high-end databases directly supported. Though the Professional edition still provides direct support for Berkely DB, MySQL, PostgreSQL, JDBC connections and generating an XML schema from a relational database structure.\nAdditionally there is a Personal addition for independent developers or free lancers paying for it themselves rather than by their employer. The Personal edition is identical to the Professional edition with regards to features, the only differences are the much lower price, though higher than a single academic license, as well as providing the full Editor edition rather than just the Author component.\nAn optional support and maintenance subscription is available which includes full upgrades, including for major releases during the maintenance period. With 17 major releases across 13 years of operation the annual maintenance offers (averaging around 20% of the full license cost) is cost effective. The term of the maintenance can be extended effectively indefinitely by renewing prior to the expiration date.\n\n\n== References ==\n\n\n== External links ==\nOxygen XML Editor Web Site\nOxygen XML Editor download page\nDocumentation (PDF and webhelp links)\nCompany Web Site\ndescription of Oxygen XML Editor (German)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Oxygen_XML_Editor", 
                "title": "Oxygen XML Editor"
            }, 
            {
                "snippet": "Oxygen toxicity is a condition resulting from the harmful effects of breathing molecular oxygen (O  2) at increased partial pressures. It is also known", 
                "pageCategories": "Articles with contributors link\nCS1 Hebrew-language sources (he)\nCS1 maint: Multiple names: authors list\nDiving medicine\nElement toxicology\nFeatured articles\nIntensive care medicine\nNeurobiological brain disorder\nOxygen\nRespiratory diseases", 
                "pageContent": "Oxygen toxicity is a condition resulting from the harmful effects of breathing molecular oxygen (O\n2) at increased partial pressures. It is also known as oxygen toxicity syndrome, oxygen intoxication, and oxygen poisoning. Historically, the central nervous system condition was called the Paul Bert effect, and the pulmonary condition the Lorrain Smith effect, after the researchers who pioneered its discovery and description in the late 19th century. Severe cases can result in cell damage and death, with effects most often seen in the central nervous system, lungs and eyes. Oxygen toxicity is a concern for underwater divers, those on high concentrations of supplemental oxygen (particularly premature babies), and those undergoing hyperbaric oxygen therapy.\nThe result of breathing increased partial pressures of oxygen is hyperoxia, an excess of oxygen in body tissues. The body is affected in different ways depending on the type of exposure. Central nervous system toxicity is caused by short exposure to high partial pressures of oxygen at greater than atmospheric pressure. Pulmonary and ocular toxicity result from longer exposure to increased oxygen levels at normal pressure. Symptoms may include disorientation, breathing problems, and vision changes such as myopia. Prolonged exposure to above-normal oxygen partial pressures, or shorter exposures to very high partial pressures, can cause oxidative damage to cell membranes, collapse of the alveoli in the lungs, retinal detachment, and seizures. Oxygen toxicity is managed by reducing the exposure to increased oxygen levels. Studies show that, in the long term, a robust recovery from most types of oxygen toxicity is possible.\nProtocols for avoidance of the effects of hyperoxia exist in fields where oxygen is breathed at higher-than-normal partial pressures, including underwater diving using compressed breathing gases, hyperbaric medicine, neonatal care and human spaceflight. These protocols have resulted in the increasing rarity of seizures due to oxygen toxicity, with pulmonary and ocular damage being mainly confined to the problems of managing premature infants.\nIn recent years, oxygen has become available for recreational use in oxygen bars. The US Food and Drug Administration has warned those suffering from problems such as heart or lung disease not to use oxygen bars. Scuba divers use breathing gases containing up to 100% oxygen, and should have specific training in using such gases.\n\n\n== Classification ==\n\nThe effects of oxygen toxicity may be classified by the organs affected, producing three principal forms:\nCentral nervous system, characterised by convulsions followed by unconsciousness, occurring under hyperbaric conditions;\nPulmonary (lungs), characterised by difficulty in breathing and pain within the chest, occurring when breathing increased pressures of oxygen for extended periods;\nOcular (retinopathic conditions), characterised by alterations to the eyes, occurring when breathing increased pressures of oxygen for extended periods.\nCentral nervous system oxygen toxicity can cause seizures, brief periods of rigidity followed by convulsions and unconsciousness, and is of concern to divers who encounter greater than atmospheric pressures. Pulmonary oxygen toxicity results in damage to the lungs, causing pain and difficulty in breathing. Oxidative damage to the eye may lead to myopia or partial detachment of the retina. Pulmonary and ocular damage are most likely to occur when supplemental oxygen is administered as part of a treatment, particularly to newborn infants, but are also a concern during hyperbaric oxygen therapy.\nOxidative damage may occur in any cell in the body but the effects on the three most susceptible organs will be the primary concern. It may also be implicated in damage to red blood cells (haemolysis), the liver, heart, endocrine glands (adrenal glands, gonads, and thyroid), or kidneys, and general damage to cells.\nIn unusual circumstances, effects on other tissues may be observed: it is suspected that during spaceflight, high oxygen concentrations may contribute to bone damage. Hyperoxia can also indirectly cause carbon dioxide narcosis in patients with lung ailments such as chronic obstructive pulmonary disease or with central respiratory depression. Hyperventilation of atmospheric air at atmospheric pressures does not cause oxygen toxicity, because sea-level air has a partial pressure of oxygen of 0.21 bar (21 kPa) whereas toxicity does not occur below 0.3 bar (30 kPa).\n\n\n== Signs and symptoms ==\n\n\n=== Central nervous system ===\nCentral nervous system oxygen toxicity manifests as symptoms such as visual changes (especially tunnel vision), ringing in the ears (tinnitus), nausea, twitching (especially of the face), behavioural changes (irritability, anxiety, confusion), and dizziness. This may be followed by a tonic\u2013clonic seizure consisting of two phases: intense muscle contraction occurs for several seconds (tonic phase); followed by rapid spasms of alternate muscle relaxation and contraction producing convulsive jerking (clonic phase). The seizure ends with a period of unconsciousness (the postictal state). The onset of seizure depends upon the partial pressure of oxygen in the breathing gas and exposure duration. However, exposure time before onset is unpredictable, as tests have shown a wide variation, both amongst individuals, and in the same individual from day to day. In addition, many external factors, such as underwater immersion, exposure to cold, and exercise will decrease the time to onset of central nervous system symptoms. Decrease of tolerance is closely linked to retention of carbon dioxide. Other factors, such as darkness and caffeine, increase tolerance in test animals, but these effects have not been proven in humans.\n\n\n=== Lungs ===\nPulmonary toxicity symptoms result from an inflammation that starts in the airways leading to the lungs and then spreads into the lungs (tracheobronchial tree). The symptoms appear in the upper chest region (substernal and carinal regions). This begins as a mild tickle on inhalation and progresses to frequent coughing. If breathing increased partial pressures of oxygen continues, patients experience a mild burning on inhalation along with uncontrollable coughing and occasional shortness of breath (dyspnoea). Physical findings related to pulmonary toxicity have included bubbling sounds heard through a stethoscope (bubbling rales), fever, and increased blood flow to the lining of the nose (hyperaemia of the nasal mucosa). X-rays of the lungs show little change in the short term, but extended exposure leads to increasing diffuse shadowing throughout both lungs. Pulmonary function measurements are reduced, as noted by a reduction in the amount of air that the lungs can hold (vital capacity) and changes in expiratory function and lung elasticity. Tests in animals have indicated a variation in tolerance similar to that found in central nervous system toxicity, as well as significant variations between species. When the exposure to oxygen above 0.5 bar (50 kPa) is intermittent, it permits the lungs to recover and delays the onset of toxicity.\n\n\n=== Eyes ===\nIn premature babies, signs of damage to the eye (retinopathy of prematurity, or ROP) are observed via an ophthalmoscope as a demarcation between the vascularised and non-vascularised regions of an infant's retina. The degree of this demarcation is used to designate four stages: (I) the demarcation is a line; (II) the demarcation becomes a ridge; (III) growth of new blood vessels occurs around the ridge; (IV) the retina begins to detach from the inner wall of the eye (choroid).\n\n\n== Causes ==\nOxygen toxicity is caused by exposure to oxygen at partial pressures greater than those to which the body is normally exposed. This occurs in three principal settings: underwater diving, hyperbaric oxygen therapy, and the provision of supplemental oxygen, particularly to premature infants. In each case, the risk factors are markedly different.\n\n\n=== Central nervous system toxicity ===\n\nExposures, from minutes to a few hours, to partial pressures of oxygen above 1.6 bars (160 kPa)\u2014about eight times normal atmospheric partial pressure\u2014are usually associated with central nervous system oxygen toxicity and are most likely to occur among patients undergoing hyperbaric oxygen therapy and divers. Since sea level atmospheric pressure is about 1 bar (100 kPa), central nervous system toxicity can only occur under hyperbaric conditions, where ambient pressure is above normal. Divers breathing air at depths beyond 60 m (200 ft) face an increasing risk of an oxygen toxicity \"hit\" (seizure). Divers breathing a gas mixture enriched with oxygen, such as nitrox, can similarly suffer a seizure at shallower depths, should they descend below the maximum operating depth allowed for the mixture.\n\n\n=== Lung toxicity ===\n\nThe lungs and the remainder of the respiratory tract are exposed to the highest concentration of oxygen in the human body and are therefore the first organs to show toxicity. Pulmonary toxicity occurs only with exposure to partial pressures of oxygen greater than 0.5 bar (50 kPa), corresponding to an oxygen fraction of 50% at normal atmospheric pressure. The earliest signs of pulmonary toxicity begin with evidence of tracheobronchitis, or inflammation of the upper airways, after an asymptomatic period between 4 and 22 hours at greater than 95% oxygen, with some studies suggesting symptoms usually begin after approximately 14 hours at this level of oxygen.\nAt partial pressures of oxygen of 2 to 3 bar (200 to 300 kPa)\u2014100% oxygen at 2 to 3 times atmospheric pressure\u2014these symptoms may begin as early as 3 hours after exposure to oxygen. Experiments on rats breathing oxygen at pressures between 1 and 3 bars (100 and 300 kPa) suggest that pulmonary manifestations of oxygen toxicity may not be the same for normobaric conditions as they are for hyperbaric conditions. Evidence of decline in lung function as measured by pulmonary function testing can occur as quickly as 24 hours of continuous exposure to 100% oxygen, with evidence of diffuse alveolar damage and the onset of acute respiratory distress syndrome usually occurring after 48 hours on 100% oxygen. Breathing 100% oxygen also eventually leads to collapse of the alveoli (atelectasis), while\u2014at the same partial pressure of oxygen\u2014the presence of significant partial pressures of inert gases, typically nitrogen, will prevent this effect.\nPreterm newborns are known to be at higher risk for bronchopulmonary dysplasia with extended exposure to high concentrations of oxygen. Other groups at higher risk for oxygen toxicity are patients on mechanical ventilation with exposure to levels of oxygen greater than 50%, and patients exposed to chemicals that increase risk for oxygen toxicity such the chemotherapeutic agent bleomycin. Therefore, current guidelines for patients on mechanical ventilation in intensive care recommends keeping oxygen concentration less than 60%. Likewise, divers who undergo treatment of decompression sickness are at increased risk of oxygen toxicity as treatment entails exposure to long periods of oxygen breathing under hyperbaric conditions, in addition to any oxygen exposure during the dive.\n\n\n=== Eye toxicity ===\n\nProlonged exposure to high inspired fractions of oxygen causes damage to the retina. Damage to the developing eye of infants exposed to high oxygen fraction at normal pressure has a different mechanism and effect from the eye damage experienced by adult divers under hyperbaric conditions. Hyperoxia may be a contributing factor for the disorder called retrolental fibroplasia or retinopathy of prematurity (ROP) in infants. In preterm infants, the retina is often not fully vascularised. Retinopathy of prematurity occurs when the development of the retinal vasculature is arrested and then proceeds abnormally. Associated with the growth of these new vessels is fibrous tissue (scar tissue) that may contract to cause retinal detachment. Supplemental oxygen exposure, while a risk factor, is not the main risk factor for development of this disease. Restricting supplemental oxygen use does not necessarily reduce the rate of retinopathy of prematurity, and may raise the risk of hypoxia-related systemic complications.\nHyperoxic myopia has occurred in closed circuit oxygen rebreather divers with prolonged exposures. It also occurs frequently in those undergoing repeated hyperbaric oxygen therapy. This is due to an increase in the refractive power of the lens, since axial length and keratometry readings do not reveal a corneal or length basis for a myopic shift. It is usually reversible with time.\n\n\n== Mechanism ==\n\nThe biochemical basis for the toxicity of oxygen is the partial reduction of oxygen by one or two electrons to form reactive oxygen species, which are natural by-products of the normal metabolism of oxygen and have important roles in cell signalling. One species produced by the body, the superoxide anion (O\n2\u2212), is possibly involved in iron acquisition. Higher than normal concentrations of oxygen lead to increased levels of reactive oxygen species. Oxygen is necessary for cell metabolism, and the blood supplies it to all parts of the body. When oxygen is breathed at high partial pressures, a hyperoxic condition will rapidly spread, with the most vascularised tissues being most vulnerable. During times of environmental stress, levels of reactive oxygen species can increase dramatically, which can damage cell structures and produce oxidative stress.\nWhile all the reaction mechanisms of these species within the body are not yet fully understood, one of the most reactive products of oxidative stress is the hydroxyl radical (\u00b7OH), which can initiate a damaging chain reaction of lipid peroxidation in the unsaturated lipids within cell membranes. High concentrations of oxygen also increase the formation of other free radicals, such as nitric oxide, peroxynitrite, and trioxidane, which harm DNA and other biomolecules. Although the body has many antioxidant systems such as glutathione that guard against oxidative stress, these systems are eventually overwhelmed at very high concentrations of free oxygen, and the rate of cell damage exceeds the capacity of the systems that prevent or repair it. Cell damage and cell death then result.\n\n\n== Diagnosis ==\nDiagnosis of central nervous system oxygen toxicity in divers prior to seizure is difficult as the symptoms of visual disturbance, ear problems, dizziness, confusion and nausea can be due to many factors common to the underwater environment such as narcosis, congestion and coldness. However, these symptoms may be helpful in diagnosing the first stages of oxygen toxicity in patients undergoing hyperbaric oxygen therapy. In either case, unless there is a prior history of epilepsy or tests indicate hypoglycaemia, a seizure occurring in the setting of breathing oxygen at partial pressures greater than 1.4 bar (140 kPa) suggests a diagnosis of oxygen toxicity.\nDiagnosis of bronchopulmonary dysplasia in newborn infants with breathing difficulties is difficult in the first few weeks. However, if the infant's breathing does not improve during this time, blood tests and x-rays may be used to confirm bronchopulmonary dysplasia. In addition, an echocardiogram can help to eliminate other possible causes such as congenital heart defects or pulmonary arterial hypertension.\nThe diagnosis of retinopathy of prematurity in infants is typically suggested by the clinical setting. Prematurity, low birth weight and a history of oxygen exposure are the principal indicators, while no hereditary factors have been shown to yield a pattern.\n\n\n== Prevention ==\n\nThe prevention of oxygen toxicity depends entirely on the setting. Both underwater and in space, proper precautions can eliminate the most pernicious effects. Premature infants commonly require supplemental oxygen to treat complications of preterm birth. In this case prevention of bronchopulmonary dysplasia and retinopathy of prematurity must be carried out without compromising a supply of oxygen adequate to preserve the infant's life.\n\n\n=== Underwater ===\n\nOxygen toxicity is a catastrophic hazard in diving, because a seizure results in near certain death by drowning. The seizure may occur suddenly and with no warning symptoms. The effects are sudden convulsions and unconsciousness, during which victims can lose their regulator and drown. One of the advantages of a full-face diving mask is prevention of regulator loss in the event of a seizure. As there is an increased risk of central nervous system oxygen toxicity on deep dives, long dives and dives where oxygen-rich breathing gases are used, divers are taught to calculate a maximum operating depth for oxygen-rich breathing gases, and cylinders containing such mixtures must be clearly marked with that depth.\nIn some diver training courses for these types of diving, divers are taught to plan and monitor what is called the oxygen clock of their dives. This is a notional alarm clock, which ticks more quickly at increased oxygen pressure and is set to activate at the maximum single exposure limit recommended in the National Oceanic and Atmospheric Administration Diving Manual. For the following partial pressures of oxygen the limits are: 45 minutes at 1.6 bar (160 kPa), 120 minutes at 1.5 bar (150 kPa), 150 minutes at 1.4 bar (140 kPa), 180 minutes at 1.3 bar (130 kPa) and 210 minutes at 1.2 bar (120 kPa), but it is impossible to predict with any reliability whether or when toxicity symptoms will occur. Many nitrox-capable dive computers calculate an oxygen loading and can track it across multiple dives. The aim is to avoid activating the alarm by reducing the partial pressure of oxygen in the breathing gas or by reducing the time spent breathing gas of greater oxygen partial pressure. As the partial pressure of oxygen increases with the fraction of oxygen in the breathing gas and the depth of the dive, the diver obtains more time on the oxygen clock by diving at a shallower depth, by breathing a less oxygen-rich gas, or by shortening the duration of exposure to oxygen-rich gases.\nDiving below 56 m (184 ft) on air would expose a diver to increasing danger of oxygen toxicity as the partial pressure of oxygen exceeds 1.4 bar (140 kPa), so a gas mixture must be used which contains less than 21% oxygen (a hypoxic mixture). Increasing the proportion of nitrogen is not viable, since it would produce a strongly narcotic mixture. However, helium is not narcotic, and a usable mixture may be blended either by completely replacing nitrogen with helium (the resulting mix is called heliox), or by replacing part of the nitrogen with helium, producing a trimix.\nPulmonary oxygen toxicity is an entirely avoidable event while diving. The limited duration and naturally intermittent nature of most diving makes this a relatively rare (and even then, reversible) complication for divers. Established guidelines enable divers to calculate when they are at risk of pulmonary toxicity.\n\n\n=== Hyperbaric setting ===\nThe presence of a fever or a history of seizure is a relative contraindication to hyperbaric oxygen treatment. The schedules used for treatment of decompression illness allow for periods of breathing air rather than 100% oxygen (oxygen breaks) to reduce the chance of seizure or lung damage. The U.S. Navy uses treatment tables based on periods alternating between 100% oxygen and air. For example, USN table 6 requires 75 minutes (three periods of 20 minutes oxygen/5 minutes air) at an ambient pressure of 2.8 standard atmospheres (280 kPa), equivalent to a depth of 18 metres (60 ft). This is followed by a slow reduction in pressure to 1.9 atm (190 kPa) over 30 minutes on oxygen. The patient then remains at that pressure for a further 150 minutes, consisting of two periods of 15 minutes air/60 minutes oxygen, before the pressure is reduced to atmospheric over 30 minutes on oxygen.\nVitamin E and selenium were proposed and later rejected as a potential method of protection against pulmonary oxygen toxicity. There is however some experimental evidence in rats that vitamin E and selenium aid in preventing in vivo lipid peroxidation and free radical damage, and therefore prevent retinal changes following repetitive hyperbaric oxygen exposures.\n\n\n=== Normobaric setting ===\nBronchopulmonary dysplasia is reversible in the early stages by use of break periods on lower pressures of oxygen, but it may eventually result in irreversible lung injury if allowed to progress to severe damage. One or two days of exposure without oxygen breaks are needed to cause such damage.\nRetinopathy of prematurity is largely preventable by screening. Current guidelines require that all babies of less than 32 weeks gestational age or having a birth weight less than 1.5 kg (3.3 lb) should be screened for retinopathy of prematurity at least every two weeks. The National Cooperative Study in 1954 showed a causal link between supplemental oxygen and retinopathy of prematurity, but subsequent curtailment of supplemental oxygen caused an increase in infant mortality. To balance the risks of hypoxia and retinopathy of prematurity, modern protocols now require monitoring of blood oxygen levels in premature infants receiving oxygen.\n\n\n=== Hypobaric setting ===\nIn low-pressure environments oxygen toxicity may be avoided since the toxicity is caused by high partial pressure of oxygen, not merely by high oxygen fraction. This is illustrated by modern pure oxygen use in spacesuits, which must operate at low pressure (also historically, very high percentage oxygen and lower than normal atmospheric pressure was used in early spacecraft, for example, the Gemini and Apollo spacecraft). In such applications as extra-vehicular activity, high-fraction oxygen is non-toxic, even at breathing mixture fractions approaching 100%, because the oxygen partial pressure is not allowed to chronically exceed 0.3 bar (4.4 psi).\n\n\n== Management ==\n\nDuring hyperbaric oxygen therapy, the patient will usually breathe 100% oxygen from a mask while inside a hyperbaric chamber pressurised with air to about 2.8 bar (280 kPa). Seizures during the therapy are managed by removing the mask from the patient, thereby dropping the partial pressure of oxygen inspired below 0.6 bar (60 kPa).\nA seizure underwater requires that the diver be brought to the surface as soon as practicable. Although for many years the recommendation has been not to raise the diver during the seizure itself, owing to the danger of arterial gas embolism (AGE), there is some evidence that the glottis does not fully obstruct the airway. This has led to the current recommendation by the Diving Committee of the Undersea and Hyperbaric Medical Society that a diver should be raised during the seizure's clonic (convulsive) phase if the regulator is not in the diver's mouth \u2013 as the danger of drowning is then greater than that of AGE \u2013 but the ascent should be delayed until the end of the clonic phase otherwise. Rescuers ensure that their own safety is not compromised during the convulsive phase. They then ensure that where the victim's air supply is established it is maintained, and carry out a controlled buoyant lift. Lifting an unconscious body is taught by most diver training agencies. Upon reaching the surface, emergency services are always contacted as there is a possibility of further complications requiring medical attention. The U.S. Navy has procedures for completing the decompression stops where a recompression chamber is not immediately available.\nThe occurrence of symptoms of bronchopulmonary dysplasia or acute respiratory distress syndrome is treated by lowering the fraction of oxygen administered, along with a reduction in the periods of exposure and an increase in the break periods where normal air is supplied. Where supplemental oxygen is required for treatment of another disease (particularly in infants), a ventilator may be needed to ensure that the lung tissue remains inflated. Reductions in pressure and exposure will be made progressively, and medications such as bronchodilators and pulmonary surfactants may be used.\nRetinopathy of prematurity may regress spontaneously, but should the disease progress beyond a threshold (defined as five contiguous or eight cumulative hours of stage 3 retinopathy of prematurity), both cryosurgery and laser surgery have been shown to reduce the risk of blindness as an outcome. Where the disease has progressed further, techniques such as scleral buckling and vitrectomy surgery may assist in re-attaching the retina.\n\n\n== Prognosis ==\nAlthough the convulsions caused by central nervous system oxygen toxicity may lead to incidental injury to the victim, it remained uncertain for many years whether damage to the nervous system following the seizure could occur and several studies searched for evidence of such damage. An overview of these studies by Bitterman in 2004 concluded that following removal of breathing gas containing high fractions of oxygen, no long-term neurological damage from the seizure remains.\nThe majority of infants who have survived following an incidence of bronchopulmonary dysplasia will eventually recover near-normal lung function, since lungs continue to grow during the first 5\u20137 years and the damage caused by bronchopulmonary dysplasia is to some extent reversible (even in adults). However, they are likely be more susceptible to respiratory infections for the rest of their lives and the severity of later infections is often greater than that in their peers.\nRetinopathy of prematurity (ROP) in infants frequently regresses without intervention and eyesight may be normal in later years. Where the disease has progressed to the stages requiring surgery, the outcomes are generally good for the treatment of stage 3 ROP, but are much worse for the later stages. Although surgery is usually successful in restoring the anatomy of the eye, damage to the nervous system by the progression of the disease leads to comparatively poorer results in restoring vision. The presence of other complicating diseases also reduces the likelihood of a favourable outcome.\n\n\n== Epidemiology ==\n\nThe incidence of central nervous system toxicity among divers has decreased since the Second World War, as protocols have developed to limit exposure and partial pressure of oxygen inspired. In 1947, Donald recommended limiting the depth allowed for breathing pure oxygen to 7.6 m (25 ft), which equates to an oxygen partial pressure of 1.8 bar (180 kPa). Over time this limit has been reduced, until today a limit of 1.4 bar (140 kPa) during a recreational dive and 1.6 bar (160 kPa) during shallow decompression stops is generally recommended. Oxygen toxicity has now become a rare occurrence other than when caused by equipment malfunction and human error. Historically, the U.S. Navy has refined its Navy Diving Manual Tables to reduce oxygen toxicity incidents. Between 1995 and 1999, reports showed 405 surface-supported dives using the helium\u2013oxygen tables; of these, oxygen toxicity symptoms were observed on 6 dives (1.5%). As a result, the U.S. Navy in 2000 modified the schedules and conducted field tests of 150 dives, none of which produced symptoms of oxygen toxicity. Revised tables were published in 2001.\nThe variability in tolerance and other variable factors such as workload have resulted in the U.S. Navy abandoning screening for oxygen tolerance. Of the 6,250 oxygen-tolerance tests performed between 1976 and 1997, only 6 episodes of oxygen toxicity were observed (0.1%).\nCentral nervous system oxygen toxicity among patients undergoing hyperbaric oxygen therapy is rare, and is influenced by a number of a factors: individual sensitivity and treatment protocol; and probably therapy indication and equipment used. A study by Welslau in 1996 reported 16 incidents out of a population of 107,264 patients (0.015%), while Hampson and Atik in 2003 found a rate of 0.03%. Yildiz, Ay and Qyrdedi, in a summary of 36,500 patient treatments between 1996 and 2003, reported only 3 oxygen toxicity incidents, giving a rate of 0.008%. A later review of over 80,000 patient treatments revealed an even lower rate: 0.0024%. The reduction in incidence may be partly due to use of a mask (rather than a hood) to deliver oxygen.\nBronchopulmonary dysplasia is among the most common complications of prematurely born infants and its incidence has grown as the survival of extremely premature infants has increased. Nevertheless, the severity has decreased as better management of supplemental oxygen has resulted in the disease now being related mainly to factors other than hyperoxia.\nIn 1997 a summary of studies of neonatal intensive care units in industrialised countries showed that up to 60% of low birth weight babies developed retinopathy of prematurity, which rose to 72% in extremely low birth weight babies, defined as less than 1 kg (2.2 lb) at birth. However, severe outcomes are much less frequent: for very low birth weight babies\u2014those less than 1.5 kg (3.3 lb) at birth\u2014the incidence of blindness was found to be no more than 8%.\n\n\n== History ==\n\nCentral nervous system toxicity was first described by Paul Bert in 1878. He showed that oxygen was toxic to insects, arachnids, myriapods, molluscs, earthworms, fungi, germinating seeds, birds, and other animals. Central nervous system toxicity may be referred to as the \"Paul Bert effect\".\nPulmonary oxygen toxicity was first described by J. Lorrain Smith in 1899 when he noted central nervous system toxicity and discovered in experiments in mice and birds that 0.43 bar (43 kPa) had no effect but 0.75 bar (75 kPa) of oxygen was a pulmonary irritant. Pulmonary toxicity may be referred to as the \"Lorrain Smith effect\". The first recorded human exposure was undertaken in 1910 by Bornstein when two men breathed oxygen at 2.8 bar (280 kPa) for 30 minutes while he went on to 48 minutes with no symptoms. In 1912, Bornstein developed cramps in his hands and legs while breathing oxygen at 2.8 bar (280 kPa) for 51 minutes. Smith then went on to show that intermittent exposure to a breathing gas with less oxygen permitted the lungs to recover and delayed the onset of pulmonary toxicity.\nAlbert R. Behnke et al. in 1935 were the first to observe visual field contraction (tunnel vision) on dives between 1.0 bar (100 kPa) and 4.1 bar (410 kPa). During World War II, Donald and Yarbrough et al. performed over 2,000 experiments on oxygen toxicity to support the initial use of closed circuit oxygen rebreathers. Naval divers in the early years of oxygen rebreather diving developed a mythology about a monster called \"Oxygen Pete\", who lurked in the bottom of the Admiralty Experimental Diving Unit \"wet pot\" (a water-filled hyperbaric chamber) to catch unwary divers. They called having an oxygen toxicity attack \"getting a Pete\".\nIn the decade following World War II, Lambertsen et al. made further discoveries on the effects of breathing oxygen under pressure and methods of prevention. Their work on intermittent exposures for extension of oxygen tolerance and on a model for prediction of pulmonary oxygen toxicity based on pulmonary function are key documents in the development of standard operating procedures when breathing increased pressures of oxygen. Lambertsen's work showing the effect of carbon dioxide in decreasing time to onset of central nervous system symptoms has influenced work from current exposure guidelines to future breathing apparatus design.\nRetinopathy of prematurity was not observed before World War II, but with the availability of supplemental oxygen in the decade following, it rapidly became one of the principal causes of infant blindness in developed countries. By 1960 the use of oxygen had become identified as a risk factor and its administration restricted. The resulting fall in retinopathy of prematurity was accompanied by a rise in infant mortality and hypoxia-related complications. Since then, more sophisticated monitoring and diagnosis have established protocols for oxygen use which aim to balance between hypoxic conditions and problems of retinopathy of prematurity.\nBronchopulmonary dysplasia was first described by Northway in 1967, who outlined the conditions that would lead to the diagnosis. This was later expanded by Bancalari and in 1988 by Shennan, who suggested the need for supplemental oxygen at 36 weeks could predict long-term outcomes. Nevertheless, Palta et al. in 1998 concluded that radiographic evidence was the most accurate predictor of long-term effects.\nBitterman et al. in 1986 and 1995 showed that darkness and caffeine would delay the onset of changes to brain electrical activity in rats. In the years since, research on central nervous system toxicity has centred on methods of prevention and safe extension of tolerance. Sensitivity to central nervous system oxygen toxicity has been shown to be affected by factors such as circadian rhythm, drugs, age, and gender. In 1988, Hamilton et al. wrote procedures for the National Oceanic and Atmospheric Administration to establish oxygen exposure limits for habitat operations. Even today, models for the prediction of pulmonary oxygen toxicity do not explain all the results of exposure to high partial pressures of oxygen.\n\n\n== Society and culture ==\n\nRecreational scuba divers commonly breathe nitrox containing up to 40% oxygen, while technical divers use pure oxygen or nitrox containing up to 80% oxygen. Divers who breathe oxygen fractions greater than of air (21%) need to be trained in the dangers of oxygen toxicity and how to prevent them. In order to buy nitrox, a diver has to show evidence of such qualification.\nSince the late 1990s the recreational use of oxygen has been promoted by oxygen bars, where customers breathe oxygen through a nasal cannula. Claims have been made that this reduces stress, increases energy, and lessens the effects of hangovers and headaches, despite the lack of any scientific evidence to support them. There are also devices on sale that offer \"oxygen massage\" and \"oxygen detoxification\" with claims of removing body toxins and reducing body fat. The American Lung Association has stated \"there is no evidence that oxygen at the low flow levels used in bars can be dangerous to a normal person's health\", but the U.S. Center for Drug Evaluation and Research cautions that people with heart or lung disease need their supplementary oxygen carefully regulated and should not use oxygen bars.\nVictorian society had a fascination for the rapidly expanding field of science. In \"Dr. Ox's Experiment\", a short story written by Jules Verne in 1872, the eponymous doctor uses electrolysis of water to separate oxygen and hydrogen. He then pumps the pure oxygen throughout the town of Quiquendone, causing the normally tranquil inhabitants and their animals to become aggressive and plants to grow rapidly. An explosion of the hydrogen and oxygen in Dr Ox's factory brings his experiment to an end. Verne summarised his story by explaining that the effects of oxygen described in the tale were his own invention. There is also a brief episode of oxygen intoxication in his \"From the Earth to the Moon\".\n\n\n== See also ==\n\nEffect of oxygen on chronic obstructive pulmonary disease\nNitrogen narcosis\n\n\n== References ==\n\n\n== Sources ==\nClark, James M; Thom, Stephen R (2003). \"Oxygen under pressure\". In Brubakk, Alf O; Neuman, Tom S. Bennett and Elliott's physiology and medicine of diving (5th ed.). United States: Saunders. pp. 358\u2013418. ISBN 978-0-7020-2571-6. OCLC 51607923. \nClark, John M; Lambertsen, Christian J (1970). \"Pulmonary oxygen tolerance in man and derivation of pulmonary oxygen tolerance curves\". IFEM Report No. 1-70. Philadelphia, PA: Environmental Biomedical Stress Data Center, Institute for Environmental Medicine, University of Pennsylvania Medical Center. Retrieved 29 April 2008. \nDonald, Kenneth W (1947). \"Oxygen Poisoning in Man: Part I\". British Medical Journal. 1 (4506): 667\u2013672. doi:10.1136/bmj.1.4506.667. PMC 2053251. PMID 20248086. \nDonald, Kenneth W (1947). \"Oxygen Poisoning in Man: Part II\". British Medical Journal. 1 (4507): 712\u2013717. doi:10.1136/bmj.1.4507.712. PMC 2053400. PMID 20248096. \nRevised version of Donald's articles also available as:\nDonald, Kenneth W (1992). Oxygen and the diver. UK: Harley Swan, 237 pages. ISBN 1-85421-176-5. OCLC 26894235. \n\nHamilton, Robert W; Thalmann, Edward D (2003). \"Decompression practice\". In Brubakk, Alf O; Neuman, Tom S. Bennett and Elliott's physiology and medicine of diving (5th ed.). United States: Saunders. pp. 475\u2013479. ISBN 978-0-7020-2571-6. OCLC 51607923. \nLang, Michael A, ed. (2001). DAN nitrox workshop proceedings. Durham, NC: Divers Alert Network, 197 pages. Retrieved 20 September 2008. \nRegillo, Carl D; Brown, Gary C; Flynn, Harry W (1998). Vitreoretinal Disease: The Essentials. New York: Thieme, 693 pages. ISBN 978-0-86577-761-3. OCLC 39170393. \nU.S. Navy Supervisor of Diving (2011). U.S. Navy Diving Manual (PDF). SS521-AG-PRO-010 0910-LP-106-0957, revision 6 with Change A entered. U.S. Naval Sea Systems Command. Retrieved 29 Jan 2015. \n\n\n== Further reading ==\nLamb, John S. (1999). The Practice of Oxygen Measurement for Divers. Flagstaff: Best Publishing, 120 pages. ISBN 0-941332-68-3. OCLC 44018369. \nLippmann, John; Bugg, Stan (1993). The Diving Emergency Handbook. Teddington, UK: Underwater World Publications. ISBN 0-946020-18-3. OCLC 52056845. \nLippmann, John; Mitchell, Simon (2005). \"Oxygen\". Deeper into Diving (2nd ed.). Victoria, Australia: J.L. Publications. pp. 121\u20134. ISBN 0-9752290-1-X. OCLC 66524750. \n\n\n== External links ==\nGeneral\nThe following external site is a compendium of resources:\nRubicon Research Repository. \u2013 Online collection of the oxygen toxicity research\nSpecialised\nThe following external sites contain resources specific to particular topics:\n2008 Divers Alert Network Technical Diving Conference. \u2013 Video of \"Oxygen Toxicity\" lecture by Dr. Richard Vann (free download, mp4, 86MB).\nPhysiology: 4/4ch7/s4ch7_7 - Essentials of Human Physiology. \u2013 Wide and detailed discussion of the effects of breathing oxygen on the respiratory system.\nRajiah, Prabhakar (11 March 2009). \"Bronchopulmonary Dysplasia\". eMedicine. Retrieved 29 June 2009.  \u2013 Concise clinical overview with extensive references.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Oxygen_toxicity", 
                "title": "Oxygen toxicity"
            }, 
            {
                "snippet": "dissolved oxygen probe such as an oxygen sensor or an optode in liquid media, usually water. The standard unit of oxygen saturation is percent (%). Oxygen saturation", 
                "pageCategories": "All articles with unsourced statements\nAquatic ecology\nArticles with unsourced statements from February 2015\nArticles with unsourced statements from March 2013\nBlood\nOxygen\nWater quality indicators", 
                "pageContent": "Oxygen saturation (symbol SO2) is a relative measure of the amount of oxygen that is dissolved or carried in a given medium. It can be measured with a dissolved oxygen probe such as an oxygen sensor or an optode in liquid media, usually water. The standard unit of oxygen saturation is percent (%).\nOxygen saturation can be measured regionally and noninvasively. Arterial oxygen saturation (SaO2) is commonly measured using pulse oximetry. Tissue saturation at peripheral scale can be measured using NIRS. This technique can be applied on both muscle and brain.\n\n\n== In medicine ==\n\nIn medicine, oxygen saturation refers to oxygenation, or when oxygen molecules (O\n2) enter the tissues of the body. In this case blood is oxygenated in the lungs, where oxygen molecules travel from the air and into the blood. Oxygen saturation ((O\n2) sats) is a measure the percentage of hemoglobin binding sites in the bloodstream occupied by oxygen. Fish, invertebrates, plants, and aerobic bacteria all require oxygen for respiration.\n\n\n== In environmental science ==\n\nIn aquatic environments, oxygen saturation is a ratio of the concentration of dissolved oxygen (O2) in the water to the maximum amount of oxygen that will dissolve in the water at that temperature and pressure under stable equilibrium. Well-aerated water (such as a fast-moving stream) without oxygen producers or consumers is 100 % saturated. \nIt is possible for stagnant water to become somewhat supersaturated with oxygen (i.e. reach more than 100 % saturation) either because of the presence of photosynthetic aquatic oxygen producers or because of a slow equilibration after a change of atmospheric conditions. Stagnant water in the presence of decaying matter will typically have an oxygen concentration much less than 100 %.\nEnvironmental oxygenation can be important to the sustainability of a particular ecosystem. Refer to ([1] for a table of maximum equilibrium dissolved oxygen concentration versus temperature at atmospheric pressure. The optimal levels in an estuary for dissolved oxygen is higher than 6 ppm. Insufficient oxygen (environmental hypoxia), often caused by the decomposition of organic matter and/or nutrient pollution, may occur in bodies of water such as ponds and rivers, tending to suppress the presence of aerobic organisms such as fish. Deoxygenation increases the relative population of anaerobic organisms such as plants and some bacteria, resulting in fish kills and other adverse events. The net effect is to alter the balance of nature by increasing the concentration of anaerobic over aerobic species.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Oxygen_saturation", 
                "title": "Oxygen saturation"
            }, 
            {
                "snippet": "There are three stable isotopes of oxygen (16O, 17O, and 18O). Radioactive isotopes with mass numbers from 12O to 24O have also been characterized, all", 
                "pageCategories": "All articles needing expert attention\nAll articles with failed verification\nAll articles with unsourced statements\nArticles needing expert attention from November 2012\nArticles with failed verification from April 2013\nArticles with unsourced statements from April 2013\nCS1 errors: dates\nIsotopes of oxygen\nLists of isotopes by element\nOxygen", 
                "pageContent": "There are three stable isotopes of oxygen (16O, 17O, and 18O). Radioactive isotopes with mass numbers from 12O to 24O have also been characterized, all short-lived, with the longest-lived being 15O with a half-life of 122.24 seconds. The shortest-lived is 12O with a half-life of 580(30)\u00d710\u221224 second.\n\n\n== Stable isotopes ==\nNaturally occurring oxygen is composed of three stable isotopes, 16O, 17O, and 18O, with 16O being the most abundant (99.762% natural abundance); thus oxygen (O) has a relative atomic mass of 15.9994(3). Known oxygen isotopes range in mass number from 12 to 24.\nThe relative and absolute abundance of 16O is high because it is a principal product of stellar evolution and because it is a primary isotope, meaning it can be made by stars that were initially made exclusively of hydrogen. Most 16O is synthesized at the end of the helium fusion process in stars; the triple-alpha reaction creates 12C, which captures an additional 4He to make 16O. The neon burning process creates additional 16O.\nBoth 17O and 18O are secondary isotopes, meaning that their nucleosynthesis requires seed nuclei. 17O is primarily made by the burning of hydrogen into helium during the CNO cycle, making it a common isotope in the hydrogen burning zones of stars. Most 18O is produced when 14N (made abundant from CNO burning) captures a 4He nucleus, making 18O common in the helium-rich zones of stars. Approximately a billion degrees Celsius is required for two oxygen nuclei to undergo nuclear fusion to form the heavier nucleus of sulfur.\nMeasurements of the ratio of oxygen-18 to oxygen-16 are often used to interpret changes in paleoclimate. The isotopic composition of oxygen atoms in the Earth's atmosphere is 99.759% 16O, 0.037% 17O and 0.204% 18O. Because water molecules containing the lighter isotope are slightly more likely to evaporate and fall as precipitation, fresh water and polar ice on earth contains slightly less (0.1981%) of the heavy isotope 18O than air (0.204%) or seawater (0.1995%). This disparity allows analysis of temperature patterns via historic ice cores.\nAn atomic mass of 16 was assigned to oxygen prior to the definition of the unified atomic mass unit based upon 12C. Since physicists referred to 16O only, while chemists meant the naturally-abundant mixture of isotopes, this led to slightly different mass scales between the two disciplines.\n\n\n== Radioisotopes ==\nFourteen radioisotopes have been characterized, with the most stable being 15O with a half-life of 122.24 s and 14O with a half-life of 70.606 s. All of the remaining radioactive isotopes have half-lives that are less than 27 s and the majority of these have half-lives that are less than 83 milliseconds (ms). For example, 24O has a half-life of 61 ms. The most common decay mode for isotopes lighter than the stable isotopes is \u03b2+ decay (to nitrogen) and the most common mode after is \u03b2\u2212 decay (to fluorine).\n\n\n=== Oxygen-13 ===\nOxygen-13 is an unstable isotope of oxygen. It consists of 8 protons and electrons, and 5 neutrons. It has a spin of 3/2-, and a half-life of 8.58 ms. Its atomic mass is 13.0248 Da. It decays to nitrogen-13 by electron capture, and has a decay energy of 17.765 MeV. Its parent nuclide is fluorine-14.\n\n\n=== Oxygen-15 ===\nOxygen-15 is an isotope of oxygen, frequently used in positron emission tomography, or PET imaging. It has 8 protons, 7 neutrons, and 8 electrons. The total atomic mass is 15.0030654 amu. It has a half-life of 122.24 seconds. Oxygen-15 is synthesized through deuteron bombardment of nitrogen-14 using a cyclotron.\n\n\n== Table ==\nOxygen isotopes with 25\u201328 nucleons exist for less than 1 microsecond, since they are beyond the neutron drip line.\n\n\n=== Notes ===\nThe precision of the isotope abundances and atomic mass is limited through variations. The given ranges should be applicable to any normal terrestrial material.\nValues marked # are not purely derived from experimental data, but at least partly from systematic trends. Spins with weak assignment arguments are enclosed in parentheses.\nUncertainties are given in concise form in parentheses after the corresponding last digits. Uncertainty values denote one standard deviation, except isotopic composition and standard atomic mass from IUPAC, which use expanded uncertainties.\nNuclide masses are given by IUPAP Commission on Symbols, Units, Nomenclature, Atomic Masses and Fundamental Constants (SUNAMCO).\nIsotope abundances are given by IUPAC Commission on Isotopic Abundances and Atomic Weights.\n\n\n== See also ==\nDole effect\n\n\n== Notes and references ==\n\n\n== References ==\nFor the table\nIsotope masses from the area:\nG. Audi; A. H. Wapstra; C. Thibault; J. Blachot; O. Bersillon (2003). \"The NUBASE evaluation of nuclear and decay properties\" (PDF). Nuclear Physics A. 729: 3\u2013128. Bibcode:2003NuPhA.729....3A. doi:10.1016/j.nuclphysa.2003.11.001. \n\nIsotopic compositions and standard atomic masses from:\nJ. R. de Laeter; J. K. B\u00f6hlke; P. De Bi\u00e8vre; H. Hidaka; H. S. Peiser; K. J. R. Rosman; P. D. P. Taylor (2003). \"Atomic weights of the elements. Review 2000 (IUPAC Technical Report)\" (PDF). Pure and Applied Chemistry. 75 (6): 683\u2013800. doi:10.1351/pac200375060683. \nM. E. Wieser (2006). \"Atomic weights of the elements 2005 (IUPAC Technical Report)\". Pure and Applied Chemistry. 78 (11): 2051\u20132066. doi:10.1351/pac200678112051. Lay summary. \n\nHalf-life, spin, and isomer data selected from the following sources. See editing notes on this article's talk page.\nG. Audi; A. H. Wapstra; C. Thibault; J. Blachot; O. Bersillon (2003). \"The NUBASE evaluation of nuclear and decay properties\" (PDF). Nuclear Physics A. 729: 3\u2013128. Bibcode:2003NuPhA.729....3A. doi:10.1016/j.nuclphysa.2003.11.001. \nNational Nuclear Data Center. \"NuDat 2.1 database\". Brookhaven National Laboratory. Retrieved September 2005.  \nN. E. Holden (2004). \"Table of the Isotopes\". In D. R. Lide. CRC Handbook of Chemistry and Physics (85th ed.). CRC Press. Section 11. ISBN 978-0-8493-0485-9. \n\nFor the prose\nCook, Gerhard A.; Lauer, Carol M. (1968). \"Oxygen\". In Clifford A. Hampel. The Encyclopedia of the Chemical Elements. New York: Reinhold Book Corporation. pp. 499\u2013512. LCCN 68-29938. \nEmsley, John (2001). \"Oxygen\". Nature's Building Blocks: An A\u2013Z Guide to the Elements. Oxford, England, UK: Oxford University Press. pp. 297\u2013304. ISBN 0-19-850340-7. \nParks, G. D.; Mellor, J. W. (1939). Mellor's Modern Inorganic Chemistry (6th ed.). London: Longmans, Green and Co.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Isotopes_of_oxygen", 
                "title": "Isotopes of oxygen"
            }, 
            {
                "snippet": "Oxygen is an American digital cable and satellite television channel that is owned by Oxygen Media, LLC, a subsidiary of the NBCUniversal Cable division", 
                "pageCategories": "American television networks\nEnglish-language television stations in the United States\nFormer Walt Disney Company subsidiaries\nNBCUniversal\nNBCUniversal networks\nTelevision channels and stations established in 1998\nWomen's interest channels", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Oxygen_(TV_channel)", 
                "title": "Oxygen (TV channel)"
            }, 
            {
                "snippet": "oxygen. The most familiar is molecular oxygen (O2), present at significant levels in Earth's atmosphere and also known as dioxygen or triplet oxygen.", 
                "pageCategories": "Allotropy\nOxygen", 
                "pageContent": "There are several known allotropes of oxygen. The most familiar is molecular oxygen (O2), present at significant levels in Earth's atmosphere and also known as dioxygen or triplet oxygen. Another is the highly reactive ozone (O3). Others include:\nAtomic oxygen (O1, a free radical).\nSinglet oxygen (O2*), either of two metastable states of molecular oxygen.\nTetraoxygen (O4), another metastable form.\nSolid oxygen, existing in six variously colored phases, of which one is O\n8 and another one metallic.\n\n\n== Atomic oxygen ==\nAtomic oxygen, denoted O(3P), O(3P) or O((3)P), is very reactive, as the single atoms of oxygen tend to quickly bond with nearby molecules; on Earth's surface it does not exist naturally for very long, though in outer space, the presence of plenty of ultraviolet radiation results in a low Earth orbit atmosphere in which 96% of the oxygen occurs in atomic form.\n\n\n== Dioxygen ==\n\nThe common allotrope of elemental oxygen on Earth, O\n2, is generally known as oxygen, but may be called dioxygen or molecular oxygen to distinguish it from the element itself. Elemental oxygen is most commonly encountered in this form, as about 21% (by volume) of Earth's atmosphere. The ground state of dioxygen is known as triplet oxygen because it has two unpaired electrons. The first excited state, singlet oxygen, has no unpaired electrons and is metastable.\nO\n2 has a bond length of 121 pm and a bond energy of 498 kJ/mol. It is a colourless gas with a boiling point of \u2212183 \u00b0C (90 K; \u2212297 \u00b0F). It can be condensed from air by cooling with liquid nitrogen, which has a boiling point of \u2212196 \u00b0C (77 K; \u2212321 \u00b0F). Liquid oxygen is pale blue in colour, and is quite markedly paramagnetic\u2014liquid oxygen contained in a flask suspended by a string is attracted to a magnet.\n\n\n=== Singlet oxygen ===\n\nSinglet oxygen is the common name used for the two metastable states of molecular oxygen (O2) with higher energy than the ground state triplet oxygen. Because of the differences in their electron shells, singlet oxygen has different chemical properties than triplet oxygen, including absorbing and emitting light at different wavelengths. It can be generated in a photosensitized process by energy transfer from dye molecules such as rose bengal, methylene blue or porphyrins, or by chemical processes such as spontaneous decomposition of hydrogen trioxide in water or the reaction of hydrogen peroxide with hypochlorite.\n\n\n== Ozone ==\n\nTriatomic oxygen (Ozone, O3), is a very reactive allotrope of oxygen that is destructive to materials like rubber and fabrics and is also damaging to lung tissue. Traces of it can be detected as a sharp, chlorine-like smell, coming from electric motors, laser printers, and photocopiers. It was named \"ozone\" by Christian Friedrich Sch\u00f6nbein, in 1840, from the Greek word \u1f60\u03b6\u03ce (ozo) for smell.\nOzone is thermodynamically unstable toward the more common dioxygen form, and is formed by reaction of O2 with atomic oxygen produced by splitting of O2 by UV radiation in the upper atmosphere. Ozone absorbs strongly in the ultraviolet and functions as a shield for the biosphere against the mutagenic and other damaging effects of solar UV radiation (see ozone layer). Ozone is formed near the Earth's surface by the photochemical disintegration of nitrogen dioxide from the exhaust of automobiles. Ground-level ozone is an air pollutant that is especially harmful for senior citizens, children, and people with heart and lung conditions such as emphysema, bronchitis, and asthma. The immune system produces ozone as an antimicrobial (see below). Liquid and solid O3 have a deeper blue color than ordinary oxygen and they are unstable and explosive.\nOzone is a pale blue gas condensable to a dark blue liquid. It is formed whenever air is subjected to an electrical discharge, and has the characteristic pungent odour of new-mown hay, or for those living in urban environments, of subways \u2013 the so-called 'electrical odour'.\n\n\n== Tetraoxygen ==\n\nTetraoxygen had been suspected to exist since the early 1900s, when it was known as oxozone, and was identified in 2001 by a team led by F. Cacace at the University of Rome. The molecule O\n4 was thought to be in one of the phases of solid oxygen later identified as O\n8. Cacace's team suggested that O\n4 probably consists of two dumbbell-like O\n2 molecules loosely held together by induced dipole dispersion forces.\n\n\n== Phases of solid oxygen ==\n\nThere are six known distinct phases of solid oxygen. One of them is a dark-red O\n8 cluster. When oxygen is subjected to a pressure of 96 GPa, it becomes metallic, in a similar manner as hydrogen, and becomes more similar to the heavier chalcogens, such as tellurium and polonium, both of which show significant metallic character. At very low temperatures, this phase also becomes superconducting.\n\n\n== References ==\n\n\n== Further reading ==\nParks, G. D.; Mellor, J. W. (1939). Mellor's Modern Inorganic Chemistry (6th ed.). London: Longmans, Green and Co. \nStwertka, Albert (1998). Guide to the Elements (Revised ed.). Oxford University Press. ISBN 0-19-508083-1.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Allotropes_of_oxygen", 
                "title": "Allotropes of oxygen"
            }, 
            {
                "snippet": "is about the liquid form of the element oxygen. For the commercial dietary supplement product, see Liquid Oxygen (supplement). \"LOX\" redirects here. For", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from April 2014\nCommons category with page title same as on Wikidata\nCryogenics\nIndustrial gases\nOxygen\nRocket oxidizers", 
                "pageContent": "Liquid oxygen\u2014abbreviated LOx, LOX or Lox in the aerospace, submarine and gas industries\u2014is one of the physical forms of elemental oxygen.\n\n\n== Physical properties ==\nLiquid oxygen has a pale blue color and is strongly paramagnetic; it can be suspended between the poles of a powerful horseshoe magnet. Liquid oxygen has a density of 1.141 g/cm3 (1.141 kg/L or 1141 kg/m3) and is cryogenic with a freezing point of 54.36 K (\u2212218.79 \u00b0C; \u2212361.82 \u00b0F) and a boiling point of 90.19 K (\u2212182.96 \u00b0C; \u2212297.33 \u00b0F) at 101.325 kPa (760 mmHg). Liquid oxygen has an expansion ratio of 1:861 under 1 standard atmosphere (100 kPa) and 20 \u00b0C (68 \u00b0F), and because of this, it is used in some commercial and military aircraft as transportable source of breathing oxygen.\nBecause of its cryogenic nature, liquid oxygen can cause the materials it touches to become extremely brittle. Liquid oxygen is also a very powerful oxidizing agent: organic materials will burn rapidly and energetically in liquid oxygen. Further, if soaked in liquid oxygen, some materials such as coal briquettes, carbon black, etc., can detonate unpredictably from sources of ignition such as flames, sparks or impact from light blows. Petrochemicals, including asphalt, often exhibit this behavior.\nThe tetraoxygen molecule (O4) was first predicted in 1924 by Gilbert N. Lewis, who proposed it to explain why liquid oxygen defied Curie's law. Modern computer simulations indicate that although there are no stable O4 molecules in liquid oxygen, O2 molecules do tend to associate in pairs with antiparallel spins, forming transient O4 units.\nLiquid nitrogen has a lower boiling point at \u2212196 \u00b0C (77 K) than oxygen's \u2212183 \u00b0C (90 K), and vessels containing liquid nitrogen can condense oxygen from air: when most of the nitrogen has evaporated from such a vessel there is a risk that liquid oxygen remaining can react violently with organic material. Conversely, liquid nitrogen or liquid air can be oxygen-enriched by letting it stand in open air; atmospheric oxygen dissolves in it, while nitrogen evaporates preferentially.\n\n\n== Uses ==\nIn commerce, liquid oxygen is classified as an industrial gas and is widely used for industrial and medical purposes. Liquid oxygen is obtained from the oxygen found naturally in air by fractional distillation in a cryogenic air separation plant.\nLiquid oxygen is a common cryogenic liquid oxidizer propellant for spacecraft rocket applications, usually in combination with liquid hydrogen, kerosene or methane. Liquid oxygen is useful in this role because it creates a high specific impulse. It was used in the very first rocket applications like the V2 missile (under the name A-Stoff and Sauerstoff) and Redstone, R-7 Semyorka, Atlas boosters, and the ascent stages of the Apollo Saturn rockets. Liquid oxygen was also used in some early ICBMs, although more modern ICBMs do not use liquid oxygen because its cryogenic properties and need for regular replenishment to replace boiloff make it harder to maintain and launch quickly. Many modern rockets use liquid oxygen, including the main engines on the now-retired Space Shuttle.\nLiquid oxygen also had extensive use in making oxyliquit explosives, but is rarely used now due to a high rate of accidents.\nIt is also used in the activated sludge processing in waste water treatment to maintain a high level of micro-organisms.\n\n\n== History ==\nBy 1845, Michael Faraday had managed to liquefy most gases then known to exist. Six gases, however, resisted every attempt at liquefaction and were known at the time as \"permanent gases\". They were oxygen, hydrogen, nitrogen, carbon monoxide, methane, and nitric oxide.\nIn 1877, Louis Paul Cailletet in France and Raoul Pictet in Switzerland succeeded in producing the first droplets of liquid air.\nThe first measurable quantity of liquid oxygen was produced by Polish professors Zygmunt Wr\u00f3blewski and Karol Olszewski (Jagiellonian University in Krak\u00f3w) on April 5, 1883.\n\n\n== See also ==\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Liquid_oxygen", 
                "title": "Liquid oxygen"
            }, 
            {
                "snippet": "Hypoxia refers to low oxygen conditions. Normally, 20.9% of the gas in the atmosphere is oxygen. The partial pressure of oxygen in the atmosphere is 20", 
                "pageCategories": "Aquatic ecology\nCS1 maint: Multiple names: authors list\nChemical oceanography\nEnvironmental science\nOxygen\nWater quality indicators", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Hypoxia_(environmental)", 
                "title": "Hypoxia (environmental)"
            }, 
            {
                "snippet": "An oxygen mask provides a method to transfer breathing oxygen gas from a storage tank to the lungs. Oxygen masks may cover the nose and mouth (oral nasal", 
                "pageCategories": "All articles with unsourced statements\nAmbulances\nArticles with unsourced statements from June 2014\nArticles with unsourced statements from September 2013\nCommons category with local link same as on Wikidata\nDosage forms\nDrug delivery devices\nMasks\nMedical equipment\nOxygen", 
                "pageContent": "An oxygen mask provides a method to transfer breathing oxygen gas from a storage tank to the lungs. Oxygen masks may cover the nose and mouth (oral nasal mask) or the entire face (full-face mask). They may be made of plastic, silicone, or rubber.\nIn certain circumstances, oxygen may be delivered via a nasal cannula instead of a mask.\n\n\n== Medical plastic oxygen masks ==\nMedical plastic oxygen masks are used primarily by medical care providers for oxygen therapy because they are disposable and so reduce cleaning costs and infection risks. Mask design can determine accuracy of oxygen delivered with many various medical situations requiring treatment with oxygen. Oxygen is naturally occurring in room air at 21% and higher percentages are often essential in medical treatment. Oxygen in these higher percentages is classified as a drug with too much oxygen being potentially harmful to a patient's health, resulting in oxygen dependence over time, and in extreme circumstances patient blindness. For these reasons oxygen therapy is closely monitored. Masks are light in weight and attached using an elasticated headband or ear loops. They are transparent for allowing the face to be visible for patient assessment by healthcare providers, and reducing a sensation of claustrophobia experienced by some patients when wearing an oxygen mask. The vast majority of patients having an operation will at some stage wear an oxygen mask; they may alternatively wear a nasal cannula but oxygen delivered in this way is less accurate and restricted in concentration.\n\n\n== Silicone and rubber masks ==\nSilicone and rubber oxygen masks are heavier than plastic masks. They are designed to provide a good seal for long-duration use by aviators, medical research subjects, and hyperbaric chamber and other patients who require administration of pure oxygen, such as carbon monoxide poisoning and decompression sickness victims. Dr. Arthur H. Bulbulian pioneered the first modern viable oxygen mask, worn by World War II pilots and used by hospitals. Valves inside these tight-fitting masks control the flow of gases into and out of the masks, so that rebreathing of exhaled gas is minimised.\n\n\n== Hoses and tubing and oxygen regulators ==\nHoses or tubing connect an oxygen mask to the oxygen supply. Hoses are larger in diameter than tubing and can allow greater oxygen flow. When a hose is used it may have a ribbed or corrugated design to allow bending of the hose while preventing twisting and cutting off the oxygen flow. The quantity of oxygen delivered from the storage tank to the oxygen mask is controlled by a valve called a regulator. Some types of oxygen masks have a breathing bag made of plastic or rubber attached to the mask or oxygen supply hose to store a supply of oxygen to allow deep breathing without waste of oxygen with use of simple fixed flow regulators.\n\n\n== Oxygen masks for aviators ==\n\nThree main kinds of oxygen masks are used by pilots and crews who fly at high altitudes: continuous flow, diluter demand, and pressure demand.\nThe first successful creation for the oxygen mask was by Armenian born Dr. Arthur Bulbulian, in the field of facial prosthetics.\nIn a continuous-flow system, oxygen is provided to the user continuously. It does not matter if the user is exhaling or inhaling as oxygen is flowing from the time the system is activated. Below the oxygen mask is a rebreather bag that collects oxygen during exhalation and as a result allows a higher flow rate during the inhalation cycle.\nDiluter-demand and pressure-demand masks supply oxygen only when the user inhales. They each require a good seal between the mask and the user\u2019s face.\nIn a diluter-demand system, as the altitude increases (ambient pressure, and therefore the partial pressure of ambient oxygen, decreases), the oxygen flow increases such that the partial pressure of oxygen is roughly constant. Diluter-demand oxygen systems can be used up to 40,000 feet (12,000 m).\nIn a pressure-demand system, oxygen in the mask is above ambient pressure, permitting breathing above 40,000 feet (12,000 m). Because the pressure inside the mask is greater than the pressure around the user\u2019s torso, inhalation is easy, but exhalation requires more effort. Aviators are trained in pressure-demand breathing in altitude chambers. Because they seal tightly, pressure-demand-type oxygen masks are also used in hyperbaric oxygen chambers and for oxygen breathing research projects with standard oxygen regulators.\nMany designs of aviator's oxygen masks contain a microphone to transmit speech to other crew members and to the aircraft's radio. Military aviators' oxygen masks have face pieces that partially cover the sides of the face and protect the face against flash burns, flying particles, and effects of a high speed air stream hitting the face during emergency evacuation from the aircraft by ejection seat or parachute. They are often part of a pressure suit or intended for use with a flight helmet.\nAn early 1919 high-altitude oxygen system used a vacuum flask of liquid oxygen to supply two people for one hour at 15,000 feet (4,600 m). The liquid passed through several warming stages before use, as expansion when it evaporated, and absorbed latent heat of vaporization, would make the gasified oxygen so cold that it could cause instant frostbite of the lungs.\n\n\n== Aviation passenger masks and emergency oxygen systems ==\n\nMost commercial aircraft are fitted with oxygen masks for use when cabin pressurization fails. In general, commercial aircraft are pressurized so that the cabin air is at a pressure equivalent to no more than 8,000 feet (2,400 m) altitude (usually somewhat lower altitude), where one can breathe normally without an oxygen mask. If the oxygen pressure in the cabin drops below a safe level, risking hypoxia, compartments containing the oxygen masks will open automatically, either above or in front of the passenger and crew seats, and in the lavatories.\nIn the early years of commercial flight, before pressurized cabins were invented, airliner passengers sometimes had to wear oxygen masks during routine flights.\n\n\n== Self-contained breathing apparatus (SCBA) ==\nFirefighters and emergency service workers use full face masks that provide breathing air as well as eye and face protection. These masks are typically attached to a tank carried upon the back of the wearer and are called self-contained breathing apparatuses (SCBA). Open circuit SCBAs do not normally supply oxygen, as it is not necessary and constitutes a fire hazard. Rebreather SCBAs usually supply oxygen as this is the lightest and most compact option, and uses a simpler mechanism than other types of rebreather.\n\n\n== Specialized masks astronauts ==\nSpecialized full-face masks that supply oxygen or other breathing gases are used by astronauts to remove nitrogen from their blood before space walks (EVA).\n\n\n== Specialized masks for pets ==\nSpecialized snout masks which supply oxygen to revive family pets have been donated to fire departments.\n\n\n== Oxygen delivery to divers ==\nDivers only use pure oxygen for accelerated decompression, or from oxygen rebreathers at shallow depths where the risk of acute oxygen toxicity is acceptable. Oxygen supply during in-water decompression is via rebreather, open circuit diving regulator, full-face mask or diving helmet which has been prepared for oxygen service.\n\n\n=== Built-in breathing system ===\n\nOxygen supply to divers in decompression chambers is preferably through a built-in breathing system, which uses an oxygen mask plumbed into supply and exhaust hoses which supply oxygen from outside the chamber, and discharge the exhaled oxygen-rich gas outside the chamber, using a system equivalent to two demand valves, one upstream of the diver, to supply oxygen on demand, and the other downstream, to exhaust exhaled gas on demand, so that the oxygen partial pressure in the chamber is limited to relatively safe levels. If oxygen masks are used that discharge into the chamber, the chamber air must be replaced frequently to keep the oxygen level within safe operating limits.\n\n\n== Anesthesia oxygen masks ==\nAnesthesia masks are face masks that are designed to administer anesthetic gases to a patient through inhalation. Anesthesia masks are either made of anti-static silicone or rubber, as a static electricity spark may ignite some anesthetic gases. They are either black rubber or clear silicone. Anesthesia masks fit over the mouth and nose and have a double hose system. One hose carries inhaled anesthetic gas to the mask and the other brings exhaled anesthetic gas back to the machine. Anesthesia masks have 4 point head strap harnesses to securely fit on the head to hold the mask in place as the anaethesia doctor controls the gases and oxygen inhaled.\n\n\n== Masks for high-altitude climbers ==\nOxygen masks are used by climbers of high peaks such as Mt. Everest. Because of the severe cold and harsh conditions oxygen masks for use at extreme altitude must be robust and effective. The oxygen storage tanks used with the masks, called oxygen bottles are made of lightweight, high-strength metals and are covered in high-strength fiber such as kevlar. These special oxygen bottles are filled with oxygen at a very high pressure which provides a longer time duration of oxygen for breathing than standard pressure oxygen bottles. These systems are generally only used above 7,000 metres (23,000 ft).\nIn recent years oxygen mask systems for high-altitude climbing which pump oxygen constantly have been increasingly replaced by systems supplying oxygen on demand via nasal cannulas.\n\n\n== Oxygen helmets ==\nOxygen helmets are used in hyperbaric oxygen chambers for oxygen administration. They are transparent, lightweight plastic helmets with a seal that goes around the wearer's neck that looks like a space suit helmet. They offer a good visual field. Light weight plastic hoses provide oxygen to the helmet and remove exhaled gas to the outside of the chamber. Oxygen helmets are often preferred for oxygen administration in hyperbaric oxygen chambers for children and patients that are uncomfortable wearing an oxygen mask.\n\n\n== Mask retention systems ==\nMedical oxygen masks are held in place by medical personnel or the user by hand, or they may be fitted with a lightweight elastic headband so the mask can be removed quickly. Full-face masks are secured by several straps. Tightly fitting oxygen masks are secured at four points by two head straps. Aviators\u2019 masks are often equipped with \u201cquick don\u201d harnesses that allow those in pressurized aircraft to rapidly don the masks in emergencies. Military aviators\u2019 oxygen masks are secured to flight helmets with quick-release systems.\n\n\n== See also ==\nFull face diving mask\nNasal cannula\nOxygen therapy\nHyperbaric oxygen therapy\nOxygen tent\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Oxygen_mask", 
                "title": "Oxygen mask"
            }
        ], 
        "phraseCharStart": "337"
    }, 
    {
        "phraseCharEnd": "418", 
        "phraseIndex": "T9", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "vessel", 
        "wikiSearchResults": [
            {
                "snippet": "Vessel or vessels may refer to:   Blood vessel, a part of the circulatory system and function to transport blood throughout the body Lymph vessel, a thin", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Vessel", 
                "title": "Vessel"
            }, 
            {
                "snippet": "Ocarinas on display            A vessel flute is a type of flute with an enclosed rather than cylindrical body (vessel rather than tube). The best-known", 
                "pageCategories": "Vessel flutes", 
                "pageContent": "A vessel flute is a type of flute with an enclosed rather than cylindrical body (vessel rather than tube). The best-known example is probably the ocarina.\nGemshorn\nPifana\n\nOcarina\nMolinukai\n\nXun\nHun\n\n\n== See also ==\nEnd-blown flute\nFipple flute\nNose flute\nOvertone flute\nSide-blown flute", 
                "titleUrl": "https://en.wikipedia.org/wiki/Vessel_flute", 
                "title": "Vessel flute"
            }, 
            {
                "snippet": "(disambiguation). A research vessel (RV or R/V) is a ship or boat designed and/or equipped to carry out research at sea. Research vessels carry out a number of", 
                "pageCategories": "Commons category with local link same as on Wikidata\nFisheries science\nHydrography\nOceanographic instrumentation\nResearch vessels\nShip types\nSurvey ships\nWikipedia articles with GND identifiers", 
                "pageContent": "A research vessel (RV or R/V) is a ship or boat designed and/or equipped to carry out research at sea. Research vessels carry out a number of roles. Some of these roles can be combined into a single vessel but others require a dedicated vessel. Due to the demanding nature of the work, research vessels are often constructed around an icebreaker hull, allowing them to operate in polar waters.\n\n\n== History ==\n\nThe research ship had origins in the early voyages of exploration. By the time of James Cook's Endeavour, the essentials of what today we would call a research ship are clearly apparent. In 1766, the Royal Society hired Cook to travel to the Pacific Ocean to observe and record the transit of Venus across the Sun. The Endeavour was a sturdy boat, well designed and equipped for the ordeals she would face, and fitted out with facilities for her \"research personnel\", Joseph Banks. And, as is common with contemporary research vessels, Endeavour carried out more than one kind of research, including comprehensive hydrographic survey work.\nSome other notable early research vessels were HMS Beagle, RV Calypso, HMS Challenger, USFC Albatross, and the Endurance and Terra Nova.\nThe names of early research vessels have been used to name later research vessels, as well as Space Shuttles.\n\n\n== Modern types ==\n\n\n=== Hydrographic survey ===\nA hydrographic survey ship is a vessel designed to conduct hydrographic research and survey. Nautical charts are produced from this information to ensure safe navigation by military and civilian shipping.\nHydrographic survey vessels also conduct seismic surveys of the seabed and the underlying geology. Apart from producing the charts, this information is useful for detecting geological features which are likely to bear oil or gas. These vessels usually mount equipment on a towed structure, for example, air cannons, used to generate a high pressure shock wave to sound the strata beneath the seabed, or mounted on the keel, for example, a depth sounder.\nIn practice, hydrographic survey vessels are often equipped to perform multiple roles. Some function also as oceanographic research ships. Naval hydrographic survey vessels often do naval research, for example, on submarine detection.\nAn example of a hydrographic survey vessel is CCGS Frederick G. Creed. For an example of the employment of a survey ship see HMS Hydra.\n\n\n=== Oceanographic research ===\nOceanographic research vessels carry out research on the physical, chemical and biological characteristics of water, the atmosphere and climate, and to these ends carry equipment for collecting water samples from a range of depths, including the deep seas, as well as equipment for the hydrographic sounding of the seabed, along with numerous other environmental sensors. These vessels often also carry scientific divers and unmanned underwater vehicles. Since the requirements of both oceanographic and hydrographic research are very different from those of fisheries research, these boats often fulfill dual roles.\nExamples of an oceanographic research vessel include the NOAAS Ronald H. Brown and the Chilean Navy Cabo de Hornos.\n\n\n=== Fisheries research ===\nA fisheries research vessel (FRV) requires platforms which are capable of towing different types of fishing nets, collecting plankton or water samples from a range of depths, and carrying acoustic fish-finding equipment. Fisheries research vessels are often designed and built along the same lines as a large fishing vessel, but with space given over to laboratories and equipment storage, as opposed to storage of the catch.\nAn example of a fisheries research vessel is FRV Scotia.\n\n\n=== Naval research ===\nNaval research vessels investigate naval concerns, such as submarine and mine detection, sonar and weapon trialling.\nAn example of a naval research vessel is the Planet of the German Navy.\n\n\n=== Polar research ===\nPolar research vessels are constructed around an icebreaker hull, allowing them to operate in polar waters. These boats usually have dual roles, particularly in the Antarctic where they function also as polar replenishment and supply vessels to the Antarctic research bases.\nAn example of a polar research vessel is USCGC Polar Star.\n\n\n=== Oil exploration ===\nOil exploration is performed in a number of ways, one of the most common being mobile drilling platforms or ships that are moved from area to area as needed to drill into the seabed to find out what deposits may or may not lie beneath it.\n\n\n== See also ==\nEuropean and American voyages of scientific exploration\nList of research vessels by country\nMarine research vessels\nOceanography\nTechnical research ship\nWeather ship\n\n\n== References ==\n\nOCEANIC International Research Vessels Database\nUnofficial (English Language) Homepage of the research icebreaker \"ARA Almirante Irizar\nAustralian research vessel facilities\nCanadian research fleet\nAlfred Wegener Institute for Polar and Marine Research - home of the \"Polarstern\"\nIfremer Fleet\nNational Institute of Oceanography and Experimental Geophysics - OGS Trieste ITALY\nNOAA Marine Operations\nScripps Institution of Oceanography\nWoods Hole Oceanographic Institution (WHOI)\nUniversity-National Oceanographic Laboratory System (UNOLS) research vessels (US academic fleet)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Research_vessel", 
                "title": "Research vessel"
            }, 
            {
                "snippet": "A mine countermeasures vessel or MCMV is a type of naval ship designed for the location of and destruction of naval mines which combines the role of a", 
                "pageCategories": "All stub articles\nCommons category with local link same as on Wikidata\nMinehunters\nMinesweepers\nNavy stubs", 
                "pageContent": "A mine countermeasures vessel or MCMV is a type of naval ship designed for the location of and destruction of naval mines which combines the role of a minesweeper and minehunter in one hull. The term MCMV is also applied collectively to minehunters and minesweepers.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Mine_countermeasures_vessel", 
                "title": "Mine countermeasures vessel"
            }, 
            {
                "snippet": "A wind turbine installation vessel is a vessel specifically designed for the installation of offshore wind turbines. Similar to a jackup rig it is self-elevating", 
                "pageCategories": "Wind turbine installation vessels", 
                "pageContent": "A wind turbine installation vessel is a vessel specifically designed for the installation of offshore wind turbines. Similar to a jackup rig it is self-elevating. To enable quick relocation in the wind farm it is self-propelled. It also has a slender ship shaped hull to achieve a quick turnaround time with the vessel carrying several foundations or wind turbines each time. Azimuth thrusters are used to position the vessel during jack-up operations.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Wind_turbine_installation_vessel", 
                "title": "Wind turbine installation vessel"
            }, 
            {
                "snippet": "A survey vessel is any type of ship or boat that is used for mapping. It is a type of research vessel.  The task of survey vessels is to map the bottom", 
                "pageCategories": "All articles lacking sources\nAll stub articles\nArticles lacking sources from November 2008\nShip type stubs\nShip types", 
                "pageContent": "A survey vessel is any type of ship or boat that is used for mapping. It is a type of research vessel.\n\n\n== Role ==\nThe task of survey vessels is to map the bottom, benthic zone, full water column, and surface for the purpose of:\nhydrography\ngeneral oceanography\nmarine habitats\nsalvage\ndredging\nmarine archaeology\n\n\n== Survey equipment ==\nTypically, modern survey vessels are equipped with one or more of the following equipment:\nGPS positioning and logging\nsingle beam sonar\nmultibeam sonar\nSide-scan sonar\ntowed magnetometer\nsubsurface profiler\ngrab sampler\nbottom coring device\nDCHP\nCTD\nInertial Measurement Unit\n\n\n== See also ==\nResearch vessel", 
                "titleUrl": "https://en.wikipedia.org/wiki/Survey_vessel", 
                "title": "Survey vessel"
            }, 
            {
                "snippet": "updated online on a continuous basis. When a ship is removed from the Naval Vessel Register in the United States, or from a Naval List of any other country", 
                "pageCategories": "All articles lacking in-text citations\nArticles lacking in-text citations from March 2013\nArticles with limited geographic scope from May 2009\nDirectories\nRoyal Navy\nUnited States Navy\nWikipedia articles needing clarification from June 2016", 
                "pageContent": "A Navy List or Naval Register is an official list of naval officers, their ranks and seniority, the ships which they command or to which they are appointed, etc., that is published by the government or naval authorities of a country.\n\n\n== Background ==\nThe Navy List fulfills an important function in international law in that warships are required by article 29 of the United Nations Convention on the Law of the Sea to be commanded by a commissioned officer whose name appears in the appropriate service list.\nPast copies of the Navy List are also important sources of information for historians and genealogists.\nThe Navy List for the Royal Navy is no longer published in hard-copy.\nThe Royal Navy (United Kingdom) publishes annual lists of active and reserve officers, and biennial lists of retired officers. The equivalent in the United States Navy is the Naval Register, which is updated online on a continuous basis. When a ship is removed from the Naval Vessel Register in the United States, or from a Naval List of any other country, the ship is said to be \"stricken\".\n\n\n== Resources ==\nGood sources of historical data on UK's Navy Lists are\nThe Naval Historical Branch, Portsmouth Naval Base.\nThe Central Library Portsmouth, Guildhall Square.\nThe National Archives, Kew, that has an almost complete set including unpublished editions produced during the Second World War for internal use by the Admiralty.\nThe Caird Library of the National Maritime Museum has in its collection bound monthly lists published by the Admiralty, and the concurrently published Steel's lists\nThe current editor of the Navy List is Cliona Willis\n\n\n== Bibliography ==\nThe 1766 Navy List, Edited by E. C. Coleman, Published by Ancholme Publishing, ISBN 0-9541443-0-9\n\n\n== See also ==\nArmy List\nNaval Vessel Register\n\n\n== References ==\n\n\n== External links ==\nNavy List 2013\nUS Naval Register (US Navy)\nNavy List Research (Royal Navy)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Navy_List", 
                "title": "Navy List"
            }, 
            {
                "snippet": "blood vessels are the part of the circulatory system that transports blood throughout the human body. There are three major types of blood vessels: the", 
                "pageCategories": "All articles needing additional references\nAngiology\nArticles needing additional references from August 2008\nCommons category with local link same as on Wikidata\nSoft tissue", 
                "pageContent": "The blood vessels are the part of the circulatory system that transports blood throughout the human body. There are three major types of blood vessels: the arteries, which carry the blood away from the heart; the capillaries, which enable the actual exchange of water and chemicals between the blood and the tissues; and the veins, which carry blood from the capillaries back toward the heart. The word vascular, meaning relating to the blood vessels, is derived from the Latin vas, meaning vessel. A few structures (such as cartilage and the lens of the eye) do not contain blood vessels and are labeled avascular.\n\n\n== StructureEdit ==\nThe arteries and veins have three layers, but the middle layer is thicker in the arteries than it is in the veins:\nTunica intima (the thinnest layer): a single layer of simple squamous endothelial cells glued by a polysaccharide intercellular matrix, surrounded by a thin layer of subendothelial connective tissue interlaced with a number of circularly arranged elastic bands called the internal elastic lamina.\nTunica media (the thickest layer in arteries): circularly arranged elastic fiber, connective tissue, polysaccharide substances, the second and third layer are separated by another thick elastic band called external elastic lamina. The tunica media may (especially in arteries) be rich in vascular smooth muscle, which controls the caliber of the vessel.\nTunica adventitia: (the thickest layer in veins) entirely made of connective tissue. It also contains nerves that supply the vessel as well as nutrient capillaries (vasa vasorum) in the larger blood vessels.\nCapillaries consist of little more than a layer of endothelium and occasional connective tissue.\nWhen blood vessels connect to form a region of diffuse vascular supply it is called an anastomosis (pl. anastomoses). Anastomoses provide critical alternative routes for blood to flow in case of blockages.\nThere is a layer of muscle surrounding the arteries and the veins which help contract and expand the vessels. This creates enough pressure for blood to be pumped around the body. Blood vessels are part of the circulatory system, together with the heart and the blood.\n\n\n=== TypesEdit ===\n\nThere are various kinds of blood vessels:\nArteries\nElastic arteries\nDistributing arteries\nArterioles\nCapillaries (the smallest blood vessels)\nVenules\nVeins\nLarge collecting vessels, such as the subclavian vein, the jugular vein, the renal vein and the iliac vein.\nVenae cavae (the two largest veins, carry blood into the heart).\n\nThey are roughly grouped as arterial and venous, determined by whether the blood in it is flowing away from (arterial) or toward (venous) the heart. The term \"arterial blood\" is nevertheless used to indicate blood high in oxygen, although the pulmonary artery carries \"venous blood\" and blood flowing in the pulmonary vein is rich in oxygen. This is because they are carrying the blood to and from the lungs, respectively, to be oxygenated.\n\n\n== PhysiologyEdit ==\nBlood vessels do not actively engage in the transport of blood (they have no appreciable peristalsis), but arteries\u2014and veins to a degree\u2014can regulate their inner diameter by contraction of the muscular layer. This changes the blood flow to downstream organs, and is determined by the autonomic nervous system. Vasodilation and vasoconstriction are also used antagonistically as methods of thermoregulation.\nOxygen (bound to hemoglobin in red blood cells) is the most critical nutrient carried by the blood. In all arteries apart from the pulmonary artery, hemoglobin is highly saturated (95-100%) with oxygen. In all veins apart from the pulmonary vein, the hemoglobin is desaturated at about 75%. (The values are reversed in the pulmonary circulation.)\nThe blood pressure in blood vessels is traditionally expressed in millimetres of mercury (1 mmHg = 133 Pa). In the arterial system, this is usually around 120 mmHg systolic (high pressure wave due to contraction of the heart) and 80 mmHg diastolic (low pressure wave). In contrast, pressures in the venous system are constant and rarely exceed 10 mmHg.\nVasoconstriction is the constriction of blood vessels (narrowing, becoming smaller in cross-sectional area) by contracting the vascular smooth muscle in the vessel walls. It is regulated by vasoconstrictors (agents that cause vasoconstriction). These include paracrine factors (e.g. prostaglandins), a number of hormones (e.g. vasopressin and angiotensin) and neurotransmitters (e.g. epinephrine) from the nervous system.\nVasodilation is a similar process mediated by antagonistically acting mediators. The most prominent vasodilator is nitric oxide (termed endothelium-derived relaxing factor for this reason).\nPermeability of the endothelium is pivotal in the release of nutrients to the tissue. It is also increased in inflammation in response to histamine, prostaglandins and interleukins, which leads to most of the symptoms of inflammation (swelling, redness, warmth and pain).\n\n\n=== Factors affecting blood flow resistanceEdit ===\nResistance occurs where the vessels away from the heart oppose the flow of blood. Resistance is an accumulation of three different factors: blood viscosity, blood vessel length, and vessel radius.\nBlood viscosity is the thickness of the blood and its resistance to flow as a result of the different components of the blood. Blood is 92% water by weight and the rest of blood is composed of protein, nutrients, electrolytes, wastes, and dissolved gases. Depending on the health of an individual, the blood viscosity can vary (i.e. anemia causing relatively lower concentrations of protein, high blood pressure an increase in dissolved salts or lipids, etc.).\nVessel length is the total length of the vessel measured as the distance away from the heart. As the total length of the vessel increases, the total resistance as a result of friction will increase.\nVessel radius also affects the total resistance as a result of contact with the vessel wall. As the radius of the wall gets smaller, the proportion of the blood making contact with the wall will increase. The greater amount of contact with the wall will increase the total resistance against the blood flow.\n\n\n== DiseaseEdit ==\n\nBlood vessels play a huge role in virtually every medical condition. Cancer, for example, cannot progress unless the tumor causes angiogenesis (formation of new blood vessels) to supply the malignant cells' metabolic demand. Atherosclerosis, the formation of lipid lumps (atheromas) in the blood vessel wall, is the most common cardiovascular disease, the main cause of death in the Western world.\nBlood vessel permeability is increased in inflammation. Damage, due to trauma or spontaneously, may lead to hemorrhage due to mechanical damage to the vessel endothelium. In contrast, occlusion of the blood vessel by atherosclerotic plaque, by an embolised blood clot or a foreign body leads to downstream ischemia (insufficient blood supply) and possibly necrosis. Vessel occlusion tends to be a positive feedback system; an occluded vessel creates eddies in the normally laminar flow or plug flow blood currents. These eddies create abnormal fluid velocity gradients which push blood elements such as cholesterol or chylomicron bodies to the endothelium. These deposit onto the arterial walls which are already partially occluded and build upon the blockage.\nVasculitis is inflammation of the vessel wall, due to autoimmune disease or infection.\n\n\n== See alsoEdit ==\nAvascular necrosis\n\n\n== ReferencesEdit ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Blood_vessel", 
                "title": "Blood vessel"
            }, 
            {
                "snippet": "The Naval Vessel Register (NVR) is the official inventory of ships and service craft in custody of or titled by the United States Navy. It contains information", 
                "pageCategories": "Ship registration\nUnited States Department of Defense publications\nUnited States Navy\nWikipedia articles incorporating text from public domain works of the United States Government", 
                "pageContent": "The Naval Vessel Register (NVR) is the official inventory of ships and service craft in custody of or titled by the United States Navy. It contains information on ships and service craft that make up the official inventory of the Navy from the time a vessel is authorized through its life cycle and disposal. It also includes ships that have been removed from the register (often termed stricken or struck), but not disposed of by sale, transfer to another government, or other means. Ships and service craft disposed of prior to 1987 are currently not included, but are gradually being added along with other updates.\nThe NVR traces its origin back to the 1880s, having evolved from several previous publications. In 1911, the Bureau of Construction and Repair published \"Ships Data US Naval Vessels\", which subsequently became the \"Ships Data Book\" in 1952 under the Bureau of Ships. The Bureau of Ordnance's \"Vessel Register\", first published in 1942 and retitled \"Naval Vessel Register\", was combined with the \"Ships Data Book\" under the Bureau of Ships in 1959.\nSince 1962, the NVR has been maintained and published by the NAVSEA Shipbuilding Support Office (NAVSHIPSO) of the Naval Sea Systems Command. Referred to by Congress in the statutes of 10 U.S.C. \u00a7\u00a7 7304\u20137308, the NVR is maintained as directed by U.S. Navy Regulations, Article 0406, of September 14, 1990.\nThe vessels are listed in the NVR when the classification and hull number(s) are assigned to ships and service craft authorized to be built by the President of the United States, or when the Chief of Naval Operations requests instatement or reinstatement of vessels as approved by the Secretary of the Navy. Once listed, the ship or service craft remains in the NVR throughout its life as a Navy asset. Afterwards, its final disposition is recorded. Many vessels struck from the NVR are transferred to the Navy Inactive Fleet or to the United States Maritime Administration (MARAD) to become part of the National Defense Reserve Fleet. Some continue limited operation in the Ready Reserve Fleet.\nThe NVR is updated weekly and now available only in electronic form and on-line. Over 6,500 separate record transactions are processed annually with each being supported by official documentation. The NVR includes a current list of ships and service craft on hand, under construction, converted, loaned/leased, or to be loaned, and those assigned to the Military Sealift Command. Ship class, fleet assignment, name, age, home port, planning yard, custodian, hull and machinery characteristics, builder, key construction dates, battle forces, local defense and miscellaneous support forces, and status conditions are some of the data elements provided.\n\n\n== See also ==\n\nDictionary of American Naval Fighting Ships\n\n\n== References ==\n This article incorporates public domain material from the United States Government document \"Naval Vessel Register\".\n\n\n== External links ==\nNaval Vessel Register\nHistory of the Naval Vessel Register\nNAVSHIPSO web site", 
                "titleUrl": "https://en.wikipedia.org/wiki/Naval_Vessel_Register", 
                "title": "Naval Vessel Register"
            }, 
            {
                "snippet": "about the naval vessel. For the Australian TV series, see Patrol Boat (TV series).           A patrol boat is a relatively small naval vessel generally designed", 
                "pageCategories": "All articles needing additional references\nArticles needing additional references from January 2013\nPatrol vessels\nShip types", 
                "pageContent": "A patrol boat is a relatively small naval vessel generally designed for coastal defence duties. There have been many designs for patrol boats. They may be operated by a nation's navy, coast guard, police force or customs and may be intended for marine (blue water) and/or estuarine or river (\"brown water\") environments. They are commonly found engaged in various border protection roles, including anti-smuggling, anti-piracy, fisheries patrols, and immigration law enforcement. They are also often called upon to participate in rescue operations. Vessels of this type include the original yacht (from Dutch/Low German jacht meaning hunting or hunt), a light, fast-sailing vessel used by the Dutch navy to pursue pirates and other transgressors around and into shallow waters.\n\n\n== Classification ==\nThey may be broadly classified as inshore patrol vessels (IPVs) and offshore patrol vessels (OPVs). They are warships typically smaller in size than a corvette and can include fast attack craft, torpedo boats and missile boats, although some are as large as a frigate. The offshore patrol vessels are usually the smallest ship in a navy's fleet that are large and seaworthy enough to patrol off-shore in the open ocean. In larger militaries, such as in the United States military, offshore patrol vessels usually serve in the coast guard, but many smaller nations navies operate these type of ships.\n\n\n== History ==\nDuring both World Wars in order to rapidly build up numbers, all sides created auxiliary patrol boats by arming motorboats and seagoing fishing trawlers with machine guns and obsolescent naval weapons. Some modern patrol vessels are still based on fishing and leisure boats. Seagoing patrol boats are typically around 30 m (100 ft) in length and usually carry a single medium caliber artillery gun as main armament, and a variety of lighter secondary armament such as machine guns or a close-in weapon system. Depending on role, vessels in this class may also have more sophisticated sensors and fire control systems that would enable them to carry torpedoes, anti-ship and surface-to-air missiles.\nMost modern designs are powered by gas turbine arrangements such as CODAG, and speeds are generally in the 25\u201330 knots (46\u201356 km/h; 29\u201335 mph) range. They are primarily used for patrol in a country's Exclusive Economic Zone. Common tasks are fisheries inspection, anti-smuggling (usually anti-narcotics) duties, illegal immigration patrols, anti-piracy patrols and search and rescue (law enforcement-type of work). The largest OPVs might also have a flight deck and helicopter embarked. In times of crisis or war, these vessels are expected to support the larger vessels in the navy.\n\nTheir small size and relatively low cost make them one of the most common type of warship in the world. Almost all navies operate at least a few offshore patrol vessels, especially those with only \"green water\" capabilities. They are useful in smaller seas such as the North Sea as well as in open oceans. Similar vessels for exclusively military duties include torpedo boats and missile boats. The United States Navy operated the Pegasus class of armed hydrofoils for years, in a patrol boat role. The River Patrol Boat (PBR, sometimes called \"Riverine\" and \"Pibber\") is a U.S. design of small patrol boat type designed to patrol waters of large rivers.\n\n\n== Specific nations ==\n\n\n=== Albania ===\nIliria (Albanian Navy Brigade)\n\n\n=== Argentina ===\n\nMantilla-class patrol vessel, Argentine Naval Prefecture\nZ-28-class patrol vessel, Argentine Naval Prefecture\n\n\n=== Australia ===\n\nAttack-class patrol boat ,(Royal Australian Navy) - 1967 to 1985\nFremantle-class patrol boat, (Royal Australian Navy) - 1979 to 2007\nArmidale-class patrol boat, (Royal Australian Navy) - 2005 to present\nBay-class patrol boat, (Customs Marine Unit) - 1999 to present\nAustralian Customs Vessel Triton, (Customs Marine Unit) - 2000 to present\nCape-class patrol boat, (Customs Marine Unit) - Bay class replacement from 2013\n\n\n=== Bahamas ===\nBahamas-class 60m patrol vessel, (Royal Bahamas Defence Force)\nProtector-class patrol boat\n\n\n=== Bangladesh ===\n\nDurjoy-class LPC\nIsland-class patrol vessel\nSea Dragon-class offshore patrol vessel\nPadma-class offshore patrol vessel (Bangladesh Navy)\nMeghna-class large patrol boats (Bangladesh Navy)\nHainan-class submarine chaser\nKraljevica-class patrol boat\nBarkat ('Haizhui' class/Type 062) small patrol boat\nAjay-class patrol boat\nShaheed ('Shanghai-II') class small patrol boat\n\n\n=== Brazil ===\nGraja\u00fa class offshore patrol vessel\nBracu\u00ed class patrol vessel (ex-River-class minesweeper)\nImperial Marinheiro-class offshore patrol vessel\nPiratini-class patrol vessel\nJ-class patrol vessel\nRoraima-class river patrol vessel\nPedro Teixeira-class river patrol vessel\nMaca\u00e9-class-offshore patrol vessel\nAmazonas-class corvette \"BAE Offshore Patrol Vessel\"\n\n\n=== Belgium ===\nP901 Castor (2014)\nP902 Pollux (2015)\n\n\n=== Canada ===\n\nKingston class, (Royal Canadian Navy)\nOrca class, (Royal Canadian Navy)\nHero class, (Canadian Coast Guard)\n\n\n=== China ===\nHarbour security boat (PBI) - 4 newly built 80 ton class harbour security / patrol boats, and more are planned in order to take over the port security / patrol duties currently performed by the obsolete Shantou, Beihai, Huangpu, and Yulin classes gunboats, which are increasingly being converted to inshore surveying boats and range support boats.\nShanghai III (Type 062-I) class gunboats - 2\nShanghai II class gunboats\nShanghai I (Type 062) classes gunboats - 150+ active and at least 100 in reserve\nHuludao (Type 206) class gunboat - 8+\nShantou class gunboats - less than 25 (in reserve, subordinated to naval militia)\nBeihai class gunboats - less than 30 (in reserve, subordinated to naval militia)\nHuangpu class gunboats - less than 15 (in reserve, subordinated to naval militia)\nYulin class gunboats - less than 40 (being transferred to logistic duties)\nHaixun class cutter(Type 718), China Coast Guard\n\n\n=== Chile ===\nPatrulleros de Zona Mar\u00edtima FASSMER OPV-80 class. 3 units built under license by ASMAR:\nOPV-81 \"Piloto Pardo\"\nOPV-82 \"Comandante Toro\"\nOPV-83 \"Marinero Fuentealba\". This unit has reinforced hull for Antarctic operations.\nOPV-84 \"Cabo Odger\" under construction in ASMAR Shipyard Talcahuano Chile.\n06 MICALVI Class Patrol vessels built in ASMAR.\n18 Protector Class Patrol crafts built under license by ASMAR.\n04 DABUR Class Patrol crafts built in Israel.\n\n\n=== Colombia ===\n\nDiligente-class patrol boat, (Colombian Navy)\nNodriza-class patrol boat, (Colombian Navy)\nPAF-I-class patrol boat, (Colombian Navy)\nPAF-II-class patrol boat, (Colombian Navy)\nPAF-III-class patrol boat, (Colombian Navy)\nPAF-IV-class patrol boat, (Colombian Navy)\nPatrullera Fluvial Ligera-class patrol boat, (Colombian Navy)\nRiohacha-class gunboat, (Colombian Navy)\nFassmer-80 class, (Colombian Navy) (Built in Colombia by COTECMAR)\n\n\n=== Denmark ===\nKnud Rasmussen-class OPV - 2 vessels\nDiana-class IPV - 6 vessels *Danish Wikipedia\nFlyvefisken-class patrol vessel\nThetis class OPV - 4 ships (classed as ocean patrol frigates)\nBeskytteren ocean patrol frigate OPV. (classed as ocean patrol frigates). Later sold to Estonia and renamed EML Admiral Pitka (A230)\nAgdlek-class IPV - 3 vessels\nBars\u00f8 class IPV - 9 vessels *Danish Wikipedia\nHvidbj\u00f8rnen class OPV - 4 ships, classed as ocean patrol frigates. (Link in Danish)\n\n\n=== Eritrea ===\nEritrea-class 60m patrol vessel, (Eritrean Navy)\nProtector-class patrol boat\n\n\n=== France ===\n\nP400 class, (French Navy)\nFlor\u00e9al class, (French Navy)\nFlamant class, (French Navy)\nEspadon 50 class, (French Navy)\nTrident-class patrol boat, (Maritime Gendarmerie)\nG\u00e9ranium-class patrol boat, (Maritime Gendarmerie)\nJonquille-class patrol boat, (Maritime Gendarmerie)\nVedette-class patrol boat, (Maritime Gendarmerie)\nP\u00e9tulante-class patrol craft, (Maritime Gendarmerie)\nPavois-class patrol craft, (Maritime Gendarmerie)\n\n\n=== Finland ===\nKiisla class (Formerly Finnish Border Guard, now Finnish Navy)\nUVL10, an offshore patrol vessel built at STX Finland Rauma shipyard in 2014\nVMV class (Finnish Navy)\n\n\n=== Germany ===\nBad Bramstedt class - 2002 to present\nPA class - 1943 to 1945\nR boats - 1929 to 1945\nType 139 patrol trawler - 1956 to mid-1970s\n\n\n=== Greece ===\n\n\n==== Hellenic Navy ====\nOsprey 55-class gunboats and derivatives HSY-55 and HSY-56A\nAsheville-class gunboats\nNasty-class Coastal Patrol Vessels, formerly torpedo boats\nEsterel-class Coastal Patrol Vessels\n\n\n==== Hellenic Coast Guard ====\nSaar 4 acting as Offshore Patrol Vessels (OPV)\nStan Patrol 5509 OPV\nVosper Europatrol 250 Mk1 OPV\nAbeking & Rasmussen Patrol Vessels, class Dilos\nPOB-24G Patrol Vessels, class Faiakas\nCB90-HCG\nLambro 57 and derivatives, all being boats for coastal patrols\n\n\n=== Hong Kong ===\n\n\n==== Hong Kong Police Force ====\nSea Panther class large command boat\n\n\n=== Iceland ===\nICGV T\u00fdr (Icelandic Coast Guard)\nICGV \u00d3\u00f0inn (Icelandic Coast Guard)\nICGV \u00de\u00f3r (Icelandic Coast Guard)\nICGV \u00c6gir (Icelandic Coast Guard)\n\n\n=== India ===\n\nCar Nicobar class fast attack craft, Indian Navy\nSaryu class patrol vessel, Indian Navy\nBangaram class patrol vessel, Indian Navy\nSukanya class patrol vessel, Indian Navy\nSamar class, Indian Coast Guard\nVishwast Class, Indian Coast Guard\nSarojini Naidu Class, Indian Coast Guard\nTara Bai class, Indian Coast Guard\nPriyadarshini class, Indian Coast Guard\nJija Bai class, Indian Coast Guard\nVikram class, Indian Coast Guard\nAadesh Class, Indian Coast Guard\n\n\n=== Indonesia ===\nFPB 28, Indonesian Police and Indonesian Customs, 28 meter long patrol boat made by local shipyard PT.PAL\nFPB 38, Indonesian Customs, 38 meter long aluminium patrol boat made by local shipyard PT.PAL\nFPB 57, Indonesian Navy, 57 meter long patrol boat designed by Luerssen and made by PT.PAL, ASM and heli deck equipped for some version.\nPC-40, Indonesian Navy, 40 meter long FRP/Aluminum patrol boat, locally made by in house Navy's workshop.\nPC-60 trimaran, Indonesian Navy, 63-meter-long composite material, is armed with 120 km range of anti-ship missile, made by PT Lundin industry\n\n\n=== Ireland ===\n\nIrish Naval Service Offshore Patrol VesselsL\u00c9 Aisling (P23)\n\nIrish Naval Service Helicopter Patrol Vessels\nL\u00c9 Eithne (P31)\n\nIrish Naval Service Coastal Patrol Vessels\nL\u00c9 Orla (P41)\nL\u00c9 Ciara (P42)\n\nIrish Naval Service Large Patrol Vessels\nL\u00c9 R\u00f3is\u00edn (P51)\nL\u00c9 Niamh (P52)\n\nSamuel Beckett-class OPV\nL\u00c9 Samuel Beckett (P61)\nL\u00c9 James Joyce (P62)\n\n\n=== Israel ===\nDabur class patrol boats - Israeli Navy\nDvora class fast patrol boat - Israeli Navy\nSuper Dvora Mk II - Israeli Navy\nSuper Dvora Mk III - Israeli Navy\nShaldag class fast patrol boats\nShaldag Mk II - Israeli Navy\n\nNachshol class patrol boats (Stingray Interceptor-2000) - Israeli Navy\n\n\n=== Italy ===\n\nZara class, (Italian Guardia di Finanza)\nSaettia class, (Italian Coast Guard)\nDiciotti - CP 902 class, (Italian Coast Guard)\nCassiopea class, (Italian Marina Militare)\nCassiopea II class, (Italian Marina Militare)\nEsploratore class, (Italian Marina Militare)\nComandanti class, (Italian Marina Militare)\n\n\n=== Japan ===\n\nShikishima (Japan Coast Guard), the largest patrol boat.\nMizuho class (Japan Coast Guard),Large patrol Vessel with helicopter deck and hangar.\nTsugaru class (Japan Coast Guard),Large patrol Vessel with helicopter deck and hangar.\nHida class (Japan Coast Guard), high-speed Large patrol Vessel with helicopter deck.\nAso class (Japan Coast Guard), high-speed Large patrol Vessel.\nAmami class (Japan Coast Guard), medium-sized patrol Vessel\nHayabusa-class (JMSDF,Japanese Navy), Corvette class patrol vessel by JMSDF(Navy) Fleet.\n\"S\u014dya\"\uff0c(Japan Coast Guard), icebreaker\n\n\n=== Latvia ===\nSkrunda class, world's first SWATH patrol boat (Latvian Naval Forces)\n\n\n=== Malaysia ===\nKedah class offshore patrol vessel, (Royal Malaysian Navy)\nGagah Class Ship\uff0cMalaysian Maritime Enforcement Agency\nRamunia Class Ship\uff0cMalaysian Maritime Enforcement Agency\nNusa Class Ship\uff0cMalaysian Maritime Enforcement Agency\nSipadan Class Ship\uff0cMalaysian Maritime Enforcement Agency\nRhu Class Ship\uff0cMalaysian Maritime Enforcement Agency\nPengawal Class Ship\uff0cMalaysian Maritime Enforcement Agency\nPeninjau Class Ship\uff0cMalaysian Maritime Enforcement Agency\nPelindung Class Ship\uff0cMalaysian Maritime Enforcement Agency\nSemilang Class Ship\uff0cMalaysian Maritime Enforcement Agency\nPenggalang Class Ship\uff0cMalaysian Maritime Enforcement Agency\nPenyelamat Class Ship\uff0cMalaysian Maritime Enforcement Agency\nPengaman Class Ship\uff0cMalaysian Maritime Enforcement Agency\nKilat Class Ship\uff0cMalaysian Maritime Enforcement Agency\nMalawali Class Ship\uff0cMalaysian Maritime Enforcement Agency\nLangkawi Class Patrol Ship\uff0cMalaysian Maritime Enforcement Agency\n\n\n=== Malta ===\n\nProtector class offshore patrol vessel (Maritime Squadron of the Armed Forces of Malta) - 2002\u2013present\nDiciotti class offshore patrol vessel (Maritime Squadron of the Armed Forces of Malta) - 2005\u2013present\nP21 class inshore patrol vessel (Maritime Squadron of the Armed Forces of Malta) - 2010\u2013present\nEmer class offshore patrol vessel (Maritime Squadron of the Armed Forces of Malta) - 2015\u2013present\n\n\n=== Mexico ===\n\nUribe class, offshore patrol vessel (Mexican Navy)\nHolzinger class, offshore patrol vessel (Mexican Navy)\nSierra class, offshore patrol vessel (Mexican Navy)\nDurango class, offshore patrol vessel (Mexican Navy)\nOaxaca class, offshore patrol vessel (Mexican Navy)\nAzteca class, coastal patrol boat (Mexican Navy)\n\n\n=== Morocco ===\n\nOPV-70 class, offshore patrol vessel (Royal Moroccan Navy)\nOPV-64 class, offshore patrol vessel (Royal Moroccan Navy)\n\n\n=== Netherlands ===\nHolland class offshore patrol vessels (Koninklijke Marine)\n\n\n=== New Zealand ===\nProtector-class OPV (Royal New Zealand Navy) (2008)\nProtector-class IPV (Royal New Zealand Navy) (2008)\nMoa-class patrol boat (Royal New Zealand Navy)(1983\u20132008)\n\n\n=== Norway ===\nRoyal Norwegian Navy\nRapp-class\nTjeld-class\nStorm-class\nSn\u00f8gg-class\nHauk-class\nSkjold-class\n\nNorwegian Coast Guard\nBarentshav class OPV\nHarstad class OPV\nNordkapp class OPV\nNornen class\nSvalbard class icebreaker\n\n\n=== Philippines ===\nPhilippine Navy\nMariano Alvarez class\nGen. Emilio Aguinaldo class\nKagitingan class\nTomas Batillo class\nConrado Yap class\nJose Andrada class\n\n\n=== Peru ===\nR\u00edo Zarumilla class, Peruvian Coast Guard\nRio Ca\u00f1ete class, Peruvian Coast Guard\n\n\n=== Portugal ===\n\nPortuguese Navy\nCentauro class\nViana do Castelo class\nAlbatroz class\nCacine class\nArgos class\nNational Republican Guard (GNR)\nRibamar class\n\n\n=== Romanian ===\nSNR-17 class patrol boats, Romanian Border Police\nStefan Cel Mare patrol vessel, Romanian Border Police\n\n\n=== Russia ===\n\nStenka class patrol boat(Type 02059), Russian Navy\nBogomol Class patrol boat(Type 02065), Russian Navy\nMirage class patrol boat (Type 14310), Russian Navy\nSvetlyak class patrol boat(Type 10410)\uff0c Russian Coast Guard\nOgonek class patrol boat(Type 12130), Russian Coast Guard\nMangust Class patrol boat(Type 12150\uff09, Russian Navy and Russian Coast Guard\nSobol class patrol boat(Type 12200\uff09, Russian Coast Guard\nTerrier class patrol boat (Type 14170), Russian Navy and Russian Coast Guard\nRubin class patrol boat(Type 22460), Russian Coast Guard\n\n\n=== Singapore ===\n\nFearless class, Republic of Singapore Navy\nPK class Interceptor Craft, Police Coast Guard\n1st Generation PT class patrol Craft, Police Coast Guard (decommissioned)\n2nd Generation PT class patrol Craft, Police Coast Guard (decommissioned)\n3rd Generation PT class patrol Craft, Police Coast Guard\n4th Generation PT class patrol Craft, Police Coast Guard\nPC class patrol Craft, Police Coast Guard\nSwift-class coastal patrol craft\n\n\n=== Slovenia ===\nSlovenian patrol boat Triglav\n\n\n=== Sri Lanka ===\nJayasagara class (Sri Lanka Navy)\nColombo class (Sri Lanka Navy)\n\n\n=== South Africa ===\nWarrior class (modified Saar 4 Open Sea Patrol Vessels)\nNamacurra class\n\n\n=== South Korea ===\nChamsuri-class (Republic of Korea Navy)\nGumdoksuri class patrol vessel\n\n\n=== Spain ===\n\nMeteoro class\nDescubierta class\nServiola class\nAnaga class\nBarcel\u00f3 class\nToralla class\nConejera class\nChilreu class\nP111 class patrol boat\nCabo Fradera class\n\n\n=== Suriname ===\nOcea Type FPB 98 class fast patrol boat\nOcea Type FPB 72 class fast patrol boat\n\n\n=== Sweden ===\nHugin-class (based on the Norwegian Storm-class) - 16 ships\nKaparen-class (Hugin-class modified with better subhunting capacity) - 8 ships\nAdditionally, the Swedish Navy also operates a smaller and less capable type of patrol boat (Bevakningsb\u00e5t = \"guard boat\")\nTyp 60 class (Decommissioned) - 17 ships\nTapper class - 12 ships\nThe Swedish Coast Guard operate an additional 22 patrol vessels of various sizes, go to article Coast Guard (Sweden)\n\n\n=== Thailand ===\nPattani class (Royal Thai Navy)\nRiver class (Royal Thai Navy)\nT.991 class (Royal Thai Navy)\n\n\n=== Turkey ===\nK\u0131l\u0131\u00e7 II class, Turkish Navy\nK\u0131l\u0131\u00e7 I class\uff0cTurkish Navy\nY\u0131ld\u0131z class\uff0cTurkish Navy\nR\u00fczgar class\uff0cTurkish Navy\nDo\u011fan class\uff0cTurkish Navy\nKartal class\uff0cTurkish Navy\nT\u00fcrk class\uff0cTurkish Navy\nTuzla class\uff0cTurkish Navy\nKAAN 15 class\uff0cTurkish Coast Guard Command\nKAAN 19 class\uff0cTurkish Coast Guard Command\nKAAN 29 class\uff0cTurkish Coast Guard Command\nKAAN 33 class\uff0cTurkish Coast Guard Command\nSAR 33 class\uff0cTurkish Coast Guard Command\nSAR 35 class\uff0cTurkish Coast Guard Command\n80 class\uff0cTurkish Coast Guard Command\n\n\n=== United Kingdom ===\n\nMotor Launch of World War II\nHarbour Defence Motor Launch of World War II\nRiver class patrol vessel\nCastle class patrol vessel\nArcher class patrol vessel\nIsland class patrol vessel\n\n\n=== United States ===\n\n\n==== United States Navy ====\n\nEagle class patrol craft - US Navy (1918-1947)\nCyclone class patrol ship - US Navy (1993- )\n\n\n==== United States Coast Guard ====\nCutters of the US Coast Guard\n87-foot Marine Protector class coastal patrol boat - USCG\n110-foot Island class patrol boat - USCG\n154-foot Sentinel class cutter- USCG\n210-foot Reliance class cutter - USCG\n270-foot Famous class cutter - USCG\n378-foot Hamilton class cutter - USCG\n418-foot Legend Class cutter - USCG\n399-foot Polar class icebreaker - USCG\n\n\n=== Vietnam ===\nType TT-120 patrol boat, Vietnam Coast Guard\nType TT-200 patrol boat, Vietnam Coast Guard\nType TT-400 patrol boat, Vietnam Coast Guard\nDN 2000(Damen 9014 class) offshore patrol vessels, Vietnam Coast Guard\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Patrol_boat", 
                "title": "Patrol boat"
            }
        ], 
        "phraseCharStart": "412"
    }, 
    {
        "phraseCharEnd": "473", 
        "phraseIndex": "T10", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "cooling tubes", 
        "wikiSearchResults": [
            {
                "snippet": "recovery ventilation, they are called earth tubes (also known as earth cooling tubes or earth warming tubes) in Europe or earth-air heat exchangers (EAHE", 
                "pageCategories": "Heat exchangers\nHeating, ventilating, and air conditioning\nLow-energy building", 
                "pageContent": "A ground-coupled heat exchanger is an underground heat exchanger that can capture heat from and/or dissipate heat to the ground. They use the Earth's near constant subterranean temperature to warm or cool air or other fluids for residential, agricultural or industrial uses. If building air is blown through the heat exchanger for heat recovery ventilation, they are called earth tubes (also known as earth cooling tubes or earth warming tubes) in Europe or earth-air heat exchangers (EAHE or EAHX) in North America. These systems are known by several other names, including: air-to-soil heat exchanger, earth channels, earth canals, earth-air tunnel systems, ground tube heat exchanger, hypocausts, subsoil heat exchangers, thermal labyrinths, underground air pipes, and others.\nEarth tubes are often a viable and economical alternative or supplement to conventional central heating or air conditioning systems since there are no compressors, chemicals or burners and only blowers are required to move the air. These are used for either partial or full cooling and/or heating of facility ventilation air. Their use can help buildings meet Passive House standards or LEED certification.\nEarth-air heat exchangers have been used in agricultural facilities (animal buildings) and horticultural facilities (greenhouses) in the United States over the past several decades and have been used in conjunction with solar chimneys in hot arid areas for thousands of years, probably beginning in the Persian Empire. Implementation of these systems in Austria, Denmark, Germany, and India has become fairly common since the mid-1990s, and is slowly being adopted in North America.\nGround-coupled heat exchanger may also use water or antifreeze as a heat transfer fluid, often in conjunction with a geothermal heat pump. See, for example downhole heat exchangers. The rest of this article deals primarily with earth-air heat exchangers or earth tubes.\n\n\n== Design ==\n\nEarth-air heat exchangers can be analyzed for performance with several software applications using weather gage data. These software applications include GAEA, AWADUKT Thermo, EnergyPlus, L-EWTSim, WKM, and others. However, numerous earth-air heat exchanger systems have been designed and constructed improperly, and failed to meet design expectations. Earth-air heat exchangers appear best suited for air pretreatment rather than for full heating or cooling. Pretreatment of air for an air-source heat pump or ground-source heat pump often provides the best economic return on investment, with simple payback often achieved within one year after installation.\nMost systems are usually constructed from 100 to 600 mm (3.9 to 23.6 in) diameter, smooth-walled (so they do not easily trap condensation moisture and mold), rigid or semi-rigid plastic, plastic-coated metal pipes or plastic pipes coated with inner antimicrobial layers, buried 1.5 to 3 m (4.9 to 9.8 ft) underground where the ambient earth temperature is typically 10 to 23 \u00b0C (50 to 73 \u00b0F) all year round in the temperate latitudes where most humans live. Ground temperature becomes more stable with depth. Smaller diameter tubes require more energy to move the air and have less earth contact surface area. Larger tubes permit a slower airflow, which also yields more efficient energy transfer and permits much higher volumes to be transferred, permitting more air exchanges in a shorter time period, when, for example, you want to clear the building of objectionable odors or smoke but suffer from poorer heat transfer from the pipe wall to the air due to increased distances.\nSome consider that it is more efficient to pull air through a long tube than to push it with a fan. A solar chimney can use natural convection (warm air rising) to create a vacuum to draw filtered passive cooling tube air through the largest diameter cooling tubes. Natural convection may be slower than using a solar-powered fan. Sharp 90-degree angles should be avoided in the construction of the tube \u2013 two 45-degree bends produce less-turbulent, more efficient air flow. While smooth-wall tubes are more efficient in moving the air, they are less efficient in transferring energy.\nThere are three configurations, a closed loop design, an open 'fresh air' system or a combination:\nClosed loop system: Air from inside the home or structure is blown through a U-shaped loop of typically 30 to 150 m (98 to 492 ft) of tube(s) where it is moderated to near earth temperature before returning to be distributed via ductwork throughout the home or structure. The closed loop system can be more effective (during air temperature extremes) than an open system, since it cools and recools the same air.\nOpen system: Outside air is drawn from a filtered air intake (Minimum Efficiency Reporting Value MERV 8+ air filter is recommended). The cooling tubes are typically 30 m (98 ft) long straight tubes into the home. An open system combined with energy recovery ventilation can be nearly as efficient (80-95%) as a closed loop, and ensures that entering fresh air is filtered and tempered.\nCombination system: This can be constructed with dampers that allow either closed or open operation, depending on fresh air ventilation requirements. Such a design, even in closed loop mode, could draw a quantity of fresh air when an air pressure drop is created by a solar chimney, clothes dryer, fireplace, kitchen or bathroom exhaust vents. It is better to draw in filtered passive cooling tube air than unconditioned outside air.\nSingle-pass earth air heat exchangers offer the potential for indoor air quality improvement over conventional systems by providing an increased supply of outdoor air. In some configurations of single-pass systems, a continuous supply of outdoor air is provided. This type of system would usually include one or more ventilation heat recovery units.\n\n\n=== Thermal Labyrinths ===\nA thermal labyrinth performs the same function as an earth tube, but they are usually formed from a larger volume rectilinear space, sometimes incorporated into building basements or under ground floors, and which are in turn divided by numerous internal walls to form a labyrinthine air path. Maximising the length of the air path ensures a better heat transfer effect. The construction of the labyrinth walls, floors, and dividing walls is normally of high thermal mass cast concrete and concrete block, with the exterior walls and floors in direct contact with the surrounding earth.\n\n\n== Safety ==\nIf humidity and associated mold colonization is not addressed in system design, occupants may face health risks. At some sites, the humidity in the earth tubes may be controlled simply by passive drainage if the water table is sufficiently deep and the soil has relatively high permeability. In situations where passive drainage is not feasible or needs to be augmented for further moisture reduction, active (dehumidifier) or passive (desiccant) systems may treat the air stream.\nFormal research indicates that earth-air heat exchangers reduce building ventilation air pollution. Rabindra (2004) states, \u201cThe tunnel [earth-Air heat exchanger] is found not to support the growth of bacteria and fungi; rather it is found to reduce the quantity of bacteria and fungi thus making the air safer for humans to inhale. It is therefore clear that the use of EAT [Earth Air Tunnel] not only helps save the energy but also helps reduce the air pollution by reducing bacteria and fungi.\u201d Likewise, Flueckiger (1999) in a study of twelve earth-air heat exchangers varying in design, pipe material, size and age, stated, \u201cThis study was performed because of concerns of potential microbial growth in the buried pipes of ground-coupled air systems. The results however demonstrate, that no harmful growth occurs and that the airborne concentrations of viable spores and bacteria, with few exceptions, even decreases after passage through the pipe-system\u201d, and further stated, \u201cBased on these investigations the operation of ground-coupled earth-to-air heat exchangers is acceptable as long as regular controls are undertaken and if appropriate cleaning facilities are available\u201d.\nWhether using earth tubes with or without antimicrobial material, it is extremely important that the underground cooling tubes have an excellent condensation drain and be installed at a 2-3 degree grade to ensure the constant removal of condensed water from the tubes. When implementing in a house without a basement on a flat lot, an external condensation tower can be installed at a depth lower than where the tube enters into the house and at a point close to the wall entry. The condensation tower installation requires the added use of a condensate pump in which to remove the water from the tower. For installations in houses with basements, the pipes are graded so that the condensation drain located within the house is at the lowest point. In either installation, the tube must continually slope towards either the condensation tower or the condensation drain. The inner surface of the tube, including all joints must be smooth to aid in the flow and removal of condensate. Corrugated or ribbed tubes and rough interior joints must not be used. Joints connecting the tubes together must be tight enough to prevent water or gas infiltration. In certain geographic areas, it is important that the joints prevent Radon gas infiltration. Porous materials like uncoated concrete tubes cannot be used. Ideally, Earth Tubes with antimicrobial inner layers should be used in installations to inhibit the potential growth of molds and bacteria within the tubes.\n\n\n== Effectiveness ==\nImplementations of earth-air heat exchangers for either partial or full cooling and/or heating of facility ventilation air have had mixed success. The literature is, unfortunately, well populated with over-generalizations about the applicability of these systems \u2013 both pro and con. A key aspect of earth-air heat exchangers is the passive nature of operation and consideration of the wide variability of conditions in natural systems.\nEarth-air heat exchangers can be very cost effective in both up-front/capital costs as well as long-term operation and maintenance costs. However, this varies widely depending on the location latitude, altitude, ambient Earth temperature, climatic temperature-and-relative-humidity extremes, solar radiation, water table, soil type (thermal conductivity), soil moisture content and the efficiency of the building's exterior envelope design / insulation. Generally, dry-and-low-density soil with little or no ground shade will yield the least benefit, while dense damp soil with considerable shade should perform well. A slow drip watering system may improve thermal performance. Damp soil in contact with the cooling tube conducts heat more efficiently than dry soil.\nEarth cooling tubes are much less effective in hot humid climates (like Florida) where the ambient temperature of the earth approaches human comfort temperature. The higher the ambient temperature of the earth, the less effective it is for cooling and dehumidification. However, the earth can be used to partially cool and dehumidify the replacement fresh air intake for passive-solar thermal buffer zone areas like the laundry room, or a solarium / greenhouse, especially those with a hot tub, swim spa, or indoor swimming pool, where warm humid air is exhausted in the summer, and a supply of cooler drier replacement air is desired.\nNot all regions and sites are suitable for earth-air heat exchangers. Conditions which may hinder or preclude proper implementation include shallow bedrock, high water table, and insufficient space, among others. In some areas, only cooling or heating may be afforded by earth-air heat exchangers. In these areas, provision for thermal recharge of the ground must especially be considered. In dual function systems (both heating and cooling), the warm season provides ground thermal recharge for the cool season and the cool season provides ground thermal recharge for the warm season, though overtaxing the thermal reservoir must be considered even with dual function systems.\nRenata Limited, a prominent pharmaceutical company in Bangladesh, tried out a pilot project trying to find out whether they could use the Earth Air Tunnel technology to complement the conventional air conditioning system. Concrete pipes (total length 60 feet, inner diameter 9 inches, outer diameter 11 inches) were placed at a depth of 9 feet underground and a blower of 1.5 kW rated power was employed. The underground temperature at that depth was found to be around 28 \u00b0C. The mean velocity of air in the tunnel was about 5 m/s. The coefficient of performance (COP) of the underground heat exchanger thus designed was poor ranging from 1.5\u20133. The results convinced the authorities that in hot and humid climates, it is unwise to implement the concept of Earth-Air heat exchanger. The cooling medium (earth itself) being at a temperature approaching that of the ambient environment happens to be the root cause of the failure of such principles in hot, humid areas (parts of Southeast Asia, Florida in the U.S. etc.). However, investigators from places like Britain and Turkey have reported very encouraging COPs-well above 20. The underground temperature seems to be of prime importance when planning an Earth-Air heat exchanger.\n\n\n== Environmental impact ==\nIn the context of today's diminishing fossil fuel reserves, increasing electrical costs, air pollution and global warming, properly designed earth cooling tubes offer a sustainable alternative to reduce or eliminate the need for conventional compressor-based air conditioning systems, in non-tropical climates. They also provide the added benefit of controlled, filtered, temperate fresh air intake, which is especially valuable in tight, well-weatherized, efficient building envelopes.\n\n\n== Water to earth ==\nAn alternative to the earth-to-air heat exchanger is the \"water\" to earth heat exchanger. This is typically similar to a geothermal heat pump tubing embedded horizontally in the soil (or could be a vertical sonde) to a similar depth of the earth-air heat exchanger. It uses approximately double the length of pipe of 35 mm diameter, e.g., around 80 m compared to an EAHX of 40 m. A heat exchanger coil is placed before the air inlet of the heat recovery ventilator. Typically a brine liquid (heavily salted water) is used as the heat exchanger fluid.\nMany European installations are now using this setup due to the ease of installation. No fall or drainage point is required and it is safe because of the reduced risk from mold.\n\n\n== See also ==\n\nPassive cooling\nSolar air conditioning\nSolar chimney\nHVAC\nRenewable energy\nGeothermal power\nGeothermal heat pump\nEarth sheltering\nSeasonal thermal energy storage\nAquifer thermal energy storage\n\n\n== References ==\n\nInternational Energy Agency, Air Infiltration and Ventilation Center, Ventilation Information Paper No. 11, 2006, \"Use of Earth to Air Heat Exchangers for Cooling\"\n\n\n== External links ==\nEnergy Savers: Earth Cooling Tubes (US Dept of Energy)\nPerformance of Single Pass Earth-Tube Heat Exchanger: An Experimental Study, Girja Sharan, Ratan Jadhav\nSmall home system using 4\" earth air pipes - 7 year retrospective: Vermont, USA", 
                "titleUrl": "https://en.wikipedia.org/wiki/Ground-coupled_heat_exchanger", 
                "title": "Ground-coupled heat exchanger"
            }, 
            {
                "snippet": "Fine Tubes is a UK-based manufacturer of metal tubes, based in Estover, Plymouth, Devon. The company has supplied cooling tubes to the Large Hadron Collider", 
                "pageCategories": "1940 establishments in England\nAll articles with topics of unclear notability\nAll stub articles\nArticles with topics of unclear notability from April 2009\nBritish companies established in 1940\nCompanies based in Plymouth, Devon\nManufacturing companies established in 1940\nManufacturing companies of the United Kingdom\nUnited Kingdom manufacturing company stubs", 
                "pageContent": "Fine Tubes is a UK-based manufacturer of metal tubes, based in Estover, Plymouth, Devon.\nThe company has supplied cooling tubes to the Large Hadron Collider at CERN.\nIn 2012, Fine Tubes was acquired along with its sister company, by The Watermill Group, an investment firm in the USA.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Fine_Tubes", 
                "title": "Fine Tubes"
            }, 
            {
                "snippet": "geothermal solar Earth cooling tubes Geothermal heat pump Heat recovery ventilation Hot water heat recycling Passive cooling Renewable heat Seasonal", 
                "pageCategories": "Architecture lists\nEnergy-related lists\nLists related to renewable energy\nLow-energy building\nSustainable building", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/List_of_low-energy_building_techniques", 
                "title": "List of low-energy building techniques"
            }, 
            {
                "snippet": " striking the engine nozzle's inner surface and tearing open three cooling tubes containing hydrogen. These ruptures resulted in a leak downstream of", 
                "pageCategories": "All Wikipedia articles written in American English\nAll articles needing additional references\nAll articles with dead external links\nArticles needing additional references from May 2009\nArticles with dead external links from July 2012\nCS1 errors: dates\nSpace Shuttle missions\nSpacecraft launched in 1999\nUse American English from January 2014\nUse dmy dates from May 2011", 
                "pageContent": "STS-93 marked the 95th launch of the Space Shuttle, the 26th launch of Columbia, and the 21st night launch of a Space Shuttle. Eileen Collins became the first female shuttle Commander on this flight. Its primary payload was the Chandra X-ray Observatory. It would also be the last mission of Columbia until March 2002. During the interim, Columbia would be out of service for upgrading, and would not fly again until STS-109. The launch was originally scheduled for 20 July but the launch was aborted at T-7 seconds. The successful launch of the flight occurred three days later.\n\n\n== Crew ==\n\n\n== Problems during ascent ==\n\nDuring the main engine ignition sequence, a gold pin used to plug an oxidizer post in the Space Shuttle's third (right) engine came loose and was violently ejected, striking the engine nozzle's inner surface and tearing open three cooling tubes containing hydrogen. These ruptures resulted in a leak downstream of the main combustion chamber. This anomalous event and the automatic response to the leak by the right engine's controller did not violate any launch commit criteria and liftoff proceeded normally. However, approximately five seconds after liftoff, an electrical short disabled the center engine's primary digital control unit, DCU-A, and the right engine's backup unit, DCU-B. The center and right engines continued to operate on their remaining DCU for the rest of powered flight to orbit. The redundant set of DCUs in each engine controller saved Columbia and her crew from potential catastrophe as shutdown of two engines at that point in the flight would have resulted in a very risky contingency abort with no guarantee of success. The electrical short was later discovered to have been caused by poorly routed wiring which had rubbed on an exposed screw head. This wiring issue led to a program-wide inspection of the wiring in all orbiters.\nBecause of the leak in the right engine, its controller sensed a decrease in power or thrust\u2014measured indirectly as main combustion chamber pressure\u2014since the leaking hydrogen was not being burned in the SSME's two pre-burners or the main combustion chamber. To bring the engine back up to the commanded thrust level, the controller opened the oxidizer values a bit more than normal. The hydrogen leak and increased oxidizer consumption resulted in the right engine deviating from the desired oxygen/hydrogen mixing ratio of 6.03 and running hotter than normal due to the mixture being closer to the stoichiometric ratio of 8:1. The increased oxidizer consumption during ascent resulted in a premature shutdown of all three engines near the end of the projected burn due to low liquid oxygen level sensed in the External Tank. Though the premature shutdown resulted in a velocity 15 ft/s lower than targeted, the vehicle safely achieved its intended orbit and completed the mission as planned. This incident brought on a maintenance practice change which required damaged oxidizer posts to be removed and replaced as opposed to being intentionally plugged, as was the practice beforehand.\nThree days previously, in the first launch attempt, the launch was stopped at T-7 seconds, just prior to the SSMEs' ignition sequence, due to a senior console operator manually triggering a cutoff in the countdown. It was later determined that the console operator, monitoring the hydrogen gas concentration in the Space Shuttle's aft compartment where the three SSMEs are located, was fooled by the gas detector's purge cycle which generated a dangerously high, but spurious, reading in the last seconds of the countdown.\n\n\n== Mission objectives ==\nThe primary objective of the STS-93 mission was to deploy the Chandra X-ray Observatory (formerly the Advanced X-ray Astrophysics Facility) with its Inertial Upper Stage booster. At its launch, Chandra was the most sophisticated X-ray observatory ever built. It is designed to observe X-rays from high energy regions of the universe, such as hot gas in the remnants of exploded stars.\nOther payloads on STS-93 included the Midcourse Space Experiment (MSX), the Shuttle Ionospheric Modification with Pulsed Local Exhaust (SIMPLEX), the Southwest Ultraviolet Imaging System (SWUIS), the Gelation of Sols: Applied Microgravity Research (GOSAMR) experiment, the Space Tissue Loss \u2013 B (STL-B) experiment, a Light mass Flexible Solar Array Hinge (LFSAH), the Cell Culture Module (CCM), the Shuttle Amateur Radio Experiment \u2013 II (SAREX \u2013 II), EarthKAM, Plant Growth Investigations in Microgravity (PGIM), the Commercial Generic Bioprocessing Apparatus (CGBA), the Micro-Electrical Mechanical System (MEMS), and the Biological Research in Canisters (BRIC).\nThe Shuttle Ionespheric Modification with Pulsed Local Exhaust (SIMPLEX) payload activity researched the source of Very High Frequency (VHF) radar echoes caused by the orbiter and its OMS engine firings. The Principal Investigator (PI) used the collected data to examine the effects of orbital kinetic energy on ionospheric irregularities and to understand the processes that take place with the venting of exhaust materials.\n\nThe Southwest Ultraviolet Imaging system (SWUIS) was based around a Maksutov-design ultraviolet (UV) telescope and a UV-sensitive, image-intensified Charge-Coupled Device (CCD) camera that frames at video frame rates. Scientists can obtain sensitive photometric measurements of astronomical targets.\nThe objective Gelation of Sols: Applied Microgravity Research (GOSAMR) experiment was to investigate the influence of microgravity on the processing of gelled sols. In particular, the purpose was to demonstrate that composite ceramic precursors composed of large particulates and small colloidal sols can be produced in space with more structural uniformity.\nThe focus of the Space Tissue Loss \u2013 B (STL-B) experiment was direct video observation of cells in culture through the use of a video microscope imaging system with the objective of demonstrating near real-time interactive operations to detect and induce cellular responses.\nThe Light mass Flexible Solar Array Hinge (LFSAH) payload consists of several hinges fabricated from shape memory alloys. Shape memory deployment hinges offered controlled shockless deployment of solar arrays and other spacecraft appendages. LFSAH demonstrated this deployment capability for a number of hinge configurations.\nThe objectives of the Cell Culture Module (CCM) were to validate models for muscle, bone, and endothelial cell biochemical and functional loss induced by microgravity stress; to evaluate cytoskeleton, metabolism, membrane integrity and protease activity in target cells; and to test tissue loss medications.\nThe Shuttle Amateur Radio Experiment (SAREX-II) demonstrated the feasibility of amateur short-wave radio contacts between the shuttle and ground-based amateur radio operators. SAREX also served as an educational opportunity for schools around the world to learn about space by speaking directly to astronauts aboard the shuttle via amateur radio.\nThe EarthKAM payload conducted Earth observations using the Electronic Still Camera (ESC) installed in the overhead starboard window of the Aft Flight Deck.\nThe Plant Growth Investigations in Microgravity (PGIM) payload experiment used plants to monitor the space flight environment for stressful conditions that affect plant growth. Because plants cannot move away from stressful conditions, they have developed mechanisms that monitor their environment and direct effective physiological responses to harmful conditions.\nThe Commercial Generic Bioprocessing Apparatus (CGBA) payload hardware allows for sample processing and stowage functions. The Generic Bioprocessing Apparatus \u2013 Isothermal Containment Module (GBA-ICM) is temperature controlled to maintain a preset temperature environment, controls the activation and termination of the experiment samples, and provides an interface for crew interaction, control and data transfer.\nThe Micro-Electrical Mechanical System (MEMS) payload examines the performance, under launch, microgravity, and reentry conditions of a suite of MEMS devices. These devices include accelerometers, gyroscopes, and environmental and chemical sensors. The MEMS payload is self-contained and requires activation and deactivation only.\nThe Biological Research in Canisters (BRIC) payload was designed to investigate the effects of space flight on small arthropod animals and plant specimens. The flight crew was available at regular intervals to monitor and control payload/experiment operations.\nColumbia's landing at Kennedy Space Center marked the twelfth night landing in the Shuttle program's history. Five had been at Edwards Air Force Base in California and the rest KSC. To date, there had been 19 consecutive landings at KSC and 25 of the last 26 had been there.\n\n\n== Special cargo ==\nIn 2001, Coin World reported the revelation (via a FOIA document request) that the Mint had struck 39 examples of the 2000 Sacagawea dollar in gold in June 1999 at the West Point Mint. The planchets came from specially prepared \u00bd troy-oz $25 American Gold Eagle Bullion Planchets. Why they were struck is not known; speculation is that this was an attempt by the mint to offer \"Premium\" collectibles in conjunction with the newly released Sacagawea dollar in 2000.\nTwenty-seven were soon melted and the remaining 12 were on board Space Shuttle Columbia for the July 1999 STS-93 mission. Two examples then popped up at two separate events; one during a Private Congressional Dinner in August 1999, and another example at the Official First-Strike ceremonies in November. The coins remained at Mint Headquarters under lock and key until they were transferred in 2001 to Fort Knox. The strikes are considered to be illegal due to the Coinage regulations in place.\nIn 2007, the Mint announced [1] it would for the first time publicly display the 12 space-flown gold dollars at the American Numismatic Association's World's Fair of Money in Milwaukee, WI.\n\n\n== Wake-up calls ==\nSleeping shuttle astronauts were often awakened with a short piece of music, a tradition that apparently began during Apollo 15. Each track was specially chosen, sometimes by their families, and usually had a special meaning to an individual member of the crew, or was applicable to their daily activities.\n\n\n== References ==\n This article incorporates public domain material from websites or documents of the National Aeronautics and Space Administration.\n\n\n== External links ==\n\nNASA PAO page about STS-93\nSTS-93 Mission Operations Director's story on ascent\nSTS-93 JSC Ascent Audio\nSTS-93 Archives\nSTS-93 Video Highlights\n\"US Mint to show unseen gold space coins\". collectSPACE. http://www.collectspace.com/news/news-071407a.html. Retrieved 2007-07-16.][1]", 
                "titleUrl": "https://en.wikipedia.org/wiki/STS-93", 
                "title": "STS-93"
            }, 
            {
                "snippet": "radiant cooling). Since PV cooling's cost effectiveness depends largely on the cooling equipment and given the poor efficiencies in electrical cooling methods", 
                "pageCategories": "All articles with specifically marked weasel-worded phrases\nAll articles with unsourced statements\nArticles with Spanish-language external links\nArticles with specifically marked weasel-worded phrases from June 2009\nArticles with unsourced statements from March 2014\nChemical processes\nHeating, ventilating, and air conditioning\nRenewable energy\nSolar architecture\nSolar thermal energy", 
                "pageContent": "Solar air conditioning refers to any air conditioning (cooling) system that uses solar power.\nThis can be done through passive solar, solar thermal energy conversion and photovoltaic conversion (sunlight to electricity). The U.S. Energy Independence and Security Act of 2007 created 2008 through 2012 funding for a new solar air conditioning research and development program, which should develop and demonstrate multiple new technology innovations and mass production economies of scale. Solar air conditioning might play an increasing role in zero-energy and energy-plus buildings design.\n\n\n== History ==\nIn the late 19th century, the most common fluid for absorption cooling was a solution of ammonia and water. Today, the combination of lithium bromide and water is also in common use. One end of the system of expansion/condensation pipes is heated, and the other end gets cold enough to make ice. Originally, natural gas was used as a heat source in the late 19th century. Today, propane is used in recreational vehicle absorption chiller refrigerators. Hot water solar thermal energy collectors can also be used as the modern \"free energy\" heat source.\n\n\n== Photovoltaic (PV) solar cooling ==\n\nPhotovoltaics can provide the power for any type of electrically powered cooling be it conventional compressor-based or adsorption/absorption-based, though the most common implementation is with compressors. For small residential and small commercial cooling (less than 5 MWh/a) PV-powered cooling has been the most frequently implemented solar cooling technology. The reason for this is debated, but commonly suggested reasons include incentive structuring, lack of residential-sized equipment for other solar-cooling technologies, the advent of more efficient electrical coolers, or ease of installation compared to other solar-cooling technologies (like radiant cooling).\nSince PV cooling's cost effectiveness depends largely on the cooling equipment and given the poor efficiencies in electrical cooling methods until recently it has not been cost effective without subsidies. Using more efficient electrical cooling methods and allowing longer payback schedules is changing that scenario.\nFor example, a 100,000 BTU U.S. Energy Star rated air conditioner with a high seasonal energy efficiency ratio (SEER) of 14 requires around 7 kW of electric power for full cooling output on a hot day. This would require over a 20 kW solar photovoltaic electricity generation system with storage.\nA solar-tracking 7 kW photovoltaic system would probably have an installed price well over $20,000 USD (with PV equipment prices currently falling at roughly 17% per year). Infrastructure, wiring, mounting, and NEC code costs may add up to an additional cost; for instance a 3120 watt solar panel grid tie system has a panel cost of $0.99/watt peak, but still costs ~$2.2/watt hour peak. Other systems of different capacity cost even more, let alone battery backup systems, which cost even more.\nA more efficient air conditioning system would require a smaller, less-expensive photovoltaic system. A high-quality geothermal heat pump installation can have a SEER in the range of 20 (\u00b1). A 100,000 BTU SEER 20 air conditioner would require less than 5 kW while operating.\nNewer and lower power technology including reverse inverter DC heat pumps can achieve SEER ratings up to 26.\nThere are new non-compressor-based electrical air conditioning systems with a SEER above 20 coming on the market. New versions of phase-change indirect evaporative coolers use nothing but a fan and a supply of water to cool buildings without adding extra interior humidity (such as at McCarran Airport Las Vegas Nevada). In dry arid climates with relative humidity below 45% (about 40% of the continental U.S.) indirect evaporative coolers can achieve a SEER above 20, and up to SEER 40. A 100,000 BTU indirect evaporative cooler would only need enough photovoltaic power for the circulation fan (plus a water supply).\nA less-expensive partial-power photovoltaic system can reduce (but not eliminate) the monthly amount of electricity purchased from the power grid for air conditioning (and other uses). With American state government subsidies of $2.50 to $5.00 USD per photovoltaic watt, the amortized cost of PV-generated electricity can be below $0.15 per kWh. This is currently cost effective in some areas where power company electricity is now $0.15 or more. Excess PV power generated when air conditioning is not required can be sold to the power grid in many locations, which can reduce (or eliminate) annual net electricity purchase requirement. (See Zero-energy building)\nSuperior energy efficiency can be designed into new construction (or retrofitted to existing buildings). Since the U.S. Department of Energy was created in 1977, their Weatherization Assistance Program has reduced heating-and-cooling load on 5.5 million low-income affordable homes an average of 31%. A hundred million American buildings still need improved weatherization. Careless conventional construction practices are still producing inefficient new buildings that need weatherization when they are first occupied.\nIt is fairly simple to reduce the heating-and-cooling requirement for new construction by one half. This can often be done at no additional net cost, since there are cost savings for smaller air conditioning systems and other benefits.\n\n\n== Geothermal cooling ==\nEarth sheltering or Earth cooling tubes can take advantage of the ambient temperature of the Earth to reduce or eliminate conventional air conditioning requirements. In many climates where the majority of humans live, they can greatly reduce the buildup of undesirable summer heat, and also help remove heat from the interior of the building. They increase construction cost, but reduce or eliminate the cost of conventional air conditioning equipment.\nEarth cooling tubes are not cost effective in hot humid tropical environments where the ambient Earth temperature approaches human temperature comfort zone. A solar chimney or photovoltaic-powered fan can be used to exhaust undesired heat and draw in cooler, dehumidified air that has passed by ambient Earth temperature surfaces. Control of humidity and condensation are important design issues.\nA geothermal heat pump uses ambient Earth temperature to improve SEER for heat and cooling. A deep well recirculates water to extract ambient Earth temperature (typically at 2 gallons of water per ton per minute). These \"open loop\" systems were the most common in early systems, however water quality could cause damage to the coils in the heat pump and shorten the life of the equipment. Another method is a closed loop system, in which a loop of tubing is run down a well or wells, or in trenches in the lawn, to cool an intermediate fluid. When wells are used, they are back-filled with Bentonite or another grout material to ensure good thermal conductivity to the earth.\nIn the past the fluid of choice was a 50/50 mixture of propylene glycol because it is non-toxic unlike ethylene glycol (which is used in car radiators). Propylene glycol is viscous, and would eventually gum up some parts in the loop(s), so it has fallen out of favor. Today, the most common transfer agent is a mixture of water and ethyl alcohol (ethanol).\nAmbient earth temperature is much lower than peak summer air temperature, and much higher than the lowest extreme winter air temperature. Water is 25 times more thermally conductive than air, so it is much more efficient than an outside air heat pump, (which becomes less effective when the outside temperature drops in Winter).\nThe same type of geothermal well can be used without a heat pump but with greatly diminished results. Ambient Earth temperature water is pumped through a shrouded radiator (like an automobile radiator). Air is blown across the radiator, which cools without a compressor-based air conditioner. Photovoltaic solar electric panels produce electricity for the water pump and fan, eliminating conventional air-conditioning utility bills. This concept is cost-effective, as long as the location has ambient Earth temperature below the human thermal comfort zone (not the tropics).\n\n\n== Solar open-loop Air Conditioning using desiccants ==\nAir can be passed over common, solid desiccants (like silica gel or zeolite) or liquid desiccants (like lithium bromide/chloride) to draw moisture from the air to allow an efficient mechanical or evaporative cooling cycle. The desiccant is then regenerated by using solar thermal energy to dehumidfy, in a cost-effective, low-energy-consumption, continuously repeating cycle. A photovoltaic system can power a low-energy air circulation fan, and a motor to slowly rotate a large disk filled with desiccant.\nEnergy recovery ventilation systems provide a controlled way of ventilating a home while minimizing energy loss. Air is passed through an \"enthalpy wheel\" (often using silica gel) to reduce the cost of heating ventilated air in the winter by transferring heat from the warm inside air being exhausted to the fresh (but cold) supply air. In the summer, the inside air cools the warmer incoming supply air to reduce ventilation cooling costs. This low-energy fan-and-motor ventilation system can be cost-effectively powered by photovoltaics, with enhanced natural convection exhaust up a solar chimney - the downward incoming air flow would be forced convection (advection).\nA desiccant like calcium chloride can be mixed with water to create an attractive recirculating waterfall, that dehumidifies a room using solar thermal energy to regenerate the liquid, and a PV-powered low-rate water pump\nActive solar cooling wherein solar thermal collectors provide input energy for a desiccant cooling system. There are several commercially available systems that blow air through a desiccant impregnated medium for both the dehumidification and the regeneration cycle. The solar heat is one way that the regeneration cycle is powered. In theory packed towers can be used to form a counter-current flow of the air and the liquid desiccant but are not normally employed in commercially available machines. Preheating of the air is shown to greatly enhance desiccant regeneration. The packed column yields good results as a dehumidifier/regenerator, provided pressure drop can be reduced with the use of suitable packing.\n\n\n== Passive solar cooling ==\n\nIn this type of cooling solar thermal energy is not used directly to create a cold environment or drive any direct cooling processes. Instead, solar building design aims at slowing the rate of heat transfer into a building in the summer, and improving the removal of unwanted heat. It involves a good understanding of the mechanisms of heat transfer: heat conduction, convective heat transfer, and thermal radiation, the latter primarily from the sun.\nFor example, a sign of poor thermal design is an attic that gets hotter in summer than the peak outside air temperature. This can be significantly reduced or eliminated with a cool roof or a green roof, which can reduce the roof surface temperature by 70 \u00b0F (40 \u00b0C) in summer. A radiant barrier and an air gap below the roof will block about 97% of downward radiation from roof cladding heated by the sun.\nPassive solar cooling is much easier to achieve in new construction than by adapting existing buildings. There are many design specifics involved in passive solar cooling. It is a primary element of designing a zero energy building in a hot climate.\n\n\n== Solar closed-loop absorption cooling ==\n\nThe following are common technologies in use for solar thermal closed-loop air conditioning.\nAbsorption: NH\n3/H\n2O or Ammonia/Water\nAbsorption: Water/Lithium Bromide\nAbsorption: Water/Lithium Chloride\nAdsorption: Water/Silica Gel or Water/Zeolite\nAdsorption: Methanol/Activated Carbon\nActive solar cooling uses solar thermal collectors to provide solar energy to thermally driven chillers (usually adsorption or absorption chillers). Solar energy heats a fluid that provides heat to the generator of an absorption chiller and is recirculated back to the collectors. The heat provided to the generator drives a cooling cycle that produces chilled water. The chilled water produced is used for large commercial and industrial cooling.\nSolar thermal energy can be used to efficiently cool in the summer, and also heat domestic hot water and buildings in the winter. Single, double or triple iterative absorption cooling cycles are used in different solar-thermal-cooling system designs. The more cycles, the more efficient they are. Absorption chillers operate with less noise and vibration than compressor-based chillers, but their capital costs are relatively high.\nEfficient absorption chillers nominally require water of at least 190 \u00b0F (88 \u00b0C). Common, inexpensive flat-plate solar thermal collectors only produce about 160 \u00b0F (71 \u00b0C) water. High temperature flat plate, concentrating or evacuated tube collectors are needed to produce the higher temperature water required. In large scale installations there are several projects successful both technical and economical in operation worldwide including, for example, at the headquarters of Caixa Geral de Dep\u00f3sitos in Lisbon with 1,579 square metres (17,000 sq ft) solar collectors and 545 kW cooling power or on the Olympic Sailing Village in Qingdao/China. In 2011 the most powerful plant at Singapore's new constructed United World College will be commissioned (1500 kW).\nThese projects have shown that flat plate solar collectors specially developed for temperatures over 200 \u00b0F (93 \u00b0C) (featuring double glazing, increased backside insulation, etc.) can be effective and cost efficient. Where water can be heated well above 190 \u00b0F (88 \u00b0C), it can be stored and used when the sun is not shining.\nThe Audubon Environmental Center at the Ernest E. Debs Regional Park in Los Angeles has an example solar air conditioning installation, which failed fairly soon after commissioning and is no longer being maintained. The Southern California Gas Co. (The Gas Company) is also testing the practicality of solar thermal cooling systems at their Energy Resource Center (ERC) in Downey, California. Solar Collectors from Sopogy and Cogenra were installed on the rooftop at the ERC and are producing cooling for the building\u2019s air conditioning system. Masdar City in the United Arab Emirates is also testing a double-effect absorption cooling plant using Sopogy parabolic trough collectors, Mirroxx Fresnel array and TVP Solar high-vacuum solar thermal panels.\nFor 150 years, absorption chillers have been used to make ice (before the electric light bulbs were invented). This ice can be stored and used as an \"ice battery\" for cooling when the sun is not shining, as it was in the 1995 Hotel New Otani Tokyo in Japan. Mathematical models are available in the public domain for ice-based thermal energy storage performance calculations.\nThe ISAAC Solar Icemaker is an intermittent solar ammonia-water absorption cycle. The ISAAC uses a parabolic trough solar collector and a compact and efficient design to produce ice with no fuel or electric input, and with no moving parts.\nProviders of solar cooling systems include ChillSolar, SOLID, Sopogy, Cogenra, Mirroxx  and TVP Solar  for commercial installations and ClimateWell, Fagor-Rotartica, SorTech and Daikin mostly for residential systems. Cogenra uses solar co-generation to produce both thermal and electric energy that can be used for cooling.\n\n\n== Zero-energy buildings ==\nGoals of zero-energy buildings include sustainable, green building technologies that can significantly reduce, or eliminate, net annual energy bills. The supreme achievement is the totally off-the-grid autonomous building that does not have to be connected to utility companies. In hot climates with significant degree days of cooling requirement, leading-edge solar air conditioning will be an increasingly important critical success factor.\n\n\n== See also ==\n\nPassive house\nPassive solar building design\nSolar powered refrigerator\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\nSolar Thermal Air Conditioning\nAbsorPilot (Spanish)\nEU: solar Heating and Cooling:.\nCooling with Solar Heat: Growing Interest in Solar Air Conditioning.\nLiquid Desiccant Waterfall for attractive building dehumidification\nPassive solar cooling\nPassive solar cooling in a hot humid climate\nSolar Heating and Cooling Programme, International Energy Agency.\nSolar Thermal Absorption Cooling System.\nFraunhofer-Institut f\u00fcr Solare Energiesysteme ISE, Solar Cooling\nDistributed Energy Resources Customer Adoption Model (DER-CAM)\nCenter for Energy and innovative Technologies\nSolar furnace and air conditioning invention", 
                "titleUrl": "https://en.wikipedia.org/wiki/Solar_air_conditioning", 
                "title": "Solar air conditioning"
            }, 
            {
                "snippet": "with water filled cooling tubes, jackets, centrifugal pumps and finned heat exchangers (the first recorded use of battery cooling) to reduce core temperatures", 
                "pageCategories": "1923 ships\nGood articles\nSubmarines of the Royal Navy\nSurface-underwater ships", 
                "pageContent": "HM Submarine X1 was conceived and designed as a submersible commerce raider for the Royal Navy; at the time of her launching she was the largest submarine in the world. The idea of a submarine cruiser had been proposed as early as 1915, but was not put into practice until 1921. X1, which was based on the uncompleted German U-173 class of 2,000-ton \"U-cruisers\", was laid down on 2 November 1921 at the Naval Dockyard Chatham and completed on 23 September 1925, commissioning in December 1925.\nThe 1922 Washington Naval Treaty, of which Britain was a signatory, did not ban submarines but it did ban their use against merchant ships, which was X1's unacknowledged purpose; its armament had been designed to successfully engage the classes of vessels likely to be escorting convoys, such as destroyers and frigates. Therefore, a certain amount of secrecy surrounded X1, the government even going to the lengths of taking a national newspaper to court over its pictures of the new submarine following her launch, all copies of the paper being seized.\n\n\n== Description ==\nThe X1's 1-inch (25 mm) thick pressure hull was 19 feet 7.5 inches (5.982 m) in diameter amidships, and was divided into 10 water-tight compartments. This was almost completely surrounded by her external hull, which also contained the main ballast tanks and most of her fuel. Her intended maximum diving depth was 500 feet (150 m), but was reduced to 350 feet (110 m) once in service.\nShe was expected to sink her targets using gunfire and so was given four 5.2-inch (130 mm) guns to be able to defeat a destroyer or armed merchant ship, although she was fitted with six bow tubes for 21-inch (530 mm) torpedoes to supplement her guns.\n\n\n=== Armament ===\nX1 carried four QF 5.2 inch Mk I guns in twin unarmoured turrets, one forward and one aft of the conning tower. They had a range of about 16,000 yards (15,000 m). A circular trunk ran from each mounting to the magazine in the pressure hull which contained 100 rounds per gun. A working chamber which was 10 feet (3.0 m) in diameter encircled the trunk between the pressure hull and the gun mount. Her ammunition hoists were problematic and could not sustain the desired rate of fire of six rounds per gun per minute. Special ballast tanks were used to compensate for the loss of weight as ammunition was fired. Working and control of the guns required no less than 58 men. The fire-control tower was in the middle of the conning tower and had a top section that could be raised 2 feet (0.61 m) when in use. The upper control room was between the tower and the pressure hull. Just aft of the control room was the rangefinding room, with a 9-foot (2.7 m) rangefinder on the bridge that could be raised 8 feet (2.4 m).\nHer six torpedo tubes came from a cancelled L-class submarine and she was provided with one reload for each tube. It took some 24 minutes to reload them all because space in the torpedo room was restricted.\n\n\n=== Propulsion ===\nThe main engines were two 8-cylinder Admiralty diesel engines with a total output of 3,000 horsepower (2,200 kW). Two auxiliary 1,200-horsepower (890 kW) MAN diesel engines taken from U-126 were installed for battery-charging purposes. For underwater propulsion, two GEC electric motors of 1,000 horsepower (750 kW) each were fitted. It was hoped to achieve over 8,000 horsepower (6,000 kW) using both diesels and electric motors together, but the highest power achieved (during a full power trial in March 1926) was 7,135 horsepower (5,321 kW). She had three groups of batteries, each with 110 cells weighing a total of 70 long tons (71 t). These batteries were also fitted with water filled cooling tubes, jackets, centrifugal pumps and finned heat exchangers (the first recorded use of battery cooling) to reduce core temperatures during aggressive charge and discharge cycles.\nIn theory she could make 19.5 knots (36.1 km/h) on the surface, and at economical speed she had a greater range than normal cruisers, but both sets of diesel engines suffered from continual mechanical problems that reduced her speed and range. The X1's average diving time (to periscope depth) was 2 minutes 20 seconds. Her handling underwater was considered superior to other submarines of the period.\n\n\n== Career ==\nAfter X1 was commissioned in December 1925 and accepted in April 1926 she made a voyage to Gibraltar after which her main engine drive wheels were found to be damaged. After repairs she was sent to the Mediterranean Sea. Her starboard camshaft driveshaft broke during a full-power run in January 1928 and a new set of gears was needed, but after refitting at Malta her port camshaft driveshaft broke in the same place in April 1928. By 1930 her commanding officer reported \"internal arrangements not very satisfactory because of overcrowding with auxiliary machinery, accommodation is cramped, ventilation poor and the ship suffers from humidity, diving arrangements good.\" Both the main and auxiliary engines were troublesome and she spent most of her time under repair, before being laid up. X1 was placed in reserve after 1930, before she was finally scrapped at Pembroke on 12 December 1936.\n\n\n== See also ==\nBritish M-class submarine \u2014 an earlier attempt at a large gun-armed submarine\nSM U-139 - U-139 class (Projekt 46); one of three German First World War submarine cruisers\nSurcouf \u2014 a similar French large gun-armed submarine\n\n\n== Notes ==\n\n\n== References ==\nAkermann, Paul (2002). Encyclopaedia of British Submarines 1901-1955 (reprint of the 1989 ed.). Penzance, Cornwall: Periscope Publishing. ISBN 1-904381-05-7. \nBrown, David K. (1982). \"X1-Cruiser Submarine\". In John Roberts. Warship VI. VI. London: Conway Maritime Press. pp. 232\u2013233. ISBN 0-85177-265-X. \n\n\n== Further reading ==\nCompton-Hall, Richard (1985). Submarine Warfare, Monsters and Midgets. Poole, Dorset, England: Blandford Press. ISBN 0-7137-1389-5. \nBranfill-Cook, Roger (2012). X.1 The Royal Navy's Mystery Submarine. Barnsley, S. Yorkshire, England: Seaforth Publishing. ISBN 978-1-84832-161-8. \n\n\n== External links ==\nSubmariners.co.uk Boat database\npicture of X-1 under construction at Chatham\nHistory of X-1", 
                "titleUrl": "https://en.wikipedia.org/wiki/HMS_X1", 
                "title": "HMS X1"
            }, 
            {
                "snippet": "nozzle strong enough to damage the gimbal actuators and regenerative cooling tubes during startup. Meticulous computational fluid dynamics (CFD) work was", 
                "pageCategories": "Commons category with local link same as on Wikidata\nRocket engines of Japan\nRocket engines using hydrogen propellant\nRocket engines using the staged combustion cycle", 
                "pageContent": "The LE-7 and its succeeding upgrade model the LE-7A are staged combustion cycle LH2/LOX liquid rocket engines produced in Japan for the H-II series of launch vehicles. Design and production work was all done domestically in Japan, the first major (main/first-stage) liquid rocket engine with that claim, in a collaborative effort from the National Space Development Agency (NASDA), Aerospace Engineering Laboratory (NAL), Mitsubishi Heavy Industries, and Ishikawajima-Harima. NASDA and NAL have since been integrated into JAXA. However, a large part of the work was contracted to Mitsubishi, with Ishikawajima-Harima providing turbomachinery, and the engine is often referred to as the Mitsubishi LE-7(A).\nThe original LE-7 was designed to be a high efficiency, medium-sized motor with sufficient thrust for use on the H-II, and classified as expendable since the engine was non-recoverable after launch.\n\n\n== H-II Flight 8, only operational LE-7 failure ==\nThe fuel turbopump had an issue using the originally designed inducer (a propeller-like axial pump used to raise the inlet pressure of the propellant ahead of the main turbopumps to prevent cavitation) where the inducer would itself begin to cavitate and cause an imbalance resulting in excessive vibration. A comprehensive post-flight analysis of the unsuccessful 8th H-II launch, including a deep ocean retrieval of the wreckage, determined that fatigue due to this vibration was the cause of premature engine failure.\n\n\n== LE-7A ==\nThe LE-7A is an upgraded model from the LE-7 rocket engine. Basic design is unchanged from the original model. The 7A had additional engineering effort placed on cost cutting, reliability, and performance developments. The renovation was undertaken to mate it with the likewise improved H-IIA launch vehicle, with the common goal being a more reliable, more powerful and flexible, and more cost effective launch system.\n\n\n=== Changes / improvements ===\nSpecific emphasis was placed on reducing or the amount of required welding by allowing for more machined or cast components, and to simplify as many of the remaining welds as possible. This resulted in a substantial rework of the pipe routing (which makes the outward appearance of the two models considerably different). To combat the fuel inducer complications described above, the fuel inducer was redesigned for the 7A. The oxidizer inducer was also redesigned, but this was primarily due to poor performance at low inlet pressures as opposed to reliability concerns. The fuel turbopump itself was also the subject of various durability enhancements. Additionally the combustion chamber/injector assembly underwent a number of small changes, like decreasing the number of injector elements, to reduce machining complexity (and thus cost) and improve reliability. While these changes overall resulted in a drop in maximum specific impulse to 440 seconds (4.3 km/s) (basically making the engine less fuel efficient), the trade off for lower cost and enhanced reliability was considered acceptable.\n\n\n=== New nozzle design (side-loading problem) ===\nFor the new engine model, a nozzle extension was designed that could be added to the base of the new standard \u201cshort\u201d nozzle when extra performance was required. But when the engine was fitted with the nozzle extension, the 7A encountered a new problem with unprecedented side-loads and irregular heating on the nozzle strong enough to damage the gimbal actuators and regenerative cooling tubes during startup. Meticulous computational fluid dynamics (CFD) work was able to sufficiently replicate and trace the dangerous transient loading and a new one-piece \u201clong\u201d nozzle with full regenerative cooling (as opposed to the original short nozzle with a separate film-cooled extension) was designed to mitigate the problem. Before this new nozzle was ready, some H-IIA\u2019s were launched using only the short nozzle. The 7A no longer uses a separate nozzle extension in any configuration.\n\n\n=== Use on H-IIB ===\nThe new H-IIB launch vehicles uses two LE-7A engines in its first stage.\n\n\n=== LE-7A specifications ===\nOperational Cycle: staged combustion\nFuel: hydrogen\nOxidizer: liquid oxygen\nMixture ratio (oxidizer to fuel): 5.90\nShort nozzle:\nRated thrust (sea level): 843 kN (190,000 lbf)\nRated thrust (vacuum): 1,074 kN (241,000 lbf)\nSpecific impulse (sea level):\nSpecific impulse (vacuum): 429 seconds (4.21 km/s)\n\nLong nozzle:\nRated thrust (sea level): 870 kN (200,000 lbf)\nRated thrust (vacuum): 1,098 kN (247,000 lbf)\nSpecific impulse (sea level): 338 seconds (3.31 km/s)\nSpecific impulse (vacuum): 440 seconds (4.3 km/s)\n\nDry mass: 1,800 kg (4,000 lb)\nLength:\nshort nozzle = 3.2 m\nlong nozzle = 3.7 m\n\nThrottle capability: 72-100%\nThrust-to-weight: 65.9\nNozzle area ratio: 51.9:1\nCombustion chamber pressure: 12.0 MPa (1,740 psi)\nLiquid hydrogen turbopump: 41,900 rpm\nLiquid oxygen turbopump: 18,300 rpm\n\n\n== See also ==\nLE-5\nH-II, H-IIA, & H-IIB\nComparison of orbital rocket engines\nliquid rocket engine\nstaged combustion cycle\nJAXA\n\n\n== References ==\n\n\n== External links ==\nEncyclopedia Astronautica info page on the LE-7\nEncyclopedia Astronautica info page on the LE-7A\nJapanese Wikipedia LE-7 page (in Japanese)\nJapanese Wikipedia LE-7A page (in Japanese)\nH-IIA Rocket Engine Development\nOverview of the H-IIA Launch Vehicle", 
                "titleUrl": "https://en.wikipedia.org/wiki/LE-7", 
                "title": "LE-7"
            }, 
            {
                "snippet": "and dry. Passive cooling which pulls air with a fan or convection from a near constant temperature air into buried Earth cooling tubes and then into the", 
                "pageCategories": "All articles lacking in-text citations\nAll pages needing cleanup\nArticles containing how-to sections\nArticles lacking in-text citations from March 2010\nArticles needing cleanup from May 2013\nBuilding engineering\nCommons category without a link on Wikidata\nEnergy conservation\nHouse types\nInterlanguage link template link number", 
                "pageContent": "Earth sheltering is the architectural practice of using earth against building walls for external thermal mass, to reduce heat loss, and to easily maintain a steady indoor air temperature. Earth sheltering has become relatively more popular in modern times, especially among environmentalists and advocates of passive solar and sustainable architecture. However, the practice has been around for nearly as long as humans have been constructing their own shelters.\n\n\n== Definition ==\nThe expression earth-sheltering is a generic term, with the general meaning: building design in which soil plays an integral part.\nA building can be described as earth-sheltered if its external envelope is in contact with a thermally significant volume of soil or substrate (where \u201cthermally significant\u201d means making a functional contribution to the thermal effectiveness of the building in question.)\nEarth-sheltered buildings consist of one or more of three types: earth-covered, earth-bunded, and subterranean. An earth-covered building is one where the thermally effective element is placed solely on the roof, but is more usually a continuation of the earth-bunding at the unexposed elevations of the building. An earth-bunded building is one where the thermally significant element insulates one or more of the sheltered elevations of the building. The bunding can be partial or total. A subterranean building is one where the thermally significant element insulates all elevations of the building, leaving only the roof exposed; or, if the building is built into an incline, it may be that the roof is covered and only one elevation is left exposed.\n\n\n== Background ==\nLiving within earth shelters has been a large part of human history. The connection to earth shelter dwellings began with the utilization of caves, and over time evolving technologies led to the construction of customized earth dwellings. Today, earth shelter construction is a rare practice, especially in the U.S.A. During the energy crisis and the 1973 Oil Crisis, along with the back-to-the-land movement, there was a surge of interest in earth shelter/underground home construction in an effort toward self-sufficient living. However, progress has been slow, and earth shelter construction is often viewed by architects, engineers, and the public alike as an unconventional method of building. Techniques of earth sheltering have not yet become common knowledge, and much of society still remains unaware of the process or benefits of this type of building construction.\n\n\n== Types of construction ==\n\nEarth berming: Earth is piled up against exterior walls and packed, sloping down away from the house. The roof may or may not be fully earth covered, and windows/openings may occur on one or more sides of the shelter. Due to the building being above ground, fewer moisture problems are associated with earth berming in comparison to underground/fully recessed construction.\nIn-hill construction: The house is set into a slope or hillside. The most practical application is using a hill facing towards the equator (south in the Northern Hemisphere and north in the Southern Hemisphere). There is only one exposed wall in this type of earth sheltering, the wall facing out of the hill, all other walls are embedded within the earth/hill.\nUnderground/fully recessed construction: The ground is excavated, and the house is set in below grade. It can also be referred to as an Atrium style due to the common atrium/courtyard constructed in the middle of the shelter to provide adequate light and ventilation.\n\n\n== Benefits ==\nThe benefits of earth sheltering are numerous. They include: taking advantage of the earth as a thermal mass, offering extra protection from the natural elements, energy savings, providing substantial privacy, efficient use of land in urban settings, shelters have low maintenance requirements, and earth sheltering commonly takes advantage of passive solar building design.\nThe Earth's mass absorbs and retains heat. Over time, this heat is released to surrounding areas, such as an earth shelter. Because of the high density of the earth, change in the earth\u2019s temperature occurs slowly. This is known as \u2018thermal lag.\u2019 Because of this principle, the earth provides a fairly constant temperature for the underground shelters, even when the outdoor temperature undergoes great fluctuation. In most of the United States, the average temperature of the earth once below the frost line is between 55 and 57 degrees Fahrenheit (13 to 14 degrees Celsius). Frost line depths vary from region to region. In the USA frost lines can range from just under the surface to more than 40 inches. Thus, at the base of a deep earth berm, the house is heated against an exterior temperature gradient of perhaps ten to fifteen degrees, instead of against a steeper temperature grade where air is on the outside of the wall instead of earth. During the summer, the temperature gradient helps to cool the house.\nThe reduction of air infiltration within an earth shelter can be highly profitable. Because three walls of the structure are mainly surrounded by earth, very little surface area is exposed to the outside air. This alleviates the problem of warm air escaping the house through gaps around windows and door. Furthermore, the earth walls protect against cold winter winds which might otherwise penetrate these gaps. However, this can also become a potential indoor air quality problem. Healthy air circulation is key.\nAs a result of the increased thermal mass of the structure, the thermal lag of the earth, the protection against unwanted air infiltration and the combined use of passive solar techniques, the need for extra heating and cooling is minimal. Therefore, there is a drastic reduction in energy consumption required for the home compared to homes of typical construction.\nEarth shelters also provide privacy from neighbours, as well as soundproofing. The ground provides acoustic protection against outside noise. This can be a major benefit in urban areas or near highways. In urban areas, another benefit of underground sheltering is the efficient use of land. Many houses can sit below grade without spoiling the habitat above ground. Each site can contain both a house and a lawn/garden.\n\n\n== Potential problems ==\nProblems of water seepage, internal condensation, bad acoustics, and poor indoor air quality can occur if an earth shelter has not been properly designed.\nIssues also include the sustainability of building materials. Earth sheltering often requires heavier construction than conventional building techniques, and many construction companies have limited or no experience with earth sheltered construction, potentially compromising the physical construction of even the best designs.\nThe threat of water seepage occurs around areas where the waterproofing layers have been penetrated. Vents and ducts emerging from the roof can cause specific problems due to the possibility of movement. Precast concrete slabs can have a deflection of 1/2 inch or more when the earth/soil is layered on top of them. If the vents or ducts are held rigidly in place during this deflection, the result is usually the failure of the waterproofing layer. To avoid this difficulty, vents can be placed on other sides of the building (besides the roof), or separate segments of pipes can be installed. A narrower pipe in the roof that fits snugly into a larger segment within the building can also be used. The threat of water seepage, condensation, and poor indoor air quality can all be overcome with proper waterproofing and ventilation.\nThe building materials for earth sheltered construction tend to be of non-biodegradable substances. Because the materials must keep water out, they are often made of plastics. Concrete is another material that is used in great quantity. More sustainable products are being tested to replace the cement within concrete (such as fly ash), as well as alternatives to reinforced concrete (see more under Materials: Structural). The excavation of a site is also drastically time- and labor-consuming. Overall, the construction is comparable to conventional construction, because the building requires minimal finishing and significantly less maintenance.\nCondensation and poor quality indoor air problems can be solved by using earthtubes, or what is known as a geothermal heat pump\u2014a concept different from earth sheltering. With modification, the idea of earthtubes can be used for underground buildings: instead of looping the earthtubes, leave one end open downslope to draw in fresh air using the chimney effect by having exhaust vents placed high in the underground building.\n\n\n== Landscape and site planning ==\nThe site planning for an earth sheltered building is an integral part of the overall design; investigating the landscape of a potential building site is crucial. There are many factors to assess when surveying a site for underground construction. The topography, regional climate, vegetation, water table and soil type of varying landscapes all play dynamic roles in the design and application of earth shelters.\n\n\n=== Topography ===\nOn land that is relatively flat, a fully recessed house with an open courtyard is the most appropriate design. On a sloping site, the house is set right into the hill. The slope will determine the location of the window wall; a south-facing exposed wall is the most practical in the Northern hemisphere (and north-facing in the Southern hemisphere) due to solar benefits. The most practical house design in the tropics (and with equal advantage in both hemispheres) is that the two shorter walls on the ends be exposed, one facing east and the other facing west.\n\n\n=== Regional climate ===\nDepending on the region and site selected for earth sheltered construction, the benefits and objectives of the earth shelter construction vary. For cool and temperate climates, objectives consist of retaining winter heat, avoiding infiltration, receiving winter sun, using thermal mass, shading and ventilating during the summer, and avoiding winter winds and cold pockets. For hot, arid climates objectives include maximizing humidity, providing summer shade, maximizing summer air movement, and retaining winter heat. For hot, humid climates objectives include avoiding summer humidity, providing summer ventilation, and retaining winter heat.\nRegions with extreme daily and seasonal temperatures emphasize the value of earth as a thermal mass. In this way, earth sheltering is most effective in regions with high cooling and heating needs, and high temperature differentials. In regions such as the south eastern United States, earth sheltering may need additional care in maintenance and construction due to condensation problems in regard to the high humidity. The ground temperature of the region may be too high to permit earth cooling if temperatures fluctuate only slightly from day to night. Preferably, there should be adequate winter solar radiation, and sufficient means for natural ventilation. Wind is a critical aspect to evaluate during site planning, for reasons regarding wind chill and heat loss, as well as ventilation of the shelter. In the Northern Hemisphere, south facing slopes tend to avoid cold winter winds typically blown in from the north. Fully recessed shelters also offer adequate protection against these harsh winds. However, atria within the structure have the ability to cause minor turbulence depending on the size. In the summer, it is helpful to take advantage of the prevailing winds. Because of the limited window arrangement in most earth shelters, and the resistance to air infiltration, the air within a structure can become stagnant if proper ventilation is not provided. By making use of the wind, natural ventilation can occur without the use of fans or other active systems. Knowing the direction, and intensity, of seasonal winds is vital in promoting cross ventilation. Vents are commonly placed in the roof of bermed or fully recessed shelters to achieve this effect.\n\n\n=== Vegetation ===\nThe plant cover of the landscape is another important factor. Adding plants can be both positive and negative. Nearby trees may be valuable in wet climates because their roots remove water. However a prospective builder should know what types of trees are in the area and how large and rapidly they tend to grow, due to possible solar-potential compromise with their growth. Vegetation can provide a windbreak for houses exposed to winter winds. The growth of small vegetation, especially those with deep roots, also helps in the prevention of erosion, on the house and in the surrounding site.\n\n\n=== Soil and drainage ===\nThe soil type is one of the most essential factors during site planning. The soil needs to provide adequate bearing capacity and drainage, and help to retain heat. With respects to drainage, the most suitable type of soil for earth sheltering is a mixture of sand and gravel. Well graded gravels have a large bearing capacity (about 8,000 pounds per square foot), excellent drainage and a low frost heave potential. Sand and clay can be susceptible to erosion. Clay soils, while least susceptible to erosion, often do not allow for proper drainage, and have a higher potential for frost heaves. Clay soils are more susceptible to thermal shrinking and expanding. Being aware of the moisture content of the soil and the fluctuation of that content throughout the year will help prevent potential heating problems. Frost heaves can also be problematic in some soil. Fine grain soils retain moisture the best and are most susceptible to heaving. A few ways to protect against capillary action responsible for frost heaves are placing foundations below the freezing zone or insulating ground surface around shallow footings, replacement of frost sensitive soils with granular material, and interrupting capillary draw of moisture by putting a drainage layer of coarser material in the existing soil.\nWater can cause potential damage to earth shelters if it ponds around the shelter. Avoiding sites with a high water table is crucial. Drainage, both surface and subsurface, must be properly dealt with. Waterproofing applied to the building is essential.\nAtrium designs have an increased risk of flooding, so the surrounding land should slope away from the structure on all sides. A drain pipe at the perimeter of the roof edge can help collect and remove additional water. For bermed homes, an interceptor drain at the crest of the berm along the edge of the roof top is recommended. An interceptor drainage swale in the middle of the berm is also helpful or the back of the berm can be terraced with retaining walls. On sloping sites runoff may cause problems. A drainage swale or gully can be built to divert water around the house, or a gravel filled trench with a drain tile can be installed along with footing drains.\nSoil stability should also be considered, especially when evaluating a sloping site. These slopes may be inherently stable when left alone, but cutting into them can greatly compromise their structural stability. Retaining walls and backfills may have to be constructed to hold up the slope prior to shelter construction.\n\n\n== Construction methods ==\n\n\n=== Current methods ===\nIn earth sheltered construction there is often extensive excavation done on the building site. An excavation several feet larger than the walls' planned perimeter is made to allow for access to the outside of the wall for waterproofing and insulation. Once the site is prepared and the utility lines installed, a foundation of reinforced concrete is poured. The walls are then installed. Usually they are either poured in place or formed either on or off site and then moved into place. Reinforced concrete is the most common choice. The process is repeated for the roof structure. If the walls, floor and roof are all to be poured in place, it is possible to make them with a single pour. This can reduce the likelihood of there being cracks or leaks at the joints where the concrete has cured at different times.\nOn the outside of the concrete a waterproofing system is applied. The most frequently used waterproofing system includes a layer of liquid asphalt onto which a heavy grade waterproof membrane is affixed, followed by a final liquid water sealant which may be sprayed on. It is very important to make sure that all of the seams are carefully sealed. It is very difficult to locate and repair leaks in the waterproofing system after the building is completed.\nOne or more layers of insulation board or foam are added on the outside of the waterproofing. If the insulation chosen is porous, a top layer of waterproofing is added. After everything is complete, earth is backfilled into the remaining space at the exterior of the wall and sometimes over the roof to accommodate a green roof. Any exposed walls and the interior are finished according to the owners' preferences.\n\n\n=== Materials ===\n\n\n==== Structural ====\nReinforced concrete is the most commonly used structural material in earth shelter construction. It is strong and readily available. Untreated wood rots within five years of use in earth shelter construction. Steel can be used, but needs to be encased by concrete to keep it from direct contact with the soil which corrodes the metal. Bricks and CMUs (concrete masonry units) are also possible options in earth shelter construction but must be reinforced to keep them from shifting under vertical pressure unless the building is constructed with arches and vaults.\nUnfortunately, reinforced concrete is not the most environmentally sustainable material. The concrete industry is working to develop products that are more earth-friendly in response to consumer demands. Products like Grancrete and Hycrete are becoming more readily available. They claim to be environmentally friendly and either reduce or eliminate the need for additional waterproofing. However, these are new products and have not been extensively used in earth shelter construction yet.\nSome unconventional approaches are also proposed. One such method is a PSP method proposed by Mike Oehler. The PSP method uses wooden posts, plastic sheeting and non-conventional ideas that allow more windows and ventilation. This design also reduces some runoff problems associated with conventional designs. The method uses wood posts, a frame that acts like a rib to distribute settling forces, specific construction methods which rely on fewer pieces of heavy equipment, plastic sheeting, and earth floors with plastic and carpeting.\n\n\n==== Waterproofing ====\nSeveral layers are used for waterproofing in earth shelter construction. The first layer is meant to seal any cracks or pores in the structural materials, also working as an adhesive for the waterproof membrane. The membrane layer is often a thick flexible polyethylene sheeting called EPDM. EPDM is the material usually used in water garden, pond and swimming pool construction. This material also prevents roots from burrowing through the waterproofing. EPDM is very heavy to work with, and can be chewed through by some common insects like fire ants. It is also made from petrochemicals, making it less than perfect environmentally.\nThere are various cementitious coatings that can be used as waterproofing. The product is sprayed directly onto the unprotected surface. It dries and acts like a huge ceramic layer between the wall and earth. The challenge with this method is, if the wall or foundation shifts in any way, it cracks and water is able to penetrate through it easily.\nBituthene (Registered name) is very similar to the three coat layering process only in one step. It comes already layered in sheets and has a self-adhesive backing. The challenge with this is the same as with the manual layering method, in addition it is sun sensitive and must be covered very soon after application.\nEco-Flex is an environmentally friendly waterproofing membrane that seems to work very well on foundations, but not much is known about its effectiveness in earth sheltering. It is among a group of liquid paint-on waterproofing products. The main challenges with these are they must be carefully applied, making sure that every area is covered to the right thickness, and that every crack or gap is tightly sealed.\nBentonite clay is the alternative that is closest to optimum on the environmental scale. It is naturally occurring and self-healing. The drawback to this system is that it is very heavy and difficult for the owner/builder to install and subject to termite damage.\nBi-membranes have been used extensively throughout Australia where 2 membranes are paired together\u2014typically 2 coats of water based epoxy as a 'sealer' and stop the internal vapor pressure of the moist concrete exploding bubbles of vapor up underneath the membrane when exposed to hot sun. The bond strength of epoxy to concrete is stronger than the internal bond strength of concrete so the membranes won't 'blow' off the wall in the sun. Epoxies are very brittle so they are paired up with an overcoat of high-build flexible water based acrylic membrane in multiple coats of different colors to ensure film coverage\u2014this is reinforced with non-woven polyproplene textile in corners and changes in direction.\n\n\n==== Insulation ====\nUnlike conventional building, earth shelters require the insulation on the exterior of the building rather than inside the wall. One reason for this is that it provides protection for the waterproof membrane against freeze damage, another is that the earth shelter is able to better retain its desired temperature. There are two types of insulation used in earth shelter construction. The first is close-celled extruded polystyrene sheets. Two to three inches glued to the outside of the waterproofing is generally sufficient. The second type of insulation is a spray on foam. This works very well where the shape of the structure is unconventional, rounded or difficult to get to. Foam insulation requires an additional protective top coat such as foil to help it resist water penetration.\nIn some low budget earth shelters, insulation may not be applied to the walls. These methods rely on the U factor or thermal heat storage capacity of the earth itself below the frost layer. These designs are the exception however and risk frost heave damage in colder climates. The theory behind no insulation designs relies on using the thermal mass of the earth to store heat, rather than relying on a heavy masonry or cement inner structures that exist in a typical passive solar house. This is the exception to the rule and cold temperatures may extend down into the earth above the frost line making insulation necessary for higher efficiencies.\n\n\n== Design for energy conservation ==\nEarth sheltered homes are often constructed with energy conservation and savings in mind. Specific designs of earth shelters allow for maximum savings. For bermed or in-hill construction, a common plan is to place all the living spaces on the side of the house facing the equator. This provides maximum solar radiation to bedrooms, living rooms, and kitchen spaces. Rooms that do not require natural daylight and extensive heating such as the bathroom, storage and utility room are typically located on the opposite (or in hill) side of the shelter. This type of layout can also be transposed to a double level house design with both levels completely underground. This plan has the highest energy efficiency of earth sheltered homes because of the compact configuration as well as the structure being submerged deeper in the earth. This provides it with a greater ratio of earth cover to exposed wall than a one story shelter would.\nWith an atrium earth shelter the living spaces are concentrated around the atrium. The atrium arrangement provides a much less compact plan than that of the one or two story bermed/inhill design; therefore it is commonly less energy efficient, in terms of heating needs. This is one of the reasons why atrium designs are classically applied to warmer climates. However, the atrium does tend to trap air within it which is then heated by the sun and helps reduce heat loss.\n\n\n== Earth sheltering with solar heating ==\nEarth sheltering is often combined with solar heating systems. Most commonly, the utilization of passive solar design techniques is used in earth shelters. In the Northern Hemisphere, a south facing structure with the north, east, and west sides covered with earth, is the most effective application for passive solar systems. A large double glazed window, triple glazed or Zomeworks beadwall (vacuum/blower pumps that filled your double pane solar windows with styrofoam balls at night for extra insulation and vacuumed the beads out in the morning, patent now expired. This changes a window from an R3 thermal resistance to an R16 to R32(depending on thickness of styrofoam bead wall)), spanning most of the length of the south wall is critical for solar heat gain. It is helpful to accompany the window with insulated drapes to protect against heat loss at night. Also, during the summer months, providing an overhang, or some sort of shading device, is useful to block out excess solar gain. Combining solar heating with earth sheltering is referred to as \"annualized geo solar design\", \"Passive annual heat storage\", or sometimes as an \"Umbrella house.\" (See Nick Pine's posting on usenet alt.homepower and alt.solar.thermal groups about this type of house.) In the umbrella house, Polystyrene insulation extends around 23 feet (7.0 m) radius from underground walls. A plastic film covers the insulation (for waterproofing), and soil is layer on top. The materials slope downward, like an umbrella. It sheds excess water while keeping the soil temperature warm and dry.\nPassive cooling which pulls air with a fan or convection from a near constant temperature air into buried Earth cooling tubes and then into the house living space. This also provides fresh air to occupants and the air exchange required by ASHRAE.\n\n\n== Earth shelter construction: history and examples ==\n\n\n=== Berming ===\n\nHistorically, earth berming was a common building practice that combined heavy timber framing and rough stone work with stacking thick layers of sod or peat against the walls and on the roof. This served as excellent protection from the elements. In a relatively short period of time the earth layers grow together leaving the structure with an appearance of a hill with a door.\nIn these early structures, the heavy timber framing acted as structural support and added comfort and warmth to the interior. Rough stone was often stacked along the outer walls with a simple lime mortar for structural support and often serves as an exterior facing wall and foundation. There is a greater use of stone work in earth shelter structures in areas where timber is scarce. These are the most sustainable of the earth shelters as far as materials go because they are able to decompose and return to earth. This is why there are few remaining example like Hvalsey Church in Greenland where only the stacked stones remain. One of the oldest examples of berming, dating back some 5,000 years, can be found at Skara Brae in the Orkney Islands off northern Scotland.\nToday\u2019s bermed earth structures are built quite differently from those of the past. Common construction employs large amounts of steel reinforced concrete acting as structural support and building shell. Bulldozers or bobcats are used to pile earth around the building and on the roof instead of stacking earth in place. One modern example of bermed earth structures is the Hockerton Housing Project, a community of 5 homes in Nottinghamshire, England.\n\n\n=== In-hill ===\nOne historical example of in-hill earth shelters would be Mesa Verde, in the southwest United States. These building are constructed directly onto the ledges and caves on the face of the cliffs. The front wall is built up with local stone and earth to enclose the structure. Similarly today, in-hill earth shelter construction utilizes the natural formation of a hillside for two to three of the exterior walls and sometimes the roof of a structure. Alternative builders craft a type of in-hill structure known as an Earthship. In Earthship construction, tires rammed with earth are used as structural materials for three of the walls and generally have a front fa\u00e7ade of windows to capture passive solar energy.\nA well-known example of an earth-sheltered home is the residence of Bill Gates, who had it built over a period of several years on a heavily wooded site on the shore of Lake Washington, USA. It is an excellent example of the lack of obtrusiveness of this kind of home, since it appears much smaller than it actually is, when seen from the lake.\n\n\n=== Underground ===\nThough underground construction is relatively uncommon in the US (except for basements where only 1 floor is underground), successful examples can be found in Australia where the ground is so hard that there is little to no need for structural supports and a pick ax and shovel are the tools of the builder/remodeler. See Coober Pedy and Lightning Ridge. The Forestiere Underground Gardens in Fresno, California is a North American example.\nIn the early 1970s, China undertook the construction of Dixia Cheng, a city underneath Beijing. It was primarily a complex of bomb shelters that could house 40% of the population at that time. It was a response to the fear of Soviet attack. Parts of it are now used in more commercial ventures.\n\n\n== Gallery ==\n\n\n== See also ==\n\nTopics:\nEarth house\nEarth structure\nGreen building\nUnderground home\nUnderground living\nTypes:\n\nApplications:\n\nProponents:\nBill Gates's house, a very large earth-sheltered home\nCoober Pedy, an Australian opal mining town famous for its underground buildings\nCosanti\u2014site of \"Earth House\" designed by architect Paolo Soleri\nEarl Young (architect)\u2014works commonly referred to as gnome homes, mushroom houses, or Hobbit houses\nMalcolm Wells, proponent of earth-sheltered building\nDavid Baggs, proponent of earth-sheltered building, author and architect of over 40 earth covered buildings in Australia\n\n\n== Notes ==\n\n\n== References ==\nBaggs, Sydney A., Baggs, Joan C. & Baggs, David W., Australian Earth-Covered Building New South Wales University Press, NSW Aus, 1991 ISBN 0-86840-060-2\nBaggs, Sydney A., Baggs, Joan C. & Baggs, David W., Australian Earth-Covered Building IP Publishers, QLD, Aus, 2005 ISBN 978-0-9756807-1-1 can be accessed online at [1]\nBerge, Bjorn. The Ecology of Building Materials. Architectural Press, 2000. This book includes detailed information about building materials.\nCampbell, Stu. The Underground House Book. Vermont: Garden Way, Inc., 1980.\nDe Mars, John. Hydrophobic Concrete Sheds Waterproofing Membrane' Concrete Products, January 2006. Concrete industry magazine it can be accessed online at [2].\nDebord, David Douglas, and Thomas R. Dunbar. Earth Sheltered Landscapes. New York: Wan Nostrand Reinhold Company, 1985.\nEdelhart, Mike. The Handbook of Earth Shelter Design. Dolphin Books, 1982. This has in depth information about earth shelter construction with many illustrations.\nMiller, David E. Toward a New Regionalism. University of Washington Press, 2005. It includes examples and information of sustainable building including earth shelters.\nReid, Esmond. Understanding Buildings. The MIT Press, 1984. This book includes detailed construction and building information.\nRoy, Robert. Earth Sheltered Houses. New Society Publishers, 2006. This book is an up to date guide of the owner builder. It features much of the information that is in his earlier book.\nRoy, Robert. Underground Houses: How to Build a Low-Cost Home. New York: Sterling Publishing Co. Inc., 1979.\nTerman, Max R. Earth Sheltered Housing: Principles in Practice. New York: Van Norstrand Reinhold Company, 1985.\nThe Underground Space Center University of Minnesota. Earth Sheltered Housing Design. Van Nostrand Reinhold Company, ed. 1978 and ed. 1979. This is an academic look at how to construct an earth shelter building.\nWade, Herb, Jeffrey Cook, Ken Labs, and Steve Selkowitz. Passive Solar: Subdivisions, windows, underground. Kansas City: American Solar Energy Society, 1983.\nOehler, Mike. The $50 & up underground house book. Mole publishing Co, 1978.\n\n\n== External links ==\nHockerton Housing Project - Community of 5 earth sheltered homes near Nottingham, UK\nStocktonUnderground : An Owner-Builder Approach\nEarth-Sheltered Houses\nAn Earth Sheltered Directory", 
                "titleUrl": "https://en.wikipedia.org/wiki/Earth_sheltering", 
                "title": "Earth sheltering"
            }, 
            {
                "snippet": "regarding passive cooling may include: improved passive cooling during warm season (mostly on still, hot days) improved night cooling rates enhanced performance", 
                "pageCategories": "Appropriate technology\nArchitectural elements\nBuilding engineering\nCommons category with local link same as on Wikidata\nConvection\nHeating, ventilating, and air conditioning\nLow-energy building\nSolar-powered devices\nSolar architecture\nSustainable building", 
                "pageContent": "This article refers to a device for ventilation. For the power generation technology, see Solar updraft tower.\nA solar chimney \u2013  often referred to as a thermal chimney \u2013  is a way of improving the natural ventilation of buildings by using convection of air heated by passive solar energy. A simple description of a solar chimney is that of a vertical shaft utilizing solar energy to enhance the natural stack ventilation through a building.\nThe solar chimney has been in use for centuries, particularly in the Middle east and Near East by the Persians, as well as in Europe by the Romans.\n\n\n== Description ==\nIn its simplest form, the solar chimney consists of a black-painted chimney. During the day solar energy heats the chimney and the air within it, creating an updraft of air in the chimney. The suction created at the chimney's base can be used to ventilate and cool the building below. In most parts of the world it is easier to harness wind power for such ventilation as with a windcatcher, but on hot windless days a Solar chimney can provide ventilation where otherwise there would be none.\nThere are however a number of solar chimney variations. The basic design elements of a solar chimney are:\nThe solar collector area: This can be located in the top part of the chimney or can include the entire shaft. The orientation, type of glazing, insulation and thermal properties of this element are crucial for harnessing, retaining and utilizing solar gains\nThe main ventilation shaft: The location, height, cross section and the thermal properties of this structure are also very important.\nThe inlet and outlet air apertures: The sizes, location as well as aerodynamic aspects of these elements are also significant.\nA principle has been proposed for solar power generation, using a large greenhouse at the base rather than relying solely on heating the chimney itself. (For further information on this issue, see Solar updraft tower.)\nSolar chimneys are painted black so that they absorb the sun's heat more effectively. When the air inside the chimney is heated, it rises and pulls cold air out from under the ground via the heat exchange tubes.\n\n\n== Solar chimney and sustainable architecture ==\n\nSolar chimneys, also called heat chimneys or heat stacks, can also be used in architectural settings to decrease the energy used by mechanical systems (systems that heat and cool the building through mechanical means). Air conditioning and mechanical ventilation have been for decades the standard method of environmental control in many building types, especially offices, in developed countries. Pollution and reallocating energy supplies have led to a new environmental approach in building design. Innovative technologies along with bioclimatic principles and traditional design strategies are often combined to create new and potentially successful design solutions. The solar chimney is one of these concepts currently explored by scientists as well as designers, mostly through research and experimentation.\nA solar chimney can serve many purposes. Direct gain warms air inside the chimney causing it to rise out the top and drawing air in from the bottom. This drawing of air can be used to ventilate a home or office, to draw air through a geothermal heat exchange, or to ventilate only a specific area such as a composting toilet.\nNatural ventilation can be created by providing vents in the upper level of a building to allow warm air to rise by convection and escape to the outside. At the same time cooler air can be drawn in through vents at the lower level. Trees may be planted on that side of the building to provide shade for cooler outside air.\nThis natural ventilation process can be augmented by a solar chimney. The chimney has to be higher than the roof level, and has to be constructed on the wall facing the direction of the sun. Absorption of heat from the sun can be increased by using a glazed surface on the side facing the sun. Heat absorbing material can be used on the opposing side. The size of the heat-absorbing surface is more important than the diameter of the chimney. A large surface area allows for more effective heat exchange with the air necessary for heating by solar radiation. Heating of the air within the chimney will enhance convection, and hence airflow through the chimney. Openings of the vents in the chimney should face away from the direction of the prevailing wind.\nTo further maximize the cooling effect, the incoming air may be led through underground ducts before it is allowed to enter the building. The solar chimney can be improved by integrating it with a trombe wall. The added advantage of this design is that the system may be reversed during the cold season, providing solar heating instead.\nA variation of the solar chimney concept is the solar attic. In a hot sunny climate the attic space is often blazingly hot in the summer. In a conventional building this presents a problem as it leads to the need for increased air conditioning. By integrating the attic space with a solar chimney, the hot air in the attic can be put to work. It can help the convection in the chimney, improving ventilation.\nThe use of a solar chimney may benefit natural ventilation and passive cooling strategies of buildings thus help reduce energy use, CO2 emissions and pollution in general. Potential benefits regarding natural ventilation and use of solar chimneys are:\n\nimproved ventilation rates on still, hot days\nreduced reliance on wind and wind driven ventilation\nimproved control of air flow though a building\ngreater choice of air intake (i.e. leeward side of building)\nimproved air quality and reduced noise levels in urban areas\nincreased night time ventilation rates\nventilation of narrow, small spaces with minimal exposure to external elements\nPotential benefits regarding passive cooling may include:\nimproved passive cooling during warm season (mostly on still, hot days)\nimproved night cooling rates\nenhanced performance of thermal mass (cooling, cool storage)\nimproved thermal comfort (improved air flow control, reduced draughts)\n\n\n== Precedent Study: The Environmental Building ==\nThe Building Research Establishment (BRE) office building in Garston, Watford, United Kingdom, incorporates solar assisted passive ventilation stacks as part of its ventilation strategy.\nDesigned by architects Feilden Clegg Bradley, the BRE offices aim to reduce energy consumption and CO2 emissions by 30% from current best practice guidelines and sustain comfortable environmental conditions without the use of air conditioning. The passive ventilation stacks, solar shading, and hollow concrete slabs with embedded under floor cooling are key features of this building. Ventilation and heating systems are controlled by the building management system (BMS) while a degree of user override is provided to adjust conditions to occupants' needs.\nThe building utilizes five vertical shafts as an integral part of the ventilation and cooling strategy. The main components of these stacks are a south facing glass-block wall, thermal mass walls and stainless steel round exhausts rising a few meters above roof level. The chimneys are connected to the curved hollow concrete floor slabs which are cooled via night ventilation. Pipes embedded in the floor can provide additional cooling utilizing groundwater.\nOn warm windy days air is drawn in through passages in the curved hollow concrete floor slabs. Stack ventilation naturally rising out through the stainless steel chimneys enhances the air flow through the building. The movement of air across the chimney tops enhances the stack effect. During warm, still days, the building relies mostly on the stack effect while air is taken from the shady north side of the building. Low-energy fans in the tops of the stacks can also be used to improve airflow.\nOvernight, control systems enable ventilation paths through the hollow concrete slab removing the heat stored during the day, which then remains cold for the following day. The exposed curved ceiling gives more surface area than a flat ceiling would, acting as a heat sink, again providing summer cooling. Research based on actual performance measurements of the passive stacks found that they enhanced the cooling ventilation of the space during warm and still days and may also have the potential to assist night-time cooling due to their thermally massive structure.\n\n\n== Passive down-draft cool tower ==\n\nA technology closely related to the solar chimney is the evaporative down-draft cooltower. In areas with a hot, arid climate this approach may contribute to a sustainable way to provide air conditioning for buildings.\nEvaporation of moisture from the pads on top of the Toguna buildings built by the Dogon people of Mali, Africa contribute to the coolness felt by the men who rest underneath. The women's buildings on the outskirts of town are functional as more conventional solar chimneys.\nThe principle is to allow water to evaporate at the top of a tower, either by using evaporative cooling pads or by spraying water. Evaporation cools the incoming air, causing a downdraft of cool air that will bring down the temperature inside the building. Airflow can be increased by using a solar chimney on the opposite side of the building to help in venting hot air to the outside. This concept has been used for the Visitor Center of Zion National Park. The Visitor Center was designed by the High Performance Buildings Research of the National Renewable Energy Laboratory (NREL).\nThe principle of the downdraft cooltower has been proposed for solar power generation as well. (See Energy tower for more information.)\n\n\n== See also ==\n\n\n== References ==\n\n\n== Sources ==\n\n\n== External links ==\nSolar Innovation Ideas \u2013 Victorian Solar Innovation Initiative \"Ventilation Systems \u2013 Solar chimney\" (PDF). Sustainability Victoria. Retrieved 2007-03-10. \nArchitectural Environmental Analysis \u2013 A guide to environmental design \"Ventilation\". EcoResearch. Retrieved 2007-03-10. \nSourcebook Passive Solar Design \"Passive Solar Guidelines\". Sustainable Sources. Retrieved 2007-03-10. \nPassive Solar Heating & Cooling Manual \"Natural Cooling\". Arizona Solar Center. Retrieved 2007-03-10. \n\"DOE: High Performance Zion Visitor Center \u2013 Cool tower\". U.S. Department of Energy. 2006-08-30. Retrieved 2007-03-10. \nSustainability at SCU \u2013 Tour Sustainable Features \"The Commons on Kennedy Mall\". Santa Clara University. Retrieved 2007-03-10.  \u2013 includes simple description and graphic of solar chimney used in a \"Green Demonstration Building\"\nSpanish pavilion: ceramic pillars for cooling", 
                "titleUrl": "https://en.wikipedia.org/wiki/Solar_chimney", 
                "title": "Solar chimney"
            }, 
            {
                "snippet": "between the two building envelopes and through a sub-floor or via earth cooling tubes. A recirculating air flow path resulted from the warm (less-dense) air", 
                "pageCategories": "All articles that may contain original research\nAll articles with unsourced statements\nArticles that may contain original research from January 2008\nArticles with unsourced statements from January 2008\nLow-energy building", 
                "pageContent": "A double envelope house is a passive solar house design which collects solar energy in a solarium and passively allows the warm air to circulate around the house between two sets of walls, a double building envelope. This design is from 1975 by Lee Porter Butler in the United States.\n\n\n== History ==\nLee Porter Butler's 1975 Double Envelope (Shell) design  received wide publicity after the U.S. solar energy tax credits were created in 1978. Versions were on the cover of Better Homes and Gardens and Popular Science  magazines.\nButler was an artistic/ecological building designer, a self-proclaimed \"Ekotect.\"  He did not hold formal qualifications as an energy engineer. Lee had built hundreds of homes, shopping centers and business buildings including banks and schools by the time he entered North Carolina State University to study architecture. He had studied engineering at Georgia Institute of Technology. Lee did not even have a high school diploma, but ended up teaching his invention of \"the gravity geo-thermal envelope\" at The University of California Berkeley in the Graduate School of design and Planning. Lee went to school for the education, not for the diploma. He was asked to speak at The Royal College of Science in London, where Sir Isaac Newton delivered Principia.\nLee's facile and curious mind led him his entire life to question methods by which humanity could live on the earth in harmony with the earth, and with each other. His most recent contribution to humanity \"Ekotecture\" is a totally self-sufficient way to live in harmony on the planet and to safeguard human life no matter what the external conditions. Lee felt that if we talk about sustainability we must talk about not just sustaining the earth, but the sustaining of human life. His contributions include a method whereby humanity can live peaceably and safely on the planet in cooperation.\n\n\n== Design and theory ==\nButler's experimental design was a form of isolated passive solar design that incorporated a passive heat distribution system. It attempted to address the problem of unequal distribution of heat that was associated with some direct gain systems. [1]. This phenomenon is observed particularly in designs with inadequate thermal mass, poor cross ventilation and excessive polar facing windows.\nButler's design essentially composed of a house within a house. Thermal energy was captured from a south-facing solarium and heat was circulated by a natural convection flow loop in the cavity between the two building envelopes and through a sub-floor or via earth cooling tubes.\nA recirculating air flow path resulted from the warm (less-dense) air rising in a south-side solarium, and cooler (denser) air falling on the north side to create pressure differentials that automatically moved excess solar thermal gain from the south to the north side of the building without forced convection systems. Air flow was proportional to the differences in temperature between the two convection paths.\nIn the summer, shading devices eliminate all direct solar gain. Vents are opened at the top to exhaust hot air. Fresh air intake uses ambient temperature Earth to cool and dehumidify replacement air at the base.\nIn winter, the air in the cavity is buffered by warm ambient-temperature Earth under the floor (which is partially recharged by the natural convection flow loop during each winter day). In the summer, the convective flow is replaced with cooler near-ambient-temperature Earth replacement air, and the warm air exhausts by natural convection.\n\n\n== Criticisms ==\nThe original explanation provided for its efficiency was the thermal buffer that existed in the double envelope cavity. However, observers have also commented that the overall insulation of the design is higher with two walls instead of just one.\nWhile the design can perform better than a conventional home, formal performance monitoring suggested there were some problems with the original design. [2].\nCommentators have criticised the design on various grounds:\nThe rock bed under the house interfered with the geothermal night time effect and slowed warming in the morning.\nRoof-angled glass (See sun path) made it difficult to control summer heat gain and winter heat loss.\nAn equator-side porch excessively shaded the lower floor from solar gain\nThe additional cavity on the north wall was excluded from the conditioned area (and created a potential fire hazard).\nDark roofs and absent radiant barrier was not adequate to inhibit summer solar gain\nThe presence of a fireplace, chimney and clothes dryer were a source of convective heat loss\nThe design was not adequate to function in polar climates.\nConcerns about condensation and moisture issues forming in the convection loop leading to maintenance or health issues.\nThe additional materials and thermal mass used in construction\nWarm air rises by natural convection. It does not go down and around. They are fire traps and are against building codes in places like Aurora CO.\nSubsequent modifications have attempted to address these issues.[3]\n\n\n== Thermal buffer zone house ==\nA modification based on Butler's original design has been described but not yet formally evaluated. It attempted to eliminate some of the initial issues.\nAn important difference is the northern limb of the convection loop (thermal buffer zone/TBZ) is employed as a utility area e.g. laundry room, closets, pantry, and storage space. The laundry room also doubled as an area for clothes drying.\nAn external window is located on the north side which can be opened to admit mild air into the TBZ during summer. An additional internal window separates the south side living quarters. This can be opened to admit warm winter air from the solarium to enter directly into northern rooms.\nThe designer states that on cold winter days, the TBZ tempered with solar-heated air could be often above 85 degrees F, while the outside air was below freezing.\n\n\n== Performance evaluation ==\nCurrent technology makes it difficult to model the effects of these convective flow designs because computer performance simulation software does not presently incorporate these processes in their calculations.\n\n\n== See also ==\nList of low-energy building techniques\nPassive solar design\nHistory of passive solar building design\nDouble-skin facade\nPassive solar design concepts\nTrombe wall\nBarra system\nPassive House\nEarth cooling tubes\nWindcatcher\nAnnualized geothermal solar\nEarth sheltering\nGeothermal exchange heat pump\nSolar-designers\nCategory:Solar building designers\nList of pioneering solar buildings\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Double_envelope_house", 
                "title": "Double envelope house"
            }
        ], 
        "phraseCharStart": "460"
    }, 
    {
        "phraseCharEnd": "618", 
        "phraseIndex": "T11", 
        "phraseGoldStandardTag": "Task", 
        "phrase": "treatment of ITER carbon co-deposits", 
        "wikiSearchResults": [], 
        "phraseCharStart": "582"
    }, 
    {
        "phraseCharEnd": "693", 
        "phraseIndex": "T12", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "divertor", 
        "wikiSearchResults": [
            {
                "snippet": "divertor was proposed as a solution to this problem. Operating on the same principle as a mass spectrometer, the plasma passes through the divertor region", 
                "pageCategories": "All stub articles\nEnergy stubs\nFusion power", 
                "pageContent": "In nuclear fusion power research, a divertor is a device within a tokamak that allows the online removal of waste material from the plasma while the reactor is still operating. This allows control over the buildup of fusion products in the fuel, and removes impurities in the plasma that have entered into it from the vessel lining.\nThe divertor was initially introduced during the earliest studies of fusion power systems in the 1950s. It was realized early on that successful fusion would result in heavier ions being created and left in the fuel (the so-called \"fusion ash\"). These impurities were responsible for the loss of heat, and caused other effects that made it more difficult to keep the reaction going. The divertor was proposed as a solution to this problem. Operating on the same principle as a mass spectrometer, the plasma passes through the divertor region where heavier ions are flung out of the fuel mass by centrifugal force, colliding with some sort of absorber material, and depositing its energy as heat. Initially considered to be a device required for operational reactors, few early designs included a divertor.\nWhen early long-shot reactors started to appear in the 1970s, a serious practical problem emerged. No matter how tightly constrained, plasma continued to leak out of the main confinement area, striking the walls of the reactor core and causing all sorts of problems. A major concern was sputtering in reactors with higher power and particle flux density, which caused ions of the vacuum chamber's metal walls to flow into the fuel and to cool it.\nDuring the 1980s it became common for reactors to include a feature known as the limiter, which is a small piece of material that projects a short distance into the outer edge of the main plasma confinement area. Ions from the fuel that are travelling outwards strike the limiter, thereby protecting the walls of the chamber from this damage. However, the problems with material being deposited into the fuel remained; the limiter simply changed where that material was coming from.\nThis led to the re-emergence of the divertor, as a device for protecting the reactor itself. In these designs, magnets pull the lower edge of the plasma to create a small region where the outer edge of the plasma, the \"Scrape-Off Layer\" (SOL), hits a limiter-like plate. The divertor improves on the limiter in several ways, but mainly because modern reactors try to create plasmas with D-shaped cross-sections (\"elongation\" and \"triangularity\") so the lower edge of the D is a natural location for the divertor. In modern examples the plates are replaced by lithium metal, which better captures the ions and causes less cooling when it enters the plasma. \nIn ITER and the latest configuration of Joint European Torus, the lowest region of the torus is configured as a divertor, while Alcator C-Mod was built with divertor channels at both top and bottom.\nA tokamak featuring a divertor is known as a divertor tokamak or divertor configuration tokamak. In this configuration, the particles escape through a magnetic \"gap\" (separatrix), which allows the energy absorbing part of the divertor to be placed outside the plasma. The divertor configuration also makes it easier to obtain a more stable \"H-mode\" of operation. The plasma facing material in the divertor faces significantly different stresses compared to the majority of the first wall.\n\n\n== See also ==\nNuclear fission\n\n\n== References ==\n\n\n== Further reading ==\nSnowflake and the multiple divertor concepts. March 2016\n\n\n== External links ==\nLimiters\nDivertors", 
                "titleUrl": "https://en.wikipedia.org/wiki/Divertor", 
                "title": "Divertor"
            }, 
            {
                "snippet": "The ULS (UMIST Linear System) is a gas target divertor simulator located on the former UMIST campus of the University of Manchester. It enables physicists", 
                "pageCategories": "Plasma physics\nUniversity of Manchester", 
                "pageContent": "The ULS (UMIST Linear System) is a gas target divertor simulator located on the former UMIST campus of the University of Manchester. It enables physicists to study the recombination processes of a detached plasma in a hydrogen target chamber.\nResearch on detached plasma and on its recombination modes is of primary importance in order to design an appropriate divertor region in a future nuclear fusion power plant, where huge amounts of energy will be deposited by the fast-moving particles generated in the main reactor. The major goal of the ULS as for many other linear divertor simulators, is to reproduce the same temperature and density conditions of the SOL (Scrape Off Layer) of a tokamak in a linear environment and therefore to make easier the study of its properties.\nIn the past few years, ULS has been used with great insight to analyze the molecular activation and the electron-ion recombination modes, and to determine the conditions for their activation: diffusive processes have also been considered. However, research on these subjects is still ongoing and our understanding of the elementary processes involved in a detached plasma is still far from being satisfactory.\n\n\n== Further reading ==\nKay, Michael J. (1998) A Study of Plasma Attenuation and Recombination in the Gas Target Chamber of a Divertor Simulator. UMIST Ph.D. thesis\nMihailjcic, B.; et al. (2007). \"Spatially resolved spectroscopy of detached recombining plasmas in the University of Manchester Linear System divertor simulator\". Physics of Plasmas. White Rose Research Online. Retrieved 1 October 2010.", 
                "titleUrl": "https://en.wikipedia.org/wiki/UMIST_linear_system", 
                "title": "UMIST linear system"
            }, 
            {
                "snippet": "ASDEX Upgrade (Axially Symmetric Divertor Experiment) is a divertor tokamak, that went into operation at the Max-Planck-Institut f\u00fcr Plasmaphysik, Garching", 
                "pageCategories": "All articles to be expanded\nArticles to be expanded from December 2015\nArticles using small message boxes\nInterlanguage link template link number\nTokamaks\nWikipedia articles with GND identifiers", 
                "pageContent": "ASDEX Upgrade (Axially Symmetric Divertor Experiment) is a divertor tokamak, that went into operation at the Max-Planck-Institut f\u00fcr Plasmaphysik, Garching in 1991. At present, it is Germany's second largest fusion experiment after stellarator Wendelstein 7X.\n\n\n== Overview ==\nTo make experiments under reactor-like conditions possible, essential plasma properties, particularly the plasma density and pressure and the wall load, have been adapted in ASDEX Upgrade to the conditions that will be present in a future fusion power plant.\nASDEX Upgrade is, compared to other international tokamaks, a midsize tokamak experiment. It began operation in 1991 and it succeeds the ASDEX experiment, which was in operation from 1980 until 1990.\nOne innovative feature of the ASDEX Upgrade experiment is its all-tungsten first wall; tungsten is a good choice for the first wall of a tokamak because of its very high melting point (over 3000 degrees Celsius) which enables it to stand up to the very high heat fluxes emanating from the hot plasma at the heart of the tokamak; however there are also problems associated with a tungsten first wall, such as tungsten's tendency to ionise at high temperatures, \"polluting\" the plasma and diluting the deuterium-tritium fuel mix. Furthermore, as a high Z material, radiation from fully ionized tungsten in the plasma is several orders of magnitude higher than that of other proposed first wall components such as carbon fibre composites (CFCs) or beryllium. This result allows for far less tungsten to \"contaminate\" a proposed break-even plasma. ASDEX Upgrade will examine ways to overcome this problem, in preparation for the construction of ITER's first wall.\nThe experiment has an overall radius of 5 metres, a gross weight of 800 metric tons, a maximum magnetic field strength of 3.1 tesla, a maximum plasma current of 1.6 megaampere and maximum heating power of up to 27 megawatt. Plasma heating and current drive in the ASDEX Upgrade is derived from several sources, namely 1 MW of ohmic heating, 20 MW of neutral beam injection, 6 MW via ion cyclotron resonance heating (ICRH) at frequencies between 30 and 120 megahertz, and 2 x 2 MW of electron cyclotron resonance heating (ECRH) at 140 gigahertz. It has 16 toroidal field coils and 12 poloidal field coils.\nThree large flywheel generators feed the 580 MVA pulsed power supply system for the magnetic confinement and plasma heating.\n\n\n== ASDEX ==\nThe original configuration was ? It went into operation in 1980. It shutdown in 1990 for a major upgrade.\n\n\n== See also ==\nList of fusion experiments\nEdge-localized mode\nBall-pen probe\n\n\n== External links ==\nASDEX Upgrade homepage\nASDEX Upgrade technical specs\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/ASDEX_Upgrade", 
                "title": "ASDEX Upgrade"
            }, 
            {
                "snippet": "system A mechanism for magnetically limiting a plasma, and hence for controlling the nuclear fusion in a tokamak; see divertor  separator (disambiguation)", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Separatrix", 
                "title": "Separatrix"
            }, 
            {
                "snippet": "Prior to October 1995, START had no rapid terminations. In October 1995, divertor coils were installed and images showed the plasma would interact with the", 
                "pageCategories": "All stub articles\nInterlanguage link template link number\nNuclear and atomic physics stubs\nTokamaks", 
                "pageContent": "The Small Tight Aspect Ratio Tokamak, or START was a nuclear fusion experiment that used magnetic confinement to hold plasma. START was the first full-sized machine to use the spherical tokamak design, which aimed to greatly reduce the aspect ratio of the traditional tokamak design.\nThe experiment began at the Culham Science Centre in the United Kingdom in 1990 and was retired in 1998. It was built as a low cost design, largely using parts already available to the START team. The START experiment revolutionized the tokamak by changing the previous toroidal shape into a tighter, almost spherical, doughnut shape. The new shape increased efficiency by reducing the cost over the conventional design, while the field required to maintain a stable plasma was a factor of 10 less.\nThe main components that comprised START included the support structure, pulse transformer, vacuum tank, toroidal and poloidal field coils, and a limiter. The support structure positioned and supported the vacuum tank which also shared the same spherical center as the large pulse transformer. The main role of the pulse transformer was to provide the current for the toroidal field coils which was supplied through fifteen irons cores that were spirally wound from a .03 millimeter iron strip. The toroidal field coil was a central conductor made of copper on the axis of the vacuum tank, and was attached to the vacuum tank through copper limbs covered by insulated clamps. START had six poloidal field coils within the vacuum tank and were encased in 3 millimeter stainless steel cases. The poloidal coils were supported from the base of the tank and each could be adjusted as necessary. The vacuum tank was the primary vessel where experiments take place; it was cylindrical in shape and was divided into three sections. The tank offered numerous ports for the attachment of pumps and diagnostics. A graphite limiter was positioned around the central stainless steel tube and this provided a simple way to measure the innermost edge of the plasma during experiments.\nIn order to successfully heat experiments in a spherical tokamak, physicists performed neutral beam injection. This involved interjecting hydrogen into hydrogen or deuterium plasmas, providing effective heating of both ions and electrons. Although the atoms were injected with no net electrostatic charge, as the beam passed through the plasma, the atoms were ionized as they bounced off the ions already in the plasma. Consequently, because the magnetic field inside the torus was circular, these fast ions were confined to the background plasma. The background plasma slowed down the confined fast ions, in a similar way to how air resistance slows down a baseball. The energy transfer from the fast ions to the plasma increased the overall plasma temperature. The neutral beam injector used in START was on loan from Oak Ridge National Laboratory.\nThe magneto-hydro-dynamic limit (MHD) was an operational limit of tokamaks, with START being no exception. The START team would test the MHD using forty-six sets of Mirnov coils at different heights on the center column of START. Plasmas being formed by compression within START limited the fluctuation of the MHD.\nPrior to October 1995, START had no rapid terminations. In October 1995, divertor coils were installed and images showed the plasma would interact with the coils before disruptions occurred. These suspicions were further strengthened when the divertor coils were moved closer to the plasma in December 1996, which resulted in a higher frequency of disruptions.\nThe characteristics of plasma within START were also measured. Typical plasma within START had an aspect ratio A=1.3, elongation k=1.8, and a temperature of 400 eV.\nA number of experiments reached 32 percent beta with START, where the previous world record for beta in a tokamak was 12.6 percent. Factors that contributed to the significantly higher beta number include better vacuum conditions, a more powerful neutral beam injection, a lower toroidal field, a higher plasma pressure, and a lower magnetic pressure.\nIn March 1998, the START experiment finished and has since been disassembled and transferred to the ENEA research laboratory at Frascati, Italy. The START team began the Mega Ampere Spherical Tokamak Experiment or MAST in 1999 which operated in the Culham Science Centre, UK until 2013.\n\n\n== References ==\n\n\n== External links ==\nMAST (START follow on experiment)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Small_Tight_Aspect_Ratio_Tokamak", 
                "title": "Small Tight Aspect Ratio Tokamak"
            }, 
            {
                "snippet": "Espa\u00f1a ASD Simplified Technical English ASDEX Upgrade (Axially Symmetric Divertor Experiment) Alliance of Socialists and Democrats for Europe  ASD (disambiguation)", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/ASDE", 
                "title": "ASDE"
            }, 
            {
                "snippet": "specialized tiles and active cooling. The advanced, so-called double null divertor plasma configuration has to be maintained through efficient feedback control", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from December 2015\nAtomic and nuclear energy research in India\nFusion Research\nInterlanguage link template link number\nNuclear technology in India\nPlasma physics\nTokamaks", 
                "pageContent": "SST-1 (steady state superconducting tokamak) is a plasma confinement experimental device in the Institute for Plasma Research (IPR), an autonomous research institute under Department of Atomic Energy, India. It belongs to a new generation of tokamaks with the major objective being steady state operation of an advanced configuration ('D' Shaped) plasma. It has been designed as a medium-sized tokamak with superconducting magnets.\nThe SST-1 project will increase India's stronghold in a selected group of countries who are capable of conceptualizing and making a fully functional fusion based reactor device. The SST-1 System is housed in Institute for Plasma Research, Gandhinagar. The SST-1 mission has been chaired by eminent Indian plasma physicists like Prof. Y.C. Saxena, Dr. Chenna Reddy, and is headed by Dr. Subrata Pradhan.\nNext stage of the SST-1 mission, the SST-2, dubbed as 'DEMO', has already been initiated.\n\n\n== History ==\nThe first talks about SST Mission started in 1994. The technical details and mechanical drawings of the system were finalized in 2001. The machine was fabricated by 2005. Godrej-Boyce Pvt. Ltd. played a crucial role in fabrication of the SST-1 coils. The assembly of SST-1 convinced the top brass of Indian bureaucracy to give a green flag to the claim of Indian physicists to join the ITER program [See Info Box]. On 17 August 2005, PM Sayeed, then India's power minister informed the Rajya Sabha about India's claim to join ITER.  A team from ITER, France visited the SST-1 mission control housed in Institute for Plasma Research to see the advances Indian scientists had made. Finally on 6 December 2005, India was officially accepted as a full partner of the ITER project.  To improve and modify some of the components, the SST-1 machine was subsequently disassembled. The improved version of the machine was completely assembled by January 2012.\nIt was fully commissioned in 2013. And by 2015, produces repeatable plasma discharges up to ~ 500 ms with plasma currents in excess of 75000 A at a central field of 1.5 T. \"SST-1 is also the only tokamak in the world with superconducting toroidal field magnets operating in two-phase helium instead of supercritical helium in a cryo-stable manner, thereby demonstrating reduced cold helium consumption. \"\nAs of Dec 2015 it is having upgrades including to the plasma facing components to allow longer pulses.\n\n\n== Objectives ==\nTraditionally the tokamaks have operated with a `transformer' action- with plasma acting as a secondary, thus having the vital `self-generated' magnetic field on top of the `externally generated' (toroidal and equilibrium) fields. This is a pretty good scheme in which creation, current-drive and heating are neatly integrated and remained a choice of the fusion community for many years until the stage came to heat the plasma to multi-keV temperatures. Heating was then accomplished separately by radio frequency (RF) waves and/or energetic neutral beam injection (NBI).\nSubsequently, excellent control got established on tokamak plasma performance by controlling the plasma-wall interaction processes at the plasma boundary so the plasma duration was limited primarily by the `transformer pulse length'. However, for relevance to future power reactors it is essential to operate these devices in a steady state mode. The very idea of steady state operation presents a series of physics and technology challenges. For example, the excellent plasma performance which was accomplished earlier, was with the surrounding material wall acting as a good 'pump' of particles, a fact which may not be true in steady state.\nSo one has to try and accomplish an equally good performance in presence of a possibly `saturated' wall. Secondly, a host of engineering and technical considerations spring up. The magnets must be superconducting type, otherwise the power dissipation in conventional (resistive) types can reach uneconomical levels. They have to be specially designed to remain superconducting in spite of their proximity to the other `warm' objects (like vacuum vessel etc.). The heat and particle exhaust must be handled in steady state with specialized tiles and active cooling. The advanced, so-called double null divertor plasma configuration has to be maintained through efficient feedback control avoiding plasma disruptions over long discharge durations.\n\n\n== Tokamak parameters ==\n\n\n== Plasma diagnostics on SST-1 ==\nSST-1 will feature many new plasma diagnostic devices, many of which are being used for the first time in fusion research in India. Some of the novel plasma diagnostics devices incorporated in SST-1 are:\nFast Scanning Langmuir probe system\nGas Puff Imaging Diagnostics\nBolometer for imaging Divertor radiations\nAlmost all of the diagnostic devices installed on SST-1 are indigenous and are designed and developed by Diagnostics Group of Institute for Plasma Research. This group is the only group working on plasma diagnostics and related technologies in Indian Subcontinent.\n\n\n== Key people ==\nDr.Subrata Pradhan - Head\n\n\n== SST-2 ==\nThe next stage of SST mission, the SST-2 fusion reactor, dubbed as 'DEMO' among Indian scientific circles has already been conceived. A group of eminent scientists from Institute for Plasma Research under the leadership of Dr. R. Srinivasan is working towards making of a full-fledged fusion reactor capable of producing electricity. Many new features like D-T plasma, Test Blanket Module, Biological shielding and an improved divertor will be incorporated in SST-2. SST-2 will also be built in the Indian state of Gujarat. The land acquisition and other basic formalities have been completed for the same.\n\n\n=== Similar projects ===\nOther designs of fusion reactor are DEMO, Wendelstein 7-X, NIF, HiPER, JET (precursor to ITER), and MAST.\n\n\n== See also ==\n\nADITYA (tokamak)\nMegaproject\nFusion for Energy\nNuclear power in India\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/SST-1_(tokamak)", 
                "title": "SST-1 (tokamak)"
            }, 
            {
                "snippet": "Hydrogen recombination modes are of vital importance in the development of divertor regions for tokamak reactors. In fact they will provide a good way for", 
                "pageCategories": "All articles lacking sources\nAll stub articles\nArticles lacking sources from October 2008\nPhysics stubs\nPlasma physics", 
                "pageContent": "Plasma recombination is a process by which positive ions of a plasma capture a free (energetic) electron and combine with electrons or negative ions to form new neutral atoms (gas). Recombination is an exothermic reaction, meaning heat releasing.\nRecombination usually take place in the whole volume of a plasma (volume recombination), although in some cases it is confined to some special region of it. Each kind of reaction is called a recombining mode and their individual rates are strongly affected by the properties of the plasma such as its energy (heat), density of each species, pressure and temperature of the surrounding environment. An everyday example of rapid plasma recombination occurs when a fluorescent lamp is switched off. The low-density plasma in the lamp (which generates the light by bombardment of the fluorescent coating on the inside of the glass wall) recombines in a fraction of a second after the plasma-generating electric field is removed by switching off the electric power source.\nHydrogen recombination modes are of vital importance in the development of divertor regions for tokamak reactors. In fact they will provide a good way for extracting the energy produced in the core of the plasma. At the present time, it is believed that the most likely plasma losses observed in the recombining region are due to two different modes: electron ion recombination (EIR) and molecular activated recombination (MAR).", 
                "titleUrl": "https://en.wikipedia.org/wiki/Plasma_recombination", 
                "title": "Plasma recombination"
            }, 
            {
                "snippet": "controlling plasma instabilities through real-time diagnostics Materials for diverters and plasma facing components Operation with \u03b2N = 2 and confinement factor", 
                "pageCategories": "All articles needing coordinates\nAll articles with dead external links\nAll articles with unsourced statements\nAnhui articles missing geocoordinate data\nArticles containing Chinese-language text\nArticles with dead external links from February 2016\nArticles with unsourced statements from February 2016\nBuildings and structures in Hefei\nInterlanguage link template link number\nNuclear energy in China", 
                "pageContent": "The Experimental Advanced Superconducting Tokamak (EAST), internal designation HT-7U) is an experimental superconducting tokamak magnetic fusion energy reactor in Hefei, China. The Hefei-based Institute of Plasma Physics is conducting the experiment for the Chinese Academy of Sciences. It has operated since 2006. It was later put under control of Hefei Institutes of Physical Science.\nIt is the first tokamak to employ superconducting toroidal and poloidal magnets. It aims for plasma pulses of up to 1000 seconds.\n\n\n== History ==\nThe project was proposed in 1996 and approved in 1998. According to a 2003 schedule, buildings and site facilities were to be constructed by 2003. Tokamak assembly was to take place from 2003 through 2005.\nConstruction was completed in March 2006 and on September 28, 2006, \"first plasma\" was achieved.\nThe reactor is an improvement over China's first superconducting tokamak device, dubbed HT-7, built by the Institute of Plasma Physics in partnership with Russia in the early 1990s.\nAccording to official reports, the project's budget is CNY \u00a5300 million (approx. USD $37 million), some 1/15 to 1/20 the cost of a comparable reactor built in other countries.\n\n\n== Operations and results ==\nOn September 28, 2006, \"first plasma\" was achieved.\nBy Jan 2007 \"the reactor created a plasma lasting nearly five seconds and generating an electrical current of 500 kilo amperes\".\nBy May 2015 it was reporting 1 MA currents, and H-mode for 6.4 seconds.\nIn February 2016, a plasma pulse was maintained for a record 102 seconds at approximately 50 million Kelvin. Plasma current of 400kA and a density of about 2.4 x 1019/m3 with slowly increasing temperature.\n\n\n== Physics objectives ==\nChina is a member of the ITER consortium, and EAST is a testbed for ITER technologies.\nEAST was designed to test:\nSuperconducting Niobium-titanium poloidal field magnets, making it the first tokamak with superconducting toroidal and poloidal magnets\nNon-inductive current drive\nPulses of up to 102 seconds with 0.5 MA plasma current\nSchemes for controlling plasma instabilities through real-time diagnostics\nMaterials for diverters and plasma facing components\nOperation with \u03b2N = 2 and confinement factor H89 > 2\n\n\n== Tokamak parameters ==\n\n\n== References ==\n\n\n== External links ==\nChinese Academy of Sciences Institute of Plasma Physics - EAST\nPeople's Daily article\nXinhua article Mar 1 2006 - Note that EAST is not the \"world's first experimental nuclear fusion device\".\nXinhua article Mar 24, 2006 Nuke fusion reactor completes test\nMainichi Daily News article Jun 2, 2006", 
                "titleUrl": "https://en.wikipedia.org/wiki/Experimental_Advanced_Superconducting_Tokamak", 
                "title": "Experimental Advanced Superconducting Tokamak"
            }, 
            {
                "snippet": "beam injector. by 2016 it was upgraded/enhanced to run with a 'snowflake' divertor    TCV Auxiliary Heating.  TCV - Development of new plasma shapes. May", 
                "pageCategories": "All articles needing additional references\nArticles needing additional references from March 2016\nFusion power\nInterlanguage link template link number\nNuclear fusion\nNuclear research institutes\nResearch projects\nTokamaks\n\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne", 
                "pageContent": "The Tokamak \u00e0 configuration variable (TCV, literally \"variable configuration tokamak\") is a research fusion reactor of the \u00c9cole polytechnique f\u00e9d\u00e9rale de Lausanne. Its distinguishing feature over other tokamaks is that its torus section is three times higher than wide. This allows studying several shapes of plasmas, which is particularly relevant since the shape of the plasma has links to the performance of the reactor. The TCV was set up in November 1992.\n\n\n== Characteristics ==\nPlasma height: 1.40 metres\nMinor radius: 0.25 metre\nMajor radius: 0.88 metre\nPlasma current: 1.2 megaamperes\nPlasma life span: 2 seconds maximum\nToroidal magnetic field: 1.43 teslas\nAdditional heating power: 4.5 megawatts\n\n\n== Main studies ==\nConfinement studies\nconfinement as a function of the shape of the plasma (triangular, square or elongated)\nImprovement of the confinement of the core\n\nStudies on vertically elongated plasmas\nStudies with ECRH and ECCD (electron cyclotron resonance heating and electron cyclotron current drive)\nBy 2012 it had 16 poloidal plasma shaping coils and could achieve a variety of field configurations and plasma shapes.\n\n\n== History ==\n1976: First proposal for an elongated tokamak by the \"New Swiss Association\"\n1985: Second proposal, with a more elongated tokamak\n1986: Acceptance of the TCV proposal (Tokamak \u00e0 Configuration Variable)\n1992: First plasma discharge\n1997: World record of plasma elongation (see plasma shaping)\nby August 2015 it had had a 19-month shutdown/upgrade to install its first neutral beam injector.\nby 2016 it was upgraded/enhanced to run with a 'snowflake' divertor\n\n\n== References ==\n\n\n== External links ==\nTCV official site\nTCV Technical data as of Oct 2012", 
                "titleUrl": "https://en.wikipedia.org/wiki/Tokamak_\u00e0_configuration_variable", 
                "title": "Tokamak \u00e0 configuration variable"
            }
        ], 
        "phraseCharStart": "685"
    }, 
    {
        "phraseCharEnd": "722", 
        "phraseIndex": "T13", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "main wall", 
        "wikiSearchResults": [
            {
                "snippet": "to articulate an extent of wall, with only an ornamental function. It consists of a flat surface raised from the main wall surface, usually treated as", 
                "pageCategories": "Architectural elements\nColumns and entablature\nCommons category with local link same as on Wikidata", 
                "pageContent": "The pilaster is an architectural element in classical architecture used to give the appearance of a supporting column and to articulate an extent of wall, with only an ornamental function. It consists of a flat surface raised from the main wall surface, usually treated as though it were a column, with a capital at the top, plinth (base) at the bottom, and the various other elements. In contrast to a pilaster, an engaged column or buttress can support the structure of a wall and roof above.\n\n\n== Definition ==\nIn discussing Leon Battista Alberti's use of pilasters, which Alberti reintroduced into wall-architecture, Rudolf Wittkower wrote, \"The pilaster is the logical transformation of the column for the decoration of a wall. It may be defined as a flattened column which has lost its three-dimensional and tactile value.\"\nA pilaster appears with a capital and entablature, also in \"low-relief\" or flattened against the wall.\nPilasters often appear on the sides of a door frame or window opening on the facade of a building, and are sometimes paired with columns or pillars set directly in front of them at some distance away from the wall, which support a roof structure above, such as a portico. These vertical elements can also be used to support a recessed archivolt around a doorway. The pilaster can be replaced by ornamental brackets supporting the entablature or a balcony over a doorway.\nWhen a pilaster appears at the corner intersection of two walls it is known as a canton.\nAs with a column, a pilaster can have a plain or fluted surface to its profile and can be represented in the mode of any architectural style. During the Renaissance and Baroque architects used a range of pilaster forms. In the giant order pilasters appear as two-storeys tall, linking floors in a single unit.\nThe fashion of using this element from Ancient Greek and Roman architecture was adopted in the Italian Renaissance, gained wide popularity with Greek Revival architecture, and continues to be seen in some modern architecture.\n\n\n== See also ==\nArchivolt\nButtress\nIonic order\nPost and lintel\nEngaged column\nLesene\nClassical architecture\nList of classical architecture terms\n\n\n== Gallery ==\n\n\n== Notes ==\n\nLewis, Philippa and Gillian Darley, Dictionary of Ornament (1986) NY: Pantheon", 
                "titleUrl": "https://en.wikipedia.org/wiki/Pilaster", 
                "title": "Pilaster"
            }, 
            {
                "snippet": "falsa braga) is a defensive wall located outside the main walls of a fortification. It is of a lower height than the main walls, and is preceded by a ditch", 
                "pageCategories": "All stub articles\nArticles containing Italian-language text\nCommons category with local link same as on Wikidata\nFortification (architectural elements)\nFortification stubs", 
                "pageContent": "A faussebraye (Italian: falsa braga) is a defensive wall located outside the main walls of a fortification. It is of a lower height than the main walls, and is preceded by a ditch. In Greek and Byzantine fortifications, the faussebraye was known as a proteichisma.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Faussebraye", 
                "title": "Faussebraye"
            }, 
            {
                "snippet": "An oriel window is a form of bay window which projects from the main wall of a building but does not reach to the ground. Supported by corbels, brackets", 
                "pageCategories": "All articles with unsourced statements\nAll stub articles\nArchitectural element stubs\nArticles containing Anglo-Norman-language text\nArticles containing Latin-language text\nArticles with unsourced statements from June 2011\nCommons category with local link same as on Wikidata\nWindows", 
                "pageContent": "An oriel window is a form of bay window which projects from the main wall of a building but does not reach to the ground. Supported by corbels, brackets or similar, an oriel window is most commonly found projecting from an upper floor but is also sometimes used on the ground floor.\nOriel windows are seen in Arab architecture in the form of mashrabiya. In Islamic culture these windows and balconies project from the street front of houses, providing an area in which women could peer out and see the activities below while remaining invisible.\n\n\n== Origins ==\nAccording to the Oxford English Dictionary, the word \"oriel\" is derived from Anglo-Norman oriell and post-classical Latin oriolum, both meaning \"gallery\" or \"porch\", perhaps from classical Latin aulaeum, \"curtain\".\nOriel College, Oxford, took its name from a balcony or oriel window forming a feature of a building which occupied the site the college now stands on.\nOriel Chambers in Liverpool was a very controversial building when it was built, featuring an entire fa\u00e7ade of glass oriel windows. It is seen as an early example of modernism.\n\n\n== Gallery ==\n\n\n== See also ==\nBay window for more details\nBow window\nBret\u00e8che\nTurret window\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Oriel_window", 
                "title": "Oriel window"
            }, 
            {
                "snippet": "assistants, the deacon and sub-deacon. The seat is often set back into the main wall of the church itself. Not all sedilia are stone; there is a timber one", 
                "pageCategories": "All stub articles\nArchitecture stubs\nArticles incorporating a citation from the 1913 Catholic Encyclopedia with Wikisource reference\nChurch architecture\nCommons category with page title same as on Wikidata\nWikipedia articles incorporating a citation from the 1911 Encyclopaedia Britannica with no article parameter\nWikipedia articles incorporating text from the 1911 Encyclop\u00e6dia Britannica", 
                "pageContent": "In ecclesiastical architecture, sedilia (the plural of Lat. sedile, seat) are seats, usually made of stone, found on the liturgical south side of an altar, often in the chancel, for use during Mass for the officiating priest and his assistants, the deacon and sub-deacon. The seat is often set back into the main wall of the church itself.\nNot all sedilia are stone; there is a timber one thought to be 15th century in St Nicholas' Church at Rodmersham in Kent.\nWhen there is only one such seat, the singular form 'sedile' is used, as for instance at St Mary's, Princes Risborough, Buckinghamshire or at St Agatha's, Coates, West Sussex.\nThe first examples in the catacombs were single inlays for the officiating priest. In time the more usual number became three, although there are examples of up to five sedilia.\nThe custom of recessing them in the thickness of the wall began about the end of the 12th century; some early examples consist only of stone benches, and there is one instance of a single seat or arm-chair in stone at Lenham in Kent.\nThe niches or recesses in which they are sunk are often richly decorated with canopies and subdivided with moulded shafts, pinnacles and tabernacle work; the seats are sometimes at different levels, the eastern being always the highest, and sometimes an additional niche is provided in which the piscina is placed.\n\n\n== References ==\n\n This article incorporates text from a publication now in the public domain: Chisholm, Hugh, ed. (1911). \"article name needed\". Encyclop\u00e6dia Britannica (11th ed.). Cambridge University Press.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Sedilia", 
                "title": "Sedilia"
            }, 
            {
                "snippet": "several places, including gable sections that project slightly over the main wall surface below, and vertically laid trim below some of the cornices. The", 
                "pageCategories": "All stub articles\nCommons category without a link on Wikidata\nCoordinates on Wikidata\nHouses completed in 1883\nHouses in Wakefield, Massachusetts\nHouses on the National Register of Historic Places in Wakefield, Massachusetts\nQueen Anne architecture in Massachusetts\nWakefield, Massachusetts Registered Historic Place stubs", 
                "pageContent": "The House at 18A and 20 Aborn Street in Wakefield, Massachusetts is a historic house and carriage house with elaborate Queen Anne styling. The 2.5 story wood frame house was built in the early 1880s, when Aborn Street and the surrounding neighborhood was developed. The house has irregular massing: the main part of the house has a rough L shape, but it is studded with porches, bay windows, and an extended porte cochere. Several architectural details appear in several places, including gable sections that project slightly over the main wall surface below, and vertically laid trim below some of the cornices. The former carriage house, which has been fully converted for residential use, carries through some of these details.\nThe property was listed on the National Register of Historic Places in 1989.\n\n\n== See also ==\nNational Register of Historic Places listings in Wakefield, Massachusetts\nNational Register of Historic Places listings in Middlesex County, Massachusetts\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/House_at_18A_and_20_Aborn_Street", 
                "title": "House at 18A and 20 Aborn Street"
            }, 
            {
                "snippet": "denoting the \"main defensive enclosure of a fortification\". For a castle this is the main defensive line of wall towers and curtain walls enclosing the", 
                "pageCategories": "All articles with unsourced statements\nAll stub articles\nArchitecture stubs\nArticles with unsourced statements from May 2015\nCastle architecture\nWikipedia articles incorporating a citation from the 1911 Encyclopaedia Britannica without Wikisource reference\nWikipedia articles incorporating text from the 1911 Encyclop\u00e6dia Britannica", 
                "pageContent": "Enceinte (from Latin incinctus: girdled, surrounded) is a French term denoting the \"main defensive enclosure of a fortification\". For a castle this is the main defensive line of wall towers and curtain walls enclosing the position. For a settlement it would be the main town wall with its associated gatehouses and towers and walls.\nAccording to the 1911 Encyclop\u00e6dia Britannica, the term was strictly applied to the continuous line of bastions and curtain walls forming \"the body of the place\", this last expression being often used as synonymous with enceinte. However, the outworks or defensive wall close to the enceinte were not considered as forming part of it. In early 20th-century fortification, the enceinte was usually simply the innermost continuous line of fortifications.\nIn architecture, generally, an enceinte is the close or precinct of a cathedral, abbey, castle, etc.\n\n\n== Features ==\nThe enceinte may be laid out as a freestanding structure or combined with buildings adjoining the outer walls. The enceinte not only provided passive protection for the areas behind it, but was usually an important component of the defence with its wall walks (often surmounted by battlements), embrasures and covered firing positions.\nThe outline of the enceinte, with its fortified towers and domestic buildings, shaped the silhouette of a castle. The ground plan of an enceinte is affected by the terrain. The enceintes of hill castles often have an irregular polygonal shape dictated by the topography, whilst lowland castles more frequently have a regular rectangular shape, as exemplified by quadrangular castles.\nFrom the 12th century onwards an additional enclosure called a zwinger was often built in front of the enceinte of many European castles. This afforded an additional layer of defence as it formed a killing ground in front of the main defensive wall. Sometimes - depending on the size and type of the surrounding fortifications \u2212 several wall systems were built (e.g. as zwingers) that could also be used to keep dogs, wild boar or bears, or even cattle in times of need. During the Baroque era it was not uncommon for these enclosures to be turned into pleasure gardens as for example in Dresden).\n\n\n== Other meaning ==\nEnceinte is also a euphemism used as an alternative to the word 'pregnant'.\n\n\n== Notes ==\n\n\n== References ==\nFriar, Stephen (2003), The Sutton Companion to Castles, Stroud: Sutton Publishing, p. 105, ISBN 978-0-7509-3994-2 \nPiper, Otto (1967), \"Burgenkunde. Bauwesen und Geschichte der Burgen\", in Piperer, R.; et al., Neue, verbesserte und erweiterte Auflage, Munich etc, p. 319 \nAttribution:\n This article incorporates text from a publication now in the public domain: Chisholm, Hugh, ed. (1911), \"Enceinte\", Encyclop\u00e6dia Britannica, 9 (11th ed.), Cambridge University Press, p. 368", 
                "titleUrl": "https://en.wikipedia.org/wiki/Enceinte", 
                "title": "Enceinte"
            }, 
            {
                "snippet": "return to the main wall via a dramatic and famous Tyrolean traverse. The spire was originally summited by lassoing the summit from the main wall and then Ax", 
                "pageCategories": "Climbing areas of the United States\nCoordinates on Wikidata\nLandforms of Mariposa County, California\nLandforms of Yosemite National Park\nRock formations of California", 
                "pageContent": "The Lost Arrow Spire is a detached pillar in Yosemite Valley, California, located immediately adjacent to Upper Yosemite Falls. The structure includes the Lost Arrow Spire Chimney route which is recognized in the historic climbing text Fifty Classic Climbs of North America. The last two pitches of Lost Arrow Spire Chimney are called the Lost Arrow Spire Tip and completes the detached portion of the spire. The Tip route is often reached by rappelling into an area known as The Notch. Once the route is completed climbers will often return to the main wall via a dramatic and famous Tyrolean traverse.\nThe spire was originally summited by lassoing the summit from the main wall and then Ax Nelson prusiked the lassoed line to the peak and was followed by Jack Arnold. While Steve Roper called this \"one of the greatest rope stunts ever pulled off in climbing history\" many climbers did not recognize this \"rope trick\" as a true ascent. An undisputed ascent was completed later that season by John Salath\u00e9 and Ax Nelson via the Lost Arrow Spire Chimney.\nLost Arrow Spire later became one of the early hotspots for Highlining, the version of slacklining on high places. The first successful walk over a slackline to the spire happened on July 13, 1985.\n\n\n== References ==\n\n\n== External links ==\nSummitpost.org", 
                "titleUrl": "https://en.wikipedia.org/wiki/Lost_Arrow_Spire", 
                "title": "Lost Arrow Spire"
            }, 
            {
                "snippet": "with little or no projection from the main wall plane, but emphasized by a different colour from the main wall. These can be seen even on small terraced", 
                "pageCategories": "All Wikipedia articles written in British (Oxford) English\nColumns and entablature\nCommons category with page title same as on Wikidata\nDoor furniture\nGeorgian architecture\nUse British (Oxford) English from May 2016\nUse dmy dates from May 2016\nWindows", 
                "pageContent": "A Gibbs surround or Gibbs Surround is a type of architectural frame surrounding a door, window or niche in the tradition of classical architecture. The formula is not fixed, but several of the following elements will be found. The door is surrounded by an architrave, or perhaps consists of or is flanked by pilasters or columns. These are with \"blocking\", where rectangular blocks stick out at intervals, usually alternating to represent half the surround. Above the opening there are large rusticated voussoirs and a keystone and a pediment above that. The most essential element is the alternation of blocking with non-blocking elements. Some definitions extend to including arches or square openings merely with alternate blocked elements that continue round the top in the same manner as the sides, as in the rectangular windows of the White House's north front basement level.\nThough intended for masonry in stone, the motif can be executed in other materials, especially brick, often masked in stucco, wood, or just paint. British vernacular housing of the late 19th century often uses alternating coloured blocks, with little or no projection from the main wall plane, but emphasized by a different colour from the main wall. These can be seen even on small terraced houses, often using cast stone, and used on both the door and ground floor windows.\n\n\n== History ==\nIt is named after the architect James Gibbs, who often used it and popularized it in England, for example at St Martin-in-the-Fields, London. Here the side doors have surrounds with all the details including pediments, while the round-topped windows along the sides have Gibbs surrounds if the broadest definition is used. However, Gibbs certainly did not invent it. The formula can be found in Ancient Roman architecture, and became popular in Renaissance architecture from the early 16th century. Gibbs illustrated a version in his pattern-book A Book of Architecture (1728), though there the blocking stopped at the edge of the architrave. More often the blocking overlies it. This was swiftly plagiarized by rival books such as William Salmon's Palladio Londinensis (1734), which credits Andrea Palladio (d. 1580) with the origin of what Salmon calls a \"Rustick Window and door\".\nThe name is mainly used in Britain and other English-speaking countries, where the type was also most popular and long-lasting. As a relatively simple but effective way of ornamenting an opening it was widely used for minor doors or windows in grand buildings, and the main door of more modest ones. The front door of Gibbs' medium-sized country house, Ditchley House, uses the device.\nA version with columns rather than a moulded architrave was illustrated by Sebastiano Serlio in 1537, where the voussoirs but not the keystone push up past the bottom edge of the pediment. Variations of this style are seen, for example, in the upper-floor windows of Palazzo Thiene in Vicenza (apparently part of the additions by Andrea Palladio), where only the keystone breaks into the pediment. The effect of a Gibbs surround is achieved round the doors of the south front of the Petit Trianon by stopping the horizontally banded rustication short in alternate levels.\nEarly examples in America, derived from the many English pattern-books used there, include the Aquia Church in Virginia of the 1750s and St. Paul's Chapel in Manhattan, completed in 1766.\n\n\n== Notes ==\n\n\n== References ==\nChitham, Robert, The Classical Orders of Architecture (2007) Routledge ISBN 9781136358951, google books\nFleming, John, Hugh Honour, Nikolaus Pevsner (1998) The Penguin Dictionary of architecture and landscape architecture, Penguin Books, 5th edition ISBN 0670880175\nLoth, Calder, \"CLASSICAL COMMENTS: THE GIBBS SURROUND\", Institute of Classical Architecture & Art, The Classicist Blog\nEnglish Heritage Thesaurus\n\n\n== External links ==\n Media related to Gibbs surround at Wikimedia Commons", 
                "titleUrl": "https://en.wikipedia.org/wiki/Gibbs_surround", 
                "title": "Gibbs surround"
            }, 
            {
                "snippet": "that some of the historic features will be retained. These include the main wall and entrance arch to the battery and some of the concrete emplacements", 
                "pageCategories": "1862 establishments in England\nAll stub articles\nBatteries\nCoordinates on Wikidata\nForts on the Isle of Wight\nGrade II listed buildings on the Isle of Wight\nGrade II listed forts\nIsle of Wight geography stubs", 
                "pageContent": "Warden Point Battery is a battery on the Isle of Wight begun in 1862, that was originally armed with 7-inch and 9-inch rifled muzzle loaders on barbette mountings.\nIt was built in the 1890s for 9.2-inch breech loader guns. In use until 1936, and in World War II as a command post and searchlight battery. The battery was a regular polygon with central caponier on the landward side, and flanking caponiers at the North and South corners. The central caponier and North -East loopholed wall remain.\nThe site spent a considerable number of years after the second world war as a holiday camp. Following the closure of that, the housing estate was started. Unconfirmed information from the builders on a visit to the site suggests that some of the historic features will be retained. These include the main wall and entrance arch to the battery and some of the concrete emplacements. The latter will be built over, but protected so that they can be \"re-discovered\" if and when the newer buildings are demolished. At time of writing, the building work has been suspended.\nIt is now a Grade II Listed Building.\n\n\n== References ==\n\n\n== External links ==\nVictorian Forts data sheet", 
                "titleUrl": "https://en.wikipedia.org/wiki/Warden_Point_Battery", 
                "title": "Warden Point Battery"
            }, 
            {
                "snippet": "extensive work has been undertaken to rebuild the dam and to prevent its main wall from leaking and it is now (June 2013) hoped that this will solve the", 
                "pageCategories": "1861 establishments in the Cape Colony\nAll articles needing additional references\nArticles needing additional references from March 2016\nCoordinates on Wikidata\nPopulated places established in 1861\nPopulated places in the Langeberg Local Municipality", 
                "pageContent": "McGregor is a small village in the mountains of the Western Cape, South Africa. It is roughly 150 km east of Cape Town. It is located in Ward 5 of the Breede River Winelands Municipality. According to the Census 2001, this Ward has a population of 10,254 people (Stats SA, 2001).\nThe village was established in 1861 and was originally called Lady Grey. It was renamed in 1905 in honour of Rev. Andrew McGregor, who had been the Dutch Reformed Church minister of the Robertson District for forty years. It has become somewhat of a magnet for alternative lifestyles. The local Waldorf school has roughly 167 children. It is part of the broader wine route network of the Western Cape and is situated at one end of the popular Boesmanskloof Trail to the town of Greyton to the south.\nThe village irrigation system dates back many years. The edge of the streets have a water channel fed from the town dam. Each landowner has a slot allocated during the week where they get a few minutes to an hour of the channel flowing into their property, with the time allocated being a function of the size of the property. In recent years some controversy has arisen due to the reduction of the roster to every second week in an effort to keep a fuller dam. However, during the summer of 2012/13 extensive work has been undertaken to rebuild the dam and to prevent its main wall from leaking and it is now (June 2013) hoped that this will solve the problems.\n\n\n== References ==\n\n\n== External links ==\nMcGregor village web site with history\nTourist and Accommodation Information - accommodation bookings, things to do, places to eat, wine cellars, official tourism site", 
                "titleUrl": "https://en.wikipedia.org/wiki/McGregor,_Western_Cape", 
                "title": "McGregor, Western Cape"
            }
        ], 
        "phraseCharStart": "713"
    }, 
    {
        "phraseCharEnd": "739", 
        "phraseIndex": "T14", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "remote parts", 
        "wikiSearchResults": [
            {
                "snippet": "Museum from 1896 until his death. He was involved in an expedition to remote parts of Western Australia from December 1894 to October 1895, travelling from", 
                "pageCategories": "1850 births\n1931 deaths\nAll stub articles\nBotanists active in Kew Gardens\nBotanists with author abbreviations\nBritish botanist stubs\nEnglish biologist stubs\nEnglish botanists\nEnglish explorers\nPeople associated with the Natural History Museum, London", 
                "pageContent": "Spencer Le Marchant Moore (1 November 1850 \u2013 14 March 1931) was an English botanist.\nMoore was born in Hampstead. He worked at the Royal Botanic Gardens, Kew from about 1870 to 1879, wrote a number of botanical papers, and then worked in an unofficial capacity at the Natural History Museum from 1896 until his death.\nHe was involved in an expedition to remote parts of Western Australia from December 1894 to October 1895, travelling from the goldfields to places like Siberia Soak (near Waverley) and Goongarrie.\nMoore is commemorated in the plant genus Spenceria.\n\n\n== Notes ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Spencer_Le_Marchant_Moore", 
                "title": "Spencer Le Marchant Moore"
            }, 
            {
                "snippet": "refers to a particular type of native-culture exploitation film set in remote parts of the Far East, Southeast Asia, Africa, South America, and the South", 
                "pageCategories": "All stub articles\nFilm genre stubs\nFilm genres", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Goona-goona_epic", 
                "title": "Goona-goona epic"
            }, 
            {
                "snippet": "supporting the Royal Australian Navy. It also conducted survey flights over remote parts of Australia and mapped the Darwin\u2013Sydney section of the Empire Air Mail", 
                "pageCategories": "All Wikipedia articles written in Australian English\nGood articles\nMilitary units and formations disestablished in 1939\nMilitary units and formations established in 1928\nRAAF squadrons\nUse Australian English from May 2015\nUse dmy dates from May 2015", 
                "pageContent": "Seaplane Squadron was a flying unit of the Royal Australian Air Force (RAAF) between the wars. It operated Supermarine Southampton flying boats from January 1928, as well as other types. Along with Fighter Squadron, Seaplane Squadron was a component of No. 1 Flying Training School, based at RAAF Point Cook, Victoria. Seaplane Squadron was responsible for coastal reconnaissance, training aircrew to operate seaplanes, and supporting the Royal Australian Navy. It also conducted survey flights over remote parts of Australia and mapped the Darwin\u2013Sydney section of the Empire Air Mail Scheme route. Seaplane Squadron was disbanded in June 1939.\n\n\n== History ==\nAlthough the first entry in Seaplane Squadron records is dated 16 February 1934, the official history of the Royal Australian Air Force (RAAF) between the wars refers to the unit as having been in operation when Australia acquired two Supermarine Southampton flying boats, which entered service in January 1928. The Southamptons formed a coastal reconnaissance flight within Seaplane Squadron, which also operated other aircraft for seaplane training. Seaplane Squadron was one of two formations raised at RAAF Point Cook, Victoria, under the auspices of No. 1 Flying Training School (No. 1 FTS), the other being Fighter Squadron, which operated Bristol Bulldogs. No. 1 FTS had been the first unit to be formed as part of the new Australian Air Force on 31 March 1921 (the prefix \"Royal\" was added in August that year).\n\nThe Southamptons (nicknamed \"Swamptons\") were the biggest aircraft in the RAAF's inventory at the time and a new seaplane hangar was specially constructed for them at Point Cook. On 22 June, one of the flying boats was overturned by strong wind on the Torrens River en route to meet the four Southamptons of the Royal Air Force's Far East Flight near Adelaide. In their naval cooperation role, the Southamptons were required to locate and shadow \"enemy\" cruisers on exercises. They also trialled radio communications between aircraft and naval ships.\nThe Southamptons were used for parachute training with the \"pull-off\" technique, which involved standing on a small platform near the outer wing struts, opening the parachute and being dragged from the aircraft by the wind. In the early 1930s, the flying boats took part in several forestry surveys in Tasmania. From June 1935 to February 1936, a Southampton was employed to map the Darwin\u2013Sydney section of the Empire Air Mail Scheme route; its survey work ultimately took it to New Guinea and around the Australian continent. Seaplane Squadron undertook search-and-rescue work with both the Southamptons and Supermarine Seagulls; the former were involved in the abortive search for the airliner Miss Hobart, a DH.86 that vanished in Bass Strait on 19 October 1934. One of the Southamptons was taken out of service in 1937; the other continued flying until 1939.\n\nIn October 1929, Seaplane Squadron received a locally designed amphibian, the Wackett Widgeon II; it crashed into the sea off Point Cook on 6 January 1930, killing all three occupants. Another Wackett design, the Warrigal II landplane, was fitted with floats and assigned to Seaplane Squadron in September 1932 for trials and possible use as a trainer and patrol aircraft; it was considered successful in the latter role but maintenance issues led to its disposal in July 1933. The squadron also operated Moths. In May 1934, one of these was flown to Darwin, Northern Territory, where it was fitted with floats and undertook reconnaissance and survey work in cooperation with HMAS Morseby, before being converted back to a landplane and returning to Point Cook in July. In December 1935, a Gipsy Moth fitted with skis embarked for Antarctica aboard the RRS Discovery II to locate missing explorer Lincoln Ellsworth. Seaplane Squadron began operating Avro Ansons for navigation courses and cross-country exercises in 1937; one exercise in November 1938 involved a round-Australia flight. A new headquarters building for the squadron was constructed at Point Cook in the late 1930s, as part of general improvements to RAAF facilities owing to the threat of war in Europe.\nThroughout their existence, Seaplane and Fighter Squadrons remained under the control of No. 1 FTS and were \"really little more than flights\", in the words of the official history. The final entry in Seaplane Squadron records was made on 30 June 1939. The unit became the nucleus for No. 10 (Reconnaissance) Squadron, formed at Point Cook the following day.\n\n\n== Notes ==\n\n\n== References ==\nCampbell-Wright, Steve (2014). An Interesting Point: A History of Military Aviation at Point Cook 1914\u20132014. Canberra: Air Power Development Centre. ISBN 978-1-925062-00-7. \nCoulthard-Clark, Chris (1991). The Third Brother: The Royal Australian Air Force 1921\u201339. North Sydney: Allen & Unwin. ISBN 0-04-442307-1. \nRAAF Historical Section (1995). Units of the Royal Australian Air Force: A Concise History. Volume 4: Maritime and Transport Units. Canberra: Australian Government Publishing Service. ISBN 0-644-42796-5. \nStephens, Alan (2006) [2001]. The Royal Australian Air Force: A History. London: Oxford University Press. ISBN 0-19-555541-4.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Seaplane_Squadron_RAAF", 
                "title": "Seaplane Squadron RAAF"
            }, 
            {
                "snippet": "be on the leash around the lawn but they are allowed off the leash in remote parts of the park. The park is open from 8 a.m. to dusk.   There are several", 
                "pageCategories": "Parks in Contra Costa County, California", 
                "pageContent": "Kennedy Grove is a 222-acre regional park located in West Contra Costa County that contains a three-mile hiking trail with an elevation of 760 feet. The Grove features many large eucalyptus trees, picnic areas, volleyball nets, playgrounds, and horseshoe pits. Bird watching is popular here because hawks are almost always spotted. Some hikers have reported seeing golden and bald eagles around the reservoir. There is no camping allowed. Parking is $5 with an extra $2 fee for a dog. Dogs have to be on the leash around the lawn but they are allowed off the leash in remote parts of the park. The park is open from 8 a.m. to dusk.\n\n\n== Trail location ==\nThere are several trails located within the recreation area ranging from very easy to somewhat difficult. All Trails lead out from the eucalyptus grove located near the base of the San Pablo Dam, adjacent to the parking area. Several trails will guide you through changing landscapes of native Oak chaparrals and fern laden Bay laurel woodlands. A segment of the Bay Area Ridge Trail begins at Kennedy Grove and runs along the western shore of San Pablo Reservoir. Eventually the trail crosses San Pablo Dam Road and then climbs up the San Pablo Ridge. *NOTE* An EBMUD permit is required for this hike.\n\n\n== History ==\nBefore this 222-acre park became a protected area it was home to wheat fields, the site of Ranchos and railroad stations for a railroad that ran from Oakland to Orinda through Berkeley and Richmond via El Sobrante. It was originally part of the 17,754-acre Rancho San Pablo. Francisco Castro acquired the rancho in 1823 and the grove later became the Clancy Ranch. By 1886 the railroad had scheduled stops from the California and Nevada railroad at Laurel Glen and Frenchman's Curve. The picnic areas around the park are named after some of the former railroad stops. The eucalyptus trees were planted in 1910. This park was honored after President John F. Kennedy. The park opened on October 22, 1967 and is now owned/operated by the East Bay Regional Park District.\n\n\n== Wildlife ==\nSeen from the Kennedy Grove area is the San Pablo Reservoir where fishing is popular. The reservoir has trout and catfish and is recognized as the one of the finest fisheries in the East Bay. However a fishing license and EBMUD fishing permit is required. You can see flocks of ducks, shorebirds, geese, and white pelicans from the trails. Sometimes deer, bobcats, foxes and coyotes can be sighted. There are also species like quail, dove, and wild turkeys. Some birds of prey that are known to take the skies above Kennedy Grove are eagles, owls, hawks, and ospreys.\n\n\n== Weather ==\nThe area surrounding San Pablo Dam is fairly temperate. The East Bay is warmer than the Peninsula. Summer temperatures tend to vary from 60-100 degrees. Fog is very common with rainfall occurring mostly during the months of October thru March. The temperatures can dip below 32 degrees in the winter.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Kennedy_Grove_Regional_Recreation_Area", 
                "title": "Kennedy Grove Regional Recreation Area"
            }, 
            {
                "snippet": "important products. There is a large Mangyan population in the more remote parts of the municipality, and programs of assistance for these people have", 
                "pageCategories": "Coordinates on Wikidata\nMunicipalities of Oriental Mindoro", 
                "pageContent": "Socorro is a third class municipality in the province of Oriental Mindoro, Philippines. It is an inland town on the Island of Mindoro at the junction of the Pola Road. According to the 2010 census, it has a population of 38,348 people.\nIn the last decade of the 20th century and the first decade of the 21st century, many roads were paved, the market rebuilt, and employment rose. Agriculture still provides the main industry with rice, fruits, and coconut products dominating. Citrus products like calamansi, dalandan and pomelo are also abundant here together with rambutan and lanzones. Fresh fish from Lake Naujan at the northern end of the municipality and Balut are also important products. There is a large Mangyan population in the more remote parts of the municipality, and programs of assistance for these people have been implemented.\n\n\n== Barangays ==\nSocorro is politically subdivided into 26 barangays.\n\n\n== Demographics ==\n\n\n== Local government ==\nMayor: Hon. Ma. Fe V. Brondial\nVice Mayor: Hon. Roy A. De Claro\n\n\n== Schools ==\nLeuteboro National High School\nSocorro Central School\nMina de Oro Catholic High School (MDOCHS)\nGrace Mission College\nIATEC Computer College\nBayuin National High School\nFortuna National High School\n\n\n== References ==\n\n\n== External links ==\nPhilippine Standard Geographic Code\nPhilippine Census Information\nLocal Governance Performance Management System", 
                "titleUrl": "https://en.wikipedia.org/wiki/Socorro,_Oriental_Mindoro", 
                "title": "Socorro, Oriental Mindoro"
            }, 
            {
                "snippet": "kilometres (350\u00a0mi) north of Sydney. Additional access points to more remote parts of the national park are via the Armidale-Grafton Road, via Marengo Road", 
                "pageCategories": "1972 establishments in Australia\nAll Wikipedia articles written in Australian English\nCoordinates on Wikidata\nGuy Fawkes\nIUCN Category Ib\nNational parks of New South Wales\nNew England (New South Wales)\nNorthern Tablelands\nProtected areas established in 1972\nUse Australian English from September 2014", 
                "pageContent": "Guy Fawkes River National Park, a national park comprising 100,590 hectares (248,600 acres), is located on the eastern edge of the New England Tablelands and the western edge of the Dorrigo Plateau, in north eastern New South Wales, Australia.\nAccess to the national park via Waterfall Way, is near Ebor, 46 kilometres (29 mi) south-west of Dorrigo and 80 kilometres (50 mi) north-east of Armidale. The national park is approximately 560 kilometres (350 mi) north of Sydney. Additional access points to more remote parts of the national park are via the Armidale-Grafton Road, via Marengo Road at Hernani, Sheepstation Creek Road at Dundurrabin, Ellis Road and Boundary Creek Road south of Nymboida; or from the Old Grafton-Glen Innes Road, via Chaelundi Road at Dalmorton. From the west, the national park is accessible via Wards Mistake.\nGuy Fawkes River National Park has over 40 different vegetation communities, 28 threatened plant species, 24 threatened fauna species and significant areas of old growth forest protected within the park. The Guy Fawkes River plunges off the Northern Tablelands at the Ebor Falls. There are spectacular examples of valley and rugged river gorges including the deeply incised Guy Fawkes River Valley along the line of an ancient fault through the park. The rugged gorges of the Aberfoyle, Sara and Henry rivers also run through the park.\n\n\n== Features ==\nWild horses have lived in this park since the 1930s. In October 2000, over 600 horses in the Guy Fawkes River National Park were shot and killed from a helicopter during a controversial cull by the NSW National Parks and Wildlife Service. In response to public outcry, the Minister for the Environment, Mr Bob Debus, commissioned a study into the heritage value of horses in the park and indicated that, should the horses be found to have genuine heritage significance, they would be humanely removed from the park so that they can be managed properly in another location by people with an interest in their heritage value. In February 2002, the final report by the Heritage Working Party found that these horses had significant historical, military and cultural value. They are direct descendants of Australia's wartime cavalry horses, known as Walers, and are the only group of Australian wild horse to have proven heritage value. Since the campaign began to remove horses from there over 400 have been passively trapped, taken from the park, and 200 of these have been found a home elsewhere. The Guy Fawkes Heritage Horse Association (GFHHA) takes possession of horses passively removed from the GFRNP and offer them for sale to the public, they also manage the horses to maintain their inherent characteristics and to preserve the unique genetics of these wild horses. A formal register and Stud Book has been established for these purposes. The GFHHA also actively promotes the GF horse versatility by sponsoring classes held at local Ag Shows and encouraging horse owners to participate in all disciplines.\nIn the 1970s the Bicentennial National Trail was plotted to run along the western side of Guy Fawkes River on what is a travelling stock route.\n\n\n== See also ==\n\nBoyd River\nChaelundi National Park\nGuy Fawkes\nProtected areas of New South Wales\nSara River\n\n\n== References ==\n\n\n== External links ==\n\"Guy Fawkes River National Park, Nature Reserve and State Conservation Area: Plan of management\" (PDF).  PDF NSW National Parks & Wildlife Service. NSW Department of Environment and Climate Change. January 2009. ISBN 978-1-74232-158-5.\n\"Guy Fawkes River National Park and adjoining parks\".   PDF 3.4MB (Map). NSW National Parks & Wildlife Service.\n\"Walks map: Guy Fawkes River National Park and Mount Hyland Nature Reserve\".   PDF 2.1MB (Map). NSW National Parks & Wildlife Service.\n\"Guy Fawkes River and Chaelundi National Parks and Reserves\".   PDF (Brochure). NSW National Parks & Wildlife Service. NSW Department of Environment and Climate Change. 2007.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Guy_Fawkes_River_National_Park", 
                "title": "Guy Fawkes River National Park"
            }, 
            {
                "snippet": "preservatives. Nonetheless, poison continues to be used as a hunting tool in remote parts of developing countries, including Africa, South America, and Asia. ", 
                "pageCategories": "Articles with Wayback Machine links\nGood articles\nHistory by topic\nPoisons\nUse dmy dates from February 2013", 
                "pageContent": "The history of poison stretches from before 4500 BC to the present day. Poisons have been used for many purposes across the span of human existence, most commonly as weapons, anti-venoms, and medicines. Poison has allowed much progress in branches, toxicology, and technology, among other sciences.\nPoison was discovered in ancient times, and was used by ancient tribes and civilizations as a hunting tool to quicken and ensure the death of their prey or enemies. This use of poison grew more advanced, and many of these ancient peoples began forging weapons designed specifically for poison enhancement. Later in history, particularly at the time of the Roman Empire, one of the more prevalent uses was assassination. As early as 331 BC, poisonings executed at the dinner table or in drinks were reported, and the practice became a common occurrence. The use of fatal substances was seen among every social class; even the nobility would often use it to dispose of unwanted political or economic opponents.\nIn Medieval Europe, poison became a more popular form of killing, though cures surfaced for many of the more widely known poisons. This was stimulated by the increased availability of poisons; shops known as apothecaries, selling various medicinal wares, were open to the public, and from there, substances that were traditionally used for curative purposes were employed for more sinister ends. At approximately the same time, in the Middle East, Arabs developed a form of arsenic that is odorless and transparent, making the poison difficult to detect. This \"poison epidemic\" was also prevalent in parts of Asia at this time, as well.\nOver the centuries, the variety of harmful uses of poisons continued to increase. The means for curing these poisons also advanced in parallel. In the modern world, intentional poisoning is less common than the Middle Ages. Rather, the more common concern is the risk of accidental poisoning from everyday substances and products.\nConstructive uses for poisons have increased considerably in the modern world. Poisons are now used as pesticides, disinfectants, cleaning solutions, and preservatives. Nonetheless, poison continues to be used as a hunting tool in remote parts of developing countries, including Africa, South America, and Asia.\n\n\n== Origins of poison ==\n\nArchaeological findings prove that while ancient mankind used conventional weapons such as axes and clubs, and later swords, they sought more subtle, destructive means of causing death\u2014something that could be achieved through poison. Grooves for storing or holding poisons such as tubocurarine have been plainly found in their hunting weapons and tools, showing that early humans had discovered poisons of varying potency and applied them to their weapons. Some speculate that this use and existence of these strange and noxious substances was kept secret within the more important and higher-ranked members of a tribe or clan, and were seen as emblems of a greater power. This may have also given birth to the concept of the stereotypical \"medicine man\" or \"witch doctor\".\nOnce the use and danger of poison was realized, it became apparent that something had to be done. Mithridates VI, King of Pontus (an ancient Hellenistic state of northern Anatolia), from around 114\u201363 BC, lived in constant fear of being assassinated through poison. He became a hard-working pioneer in the search for a cure for poisons. In his position of power, he was able to test poisons on criminals facing execution, and then if there was a possible antidote. He was paranoid to the point that he administered daily amounts of poisons in an attempt to make himself immune to as many poisons as he could. Eventually, he discovered a formula that combined small portions of dozens of the best-known herbal remedies of the time, which he named Mithridatium. This was kept secret until his kingdom was invaded by Pompey the Great, who took it back to Rome. After being defeated by Pompey, Mithridates' antidote prescriptions and notes of medicinal plants were taken by the Romans and translated into Latin.\nPliny the Elder describes over 7000 different poisons. One he describes as \"The blood of a duck found in a certain district of Pontus, which was supposed to live on poisonous food, and the blood of this duck was afterwards used in the preparation of the Mithridatum, because it fed on poisonous plants and suffered no harm.\"\n\n\n=== India ===\nIndian surgeon Sushruta defined the stages of slow poisoning and the remedies of slow poisoning. He also mentions antidotes and the use of traditional substances to counter the effects of poisoning.\nPoisoned weapons were used in ancient India, and war tactics in ancient India have references to poison. A verse in Sanskrit reads \"Jalam visravayet sarmavamavisravyam ca dusayet,\" which translates to \"Waters of wells were to be mixed with poison and thus polluted.\"\nCh\u0101nakya (c. 350\u2013283 BC), also known as Kautilya, was adviser and prime minister to the first Maurya Emperor Chandragupta (c. 340\u2013293 BC). Kautilya suggested employing means such as seduction, secret use of weapons, and poison for political gain. He also urged detailed precautions against assassination\u2014tasters for food and elaborate ways to detect poison. In addition, the death penalty for violations of royal decrees was frequently administered through the use of poison.\n\n\n=== Egypt ===\nUnlike many civilizations, records of Egyptian knowledge and use of poisons can only be dated back to approximately 300 BC. However, it is believed that the earliest known Egyptian pharaoh, Menes, studied the properties of poisonous plants and venoms, according to early records.\nThe Egyptians are also thought to have come into knowledge about elements such as antimony, copper, crude arsenic, lead, opium, and mandrake (among others) which are mentioned in papyri. Egyptians are now thought to be the first to master distillation, and to manipulate the poison that can be retrieved from apricot kernels.\nCleopatra is said to have poisoned herself with an asp after hearing of Marc Antony's demise. Prior to her death, she was said to have sent many of her maidservants to act as guinea pigs to test different poisons, including belladonna, henbane, and the strychnine tree's seed.\nAfter this, the alchemist Agathodaemon (around AD 300) spoke of a mineral that when mixed with natron produced a 'fiery poison'. He described this poison as 'disappearing in water', giving a clear solution. Emsley speculates that the 'fiery poison' was arsenic trioxide, the unidentified mineral having to have been either realgar or orpiment, due to the relation between the unidentified mineral and his other writings.\n\n\n=== Rome ===\n\nIn Roman times, poisoning carried out at the dinner table or common eating or drinking area was not unheard of, or even uncommon, and was happening as early as 331 BC. These poisonings would have been used for self-advantageous reasons in every class of the social order. The writer Livy describes the poisoning of members of the upper class and nobles of Rome, and Roman emperor Nero is known to have favored the use of poisons on his relatives, even hiring a personal poisoner. His preferred poison was said to be cyanide.\nNero's predecessor, Claudius, was allegedly poisoned with mushrooms or alternatively poison herbs. However, accounts of the way Claudius died vary greatly. Halotus, his taster, Xenophon, his doctor, and the infamous poisoner Locusta have all been accused of possibly being the administrator of the fatal substance, but Agrippina, his final wife, is considered to be the most likely to have arranged his murder and may have even administered the poison herself. Some report that he died after prolonged suffering following a single dose at his evening meal, while some say that he recovered somewhat, only to be poisoned once more by a feather dipped in poison which was pushed down his throat under the pretense of helping him to vomit, or by poisoned gruel or an enema. Agrippina is considered to be the murderer, because she was ambitious for her son, Nero, and Claudius had become suspicious of her intrigues.\n\n\n== Later imperial Asia ==\nDespite the negative effects of poison, which were so evident in these times, cures were being found in poison, even at such a time where it was hated by the most of the general public. An example can be found in the works of Iranian born Persian physician, philosopher, and scholar Rhazes, writer of Secret of Secrets, which was a long list of chemical compounds, minerals and appratus, the first man to distil alcohol and use it as an anti-septic, and the person who suggested mercury be used as a laxative. He made discoveries relating to a mercury chloride called corrosive sublimate. An ointment derived from this sublimate was used to cure what Rhazes described as 'the itch', which is now referred to as scabies. This proved an effective treatment because of mercury's poisonous nature and ability to penetrate the skin, allowing it to eliminate the disease and the itch.\nIn India, the troubled 14th and 15th centuries in Rajasthan saw invasions in the Rajput heartlands. Rajput women practiced a custom of jauhar ( literally the taking of life) when their sons, brothers, or husbands faced certain death in battle. Jauhar was practiced within the Kshatriya warrior class to avoid the fate of subservience, slavery, rape, or slaughter at the hands of the invading forces.\n\n\n=== Nazi suicides by poison ===\nNazi war leader Hermann G\u00f6ring used cyanide to kill himself the night before he was supposed to be hanged during the Nuremberg Trials. Adolf Hitler had also taken a pill of cyanide but he bit down on the capsule and shot himself in the right temple shortly before the fall of Berlin along with his wife, Eva Braun.\n\n\n== Present day ==\nIn the late 20th century, an increasing number of products used for everyday life proved to be poisonous. The risk of being poisoned nowadays lies more in the accidental factor, where poison be induced or taken by accident. Poisoning is the 4th most common cause of death within young people. Accidental ingestions are most common in children less than 5 years old.\nHowever, hospital and emergency facilities are much enhanced compared to the first half of the 20th century and before, and antidotes are more available. Antidotes have been found for many poisons, and the antidotes for some of the most commonly known poisons are shown in the table above:\nHowever, poison still exists as a murderous entity today, but it is not as popular form of conducting murder as it used to be in past times, probably because of the wider range of ways to kill people and other factors that must be taken into consideration. One of the more recent deaths by poisoning was that of Russian dissident Alexander Litvinenko in 2006 from lethal polonium-210 radiation poisoning.\n\n\n=== Other uses ===\nToday, poison is used for a wider variety of purposes than it used to be. For example, poison can be used to rid an unwanted infestation by pests or to kill weeds. Such chemicals, known as pesticides, have been known to be used in some form since about 2500 BC. However, the use of pesticides has increased staggeringly from 1950, and presently approximately 2.5 million tons of industrial pesticides are used each year. Other poisons can also be used to preserve foods and building material.\n\n\n=== In culture ===\nToday, in many developing peoples of countries such as certain parts of Africa, South America and Asia, the use of poison as an actual weapon of hunting and attack still endures.\nIn Africa, certain arrow poisons are made using floral ingredients, such as of that taken from the plant Acokanthera. This plant contains ouabain, which is a cardiac glycoside, oleander, and milkweeds. Poisoned arrows are also still used in the jungle areas of Assam, Burma and Malaysia. The ingredients for the creation of these poisons are mainly extracted from plants of the Antiaris, Strychnos and Strophanthus genera, and Antiaris toxicaria (a tree of the mulberry and breadfruit family), for example, is used in the Java island of Indonesia, as well as several of its surrounding islands. The juice or liquid extracts are smeared on the head of the arrow, and inflicts the target paralysis, convulsions and/or cardiac arrest, virtually on strike due to the speed in which the extracts can affect a victim.\nAs well as plant based poisons, there are others that are made that are based on animals. For example, the larva or pupae of a beetle genus of the Northern Kalahari Desert is used to create a slow-acting poison that can be quite useful when hunting. The beetle itself is applied to the arrow head, by squeezing the contents of the beetle right onto the head. Plant sap is then mixed and serves as an adhesive. However, instead of the plant sap, a powder made from the dead, eviscerated larva can be used.\n\n\n== See also ==\nForensic science\nList of chemical elements\nList of Extremely Hazardous Substances\nToxicity\nList of poisonings\nPoison\n\n\n== References and notes ==\n\n\n== External links ==\nHistory of Poisons\nwww.poison.org\nDark History of Poison\nArsenic Poisoning History\n\"The Savior from Demise: A Book on Withstanding the Harms of Deadly Poisons\" from 1431 (in Arabic)", 
                "titleUrl": "https://en.wikipedia.org/wiki/History_of_poison", 
                "title": "History of poison"
            }, 
            {
                "snippet": "composer Herbert Hughes. Together, they collected traditional airs from the remote parts of County Donegal. While on holidays in Donegal, Hughes had learned the", 
                "pageCategories": "Irish folk songs\nIrish songs", 
                "pageContent": "\"My Lagan Love\" is a song to a traditional Irish air collected in 1903 in northern Donegal.\nThe English lyrics have been credited to Joseph Campbell (1879\u20131944, AKA Seosamh MacCathmhaoil and Joseph McCahill, among others). Campbell was a Belfast man whose grandparents came from the Irish-speaking area of Flurrybridge, South Armagh. He started collecting songs in County Antrim. In 1904 he began a collaboration with composer Herbert Hughes. Together, they collected traditional airs from the remote parts of County Donegal. While on holidays in Donegal, Hughes had learned the air from Proinseas mac Suibhne, who had learned it from his father Seaghan mac Suibhne, who in turn had learned it fifty years previously.\nThe Lagan referred to in the title most likely pertains to the area of good farming land between Donegal and Derry known in Irish as An Lag\u00e1n. The Lagan is the river that runs through Belfast. However, some argue that the Lagan in the song refers to a stream that empties into Lough Swilly in County Donegal, not far from where Herbert Hughes collected the song.\nThe song was arranged in a classical style by Hamilton Harty; this was used by Mary O'Hara and Charlotte Church.\n\n\n== Covers ==\n1910: John McCormack\n1953: Margaret Barry on I Sang Through the Fairs (The Alan Lomax Portrait Series)\n1967: Dusty Springfield on her BBCTV series.\n1968: Emmet Spiceland on The First\n1976: Horslips on The Book of Invasions: A Celtic Symphony (instrumental)\n1977: Bob McGrath on Sleepytime Bird\n1985: Kate Bush, bonus material on 1997 re-release of Hounds Of Love with some new lyrics written by Kate\n1987: Vermilion Sands on Water Blue\n1987: Capercaillie on Crosswinds as \"My Laggan Love/Fox on the Town\" (instrumental)\n1988: Van Morrison and The Chieftains on Irish Heartbeat\n1990: Meg Davis on The Claddagh Walk\n1994: Jean Redpath on Jean Redpath\n1995: Caroline Lavelle on Spirit as \"Lagan Love\"\n1997: James Galway and Phil Coulter on \"Legends\"\n1998: Charlotte Church on Voice of an Angel\n1998: Pentangle on Passe Avant\n1999: Sheila Chandra on Moonsung as \"Lagan Love/Nada Brahma\"\n1999: Carol Noonan on her recording Self-Titled as \"Lagan Love\"\n2002: Sin\u00e9ad O'Connor on Sean-N\u00f3s Nua\n2003: Anuna on Invocation\n2004: Sharon Knight on Song of the Sea.\n2005: The Corrs on Home\n2009: Celtic Woman on Celtic Woman: Songs from the Heart\n2010: Fionnuala Sherry on Songs From Before\n2011: Celtic Thunder on Storm\n2011: Lone Raven/Kara Markley on Flight to the Hinterlands\n2012: Lisa Hannigan and The Chieftains on The Chieftains: Voice of Ages\n2012: Niopha Keegan on lead vocals with The Unthanks on The Unthanks with Brighouse and Rastrick Brass Band\n\n\n== References ==\n\nJohn A. McLaughlin: One Green Hill, 2003. Beyond the Pale, Belfast. ISBN 978-1-900960-21-2.\n\n\n== External links ==\nAntony Kearns : Irish Repertoire\nFolk & Traditional Song Lyrics - My Lagan Love", 
                "titleUrl": "https://en.wikipedia.org/wiki/My_Lagan_Love", 
                "title": "My Lagan Love"
            }, 
            {
                "snippet": "and Monty Halls' Great Irish Escape) in which he lived and worked in remote parts of the UK and Ireland with his dog-and-best-friend Reuben. Halls' other", 
                "pageCategories": "1966 births\nAll articles with unsourced statements\nAlumni of the University of Plymouth\nArticles with hCards\nArticles with unsourced statements from June 2015\nCS1 maint: Unfit url\nEnglish explorers\nEnglish marine biologists\nEnglish photographers\nEnglish soldiers", 
                "pageContent": "Monty Halls (born 5 November 1966, England) is a TV broadcaster, explorer and marine biologist. He is most well known for his BBC Great Escape series (Monty Halls' Great Escape, Monty Halls' Great Hebridean Escape and Monty Halls' Great Irish Escape) in which he lived and worked in remote parts of the UK and Ireland with his dog-and-best-friend Reuben. Halls' other TV programmes include Great Ocean Adventures, Scubazoo, Animal Planet and Perfect Weapon. He has also led a number of diving expeditions. He was an expedition leader for the marine conservation charity Coral Cay Conservation.\n\n\n== Background and career ==\nHalls attended Bedstone College, where he was head boy, after which he was commissioned as a Royal Marines officer. His time in the Marines included a period in the British Military Assistance and Training Team in South Africa, where he assisted with the integration of former ANC guerrillas into the South African Army.\nAt 29, having left the Marines, Halls studied Marine Biology at the University of Plymouth where he became involved with projects including the underwater filming of a rare species of crocodile in Belize in partnership with the Natural History Museum. He graduated with a First Class Honours degree in 1999. In 2002, Halls led a group of adventurers and scientists (from the Scientific Exploration Society) to the sunken city of Mahabalipuram off Tamil Nadu (India), researched by Graham Hancock, and discovered 6 temples, which are still being explored.\n\n\n=== Television work ===\nIn 2005, he presented a nine-part TV series called Great Ocean Adventures (co-produced by RDF Television and Channel Five). A second series was broadcast in 2007. Memorable moments include Halls diving among a large number of Giant Humboldt Squid. In 2008 he co-hosted a National Geographic series, Perfect Weapon, looking at aspects of medieval weaponry. In 2008, Halls left his home in Bristol for Applecross on the west coast of Scotland for the first of the three BBC 'Great Escape' series, Monty Halls' Great Escape, where he tried to recreate the life of crofters for 6 months. In 2009 Halls then left for the Outer Hebrides to live and work as a nature warden on North Uist for six months. 2010 saw Monty live for six months in Connemara working with Irish Whale & Dolphin Conservation Group (IWDG). All of these series featured his dog Reuben.\n\n\n=== Charity work ===\nWith his background as a marine Halls was deeply moved when his close friend and best man, Major Jason Ward RM was killed in a helicopter crash on 21 March 2003, the second day of the Iraq War (Operation Telic). Halls supports military charities, in particular he is a patron of Help for Heroes, a charity committed to improving the facilities for injured servicemen and women. As a marine biologist he supports charities associated with the marine environment, and is also a patron of Shark Trust.\n\n\n=== Writing ===\nMonty Halls has written a number of books about diving, and writes a regular column for Dive Magazine in the UK.\n\n\n== Awards ==\nIn 2003, Halls was awarded the Bish Medal by the \"Scientific Exploration Society\" for his services to exploration. In December 2010, he was awarded an Honorary Doctor of Science Degree by Plymouth University.\n\n\n== Personal life ==\nHalls lives in Devon, England with his wife Tamsyn Smith and their daughter Isla Grace.\n\n\n== Filmography ==\nTelevision\nContestant and winner of Superhuman (2004) on Channel 4\nGreat Ocean Adventures (Season 1, nine episodes in 2005)\nGreat Ocean Adventures (Season 2, eight episodes 2007)\nPerfect Weapon (Six episodes in 2008)\nMonty Halls' Great Escape (BBC Two, from 1 March 2009; broadcast as Beachcomber Cottage in Australia and New Zealand)\nMonty Halls' Great Hebridean Escape (BBC Two, from 21 April 2010; broadcast as Monty Halls' Island Escape in Australia from 28 December 2012)\nMonty Halls' Great Irish Escape (BBC Two, from 11 August 2011)\nGreat Barrier Reef (BBC Two, 1\u201322 January 2012, 3 episodes)\nThe Fisherman's Apprentice (BBC Two, from 29 February 2012, 6 episodes)\n\n\n== References ==\n\n\n== External links ==\nMonty Halls' website\nMonty Halls at The Underwater TV Channel\nHelp for Heroes\nShark Trust\nScubazoo\nTalking about his 2013 series Mysteries of the Deep", 
                "titleUrl": "https://en.wikipedia.org/wiki/Monty_Halls", 
                "title": "Monty Halls"
            }, 
            {
                "snippet": "Business Park. Outside Lerwick there are Teaching Centres based in more remote parts of Shetland from which daily travel to Lerwick would not be practicable", 
                "pageCategories": "1970 establishments in Scotland\nAll stub articles\nEducation in Shetland\nEducational institutions established in 1970\nFurther education colleges in Scotland\nHigher education colleges in Scotland\nLerwick\nSchools in Shetland\nScotland education stubs\nScottish organisation stubs", 
                "pageContent": "Shetland College is a further and higher education college in Lerwick, Shetland, Scotland. It is part of the University of the Highlands and Islands.\nThe main campus is located at Gremista, on the outskirts of Lerwick. There are also dedicated premises in Lerwick for teaching Hospitality, at Anderson High School; and Health and Care at North Ness Business Park. Outside Lerwick there are Teaching Centres based in more remote parts of Shetland from which daily travel to Lerwick would not be practicable. These are to be found at Unst, Whalsay, North Mainland, West Mainland and Ness, in the south mainland.\nThe college also uses video conferencing to deliver some course material.\nThe college offers subsidised day-care facilities to students via a nursery near Anderson High School in Lerwick.\nA review of performance in 2014 by Education Scotland rated the college as effective and highlighted many positive findings.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Shetland_College", 
                "title": "Shetland College"
            }
        ], 
        "phraseCharStart": "727"
    }, 
    {
        "phraseCharEnd": "802", 
        "phraseIndex": "T15", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "tritium", 
        "wikiSearchResults": [
            {
                "snippet": "For Italian football team, see Tritium Calcio 1908. \"3H\" redirects here. For other uses, see 3H (disambiguation). Tritium (/\u02c8tr\u026ati\u0259m/ or /\u02c8tr\u026a\u0283i\u0259m/; symbol", 
                "pageCategories": "All articles needing additional references\nArticles needing additional references from June 2016\nCS1 maint: Multiple names: authors list\nEnvironmental isotopes\nIsotopes of hydrogen\nNuclear fusion fuels\nRadiochemistry\nRadioisotope fuels\nUse dmy dates from September 2010\nWikipedia articles with GND identifiers", 
                "pageContent": "Tritium (/\u02c8tr\u026ati\u0259m/ or /\u02c8tr\u026a\u0283i\u0259m/; symbol T or 3H, also known as hydrogen-3) is a radioactive isotope of hydrogen. The nucleus of tritium (sometimes called a triton) contains one proton and two neutrons, whereas the nucleus of protium (by far the most abundant hydrogen isotope) contains one proton and no neutrons. Naturally occurring tritium is extremely rare on Earth, where trace amounts are formed by the interaction of the atmosphere with cosmic rays. It is produced by irradiating lithium metal in a nuclear reactor. Tritium is used as a radioactive tracer, in radioluminescent light sources for watches and instruments, and in nuclear weapons. The name of this isotope is formed from the Greek word \u03c4\u03c1\u03af\u03c4\u03bf\u03c2 (tr\u00edtos) meaning \"third\".\n\n\n== Decay ==\nWhile tritium has several different experimentally determined values of its half-life, the National Institute of Standards and Technology lists 4,500 \u00b1 8 days (12.32 \u00b1 0.02 years). It decays into helium-3 by beta decay as in this nuclear equation:\n\nand it releases 18.6 keV of energy in the process. The electron's kinetic energy varies, with an average of 5.7 keV, while the remaining energy is carried off by the nearly undetectable electron antineutrino. Beta particles from tritium can penetrate only about 6.0 mm of air, and they are incapable of passing through the dead outermost layer of human skin. The unusually low energy released in the tritium beta decay makes the decay (along with that of rhenium-187) appropriate for absolute neutrino mass measurements in the laboratory (the most recent experiment being KATRIN).\nThe low energy of tritium's radiation makes it difficult to detect tritium-labeled compounds except by using liquid scintillation counting.\n\n\n== Production ==\n\n\n=== Lithium ===\nTritium is produced in nuclear reactors by neutron activation of lithium-6. This is possible with neutrons of any energy, and is an exothermic reaction yielding 4.8 MeV. In comparison, the fusion of deuterium with tritium releases about 17.6 MeV of energy.\n\nHigh-energy neutrons can also produce tritium from lithium-7 in an endothermic reaction, consuming 2.466 MeV. This was discovered when the 1954 Castle Bravo nuclear test produced an unexpectedly high yield.\n\nHigh-energy neutrons irradiating boron-10 will also occasionally produce tritium:\n\nA more common result of boron-10 neutron capture is 7Li and a single alpha particle.\nThe reactions requiring high neutron energies are not attractive production methods for peaceful applications.\n\n\n=== Deuterium ===\n\nTritium is also produced in heavy water-moderated reactors whenever a deuterium nucleus captures a neutron. This reaction has a quite small absorption cross section, making heavy water a good neutron moderator, and relatively little tritium is produced. Even so, cleaning tritium from the moderator may be desirable after several years to reduce the risk of its escaping to the environment. Ontario Power Generation's \"Tritium Removal Facility\" processes up to 2,500 tonnes (2,500 long tons; 2,800 short tons) of heavy water a year, and it separates out about 2.5 kg (5.5 lb) of tritium, making it available for other uses.\nDeuterium's absorption cross section for thermal neutrons is about 0.52 millibarns, whereas that of oxygen-16 (16\n8O) is about 0.19 millibarns and that of oxygen-17 (17\n8O) is about 240 millibarns.\n\n\n=== Fission ===\nTritium is an uncommon product of the nuclear fission of uranium-235, plutonium-239, and uranium-233, with a production of about one atom per each 10,000 fissions. The release or recovery of tritium needs to be considered in the operation of nuclear reactors, especially in the reprocessing of nuclear fuels and in the storage of spent nuclear fuel. The production of tritium is not a goal, but rather a side-effect.\n\n\n==== Fukushima Daiichi ====\n\nIn January 2014 it was made public that a total of 875 TBq (2.45 g) of tritium are on the site of Fukushima Daiichi, and the amount of tritium contained in the contaminated water is increasing by approximately 230 TBq (0.64 g) per year. According to a report by Tepco \"Tritium could be separated theoretically, but there is no practical separation technology on an industrial scale.\"\n\n\n=== Helium-3 and tritium ===\nTritium's decay product, helium-3, has a very large cross section for reacting with thermal neutrons, expelling a proton, hence it is rapidly converted back to tritium in nuclear reactors.\n3\n2He + n \u2192 1\n1H + 3\n1H\n\n\n=== Cosmic rays ===\nTritium occurs naturally due to cosmic rays interacting with atmospheric gases. In the most important reaction for natural production, a fast neutron (which must have energy greater than 4.0 MeV) interacts with atmospheric nitrogen:\n\nWorldwide, the production of tritium from natural sources is 148,000 terabecquerels per year. The global equilibrium inventory of tritium created by natural sources remains approximately constant at 2,590,000 terabecquerels. This is due to a fixed production rate and losses proportional to the inventory.\n\n\n=== Production history ===\nAccording to the Institute for Energy and Environmental Research report in 1996 about the U.S. Department of Energy, only 225 kg (496 lb) of tritium has been produced in the United States since 1955. Since it continually decays into helium-3, the total amount remaining was about 75 kg (165 lb) at the time of the report.\nTritium for American nuclear weapons was produced in special heavy water reactors at the Savannah River Site until their closures in 1988. With the Strategic Arms Reduction Treaty (START) after the end of the Cold War, the existing supplies were sufficient for the new, smaller number of nuclear weapons for some time.\nThe production of tritium was resumed with irradiation of rods containing lithium (replacing the usual control rods containing boron, cadmium, or hafnium), at the reactors of the commercial Watts Bar Nuclear Generating Station in 2003\u20132005 followed by extraction of tritium from the rods at the new Tritium Extraction Facility at the Savannah River Site beginning in November 2006. Tritium leakage from the rods during reactor operations limits the number that can be used in any reactor without exceeding the maximum allowed tritium levels in the coolant.\n\n\n== Properties ==\nTritium has an atomic mass of 3.0160492 u. It is a gas (T2 or 3H2) at standard temperature and pressure. It combines with oxygen to form a liquid called tritiated water, T2O.\nTritium's specific activity is 9,650 curies (3.57\u00d71014 Bq) per gram.\nTritium figures prominently in studies of nuclear fusion because of its favorable reaction cross section and the large amount of energy (17.6 MeV) produced through its reaction with deuterium:\n\nAll atomic nuclei, being composed of protons and neutrons, repel one another because of their positive charge. However, if the atoms have a high enough temperature and pressure (for example, in the core of the Sun), then their random motions can overcome such electrical repulsion (called the Coulomb force), and they can come close enough for the strong nuclear force to take effect, fusing them into heavier atoms.\nThe tritium nucleus, containing one proton and two neutrons, has the same charge as the nucleus of ordinary hydrogen, and it experiences the same electrostatic repulsive force when brought close to another atomic nucleus. However, the neutrons in the tritium nucleus increase the attractive strong nuclear force when brought close enough to another atomic nucleus. As a result, tritium can more easily fuse with other light atoms, compared with the ability of ordinary hydrogen to do so.\nThe same is true, albeit to a lesser extent, of deuterium. This is why brown dwarfs (so-called failed stars) cannot utilize ordinary hydrogen, but they do fuse the small minority of deuterium nuclei.\n\nLike the other isotopes of hydrogen, tritium is difficult to confine. Rubber, plastic, and some kinds of steel are all somewhat permeable. This has raised concerns that if tritium were used in large quantities, in particular for fusion reactors, it may contribute to radioactive contamination, although its short half-life should prevent significant long-term accumulation in the atmosphere.\nThe high levels of atmospheric nuclear weapons testing that took place prior to the enactment of the Partial Test Ban Treaty proved to be unexpectedly useful to oceanographers. The high levels of tritium oxide introduced into upper layers of the oceans have been used in the years since then to measure the rate of mixing of the upper layers of the oceans with their lower levels.\n\n\n== Health risks ==\nTritium is an isotope of hydrogen, which allows it to readily bind to hydroxyl radicals, forming tritiated water (HTO), and to carbon atoms. Since tritium is a low energy beta emitter, it is not dangerous externally (its beta particles are unable to penetrate the skin), but it can be a radiation hazard when inhaled, ingested via food or water, or absorbed through the skin. HTO has a short biological half-life in the human body of 7 to 14 days, which both reduces the total effects of single-incident ingestion and precludes long-term bioaccumulation of HTO from the environment. Biological half life of tritiated water in human body, which is a measure of body water turn over, varies with season. Studies on biological half life of occupational radiation workers for free water tritium in the coastal region of Karnataka, India show that the biological half life in winter season is twice that of the summer season.\n\n\n== Environmental contamination ==\nTritium has leaked from 48 of 65 nuclear sites in the US. In one case, leaking water contained 7.5 microcuries (0.28 MBq) of tritium per litre, which is 375 times the EPA limit for drinking water.\nThe US Nuclear Regulatory Commission states that in normal operation in 2003, 56 pressurized water reactors released 40,600 curies (1.50 PBq) of tritium (maximum: 2,080; minimum: 0.1; average: 725) and 24 boiling water reactors released 665 curies (24.6 TBq) (maximum: 174; minimum: 0; average: 27.7), in liquid effluents.\nAccording to the U.S. EPA, \"a recently documented source of tritium in the environment is [self-illuminating] exit signs that have been illegally disposed of in municipal landfills. Water, which seeps through the landfill, is contaminated with tritium from broken signs and can pass into water ways, carrying the tritium with it.\"\n\n\n=== Regulatory limits ===\nThe legal limits for tritium in drinking water vary from country to country. Some figures are given below:\nThe American limit is calculated to yield a dose of 4.0 millirems (or 40 microsieverts in SI units) per year. This is about 1.3% of the natural background radiation (roughly 3000 microsieverts).\n\n\n== Use ==\n\n\n=== Self-powered lighting ===\n\nThe emitted electrons from the radioactive decay of small amounts of tritium cause phosphors to glow so as to make self-powered lighting devices called betalights, which are now used in firearm night sights, watches, exit signs, map lights, knives and a variety of other devices. This takes the place of radium, which can cause bone cancer and has been banned in most countries for decades. Commercial demand for tritium is 400 grams per year and the cost is approximately US $30,000 per gram.\n\n\n=== Nuclear weapons ===\nTritium is an important component in nuclear weapons. It is used to enhance the efficiency and yield of fission bombs and the fission stages of hydrogen bombs in a process known as \"boosting\" as well as in external neutron initiators for such weapons.\n\n\n==== Neutron initiator ====\nActuated by an ultrafast switch like a krytron, a small particle accelerator drives ions of tritium and deuterium to energies above the 15 kilo-electron-volts or so needed for deuterium-tritium fusion and directs them into a metal target where the tritium and deuterium are adsorbed as hydrides. High-energy fusion neutrons from the resulting fusion radiate in all directions. Some of these strike plutonium or uranium nuclei in the primary's pit, initiating nuclear chain reaction. The quantity of neutrons produced is large in absolute numbers, allowing the pit to quickly achieve neutron levels that would otherwise need many more generations of chain reaction, though still small compared to the total number of nuclei in the pit.\n\n\n==== Boosting ====\n\nBefore detonation, a few grams of tritium-deuterium gas are injected into the hollow \"pit\" of fissile plutonium or uranium. The early stages of the fission chain reaction supply enough heat and compression to start deuterium-tritium fusion, then both fission and fusion proceed in parallel, the fission assisting the fusion by continuing heating and compression, and the fusion assisting the fission with highly energetic (14.1 MeV) neutrons. As the fission fuel depletes and also explodes outward, it falls below the density needed to stay critical by itself, but the fusion neutrons make the fission process progress faster and continue longer than it would without boosting. Increased yield comes overwhelmingly from the increase in fission. The energy released by the fusion itself is much smaller because the amount of fusion fuel is so much smaller. The effects of boosting include:\nincreased yield (for the same amount of fission fuel, compared to detonation without boosting)\nthe possibility of variable yield by varying the amount of fusion fuel\nallowing the bomb to require a smaller amount of the very expensive fissile material \u2013 and also eliminating the risk of predetonation by nearby nuclear explosions\nnot so stringent requirements on the implosion setup, allowing for a smaller and lighter amount of high-explosives to be used\nThe tritium in a warhead is continually undergoing radioactive decay, hence becoming unavailable for fusion. Furthermore its decay product, helium-3, absorbs neutrons if exposed to the ones emitted by nuclear fission. This potentially offsets or reverses the intended effect of the tritium, which was to generate many free neutrons, if too much helium-3 has accumulated from the decay of tritium. Therefore, it is necessary to replenish tritium in boosted bombs periodically. The estimated quantity needed is 4 grams per warhead. To maintain constant levels of tritium, about 0.20 grams per warhead per year must be supplied to the bomb.\nOne mole of deuterium-tritium gas would contain about 3.0 grams of tritium and 2.0 grams of deuterium. In comparison, the 20 moles of plutonium in a nuclear bomb consists of about 4.5 kilograms of plutonium-239.\n\n\n==== Tritium in hydrogen bomb secondaries ====\n\nSince tritium undergoes radioactive decay, and it is also difficult to confine physically, the much larger secondary charge of heavy hydrogen isotopes needed in a true hydrogen bomb uses solid lithium deuteride as its source of deuterium and tritium, where the lithium is all in the form of the lithium-6 isotope.\nDuring the detonation of the primary fission bomb stage in a thermonuclear weapon (Teller-Ullam staging), the sparkplug, a cylinder of U235/P239 at the center of the fusion stage(s), begins to fission in a chain reaction, from excess neutrons channeled from the primary. The neutrons released from the fission of the sparkplug split lithium-6 into tritium plus helium-4 and lithium-7 into helium-4 plus tritium plus one neutron. As these reactions occur, the fusion stage is compressed by photons from the primary and fission of the U-238 or U-238/U-235 jacket surrounding the fusion stage. Therefore, the fusion stage breeds its own tritium as the device detonates. In the extreme heat and pressure of the explosion, some of the tritium is then forced into fusion with deuterium, and that reaction releases even more neutrons.\nSince this fusion process requires an extremely high temperature for ignition, and it produces fewer and less energetic neutrons (only fission, deuterium-tritium fusion, and 7\n3Li splitting are net neutron producers), lithium deuteride is not used in boosted bombs, but rather, for multistage hydrogen bombs.\n\n\n=== Controlled nuclear fusion ===\nTritium is an important fuel for controlled nuclear fusion in both magnetic confinement and inertial confinement fusion reactor designs. The experimental fusion reactor ITER and the National Ignition Facility (NIF) will use deuterium-tritium fuel. The deuterium-tritium reaction is favorable since it has the largest fusion cross-section (about 5.0 barns) and it reaches this maximum cross-section at the lowest energy (about 65 keV center-of-mass) of any potential fusion fuel.\nThe Tritium Systems Test Assembly (TSTA) was a facility at the Los Alamos National Laboratory dedicated to the development and demonstration of technologies required for fusion-relevant deuterium-tritium processing.\n\n\n=== Analytical chemistry ===\nTritium is sometimes used as a radiolabel. It has the advantage that hydrogen appears in almost all organic chemicals making it easy to find a place to put tritium on the molecule under investigation. It has the disadvantage of producing a comparatively weak signal.\n\n\n== Use as an oceanic transient tracer ==\nAside from chlorofluorocarbons, tritium can act as a transient tracer and has the ability to \"outline\" the biological, chemical, and physical paths throughout the world oceans because of its evolving distribution. Tritium has thus been used as a tool to examine ocean circulation and ventilation and, for such purposes, is usually measured in Tritium Units where 1 TU is defined as the ratio of 1 tritium atom to 1018 hydrogen atoms. As noted earlier, nuclear weapons testing, primarily in the high-latitude regions of the Northern Hemisphere, throughout the late 1950s and early 1960s introduced large amounts of tritium into the atmosphere, especially the stratosphere. Before these nuclear tests, there were only about 3 to 4 kilograms of tritium on the Earth's surface; but these amounts rose by 2 or 3 orders of magnitude during the post-test period.\n\n\n=== North Atlantic Ocean ===\nWhile in the stratosphere (post-test period), the tritium interacted with and oxidized to water molecules and was present in much of the rapidly produced rainfall, making tritium a prognostic tool for studying the evolution and structure of the hydrologic cycle as well as the ventilation and formation of water masses in the North Atlantic Ocean. Bomb-tritium data were used from the Transient Tracers in the Ocean (TTO) program in order to quantify the replenishment and overturning rates for deep water located in the North Atlantic. Bomb-tritium also enters the deep ocean around the Antarctic. Most of the bomb tritiated water (HTO) throughout the atmosphere can enter the ocean through the following processes: a) precipitation, b) vapor exchange, and c) river runoff \u2013 these processes make HTO a great tracer for time-scales up to a few decades. Using the data from these processes for 1981, the 1 TU isosurface lies between 500 and 1,000 meters deep in the subtropical regions and then extends to 1,500\u20132,000 meters south of the Gulf Stream due to recirculation and ventilation in the upper portion of the Atlantic Ocean. To the north, the isosurface deepens and reaches the floor of the abyssal plain which is directly related to the ventilation of the ocean floor over 10 to 20 year time-scales.\nAlso evident in the Atlantic Ocean is the tritium profile near Bermuda between the late 1960s and late 1980s. There is a downward propagation of the tritium maximum from the surface (1960s) to 400 meters (1980s), which corresponds to a deepening rate of approximately 18 meters per year. There are also tritium increases at 1,500 meters depth in the late 1970s and 2,500 meters in the middle of the 1980s, both of which correspond to cooling events in the deep water and associated deep water ventilation.\nFrom a study in 1991, the tritium profile was used as a tool for studying the mixing and spreading of newly formed North Atlantic Deep Water (NADW), corresponding to tritium increases to 4 TU. This NADW tends to spill over sills that divide the Norwegian Sea from the North Atlantic Ocean and then flows to the west and equatorward in deep boundary currents. This process was explained via the large-scale tritium distribution in the deep North Atlantic between 1981 and 1983. The sub-polar gyre tends to be freshened (ventilated) by the NADW and is directly related to the high tritium values (> 1.5 TU). Also evident was the decrease in tritium in the deep western boundary current by a factor of 10 from the Labrador Sea to the Tropics, which is indicative of loss to ocean interior due to turbulent mixing and recirculation.\n\n\n=== Pacific and Indian Oceans ===\nIn a 1998 study, tritium concentrations in surface seawater and atmospheric water vapor (10 meters above the surface) were sampled at the following locations: the Sulu Sea, the Fremantle Bay, the Bay of Bengal, the Penang Bay, and the Strait of Malacca. Results indicated that the tritium concentration in surface seawater was highest at the Fremantle Bay (approximately 0.40 Bq/liter), which could be accredited to the mixing of runoff of freshwater from nearby lands due to large amounts found in coastal waters. Typically, lower concentrations were found between 35 and 45 degrees south latitude and near the equator. Results also indicated that (in general) tritium has decreased over the years (up to 1997) due to the physical decay of bomb tritium in the Indian Ocean. As for water vapor, the tritium concentration was approximately one order of magnitude greater than surface seawater concentrations (ranging from 0.46 to 1.15 Bq/liter). Therefore, the water vapor tritium is not affected by the surface seawater concentration; thus, the high tritium concentrations in the vapor were concluded to be a direct consequence of the downward movement of natural tritium from the stratosphere to the troposphere (therefore, the ocean air showed a dependence on latitudinal change).\nIn the North Pacific Ocean, the tritium (introduced as bomb tritium in the Northern Hemisphere) spread in three dimensions. There were subsurface maxima in the middle and low latitude regions, which is indicative of lateral mixing (advection) and diffusion processes along lines of constant potential density (isopycnals) in the upper ocean. Some of these maxima even correlate well with salinity extrema. In order to obtain the structure for ocean circulation, the tritium concentrations were mapped on 3 surfaces of constant potential density (23.90, 26.02, and 26.81). Results indicated that the tritium was well-mixed (at 6 to 7 TU) on the 26.81 isopycnal in the subarctic cyclonic gyre and there appeared to be a slow exchange of tritium (relative to shallower isopycnals) between this gyre and the anticyclonic gyre to the south; also, the tritium on the 23.90 and 26.02 surfaces appeared to be exchanged at a slower rate between the central gyre of the North Pacific and the equatorial regions.\nThe depth penetration of bomb tritium can be separated into 3 distinct layers. Layer 1 is the shallowest layer and includes the deepest, ventilated layer in winter; it has received tritium via radioactive fallout and lost some due to advection and/or vertical diffusion and contains approximately 28% of the total amount of tritium. Layer 2 is below the first layer but above the 26.81 isopycnal and is no longer part of the mixed layer. Its 2 sources are diffusion downward from the mixed layer and lateral expansions outcropping strata (poleward); it contains about 58% of the total tritium. Layer 3 is representative of waters that are deeper than the outcrop isopycnal and can only receive tritium via vertical diffusion; it contains the remaining 14% of the total tritium.\n\n\n=== Mississippi River System ===\nThe impacts of the nuclear fallout were felt in the United States throughout the Mississippi River System. Tritium concentrations can be used to understand the residence times of continental hydrologic systems (as opposed to the usual oceanic hydrologic systems) which include surface waters such as lakes, streams, and rivers. Studying these systems can also provide societies and municipals with information for agricultural purposes and overall river water quality.\nIn a 2004 study, several rivers were taken into account during the examination of tritium concentrations (starting in the 1960s) throughout the Mississippi River Basin: Ohio River (largest input to the Mississippi River flow), Missouri River, and Arkansas River. The largest tritium concentrations were found in 1963 at all the sampled locations throughout these rivers and correlate well with the peak concentrations in precipitation due to the nuclear bomb tests in 1962. The overall highest concentrations occurred in the Missouri River (1963) and were greater than 1,200 TU while the lowest concentrations were found in the Arkansas River (never greater than 850 TU and less than 10 TU in the mid-1980s).\nSeveral processes can be identified using the tritium data from the rivers: direct runoff and outflow of water from groundwater reservoirs. Using these processes, it becomes possible to model the response of the river basins to the transient tritium tracer. Two of the most common models are the following:\nPiston-flow approach \u2013 tritium signal appears immediately; and\nWell-mixed reservoir approach \u2013 outflow concentration depends upon the residence time of the basin water\nUnfortunately, both models fail to reproduce the tritium in river waters; thus, a two-member mixing model was developed that consists of 2 components: a prompt-flow component (recent precipitation \u2013 \"piston\") and a component where waters reside in the basin for longer than 1 year (\"well-mixed reservoir\"). Therefore, the basin tritium concentration becomes a function of the residence times within the basin, sinks (radioactive decay) or sources of tritium, and the input function.\nFor the Ohio River, the tritium data indicated that about 40% of the flow was composed of precipitation with residence times of less than 1 year (in the Ohio basin) and older waters consisted of residence times of about 10 years. Thus, the short residence times (less than 1 year) corresponded to the \"prompt-flow\" component of the two-member mixing model. As for the Missouri River, results indicated that residence times were approximately 4 years with the prompt-flow component being around 10% (these results are due to the series of dams in the area of the Missouri River).\nAs for the mass flux of tritium through the main stem of the Mississippi River into the Gulf of Mexico, data indicated that approximately 780 grams of tritium has flowed out of the River and into the Gulf between 1961 and 1997. And current fluxes through the Mississippi River are about 1 to 2 grams per year as opposed to the pre-bomb period fluxes of roughly 0.4 grams per year.\n\n\n== History ==\nTritium was first produced in 1934 from deuterium, another isotope of hydrogen, by Ernest Rutherford, Mark Oliphant, and Paul Harteck. However, their experiment could not isolate tritium, which was later accomplished by Luis Alvarez and Robert Cornog, who also realized tritium's radioactivity. Willard F. Libby recognized that tritium could be used for radiometric dating of water and wine.\n\n\n== See also ==\nHypertriton\nLuminox\nDeuterium\n\n\n== References ==\n\n\n== Bibliography ==\nB.M. Andreev, E. P. Magomedbekov, G.H. Sicking, Interaction of hydrogen isotopes with transition metals and intermetallic compounds, Springer Tracts in Modern physics, Springer, Berlin, (1996)\n\n\n== External links ==\nAnnotated bibliography for tritium from the Alsos Digital Library\nNLM Hazardous Substances Databank \u2013 Tritium, Radioactive\nNuclear Data Evaluation Lab\nReview of risks from tritium. Report of the independent Advisory Group on Ionising Radiation. Health Protection Agency. November 2007. RCE-4. \nTritium on Ice: The Dangerous New Alliance of Nuclear Weapons and Nuclear Power by Kenneth D. Bergeron\nTritium production and recovery in the United States in FY2011\nTritium removal mass transfer coefficient", 
                "titleUrl": "https://en.wikipedia.org/wiki/Tritium", 
                "title": "Tritium"
            }, 
            {
                "snippet": "Tritium illumination is the use of gaseous tritium, a radioactive isotope of hydrogen, to create visible light. Tritium emits electrons through beta decay", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from April 2015\nArticles with unsourced statements from August 2014\nArticles with unsourced statements from March 2014\nArticles with unsourced statements from September 2008\nLighting\nNuclear technology\nVague or ambiguous time from April 2015", 
                "pageContent": "Tritium illumination is the use of gaseous tritium, a radioactive isotope of hydrogen, to create visible light. Tritium emits electrons through beta decay, and, when they interact with a phosphor material, fluorescent light is created, a process called radioluminescence. As tritium illumination requires no electrical energy, it found wide use in applications such as emergency exit signs and illumination of wristwatches. More recently, many applications using radioactive materials have been replaced with photoluminescent materials.\n\n\n== Design ==\n\nTritium lighting is made using glass tubes with a phosphor layer in them and tritium gas inside the tube. Such a tube is known as a \"gaseous tritium light source\" (GTLS), or beta light, (since the tritium undergoes beta decay).\nThe tritium in a gaseous tritium light source undergoes beta decay, releasing electrons that cause the phosphor layer to fluoresce.\nDuring manufacture, a length of borosilicate glass tube that has had the inside surface coated with a phosphor-containing material is filled with the radioactive tritium. The tube is then fused with a carbon dioxide laser at the desired length. Borosilicate is preferred for its strength and resistance to breakage. In the tube, the tritium gives off a steady stream of electrons due to beta decay. These particles excite the phosphor, causing it to emit a low, steady glow. Tritium is not the only material that can be used for self-powered lighting. Other beta particle-emitting radioisotopes can also serve. Radium was used to make self-luminous paint from the early years of the 20th Century until approximately 1970, but it has been replaced by tritium, which is less hazardous.\nVarious preparations of the phosphor compound can be used to produce different colors of light. Some of the colors that have been manufactured in addition to the common phosphors are green, red, blue, yellow, purple, orange, and white.\nThe types of GTLS used in watches give off a small amount of light: not enough to be seen in daylight, but enough to be visible in the dark from a distance of several meters. The average such GTLS has a useful life of 10\u201320 years. As the tritium component of the lighting is often more expensive than the rest of the watch itself, manufacturers try to use as little as possible. Being an unstable isotope with a half-life of 12.32 years, tritium loses half its brightness in that period. The more tritium that is initially placed in the tube, the brighter it is to begin with, and the longer its useful life. Tritium exit signs usually come in three brightness levels guaranteed for 10-, 15-, or 20-year useful life expectancies. The difference between the signs is how much tritium the manufacturer installs.\nThe light produced by GTLSs varies in colour and size. Green is usually the brightest color and white the least bright. Sizes can be found ranging from tiny tubes small enough to fit on the hand of a watch to ones the size of a pencil. When it comes to large ones (5mm diameter and up to 100mm long), they are usually only found in green and can surprisingly be not as bright as the standard 22.5mm x 3mm sized tritium; this smaller size is usually the brightest and is used mainly in key chains available commercially.\n\n\n== Uses ==\n\nThese light sources are most often seen as \"permanent\" illumination for the hands of wristwatches intended for diving, nighttime, or combat use. They are also used in glowing novelty keychains and in self-illuminated exit signs. They are favored by the military for applications where a power source may not be available, such as for instrument dials in aircraft, compasses, and sights for weapons.\nTritium lights are also found in some old rotary dial telephones, though due to their age they no longer produce a useful amount of light. Tritium lights or beta lights were formerly used in fishing lures. Some flashlights have slots for tritium vials so that the flashlight can be easily located in the dark.\nTritium is used to illuminate the iron sights of some small arms. The reticle on the SA80's optical SUSAT sight as well as the LPS 4x6\u00b0 TIP2 telescopic sight of a PSL rifle, contains a small amount of tritium for the same effect as an example of tritium use on a rifle sight. The electrons emitted by the radioactive decay of the tritium cause phosphor to glow, thus providing a long-lasting (several years) and non-battery-powered firearms sight that is visible in dim lighting conditions. The tritium glow is not noticeable in bright conditions such as during daylight, however. As a result, some manufacturers have started to integrate fiber optic sights with tritium vials to provide bright, high-contrast firearms sights in both bright and dim conditions.\n\n\n== Safety ==\n\nWhile these devices contain a radioactive substance, it is currently believed that self-powered lighting does not pose a significant health concern. A 2007 report by the UK government's Health Protection Agency Advisory Group on Ionizing Radiation declared the health risks of tritium exposure to be double than previously set by the International Commission on Radiological Protection, but encapsulated tritium lighting devices, typically taking the form of a luminous glass tube embedded in a thick block of clear plastic, prevent the user from being exposed to the tritium at all unless the device is broken apart.\nTritium presents no external radiation threat via beta radiation when encapsulated in non-hydrogen-permeable containers due to its low penetration depth, which is insufficient to penetrate intact human skin. However, GTLS devices do emit low levels of X-rays due to bremsstrahlung. The primary danger from tritium arises if it is inhaled, ingested, injected, or absorbed into the body. This results in the absorption of the emitted radiation in a relatively small region of the body, again due to the low penetration depth. The biological half-life of tritium\u2014the time it takes for half of an ingested dose to be expelled from the body\u2014is low, at only 12 days. Tritium excretion can be accelerated further by increasing water intake to 3\u20134 liters/day.\nDirect, short-term exposure to small amounts of tritium is mostly harmless. If a tritium tube breaks, one should leave the area and allow the gas to diffuse into the air. Tritium exists naturally in the environment, but in very small quantities. Persons working with the gas face another hazard: tritium reacts with the oxygen in air, forming tritiated water. This moisture is readily ingested by the body. When in contact with any hydrocarbon the tritium atom replaces the natural hydrogen. This can even occur with the natural oil on a person's skin.\n\n\n== Legislation ==\nBecause tritium in particular is an integral part of certain thermonuclear devices (though in quantities several thousand times larger than that in a keychain), consumer and safety devices containing tritium for use in the United States are subject to certain possession, resale, disposal, and use restrictions. In the US, devices such as self-luminous exit signs, gauges, wrist watches, etc. that contain small amounts of tritium are under the jurisdiction of the Nuclear Regulatory Commission, and are subject to possession, distribution, and import and export regulations found in 10 CFR Parts, 30, 32, and 110. They are also subject to regulations for possession, use and disposal in certain states. Luminous products containing quantities of tritium greater than that which is used for a wrist watch are not widely available at retail outlets in the United States.\nThey are readily sold and used in the US and are widely available in the UK and are regulated in England and Wales by environmental health departments of local councils. Tritium lighting is legal in most of Asia and Australia.\n\n\n== See also ==\nExit sign\nList of light sources\n\n\n== References ==\n\n\n== External links ==\nCleanup of a broken tritium sign\nRadioluminescent items", 
                "titleUrl": "https://en.wikipedia.org/wiki/Tritium_illumination", 
                "title": "Tritium illumination"
            }, 
            {
                "snippet": "Tritium Calcio 1908 was an Italian association football club located in Trezzo sull'Adda, Lombardy. The noun Tritium comes from the Latin name of the city", 
                "pageCategories": "1908 establishments in Italy\nAll stub articles\nArticles with Italian-language external links\nAssociation football clubs established in 1908\nFootball clubs in Italy\nFootball clubs in Lombardy\nItalian football club stubs\nLega Pro Prima Divisione clubs\nLega Pro Seconda Divisione clubs\nUse dmy dates from January 2011", 
                "pageContent": "Tritium Calcio 1908 was an Italian association football club located in Trezzo sull'Adda, Lombardy.\nThe noun Tritium comes from the Latin name of the city in which it took the life activities of the club.\nThe last played season was in Promozione.\n\n\n== History ==\nThe club was born in 1908 as Societ\u00e0 Ginnastica Tritium with sections for all kinds of sports. The main activities were cyclistic journeys and football was played locally. Soon after the end of World War I their home ground was enlarged to allow meeting the Italian F.I.G.C. standards but up to 1927 main matches were against local amateurs. In 1925 the club changed name to Societ\u00e0 Sportiva and later to Fascio Giovanile di Combattimento and Associazione Sportiva Trezzo, due to the fascist influence up to 1945.\nThe football team always took part to regional amateur championships excepting two times when it was promoted to national amateur league (Serie D) in 1976 and 2005.\nIn the season 2010\u201311 from Lega Pro Seconda Divisione it was promoted to Lega Pro Prima Divisione with the head coach Stefano Vecchi.\nIn summer 2010 after the promotion to Lega Pro Seconda Divisione it changed its name to Tritium Calcio 1908. In the next season it was promoted to Lega Pro Prima Divisione.\nAt the end of the 2012-13 Lega Pro Prima Divisione season the club despite the conquest of salvation after play-off, didn't enroll in the 2013-14 Lega Pro Prima Divisione league, restarting from Promozione. At the end of the 2013\u201314 Promozione season the club didn't enroll in the 2014\u201315 Promozione league and dead. \n\n\n== Colors and badge ==\nIts colors are white and blue.\n\n\n== Youth Academy ==\nFrom the 2014's summer Tritium is reborn only for youth academy.\nTritium is known in his area for the youth academy.\nThe school football team Tritium is directed by the great former footballer Paolo Pulici. Here children from 5 to 9 years enjoy learn to play football.\nCurrently these are the teams that make up the field of youth\nPulcini to 5 1998\nPulcini to 5 1998\nPulcini to 7 1997 \u2013 1996\nPulcini to 7 1997\nPulcini to 9 1996\nDebut years 1995 B\nDebut years 1994 A\nTeenagers years 1993 Regional B\nTeenagers years 1992 Regional A\nStudents years 1990\u201391 Regional\nJuniors Team\n\n\n== Chronology ==\n\n\n== Current Structure ==\nPresident Floriana Mancini\nTeam Manager Nicola Bassani \nSecretary Mirko Rizzi\nInstitution Information Forza Tritium\n\n\n== References ==\n\n\n== External links ==\nOfficial Site (Italian)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Tritium_Calcio_1908", 
                "title": "Tritium Calcio 1908"
            }, 
            {
                "snippet": "KATRIN is a German acronym (Karlsruhe Tritium Neutrino Experiment) for an undertaking to measure the mass of the electron antineutrino with sub-eV precision", 
                "pageCategories": "All Wikipedia articles in need of updating\nCommons category with page title same as on Wikidata\nCoordinates on Wikidata\nNeutrino experiments\nUse dmy dates from September 2010\nWikipedia articles in need of updating from September 2010", 
                "pageContent": "KATRIN is a German acronym (Karlsruhe Tritium Neutrino Experiment) for an undertaking to measure the mass of the electron antineutrino with sub-eV precision by examining the spectrum of electrons emitted from the beta decay of tritium. The core of the apparatus is a 200-ton spectrometer. As of 2015, spectrometer had completed commissioning measurements, successfully verifying the spectrometer's basic vacuum, transmission and background properties.\n\n\n== Construction and assembly ==\nThe spectrometer was built by MAN DWE GmbH in Deggendorf and was shipped to Karlsruhe via an 8600 km route involving the Black Sea, the Mediterranean Sea, the Atlantic Ocean and the Rhine. This detour was necessary as shorter routes (the shortest route on land would have been only 350 km) could not been used due to the tank's size. The construction is proceeding well with several of the major components being already on-site. The main spectrometer test program will start in 2013 and the complete system integration is planned for 2014. The experiment is located at the former Forschungszentrum Karlsruhe, now Campus Nord of the Karlsruhe Institute of Technology.\n\n\n== Experiment ==\n\nThe beta decay of tritium is one of the least energetic beta decays. The electron and the neutrino which are emitted share only 18.6 keV of energy between them. KATRIN is designed to produce a very accurate spectrum of the numbers of electrons emitted with energies very close to this total energy (only a few eV away), which correspond to very low energy neutrinos. If the neutrino is a massless particle, there is no lower bound to the energy the neutrino can carry, so the electron energy spectrum should extend all the way to the 18.6 keV limit. On the other hand, if the neutrino has mass, then it must always carry away at least the amount of energy equivalent to its mass by E = mc2, and the electron spectrum should drop off short of the total energy limit and have a different shape.\nIn most beta decay events, the electron and the neutrino carry away roughly equal amounts of energy. The events of interest to KATRIN, in which the electron takes almost all the energy and the neutrino almost none, are very rare, occurring roughly once in a trillion decays. In order to filter out the common events so the detector is not overwhelmed, the electrons must pass through an electric potential that stops all electrons below a certain threshold, which is set a few eV below the total energy limit. Only electrons that have enough energy to pass through the potential are counted.\n\n\n== Importance ==\nThe precise mass of the neutrino is important not only for particle physics, but also for cosmology. The observation of neutrino oscillation is strong evidence in favor of massive neutrinos, but gives only a weak lower bound, which furthermore depends on whether the neutrino is its own antiparticle or not, i.e., whether it has Majorana mass or Dirac mass.\nAlong with the possible observation of neutrinoless double beta decay, KATRIN is one of the neutrino experiments most likely to yield significant results in the near future.\n\n\n== External links ==\nKATRIN homepage\nArrival of KATRIN main spectrometer at Karlsruhe (pictures)\nKATRIN design report 2004\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/KATRIN", 
                "title": "KATRIN"
            }, 
            {
                "snippet": "where the usual hydrogen atoms are replaced with tritium. In its pure form it may be called tritium oxide (T2O or 3H2O) or super-heavy water. Pure T2O", 
                "pageCategories": "All articles needing additional references\nArticles needing additional references from May 2013\nArticles with changed CASNo identifier\nArticles with changed ChemSpider identifier\nArticles with changed EBI identifier\nArticles with changed InChI identifier\nArticles without KEGG source\nArticles without UNII source\nBody water\nChemboxes which contain changes to verified fields", 
                "pageContent": "Tritiated water is a radioactive form of water where the usual hydrogen atoms are replaced with tritium. In its pure form it may be called tritium oxide (T2O or 3H2O) or super-heavy water. Pure T2O is corrosive due to self-radiolysis. Diluted, tritiated water is mainly H2O plus some HTO (3HOH). It is also used as a tracer for water transport studies in life-science research. Furthermore, since it naturally occurs in minute quantities, it can be used to determine the age of various water-based liquids, such as vintage wines.\nIt should not be confused with heavy water, which is deuterium oxide.\n\n\n== Applications ==\nTritiated water can be used to measure the total volume of water in one's body. Tritiated water distributes itself into all body compartments relatively quickly. The concentration of tritiated water in urine is assumed to be similar to the concentration of tritiated water in the body. Knowing the original amount of tritiated water that was ingested and the concentration, one can calculate the volume of water in the body.\nAmount of tritiated water (mg) = Concentration of tritiated water (mg/ml) x Volume of body water (ml)\nVolume of body water (ml) = [Amount of tritiated water (mg) - Amount excreted (mg)] / Concentration of tritiated water (mg/ml)\n\n\n== Adverse health effects ==\nTritiated water contains the radioactive hydrogen isotope tritium; as a low energy beta emitter with a half-life of about 12 years, it is not dangerous externally (its beta particles are unable to penetrate the skin), but it is a radiation hazard when inhaled, ingested via food or water, or absorbed through the skin. HTO has a short biological half-life in the human body of 7 to 14 days, which both reduces the total effects of single-incident ingestion and precludes long-term bioaccumulation of HTO from the environment. Biological half-life of tritiated water in the human body, which is a measure of body water turnover, varies with season. Studies on the biological half-life of occupational radiation workers for free water tritium in the coastal region of Karnataka, India show that the biological half-life in the winter season is twice that of the summer season.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Tritiated_water", 
                "title": "Tritiated water"
            }, 
            {
                "snippet": "ingested. Since tritium is a gas, if a tritium tube breaks, the gas dissipates in the air and is diluted to safe concentrations. Tritium has a half-life", 
                "pageCategories": "All articles lacking sources\nAll articles with unsourced statements\nArticles lacking sources from December 2009\nArticles with unsourced statements from August 2016\nLuminescence", 
                "pageContent": "Radioluminescence is the phenomenon by which light is produced in a material by bombardment with ionizing radiation such as alpha particles, beta particles, or gamma rays. Radioluminescence is used as a low level light source for night illumination of instruments or signage or other applications where light must be produced for long periods without external energy sources. Radioluminescent paint used to be used for clock hands and instrument dials, enabling them to be read in the dark. Radioluminescence is also sometimes seen around high-power radiation sources, such as nuclear reactors and radioisotopes.\n\n\n== Mechanism ==\nRadioluminescence occurs when an incoming radiation particle collides with an atom or molecule, exciting an orbital electron to a higher energy level. The particle usually comes from the radioactive decay of an atom of a radioisotope, an isotope of an element which is radioactive. The electron then returns to its ground energy level by emitting the extra energy as a photon of light. The emitted photon is often in the ultraviolet energy range or higher and so invisible to the human eye. Therefore, in radioluminescent light sources, the radioactive substance is mixed with a phosphor, a chemical that releases light of a particular color when struck by the particle.\n\n\n== Applications ==\n\nSince radioactivity was discovered around the turn of the 20th century, the main application of radioluminescence has been in radioluminescent paint, used on watch and compass dials, gunsights, aircraft flight instrument faces, and other instruments, to allow them to be seen in the dark. Radioluminescent paint consists of a mixture of a chemical containing a radioisotope with a radioluminescent chemical (phosphor). The continuous radioactive decay of the isotope's atoms releases radiation particles which strike the molecules of the phosphor, causing them to give off light. The constant bombardment by radioactive particles causes the chemical breakdown of many types of phosphor, so radioluminescent paints lose some of their luminosity over their working life.\n\n\n=== Tritium ===\n\nThe latest generation of radioluminescent materials is based on tritium, a radioactive isotope of hydrogen with half-life of 12.32 years that emits very low-energy beta radiation. It is used on wristwatch faces, gun sights, and emergency exit signs. The tritium gas is contained in a small glass tube, coated with a phosphor on the inside. Beta particles emitted by the tritium strike the phosphor coating and cause it to fluoresce, emitting light, usually yellow-green.\nTritium is used because it is believed to pose a negligible threat to human health, in contrast to the previous radioluminescent source, radium (below), which proved to be a significant radiological hazard. The low-energy 5.7 keV beta particles emitted by tritium cannot pass through the enclosing glass tube. Even if they could, they are not able to penetrate human skin. Tritium is only a health threat if ingested. Since tritium is a gas, if a tritium tube breaks, the gas dissipates in the air and is diluted to safe concentrations.\nTritium has a half-life of 12.3 years, so the brightness of a tritium light source will decline to half its initial value in that time.\n\n\n=== Promethium ===\n\nIn the second half of the 20th century, radium was progressively replaced with paint containing promethium-147. Promethium is a low-energy beta-emitter, which, unlike alpha emitters like radium does not degrade the phosphor lattice, so the luminosity of the material does not degrade so fast. It also does not emit the penetrating gamma rays which radium does. The half-life of 147Pm is only 2.62 years, so in a decade the radioactivity of a promethium dial will decline to only 1/16 of its original value, making it safer to dispose of, compared to radium with its half life of 1600 years. However this short half-life meant that the luminosity of promethium dials also dropped by half every 2.62 years, giving them a short useful life, which led to promethium's replacement by tritium (above).\nPromethium-based paint was used to illuminate Apollo Lunar Module electrical switch tips and painted on control panels of the Lunar Roving Vehicle.\n\n\n=== Radium ===\n\nBeginning in 1908, luminous paint containing a mixture of radium and copper-doped zinc sulfide was used to paint watch faces and instrument dials, giving a greenish glow. Phosphors containing copper-doped zinc sulfide (ZnS:Cu) yield blue-green light; copper and manganese-doped zinc sulfide (ZnS:Cu,Mn), yielding yellow-orange light, are also used. Radium-based luminescent paint is no longer used due to the radiation hazard posed to those manufacturing the dials. These phosphors are not suitable for use in layers thicker than 25 mg/cm2, as the self-absorption of the light then becomes a problem. Furthermore, zinc sulfide undergoes degradation of its crystal lattice structure, leading to gradual loss of brightness significantly faster than the depletion of radium.\nZnS:Ag coated spinthariscope screens were used by Ernest Rutherford in his experiments discovering the atomic nucleus.\nRadium was used in luminous paint until the 1960s, when it was replaced with the other radioisotopes above due to health concerns. In addition to alpha and beta rays, radium emits penetrating gamma rays, which can pass through the metal and glass of a watch dial, and skin. A typical older radium wristwatch dial has a radioactivity of 3-10 kBq and could expose its wearer to an annual dose of 24 millisieverts if worn continuously. Another health hazard is its decay product, the radioactive gas radon, which constitutes a significant risk even at extremely low concentrations when breathed. Radium's long half-life of 1600 years means that surfaces coated with radium paint, such as watch faces and hands, remain a health hazard long after their useful life is over. There are still millions of luminous radium clock, watch, and compass faces and aircraft instrument dials owned by the public. The case of the \"Radium Girls\", workers in watch factories in the early 1920s who painted watch faces with radium paint and later contracted fatal cancer through ingesting radium when they pointed their brushes with their lips, increased public awareness of the hazards of radioluminescent materials, and radioactivity in general.\n\n\n== See also ==\nCherenkov radiation\nGas mantle\nList of light sources\nUndark\nRadium Dial Company\nUnited States Radium Corporation\nRadioluminescence in phosphor\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Radioluminescence", 
                "title": "Radioluminescence"
            }, 
            {
                "snippet": "getting out. Small amounts of tritium are produced naturally by the interaction of cosmic rays with atmospheric gases; tritium has also been released during", 
                "pageCategories": "Airship technology\nBiology and pharmacology of chemical elements\nCS1 maint: Multiple names: authors list\nChemical elements\nDiatomic nonmetals\nFeatured articles\nGaseous signaling molecules\nHydrogen\nNuclear fusion fuels\nReducing agents", 
                "pageContent": "Hydrogen is a chemical element with chemical symbol H and atomic number 1. With an atomic weight of 7000100794000000000\u26601.00794 u, hydrogen is the lightest element on the periodic table. Its monatomic form (H) is the most abundant chemical substance in the Universe, constituting roughly 75% of all baryonic mass. Non-remnant stars are mainly composed of hydrogen in the plasma state. The most common isotope of hydrogen, termed protium (name rarely used, symbol 1H), has one proton and no neutrons.\nThe universal emergence of atomic hydrogen first occurred during the recombination epoch. At standard temperature and pressure, hydrogen is a colorless, odorless, tasteless, non-toxic, nonmetallic, highly combustible diatomic gas with the molecular formula H2. Since hydrogen readily forms covalent compounds with most nonmetallic elements, most of the hydrogen on Earth exists in molecular forms such as water or organic compounds. Hydrogen plays a particularly important role in acid\u2013base reactions because most acid-base reactions involve the exchange of protons between soluble molecules. In ionic compounds, hydrogen can take the form of a negative charge (i.e., anion) when it is known as a hydride, or as a positively charged (i.e., cation) species denoted by the symbol H+. The hydrogen cation is written as though composed of a bare proton, but in reality, hydrogen cations in ionic compounds are always more complex. As the only neutral atom for which the Schr\u00f6dinger equation can be solved analytically, study of the energetics and bonding of the hydrogen atom has played a key role in the development of quantum mechanics.\nHydrogen gas was first artificially produced in the early 16th century by the reaction of acids on metals. In 1766\u201381, Henry Cavendish was the first to recognize that hydrogen gas was a discrete substance, and that it produces water when burned, the property for which it was later named: in Greek, hydrogen means \"water-former\".\nIndustrial production is mainly from steam reforming natural gas, and less often from more energy-intensive methods such as the electrolysis of water. Most hydrogen is used near the site of its production site, the two largest uses being fossil fuel processing (e.g., hydrocracking) and ammonia production, mostly for the fertilizer market. Hydrogen is a concern in metallurgy as it can embrittle many metals, complicating the design of pipelines and storage tanks.\n\n\n== PropertiesEdit ==\n\n\n=== CombustionEdit ===\n\nHydrogen gas (dihydrogen or molecular hydrogen) is highly flammable and will burn in air at a very wide range of concentrations between 4% and 75% by volume. The enthalpy of combustion is \u2212286 kJ/mol:\n2 H2(g) + O2(g) \u2192 2 H2O(l) + 572 kJ (286 kJ/mol)\nHydrogen gas forms explosive mixtures with air in concentrations from 4\u201374% and with chlorine at 5\u201395%. The explosive reactions may be triggered by spark, heat, or sunlight. The hydrogen autoignition temperature, the temperature of spontaneous ignition in air, is 500 \u00b0C (932 \u00b0F). Pure hydrogen-oxygen flames emit ultraviolet light and with high oxygen mix are nearly invisible to the naked eye, as illustrated by the faint plume of the Space Shuttle Main Engine, compared to the highly visible plume of a Space Shuttle Solid Rocket Booster, which uses an ammonium perchlorate composite. The detection of a burning hydrogen leak may require a flame detector; such leaks can be very dangerous. Hydrogen flames in other conditions are blue, resembling blue natural gas flames.\nThe destruction of the Hindenburg airship was a notorious example of hydrogen combustion and the cause is still debated. The visible orange flames in that incident were the result of a rich mixture of hydrogen to oxygen combined with carbon compounds from the airship skin.\nH2 reacts with every oxidizing element. Hydrogen can react spontaneously and violently at room temperature with chlorine and fluorine to form the corresponding hydrogen halides, hydrogen chloride and hydrogen fluoride, which are also potentially dangerous acids.\n\n\n=== Electron energy levelsEdit ===\n\nThe ground state energy level of the electron in a hydrogen atom is \u221213.6 eV, which is equivalent to an ultraviolet photon of roughly 91 nm wavelength.\nThe energy levels of hydrogen can be calculated fairly accurately using the Bohr model of the atom, which conceptualizes the electron as \"orbiting\" the proton in analogy to the Earth's orbit of the Sun. However, the atomic electron and proton are held together by electromagnetic force, while planets and celestial objects are held by gravity. Because of the discretization of angular momentum postulated in early quantum mechanics by Bohr, the electron in the Bohr model can only occupy certain allowed distances from the proton, and therefore only certain allowed energies.\nA more accurate description of the hydrogen atom comes from a purely quantum mechanical treatment that uses the Schr\u00f6dinger equation, Dirac equation or even the Feynman path integral formulation to calculate the probability density of the electron around the proton. The most complicated treatments allow for the small effects of special relativity and vacuum polarization. In the quantum mechanical treatment, the electron in a ground state hydrogen atom has no angular momentum at all\u2014illustrating how the \"planetary orbit\" differs from electron motion.\n\n\n=== Elemental molecular formsEdit ===\n\nThere exist two different spin isomers of hydrogen diatomic molecules that differ by the relative spin of their nuclei. In the orthohydrogen form, the spins of the two protons are parallel and form a triplet state with a molecular spin quantum number of 1 (\u200a1\u20442+\u200a1\u20442); in the parahydrogen form the spins are antiparallel and form a singlet with a molecular spin quantum number of 0 (\u200a1\u20442\u2013\u200a1\u20442). At standard temperature and pressure, hydrogen gas contains about 25% of the para form and 75% of the ortho form, also known as the \"normal form\". The equilibrium ratio of orthohydrogen to parahydrogen depends on temperature, but because the ortho form is an excited state and has a higher energy than the para form, it is unstable and cannot be purified. At very low temperatures, the equilibrium state is composed almost exclusively of the para form. The liquid and gas phase thermal properties of pure parahydrogen differ significantly from those of the normal form because of differences in rotational heat capacities, as discussed more fully in spin isomers of hydrogen. The ortho/para distinction also occurs in other hydrogen-containing molecules or functional groups, such as water and methylene, but is of little significance for their thermal properties.\nThe uncatalyzed interconversion between para and ortho H2 increases with increasing temperature; thus rapidly condensed H2 contains large quantities of the high-energy ortho form that converts to the para form very slowly. The ortho/para ratio in condensed H2 is an important consideration in the preparation and storage of liquid hydrogen: the conversion from ortho to para is exothermic and produces enough heat to evaporate some of the hydrogen liquid, leading to loss of liquefied material. Catalysts for the ortho-para interconversion, such as ferric oxide, activated carbon, platinized asbestos, rare earth metals, uranium compounds, chromic oxide, or some nickel compounds, are used during hydrogen cooling.\n\n\n=== PhasesEdit ===\nGaseous hydrogen\nLiquid hydrogen\nSlush hydrogen\nSolid hydrogen\nMetallic hydrogen\n\n\n=== CompoundsEdit ===\n\n\n==== Covalent and organic compoundsEdit ====\nWhile H2 is not very reactive under standard conditions, it does form compounds with most elements. Hydrogen can form compounds with elements that are more electronegative, such as halogens (e.g., F, Cl, Br, I), or oxygen; in these compounds hydrogen takes on a partial positive charge. When bonded to fluorine, oxygen, or nitrogen, hydrogen can participate in a form of medium-strength noncovalent bonding with the hydrogen of other similar molecules, a phenomenon called hydrogen bonding that is critical to the stability of many biological molecules. Hydrogen also forms compounds with less electronegative elements, such as metals and metalloids, where it takes on a partial negative charge. These compounds are often known as hydrides.\nHydrogen forms a vast array of compounds with carbon called the hydrocarbons, and an even vaster array with heteroatoms that, because of their general association with living things, are called organic compounds. The study of their properties is known as organic chemistry and their study in the context of living organisms is known as biochemistry. By some definitions, \"organic\" compounds are only required to contain carbon. However, most of them also contain hydrogen, and because it is the carbon-hydrogen bond which gives this class of compounds most of its particular chemical characteristics, carbon-hydrogen bonds are required in some definitions of the word \"organic\" in chemistry. Millions of hydrocarbons are known, and they are usually formed by complicated synthetic pathways that seldom involve elementary hydrogen.\n\n\n==== HydridesEdit ====\n\nCompounds of hydrogen are often called hydrides, a term that is used fairly loosely. The term \"hydride\" suggests that the H atom has acquired a negative or anionic character, denoted H\u2212, and is used when hydrogen forms a compound with a more electropositive element. The existence of the hydride anion, suggested by Gilbert N. Lewis in 1916 for group 1 and 2 salt-like hydrides, was demonstrated by Moers in 1920 by the electrolysis of molten lithium hydride (LiH), producing a stoichiometry quantity of hydrogen at the anode. For hydrides other than group 1 and 2 metals, the term is quite misleading, considering the low electronegativity of hydrogen. An exception in group 2 hydrides is BeH\n2, which is polymeric. In lithium aluminium hydride, the AlH\u2212\n4 anion carries hydridic centers firmly attached to the Al(III).\nAlthough hydrides can be formed with almost all main-group elements, the number and combination of possible compounds varies widely; for example, more than 100 binary borane hydrides are known, but only one binary aluminium hydride. Binary indium hydride has not yet been identified, although larger complexes exist.\nIn inorganic chemistry, hydrides can also serve as bridging ligands that link two metal centers in a coordination complex. This function is particularly common in group 13 elements, especially in boranes (boron hydrides) and aluminium complexes, as well as in clustered carboranes.\n\n\n==== Protons and acidsEdit ====\n\nOxidation of hydrogen removes its electron and gives H+, which contains no electrons and a nucleus which is usually composed of one proton. That is why H+ is often called a proton. This species is central to discussion of acids. Under the Bronsted-Lowry theory, acids are proton donors, while bases are proton acceptors.\nA bare proton, H+, cannot exist in solution or in ionic crystals because of its unstoppable attraction to other atoms or molecules with electrons. Except at the high temperatures associated with plasmas, such protons cannot be removed from the electron clouds of atoms and molecules, and will remain attached to them. However, the term 'proton' is sometimes used loosely and metaphorically to refer to positively charged or cationic hydrogen attached to other species in this fashion, and as such is denoted \"H+\" without any implication that any single protons exist freely as a species.\nTo avoid the implication of the naked \"solvated proton\" in solution, acidic aqueous solutions are sometimes considered to contain a less unlikely fictitious species, termed the \"hydronium ion\" (H\n3O+). However, even in this case, such solvated hydrogen cations are more realistically conceived as being organized into clusters that form species closer to H\n9O+\n4. Other oxonium ions are found when water is in acidic solution with other solvents.\nAlthough exotic on Earth, one of the most common ions in the universe is the H+\n3 ion, known as protonated molecular hydrogen or the trihydrogen cation.\n\n\n=== IsotopesEdit ===\n\nHydrogen has three naturally occurring isotopes, denoted 1H, 2H and 3H. Other, highly unstable nuclei (4H to 7H) have been synthesized in the laboratory but not observed in nature.\n1H is the most common hydrogen isotope with an abundance of more than 99.98%. Because the nucleus of this isotope consists of only a single proton, it is given the descriptive but rarely used formal name protium.\n2H, the other stable hydrogen isotope, is known as deuterium and contains one proton and one neutron in the nucleus. All deuterium in the universe is thought to have been produced at the time of the Big Bang, and has endured since that time. Deuterium is not radioactive, and does not represent a significant toxicity hazard. Water enriched in molecules that include deuterium instead of normal hydrogen is called heavy water. Deuterium and its compounds are used as a non-radioactive label in chemical experiments and in solvents for 1H-NMR spectroscopy. Heavy water is used as a neutron moderator and coolant for nuclear reactors. Deuterium is also a potential fuel for commercial nuclear fusion.\n3H is known as tritium and contains one proton and two neutrons in its nucleus. It is radioactive, decaying into helium-3 through beta decay with a half-life of 12.32 years. It is so radioactive that it can be used in luminous paint, making it useful in such things as watches. The glass prevents the small amount of radiation from getting out. Small amounts of tritium are produced naturally by the interaction of cosmic rays with atmospheric gases; tritium has also been released during nuclear weapons tests. It is used in nuclear fusion reactions, as a tracer in isotope geochemistry, and in specialized self-powered lighting devices. Tritium has also been used in chemical and biological labeling experiments as a radiolabel.\nHydrogen is the only element that has different names for its isotopes in common use today. During the early study of radioactivity, various heavy radioactive isotopes were given their own names, but such names are no longer used, except for deuterium and tritium. The symbols D and T (instead of 2H and 3H) are sometimes used for deuterium and tritium, but the corresponding symbol for protium, P, is already in use for phosphorus and thus is not available for protium. In its nomenclatural guidelines, the International Union of Pure and Applied Chemistry allows any of D, T, 2H, and 3H to be used, although 2H and 3H are preferred.\n\n\n== HistoryEdit ==\n\n\n=== Discovery and useEdit ===\n\nIn 1671, Robert Boyle discovered and described the reaction between iron filings and dilute acids, which results in the production of hydrogen gas. In 1766, Henry Cavendish was the first to recognize hydrogen gas as a discrete substance, by naming the gas from a metal-acid reaction \"inflammable air\". He speculated that \"inflammable air\" was in fact identical to the hypothetical substance called \"phlogiston\" and further finding in 1781 that the gas produces water when burned. He is usually given credit for the discovery of hydrogen as an element. In 1783, Antoine Lavoisier gave the element the name hydrogen (from the Greek \u1f51\u03b4\u03c1\u03bf- hydro meaning \"water\" and -\u03b3\u03b5\u03bd\u03ae\u03c2 genes meaning \"creator\") when he and Laplace reproduced Cavendish's finding that water is produced when hydrogen is burned.\n\nLavoisier produced hydrogen for his experiments on mass conservation by reacting a flux of steam with metallic iron through an incandescent iron tube heated in a fire. Anaerobic oxidation of iron by the protons of water at high temperature can be schematically represented by the set of following reactions:\n   Fe +    H2O \u2192 FeO + H2\n2 Fe + 3 H2O \u2192 Fe2O3 + 3 H2\n3 Fe + 4 H2O \u2192 Fe3O4 + 4 H2\nMany metals such as zirconium undergo a similar reaction with water leading to the production of hydrogen.\nHydrogen was liquefied for the first time by James Dewar in 1898 by using regenerative cooling and his invention, the vacuum flask. He produced solid hydrogen the next year. Deuterium was discovered in December 1931 by Harold Urey, and tritium was prepared in 1934 by Ernest Rutherford, Mark Oliphant, and Paul Harteck. Heavy water, which consists of deuterium in the place of regular hydrogen, was discovered by Urey's group in 1932. Fran\u00e7ois Isaac de Rivaz built the first de Rivaz engine, an internal combustion engine powered by a mixture of hydrogen and oxygen in 1806. Edward Daniel Clarke invented the hydrogen gas blowpipe in 1819. The D\u00f6bereiner's lamp and limelight were invented in 1823.\nThe first hydrogen-filled balloon was invented by Jacques Charles in 1783. Hydrogen provided the lift for the first reliable form of air-travel following the 1852 invention of the first hydrogen-lifted airship by Henri Giffard. German count Ferdinand von Zeppelin promoted the idea of rigid airships lifted by hydrogen that later were called Zeppelins; the first of which had its maiden flight in 1900. Regularly scheduled flights started in 1910 and by the outbreak of World War I in August 1914, they had carried 35,000 passengers without a serious incident. Hydrogen-lifted airships were used as observation platforms and bombers during the war.\nThe first non-stop transatlantic crossing was made by the British airship R34 in 1919. Regular passenger service resumed in the 1920s and the discovery of helium reserves in the United States promised increased safety, but the U.S. government refused to sell the gas for this purpose. Therefore, H2 was used in the Hindenburg airship, which was destroyed in a midair fire over New Jersey on 6 May 1937. The incident was broadcast live on radio and filmed. Ignition of leaking hydrogen is widely assumed to be the cause, but later investigations pointed to the ignition of the aluminized fabric coating by static electricity. But the damage to hydrogen's reputation as a lifting gas was already done.\nIn the same year the first hydrogen-cooled turbogenerator went into service with gaseous hydrogen as a coolant in the rotor and the stator in 1937 at Dayton, Ohio, by the Dayton Power & Light Co.; because of the thermal conductivity of hydrogen gas, this is the most common type in its field today.\nThe nickel hydrogen battery was used for the first time in 1977 aboard the U.S. Navy's Navigation technology satellite-2 (NTS-2). For example, the ISS, Mars Odyssey and the Mars Global Surveyor are equipped with nickel-hydrogen batteries. In the dark part of its orbit, the Hubble Space Telescope is also powered by nickel-hydrogen batteries, which were finally replaced in May 2009, more than 19 years after launch and 13 years beyond their design life.\n\n\n=== Role in quantum theoryEdit ===\n\nBecause of its simple atomic structure, consisting only of a proton and an electron, the hydrogen atom, together with the spectrum of light produced from it or absorbed by it, has been central to the development of the theory of atomic structure. Furthermore, study of the corresponding simplicity of the hydrogen molecule and the corresponding cation H+\n2 brought understanding of the nature of the chemical bond, which followed shortly after the quantum mechanical treatment of the hydrogen atom had been developed in the mid-1920s.\nOne of the first quantum effects to be explicitly noticed (but not understood at the time) was a Maxwell observation involving hydrogen, half a century before full quantum mechanical theory arrived. Maxwell observed that the specific heat capacity of H2 unaccountably departs from that of a diatomic gas below room temperature and begins to increasingly resemble that of a monatomic gas at cryogenic temperatures. According to quantum theory, this behavior arises from the spacing of the (quantized) rotational energy levels, which are particularly wide-spaced in H2 because of its low mass. These widely spaced levels inhibit equal partition of heat energy into rotational motion in hydrogen at low temperatures. Diatomic gases composed of heavier atoms do not have such widely spaced levels and do not exhibit the same effect.\nAntihydrogen (H) is the antimatter counterpart to hydrogen. It consists of an antiproton with a positron. Antihydrogen is the only type of antimatter atom to have been produced as of 2015.\n\n\n== Natural occurrenceEdit ==\n\nHydrogen, as atomic H, is the most abundant chemical element in the universe, making up 75% of normal matter by mass and more than 90% by number of atoms. (Most of the mass of the universe, however, is not in the form of chemical-element type matter, but rather is postulated to occur as yet-undetected forms of mass such as dark matter and dark energy.) This element is found in great abundance in stars and gas giant planets. Molecular clouds of H2 are associated with star formation. Hydrogen plays a vital role in powering stars through the proton-proton reaction and the CNO cycle nuclear fusion.\nThroughout the universe, hydrogen is mostly found in the atomic and plasma states, with properties quite different from those of molecular hydrogen. As a plasma, hydrogen's electron and proton are not bound together, resulting in very high electrical conductivity and high emissivity (producing the light from the Sun and other stars). The charged particles are highly influenced by magnetic and electric fields. For example, in the solar wind they interact with the Earth's magnetosphere giving rise to Birkeland currents and the aurora. Hydrogen is found in the neutral atomic state in the interstellar medium. The large amount of neutral hydrogen found in the damped Lyman-alpha systems is thought to dominate the cosmological baryonic density of the Universe up to redshift z=4.\nUnder ordinary conditions on Earth, elemental hydrogen exists as the diatomic gas, H2. However, hydrogen gas is very rare in the Earth's atmosphere (1 ppm by volume) because of its light weight, which enables it to escape from Earth's gravity more easily than heavier gases. However, hydrogen is the third most abundant element on the Earth's surface, mostly in the form of chemical compounds such as hydrocarbons and water. Hydrogen gas is produced by some bacteria and algae and is a natural component of flatus, as is methane, itself a hydrogen source of increasing importance.\nA molecular form called protonated molecular hydrogen (H+\n3) is found in the interstellar medium, where it is generated by ionization of molecular hydrogen from cosmic rays. This charged ion has also been observed in the upper atmosphere of the planet Jupiter. The ion is relatively stable in the environment of outer space due to the low temperature and density. H+\n3 is one of the most abundant ions in the Universe, and it plays a notable role in the chemistry of the interstellar medium. Neutral triatomic hydrogen H3 can exist only in an excited form and is unstable. By contrast, the positive hydrogen molecular ion (H+\n2) is a rare molecule in the universe.\n\n\n== ProductionEdit ==\n\nH\n2 is produced in chemistry and biology laboratories, often as a by-product of other reactions; in industry for the hydrogenation of unsaturated substrates; and in nature as a means of expelling reducing equivalents in biochemical reactions.\n\n\n=== Steam reformingEdit ===\nHydrogen can be prepared in several different ways, but economically the most important processes involve removal of hydrogen from hydrocarbons, as about 95% of hydrogen production came from steam reforming around year 2000. Commercial bulk hydrogen is usually produced by the steam reforming of natural gas. At high temperatures (1000\u20131400 K, 700\u20131100 \u00b0C or 1300\u20132000 \u00b0F), steam (water vapor) reacts with methane to yield carbon monoxide and H\n2.\nCH\n4 + H\n2O \u2192 CO + 3 H\n2\nThis reaction is favored at low pressures but is nonetheless conducted at high pressures (2.0  MPa, 20 atm or 600 inHg). This is because high-pressure H\n2 is the most marketable product and Pressure Swing Adsorption (PSA) purification systems work better at higher pressures. The product mixture is known as \"synthesis gas\" because it is often used directly for the production of methanol and related compounds. Hydrocarbons other than methane can be used to produce synthesis gas with varying product ratios. One of the many complications to this highly optimized technology is the formation of coke or carbon:\nCH\n4 \u2192 C + 2 H\n2\nConsequently, steam reforming typically employs an excess of H\n2O. Additional hydrogen can be recovered from the steam by use of carbon monoxide through the water gas shift reaction, especially with an iron oxide catalyst. This reaction is also a common industrial source of carbon dioxide:\nCO + H\n2O \u2192 CO\n2 + H\n2\nOther important methods for H\n2 production include partial oxidation of hydrocarbons:\n2 CH\n4 + O\n2 \u2192 2 CO + 4 H\n2\nand the coal reaction, which can serve as a prelude to the shift reaction above:\nC + H\n2O \u2192 CO + H\n2\nHydrogen is sometimes produced and consumed in the same industrial process, without being separated. In the Haber process for the production of ammonia, hydrogen is generated from natural gas. Electrolysis of brine to yield chlorine also produces hydrogen as a co-product.\n\n\n=== Metal-acidEdit ===\nIn the laboratory, H\n2 is usually prepared by the reaction of dilute non-oxidizing acids on some reactive metals such as zinc with Kipp's apparatus.\nZn + 2 H+ \u2192 Zn2+ + H\n2\nAluminium can also produce H\n2 upon treatment with bases:\n2 Al + 6 H\n2O + 2 OH\u2212 \u2192 2 Al(OH)\u2212\n4 + 3 H\n2\nThe electrolysis of water is a simple method of producing hydrogen. A low voltage current is run through the water, and gaseous oxygen forms at the anode while gaseous hydrogen forms at the cathode. Typically the cathode is made from platinum or another inert metal when producing hydrogen for storage. If, however, the gas is to be burnt on site, oxygen is desirable to assist the combustion, and so both electrodes would be made from inert metals. (Iron, for instance, would oxidize, and thus decrease the amount of oxygen given off.) The theoretical maximum efficiency (electricity used vs. energetic value of hydrogen produced) is in the range 80\u201394%.\n2 H\n2O(l) \u2192 2 H\n2(g) + O\n2(g)\nAn alloy of aluminium and gallium in pellet form added to water can be used to generate hydrogen. The process also produces alumina, but the expensive gallium, which prevents the formation of an oxide skin on the pellets, can be re-used. This has important potential implications for a hydrogen economy, as hydrogen can be produced on-site and does not need to be transported.\n\n\n=== ThermochemicalEdit ===\nThere are more than 200 thermochemical cycles which can be used for water splitting, around a dozen of these cycles such as the iron oxide cycle, cerium(IV) oxide\u2013cerium(III) oxide cycle, zinc zinc-oxide cycle, sulfur-iodine cycle, copper-chlorine cycle and hybrid sulfur cycle are under research and in testing phase to produce hydrogen and oxygen from water and heat without using electricity. A number of laboratories (including in France, Germany, Greece, Japan, and the USA) are developing thermochemical methods to produce hydrogen from solar energy and water.\n\n\n=== Anaerobic corrosionEdit ===\nUnder anaerobic conditions, iron and steel alloys are slowly oxidized by the protons of water concomitantly reduced in molecular hydrogen (H\n2). The anaerobic corrosion of iron leads first to the formation of ferrous hydroxide (green rust) and can be described by the following reaction:\nFe + 2 H\n2O \u2192 Fe(OH)\n2 + H\n2\nIn its turn, under anaerobic conditions, the ferrous hydroxide (Fe(OH)\n2 ) can be oxidized by the protons of water to form magnetite and molecular hydrogen. This process is described by the Schikorr reaction:\n3 Fe(OH)\n2 \u2192 Fe\n3O\n4 + 2 H\n2O + H\n2\nferrous hydroxide \u2192 magnetite + water + hydrogen\nThe well crystallized magnetite (Fe\n3O\n4) is thermodynamically more stable than the ferrous hydroxide (Fe(OH)\n2 ).\nThis process occurs during the anaerobic corrosion of iron and steel in oxygen-free groundwater and in reducing soils below the water table.\n\n\n=== Geological occurrence: the serpentinization reactionEdit ===\nIn the absence of atmospheric oxygen (O\n2), in deep geological conditions prevailing far away from Earth atmosphere, hydrogen (H\n2) is produced during the process of serpentinization by the anaerobic oxidation by the water protons (H+) of the ferrous (Fe2+) silicate present in the crystal lattice of the fayalite (Fe\n2SiO\n4, the olivine iron-endmember). The corresponding reaction leading to the formation of magnetite (Fe\n3O\n4), quartz (SiO\n2) and hydrogen (H\n2) is the following:\n3Fe\n2SiO\n4 + 2 H\n2O \u2192 2 Fe\n3O\n4 + 3 SiO\n2 + 3 H\n2\nfayalite + water \u2192 magnetite + quartz + hydrogen\nThis reaction closely resembles the Schikorr reaction observed in the anaerobic oxidation of the ferrous hydroxide in contact with water.\n\n\n=== Formation in transformersEdit ===\nFrom all the fault gases formed in power transformers, hydrogen is the most common and is generated under most fault conditions; thus, formation of hydrogen is an early indication of serious problems in the transformer's life cycle.\n\n\n== ApplicationsEdit ==\n\n\n=== Consumption in processesEdit ===\nLarge quantities of H\n2 are needed in the petroleum and chemical industries. The largest application of H\n2 is for the processing (\"upgrading\") of fossil fuels, and in the production of ammonia. The key consumers of H\n2 in the petrochemical plant include hydrodealkylation, hydrodesulfurization, and hydrocracking. H\n2 has several other important uses. H\n2 is used as a hydrogenating agent, particularly in increasing the level of saturation of unsaturated fats and oils (found in items such as margarine), and in the production of methanol. It is similarly the source of hydrogen in the manufacture of hydrochloric acid. H\n2 is also used as a reducing agent of metallic ores.\nHydrogen is highly soluble in many rare earth and transition metals and is soluble in both nanocrystalline and amorphous metals. Hydrogen solubility in metals is influenced by local distortions or impurities in the crystal lattice. These properties may be useful when hydrogen is purified by passage through hot palladium disks, but the gas's high solubility is a metallurgical problem, contributing to the embrittlement of many metals, complicating the design of pipelines and storage tanks.\nApart from its use as a reactant, H\n2 has wide applications in physics and engineering. It is used as a shielding gas in welding methods such as atomic hydrogen welding. H2 is used as the rotor coolant in electrical generators at power stations, because it has the highest thermal conductivity of any gas. Liquid H2 is used in cryogenic research, including superconductivity studies. Because H\n2 is lighter than air, having a little more than \u200a1\u204414 of the density of air, it was once widely used as a lifting gas in balloons and airships.\nIn more recent applications, hydrogen is used pure or mixed with nitrogen (sometimes called forming gas) as a tracer gas for minute leak detection. Applications can be found in the automotive, chemical, power generation, aerospace, and telecommunications industries. Hydrogen is an authorized food additive (E 949) that allows food package leak testing among other anti-oxidizing properties.\nHydrogen's rarer isotopes also each have specific applications. Deuterium (hydrogen-2) is used in nuclear fission applications as a moderator to slow neutrons, and in nuclear fusion reactions. Deuterium compounds have applications in chemistry and biology in studies of reaction isotope effects. Tritium (hydrogen-3), produced in nuclear reactors, is used in the production of hydrogen bombs, as an isotopic label in the biosciences, and as a radiation source in luminous paints.\nThe triple point temperature of equilibrium hydrogen is a defining fixed point on the ITS-90 temperature scale at 13.8033 kelvins.\n\n\n=== CoolantEdit ===\n\nHydrogen is commonly used in power stations as a coolant in generators due to a number of favorable properties that are a direct result of its light diatomic molecules. These include low density, low viscosity, and the highest specific heat and thermal conductivity of all gases.\n\n\n=== Energy carrierEdit ===\n\nHydrogen is not an energy resource, except in the hypothetical context of commercial nuclear fusion power plants using deuterium or tritium, a technology presently far from development. The Sun's energy comes from nuclear fusion of hydrogen, but this process is difficult to achieve controllably on Earth. Elemental hydrogen from solar, biological, or electrical sources require more energy to make it than is obtained by burning it, so in these cases hydrogen functions as an energy carrier, like a battery. Hydrogen may be obtained from fossil sources (such as methane), but these sources are unsustainable.\nThe energy density per unit volume of both liquid hydrogen and compressed hydrogen gas at any practicable pressure is significantly less than that of traditional fuel sources, although the energy density per unit fuel mass is higher. Nevertheless, elemental hydrogen has been widely discussed in the context of energy, as a possible future carrier of energy on an economy-wide scale. For example, CO\n2 sequestration followed by carbon capture and storage could be conducted at the point of H\n2 production from fossil fuels. Hydrogen used in transportation would burn relatively cleanly, with some NOx emissions, but without carbon emissions. However, the infrastructure costs associated with full conversion to a hydrogen economy would be substantial. Fuel cells can convert hydrogen and oxygen directly to electricity more efficiently than internal combustion engines.\n\n\n=== Semiconductor industryEdit ===\nHydrogen is employed to saturate broken (\"dangling\") bonds of amorphous silicon and amorphous carbon that helps stabilizing material properties. It is also a potential electron donor in various oxide materials, including ZnO, SnO2, CdO, MgO, ZrO2, HfO2, La2O3, Y2O3, TiO2, SrTiO3, LaAlO3, SiO2, Al2O3, ZrSiO4, HfSiO4, and SrZrO3.\n\n\n== Biological reactionsEdit ==\n\nH2 is a product of some types of anaerobic metabolism and is produced by several microorganisms, usually via reactions catalyzed by iron- or nickel-containing enzymes called hydrogenases. These enzymes catalyze the reversible redox reaction between H2 and its component two protons and two electrons. Creation of hydrogen gas occurs in the transfer of reducing equivalents produced during pyruvate fermentation to water. The natural cycle of hydrogen production and consumption by organisms is called the hydrogen cycle.\nWater splitting, in which water is decomposed into its component protons, electrons, and oxygen, occurs in the light reactions in all photosynthetic organisms. Some such organisms, including the alga Chlamydomonas reinhardtii and cyanobacteria, have evolved a second step in the dark reactions in which protons and electrons are reduced to form H2 gas by specialized hydrogenases in the chloroplast. Efforts have been undertaken to genetically modify cyanobacterial hydrogenases to efficiently synthesize H2 gas even in the presence of oxygen. Efforts have also been undertaken with genetically modified alga in a bioreactor.\n\n\n== Safety and precautionsEdit ==\n\nHydrogen poses a number of hazards to human safety, from potential detonations and fires when mixed with air to being an asphyxiant in its pure, oxygen-free form. In addition, liquid hydrogen is a cryogen and presents dangers (such as frostbite) associated with very cold liquids. Hydrogen dissolves in many metals, and, in addition to leaking out, may have adverse effects on them, such as hydrogen embrittlement, leading to cracks and explosions. Hydrogen gas leaking into external air may spontaneously ignite. Moreover, hydrogen fire, while being extremely hot, is almost invisible, and thus can lead to accidental burns.\nEven interpreting the hydrogen data (including safety data) is confounded by a number of phenomena. Many physical and chemical properties of hydrogen depend on the parahydrogen/orthohydrogen ratio (it often takes days or weeks at a given temperature to reach the equilibrium ratio, for which the data is usually given). Hydrogen detonation parameters, such as critical detonation pressure and temperature, strongly depend on the container geometry.\n\n\n== NotesEdit ==\n\n\n== ReferencesEdit ==\n\n\n== Further readingEdit ==\nChart of the Nuclides (17th ed.). Knolls Atomic Power Laboratory. 2010. ISBN 978-0-9843653-0-2. \nFerreira-Aparicio, P; Benito, M. J.; Sanz, J. L. (2005). \"New Trends in Reforming Technologies: from Hydrogen Industrial Plants to Multifuel Microreformers\". Catalysis Reviews. 47 (4): 491\u2013588. doi:10.1080/01614940500364958. \nNewton, David E. (1994). The Chemical Elements. New York: Franklin Watts. ISBN 0-531-12501-7. \nRigden, John S. (2002). Hydrogen: The Essential Element. Cambridge, Massachusetts: Harvard University Press. ISBN 0-531-12501-7. \nRomm, Joseph, J. (2004). The Hype about Hydrogen, Fact and Fiction in the Race to Save the Climate. Island Press. ISBN 1-55963-703-X.  CS1 maint: Multiple names: authors list (link)\nScerri, Eric (2007). The Periodic System, Its Story and Its Significance. New York: Oxford University Press. ISBN 0-19-530573-6. \n\n\n== External linksEdit ==\n\nBasic Hydrogen Calculations of Quantum Mechanics\nHydrogen at The Periodic Table of Videos (University of Nottingham)\nHigh temperature hydrogen phase diagram\nWavefunction of hydrogen", 
                "titleUrl": "https://en.wikipedia.org/wiki/Hydrogen", 
                "title": "Hydrogen"
            }, 
            {
                "snippet": "isotope is usually called tritium. The symbols D and T (instead of 2H and 3H) are sometimes used for deuterium and tritium. The IUPAC states in the 2005", 
                "pageCategories": "CS1 errors: dates\nHydrogen\nIsotopes of hydrogen\nLists of isotopes by element\nWikipedia articles with GND identifiers", 
                "pageContent": "Hydrogen (H) (relative atomic mass: 1.00794) has three naturally occurring isotopes, sometimes denoted 1H, 2H, and 3H. The first two of these are stable while 3H has a half-life of 12.32 years. All heavier isotopes are synthetic and have a half-life less than one zeptosecond (10\u221221 second). Of these, 5H is the most stable, and 7H is the least.\nHydrogen is the only element whose isotopes have different names that are in common use today. The 2H (or hydrogen-2) isotope is usually called deuterium, while the 3H (or hydrogen-3) isotope is usually called tritium. The symbols D and T (instead of 2H and 3H) are sometimes used for deuterium and tritium. The IUPAC states in the 2005 Red Book that while the use of D and T is common, it is not preferred because it can cause problems in the alphabetic sorting of chemical formulae. The ordinary isotope of hydrogen, with no neutrons, is sometimes called \"protium\". (During the early study of radioactivity, some other heavy radioactive isotopes were given names, but such names are rarely used today.)\n\n\n== Hydrogen-1 (protium) ==\n\n1H (atomic mass 1.00782504(7) u) is the most common hydrogen isotope with an abundance of more than 99.98%. Because the nucleus of this isotope consists of only a single proton, it is given the descriptive but rarely used formal name protium.\nThe proton has never been observed to decay and hydrogen-1 is therefore considered a stable isotope. Some grand unified theories proposed in the 1970s predict that proton decay can occur with a half-life between 1031 and 1036 years. If this prediction is found to be true, then hydrogen-1 (and indeed all nuclei now believed to be stable) are only observationally stable. To date however, experiments have shown that the minimum proton half life is in excess of 1034 years.\n\n\n== Hydrogen-2 (deuterium) ==\n\n2H (atomic mass 2.013553212724(78) u), the other stable hydrogen isotope, is known as deuterium and contains one proton and one neutron in its nucleus. The nucleus of deuterium is called a deuteron. Deuterium comprises 0.0026 \u2013 0.0184% (by population, not by mass) of hydrogen samples on Earth, with the lower number tending to be found in samples of hydrogen gas and the higher enrichment (0.015% or 150 ppm) typical of ocean water. Deuterium on Earth has been enriched with respect to its initial concentration in the Big Bang and the outer solar system (about 27 ppm, by atom fraction) and its concentration in older parts of the Milky Way galaxy (about 23 ppm). Presumably the differential concentration of deuterium in the inner solar system is due to the lower volatility of deuterium gas and compounds, enriching deuterium fractions in comets and planets exposed to significant heat from the Sun over billions of years of solar system evolution.\nDeuterium is not radioactive, and does not represent a significant toxicity hazard. Water enriched in molecules that include deuterium instead of protium is called heavy water. Deuterium and its compounds are used as a non-radioactive label in chemical experiments and in solvents for 1H-NMR spectroscopy. Heavy water is used as a neutron moderator and coolant for nuclear reactors. Deuterium is also a potential fuel for commercial nuclear fusion.\n\n\n== Hydrogen-3 (tritium) ==\n\n3H (atomic mass 3.0160492 u) is known as tritium and contains one proton and two neutrons in its nucleus. It is radioactive, decaying into helium-3 through \u03b2\u2212 decay with a half-life of 12.32 years. Small amounts of tritium occur naturally because of the interaction of cosmic rays with atmospheric gases. Tritium has also been released during nuclear weapons tests. It is used in thermonuclear fusion weapons, as a tracer in isotope geochemistry, and specialized in self-powered lighting devices.\nThe most common method of producing tritium is by bombarding a natural isotope of lithium, lithium-6, with neutrons in a nuclear reactor.\nTritium was once used routinely in chemical and biological labeling experiments as a radiolabel, which has become less common in recent times. D-T nuclear fusion uses tritium as its main reactant, along with deuterium, liberating energy through the loss of mass when the two nuclei collide and fuse at high temperatures.\n\n\n== Hydrogen-4 ==\n4H contains one proton and three neutrons in its nucleus. It is a highly unstable isotope of hydrogen. It has been synthesised in the laboratory by bombarding tritium with fast-moving deuterium nuclei. In this experiment, the tritium nucleus captured a neutron from the fast-moving deuterium nucleus. The presence of the hydrogen-4 was deduced by detecting the emitted protons. Its atomic mass is 4.02781 \u00b1 0.00011. It decays through neutron emission into hydrogen-3 (tritium) with a half-life of about 139 yoctoseconds ((1.39 \u00b1 0.10) \u00d7 10\u221222 seconds).\nIn the 1955 satirical novel The Mouse That Roared, the name quadium was given to the hydrogen-4 isotope that powered the Q-bomb that the Duchy of Grand Fenwick captured from the United States.\n\n\n== Hydrogen-5 ==\n5H is a highly unstable isotope of hydrogen. The nucleus consists of a proton and four neutrons. It has been synthesised in the laboratory by bombarding tritium with fast-moving tritium nuclei. In this experiment, one tritium nucleus captures two neutrons from the other, becoming a nucleus with one proton and four neutrons. The remaining proton may be detected, and the existence of hydrogen-5 deduced. It decays through double neutron emission into hydrogen-3 (tritium) and has a half-life of at least 910 yoctoseconds (9.1 \u00d7 10\u221222 seconds).\n\n\n== Hydrogen-6 ==\n6H decays either through triple neutron emission into hydrogen-3 (tritium) or quadruple neutron emission into hydrogen-2 (deuterium) and has a half-life of 290 yoctoseconds (2.9\u00d710\u221222 seconds).\n\n\n== Hydrogen-7 ==\n7H consists of a proton and six neutrons. It was first synthesised in 2003 by a group of Russian, Japanese and French scientists at RIKEN's RI Beam Science Laboratory by bombarding hydrogen with helium-8 atoms. In the resulting reaction, all six of the helium-8's neutrons were donated to the hydrogen's nucleus. The two remaining protons were detected by the \"RIKEN telescope\", a device composed of several layers of sensors, positioned behind the target of the RI Beam cyclotron. Hydrogen-7 has a half life of 23 yoctoseconds (2.3\u00d710\u221223 seconds).\n\n\n== Table ==\n\n\n=== Notes ===\nCommercially available materials may have been subjected to an undisclosed or inadvertent isotopic fractionation. Substantial deviations from the given mass and composition can occur.\nValues marked # are not purely derived from experimental data, but at least partly from systematic trends. Spins with weak assignment arguments are enclosed in parentheses.\nUncertainties are given in concise form in parentheses after the corresponding last digits. Uncertainty values denote one standard deviation, except isotopic composition and standard atomic mass from IUPAC, which use expanded uncertainties.\nNuclide masses are given by IUPAP Commission on Symbols, Units, Nomenclature, Atomic Masses and Fundamental Constants (SUNAMCO)\nIsotope abundances are given by IUPAC Commission on Isotopic Abundances and Atomic Weights\n\n\n== Decay chains ==\nThe majority of heavy hydrogen isotopes decay directly to 3H, which then decays to the stable isotope 3He. However, 6H has occasionally been observed to decay directly to stable 2H.\n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              1\n            \n            \n              3\n            \n          \n          H\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                12.32\n                y\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              2\n            \n            \n              3\n            \n          \n          H\n          e\n        \n        +\n        \n          e\n          \n            \n\n            \n            \n\n            \n            \n              \u2212\n            \n          \n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{1}^{3}H} \\ {\\xrightarrow {\\ \\mathrm {12.32y} }}\\ \\mathrm {{}_{2}^{3}He} +\\mathrm {e{}_{}^{-}} }\n  \n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              1\n            \n            \n              4\n            \n          \n          H\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                139\n                y\n                s\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              1\n            \n            \n              3\n            \n          \n          H\n        \n        +\n        \n          \n            \n\n            \n            \n              0\n            \n            \n              1\n            \n          \n          n\n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{1}^{4}H} \\ {\\xrightarrow {\\ \\mathrm {139ys} }}\\ \\mathrm {{}_{1}^{3}H} +\\mathrm {{}_{0}^{1}n} }\n  \n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              1\n            \n            \n              5\n            \n          \n          H\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                >\n                910\n                y\n                s\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              1\n            \n            \n              3\n            \n          \n          H\n        \n        +\n        \n          2\n          \n            \n\n            \n            \n              0\n            \n            \n              1\n            \n          \n          n\n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{1}^{5}H} \\ {\\xrightarrow {\\ \\mathrm {>910ys} }}\\ \\mathrm {{}_{1}^{3}H} +\\mathrm {2{}_{0}^{1}n} }\n  \n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              1\n            \n            \n              6\n            \n          \n          H\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                290\n                y\n                s\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              1\n            \n            \n              3\n            \n          \n          H\n        \n        +\n        \n          3\n          \n            \n\n            \n            \n              0\n            \n            \n              1\n            \n          \n          n\n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{1}^{6}H} \\ {\\xrightarrow {\\ \\mathrm {290ys} }}\\ \\mathrm {{}_{1}^{3}H} +\\mathrm {3{}_{0}^{1}n} }\n  \n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              1\n            \n            \n              6\n            \n          \n          H\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                290\n                y\n                s\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              1\n            \n            \n              2\n            \n          \n          H\n        \n        +\n        \n          4\n          \n            \n\n            \n            \n              0\n            \n            \n              1\n            \n          \n          n\n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{1}^{6}H} \\ {\\xrightarrow {\\ \\mathrm {290ys} }}\\ \\mathrm {{}_{1}^{2}H} +\\mathrm {4{}_{0}^{1}n} }\n  \n\n  \n    \n      \n        \n          \n            \n\n            \n            \n              1\n            \n            \n              7\n            \n          \n          H\n        \n         \n        \n          \n            \u2192\n            \n               \n              \n                23\n                y\n                s\n              \n            \n          \n        \n         \n        \n          \n            \n\n            \n            \n              1\n            \n            \n              3\n            \n          \n          H\n        \n        +\n        \n          4\n          \n            \n\n            \n            \n              0\n            \n            \n              1\n            \n          \n          n\n        \n      \n    \n    {\\displaystyle \\mathrm {{}_{1}^{7}H} \\ {\\xrightarrow {\\ \\mathrm {23ys} }}\\ \\mathrm {{}_{1}^{3}H} +\\mathrm {4{}_{0}^{1}n} }\n  \nnote that the decay times are in yoctoseconds for all isotopes except 3H, which is expressed in years.\n\n\n== See also ==\nIsotopes of helium\nHydrogen isotope biogeochemistry\n\n\n== References ==\nNotes\n\nGeneral references\nIsotope masses from:\nG. Audi; A. H. Wapstra; C. Thibault; J. Blachot; O. Bersillon (2003). \"The NUBASE evaluation of nuclear and decay properties\" (PDF). Nuclear Physics. 729: 3\u2013128. Bibcode:2003NuPhA.729....3A. doi:10.1016/j.nuclphysa.2003.11.001. \n\nIsotopic compositions and standard atomic masses from:\nJ. R. de Laeter; J. K. B\u00f6hlke; P. De Bi\u00e8vre; H. Hidaka; H. S. Peiser; K. J. R. Rosman; P. D. P. Taylor (2003). \"Atomic weights of the elements. Review 2000 (IUPAC Technical Report)\". Pure and Applied Chemistry. 75 (6): 683\u2013800. doi:10.1351/pac200375060683. \nM. E. Wieser (2006). \"Atomic weights of the elements 2005 (IUPAC Technical Report)\". Pure and Applied Chemistry. 78 (11): 2051\u20132066. doi:10.1351/pac200678112051. Lay summary. \n\nHalf-life, spin, and isomer data selected from the following sources. See editing notes on this article's talk page.\nG. Audi; A. H. Wapstra; C. Thibault; J. Blachot; O. Bersillon (2003). \"The NUBASE evaluation of nuclear and decay properties\" (PDF). Nuclear Physics A. 729: 3\u2013128. Bibcode:2003NuPhA.729....3A. doi:10.1016/j.nuclphysa.2003.11.001. \nNational Nuclear Data Center. \"NuDat 2.1 database\". Brookhaven National Laboratory. Retrieved September 2005.  \nN. E. Holden (2004). \"Table of the Isotopes\". In D. R. Lide. CRC Handbook of Chemistry and Physics (85th ed.). CRC Press. Section 11. ISBN 978-0-8493-0485-9. \n\n\n== Further reading ==\nB. Dum\u00e9 (7 March 2003). \"Hydrogen-7 makes its debut\". Physics World.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Isotopes_of_hydrogen", 
                "title": "Isotopes of hydrogen"
            }, 
            {
                "snippet": "The Tritium Systems Test Assembly (TSTA) was a facility at Los Alamos National Laboratory dedicated to the development and demonstration of technologies", 
                "pageCategories": "Los Alamos National Laboratory", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Tritium_Systems_Test_Assembly_(TSTA)", 
                "title": "Tritium Systems Test Assembly (TSTA)"
            }, 
            {
                "snippet": "that acts as a beta-adrenergic blocker. When the extra hydrogen atoms are tritium, it is a radiolabeled form of alprenolol, which is used to label beta-adrenergic", 
                "pageCategories": "All stub articles\nArticles containing unverified chemical infoboxes\nArticles without EBI source\nArticles without KEGG source\nArticles without UNII source\nBeta blockers\nBiochemistry stubs", 
                "pageContent": "Dihydroalprenolol (DHA) is a hydrogenated alprenolol derivative that acts as a beta-adrenergic blocker. When the extra hydrogen atoms are tritium, it is a radiolabeled form of alprenolol, which is used to label beta-adrenergic receptors for isolation.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Dihydroalprenolol", 
                "title": "Dihydroalprenolol"
            }
        ], 
        "phraseCharStart": "795"
    }, 
    {
        "phraseCharEnd": "840", 
        "phraseIndex": "T16", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "carbon layers", 
        "wikiSearchResults": [
            {
                "snippet": "compound not every layer is necessarily occupied by guests. In so-called stage 1 compounds, graphite layers and intercalated layers alternate and in stage", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from April 2015\nArticles with unsourced statements from April 2016\nCommons category with local link same as on Wikidata\nInorganic carbon compounds\nSupramolecular chemistry", 
                "pageContent": "Graphite intercalation compounds (GICs) are complex materials having a formula CXm where the ion Xn+ or Xn\u2212 is inserted (intercalated) between the oppositely charged carbon layers. Typically m is much less than 1. These materials are deeply colored solids that exhibit a range of electrical and redox properties of potential applications.\n\n\n== Preparation and structure ==\nThese materials are prepared by treating graphite with a strong oxidant or a strong reducing agent:\nC + m X \u2192 CXm\nThe reaction is reversible.\nThe host (graphite) and the guest X interact by charge transfer. A analogous process is the basis of commercial lithium-ion batteries.\nIn a graphite intercalation compound not every layer is necessarily occupied by guests. In so-called stage 1 compounds, graphite layers and intercalated layers alternate and in stage 2 compounds, two graphite layers with no guest material in between alternate with an intercalated layer. The actual composition may vary and therefore these compounds are an example of non-stoichiometric compounds. It is customary to specify the composition together with the stage. The layers are pushed apart upon incorporation of the guest ions.\n\n\n== Examples ==\n\n\n=== Alkali and alkaline earth derivatives ===\n\nOne of the best studied graphite intercalation compounds, KC8, is prepared by melting potassium over graphite powder. The potassium is absorbed into the graphite and the material changes color from black to bronze. The resulting solid is pyrophoric. The composition is explained by assuming that the potassium to potassium distance is twice the distance between hexagons in the carbon framework. The bond between anionic graphite layers and potassium cations is ionic. The electrical conductivity of the material is greater than that of \u03b1-graphite. KC8 is a superconductor with a very low critical temperature Tc = 0.14 K. Heating KC8 leads to the formation of a series of decomposition products as the K atoms are eliminated:\n3 KC8 \u2192 KC24 + 2 K\nVia the intermediates KC24 (blue in color), KC36, KC48, ultimately the compound KC60 results.\nThe stoichiometry MC8 is observed for M = K, Rb and Cs. For smaller ions M = Li+, Sr2+, Ba2+, Eu2+, Yb3+, and Ca2+, the limiting stoichiometry is MC6 (X. Calcium graphite CaC\n6 is obtained by immersing highly oriented pyrolytic graphite in liquid Li\u2013Ca alloy for 10 days at 350 \u00b0C. The crystal structure of CaC\n6 belongs to the R-3m space group. The graphite interlayer distance increases upon Ca intercalation from 3.35 to 4.524 \u00c5, and the carbon-carbon distance increases from 1.42 to 1.444 \u00c5.\n\nWith barium and ammonia, the cations are solvated, giving the stoichiometry (Ba(NH3)2.5C10.9(stage 1)) or those with caesium, hydrogen and potassium (CsC8\u00b7K2H4/3C8(stage 1)).\nInterestingly, different from other alkali metals, the amount of Na intercalation is very small. Quantum-mechanical calculations show that this originate from a quite general phenomenon: among the alkali and alkaline earth metals, Na and Mg generally have the weakest chemical binding to a given substrate, compared with the other elements in the same group of the periodic table. The phenomenon arises from the competition between trends in the ionization energy and the ion\u2013substrate coupling, down the columns of the periodic table.\n\n\n=== Graphite bisulfate, perchlorate, hexafluoroarsenate: oxidized carbons ===\nThe intercalation compounds graphite bisulfate and graphite perchlorate can be prepared by treating graphite with strong oxidizing agents in the presence of strong acids. In contrast to the potassium and calcium graphites, the carbon layers are oxidized in this process: 48 C + 0.25 O2 + 3 H2SO4 \u2192 [C24]+[HSO4]\u2212.2H2SO4 + 0.5 H2O\nIn graphite perchlorate, planar layers of carbon atoms are 794 picometers apart, separated by ClO4\u2212 ions. Cathodic reduction of graphite perchlorate is analogous to heating KC8, which leads to a sequential elimination of HClO4.\nBoth graphite bisulfate and graphite perchlorate are better conductors as compared to graphite, as predicted by using a positive-hole mechanism. Reaction of graphite with [O2]+[AsF6]\u2212 affords the salt [C8]+[AsF6]\u2212.\n\n\n=== Metal halide derivatives ===\nA number of metal halides intercalate into graphite. The chloride derivatives have been most extensively studied. Examples include MCl2 (M = Zn, Ni, Cu, Mn), MCl3 (M = Al, Fe, Ga), MCl4 (M = Zr, Pt), etc. The materials consists of layers of close-packed metal halide layers between sheets of carbon. The derivative C~8FeCl3 exhibits spin glass behavior. It proved to be a particularly fertile system on which to study phase transitions. A stage n magnetic GIC has n graphite layers separating successive magnetic layers. As the stage number increases the interaction between spins in successive magnetic layers becomes weaker and 2D magnetic behaviour may arise.\n\n\n=== Halogen- and oxide-graphite compounds ===\nChlorine and bromine reversibly intercalate into graphite. Iodine does not. Fluorine reacts irreversibly. In the case of bromine, the following stoichiometries are known: CnBr for n = 8, 12, 14, 16, 20, and 28.\nBecause it forms irreversibly, carbon monofluoride is often not classified as an intercalation compound. It has the formula (CF)x. It is prepared by reaction of gaseous fluorine with graphitic carbon at 215\u2013230 \u00b0C. The color is greyish, white, or yellow. The bond between the carbon and fluorine atoms is covalent. Tetracarbon monofluoride (C4F) is prepared by treating graphite with a mixture of fluorine and hydrogen fluoride at room temperature. The compound has a blackish-blue color. Carbon monofluoride is not electrically conductive. It has been studied as a cathode material in one type of primary (non-rechargeable) lithium batteries.\nGraphite oxide is an unstable yellow solid.\n\n\n== Properties and applications ==\nGraphite intercalation compounds have fascinated materials scientists for many years owing to their diverse electronic and electrical properties.\n\n\n=== Superconductivity ===\nAmong the superconducting graphite intercalation compounds, CaC\n6 exhibits the highest critical temperature Tc = 11.5 K, which further increases under applied pressure (15.1 K at 8 GPa). Superconductivity in these compounds is thought to be related to the role of an interlayer state, a free electron like band lying roughly 2 eV (0.32 aJ) above the Fermi level; superconductivity only occurs if the interlayer state is occupied. Analysis of pure CaC\n6 using a high quality ultraviolet light revealed to conduct angle-resolved photoemission spectroscopy measurements. The opening of a superconducting gap in the \u03c0* band revealed a substantial contribution to the total electron\u2013phonon-coupling strength from the \u03c0*-interlayer interband interaction.\n\n\n=== Reagents in chemical synthesis: KC8 ===\nThe bronze-colored material KC8 is one of the strongest reducing agents known. It has also been used as a catalyst in polymerizations and as a coupling reagent for aryl halides to biphenyls. In one study, freshly prepared KC8 was treated with 1-iodododecane delivering a modification (micrometre scale carbon platelets with long alkyl chains sticking out providing solubility) that is soluble in chloroform. Another potassium graphite compound, KC24, has been used as a neutron monochromator. A new essential application for potassium graphite was introduced by the invention of the potassium-ion battery. Like the lithium-ion battery, the potassium-ion battery should use a carbon-based anode instead of a metallic anode. In this circumstance, the stable structure of potassium graphite is an important advantage.\n\n\n== See also ==\nCovalent superconductors\nMagnesium diboride, which uses hexagonal planar boron sheets instead of carbon\nPyrolytic graphite\n\n\n== References ==\n\n\n== Further reading ==\nT. Enoki, M. Suzuki and M. Endo (2003). Graphite intercalation compounds and applications. Oxford University Press. ISBN 0-19-512827-3. \nM.S. Dresselhaus and G. Dresselhaus Review: (1981). \"Intercalation compounds of graphite\". Advances in Physics. 30 (2): 139. Bibcode:1981AdPhy..30..139D. doi:10.1080/00018738100101367.  (187 pages), also reprinted as Dresselhaus, M. S.; Dresselhaus, G. (2002). \"Intercalation compounds of graphite\". Advances in Physics. 51: 1. Bibcode:2002AdPhy..51....1D. doi:10.1080/00018730110113644. \nD. Savoia; Trombini, C.; Umani-Ronchi, A.; et al. (1985). \"Applications of potassium-graphite and metals dispersed on graphite in organic synthesis\" (PDF). Pure & Appl. Chem. (PDF). 57 (12): 1887. doi:10.1351/pac198557121887. \nSuzuki, Itsuko S.; Ting-Yu Huang; Masatsugu Suzuki (13 June 2002). \"Magnetic phase diagram of the stage-1 CoCl2 graphite intercalation compound: Existence of metamagnetic transition and spin-flop transitions\". Phys. Rev. B. 65: 224432. doi:10.1103/PhysRevB.65.224432. \nRancourt, DG; C Meschi; S Flandrois (1986). \"S=1/2 antiferromagnetic finite chains effectively isolated by frustration: CuCl2-intercalated graphite.\". Phys Rev B Condens Matter. 33 (1): 347\u2013355. doi:10.1103/PhysRevB.33.347. PMID 9937917. \n\n\n== External links ==\nPreparation of Potassium Graphite\nstructure of CaC6", 
                "titleUrl": "https://en.wikipedia.org/wiki/Graphite_intercalation_compound", 
                "title": "Graphite intercalation compound"
            }, 
            {
                "snippet": "compound, a complex material consisting of charged ions inserted in carbon layers. Guaranteed Investment Certificate, a financial instrument Guaranteed", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages\nThree-letter disambiguation pages", 
                "pageContent": "Gic is a village in Veszpr\u00e9m County, Hungary.\n\n\n== External links ==\nStreet map (Hungarian)", 
                "titleUrl": "https://en.wikipedia.org/wiki/GIC", 
                "title": "GIC"
            }, 
            {
                "snippet": "diversify with carbon composite frames using Resin Transfer Molding (RTM) technology. TIME weaves its own custom tubular carbon layers in its RTM factory", 
                "pageCategories": "Cycle manufacturers of France\nPages using infobox company with unsupported parameters", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Time_(bicycles)", 
                "title": "Time (bicycles)"
            }, 
            {
                "snippet": "aluminium, carbon fibre reinforced plastic, or a combination of materials. Such shafts are typically made from an aluminium core wrapped with a carbon fibre", 
                "pageCategories": "All articles with unsourced statements\nArchaeological artefact types\nArchery\nArticles with unsourced statements from July 2012\nArticles with unsourced statements from November 2015\nCommons category with local link same as on Wikidata\nHunting equipment\nLithics\nProjectile weapons\nProjectiles", 
                "pageContent": "An arrow is a shafted projectile that is shot with a bow. It predates recorded history and is common to most cultures. An arrow usually consists of a shaft with an arrowhead attached to the front end, with fletchings and a nock at the other.\n\n\n== History ==\n\nThe oldest evidence of stone-tipped projectiles, which may or may not have been propelled by a bow (c.f. atlatl), dating to c. 64,000 years ago, were found in Sibudu Cave, South Africa. The oldest evidence of the use of bows to shoot arrows dates to about 10,000 years ago; it is based on pinewood arrows found in the Ahrensburg valley north of Hamburg. They had shallow grooves on the base, indicating that they were shot from a bow. The oldest bow so far recovered is about 8,000 years old, found in the Holmeg\u00e5rd swamp in Denmark. Archery seems to have arrived in the Americas with the Arctic small tool tradition, about 4,500 years ago.\n\n\n== Size ==\n\nArrow sizes vary greatly across cultures, ranging from eighteen inches to five feet (45 cm to 150 cm). However, most modern arrows are 75 centimetres (30 in) to 96 centimetres (38 in); most war arrows from an English ship sunk in 1545 were 76 centimetres (30 in). Very short arrows have been used, shot through a guide attached either to the bow (an \"overdraw\") or to the archer's wrist (the Turkish \"siper\"). These may fly farther than heavier arrows, and an enemy without suitable equipment may find himself unable to return them.\n\n\n=== Shaft ===\n\nThe shaft is the primary structural element of the arrow, to which the other components are attached. Traditional arrow shafts are made from lightweight wood, bamboo or reeds, while modern shafts may be made from aluminium, carbon fibre reinforced plastic, or a combination of materials. Such shafts are typically made from an aluminium core wrapped with a carbon fibre outer.\nThe stiffness of the shaft is known as its spine, referring to how little the shaft bends when compressed. Hence, an arrow which bends less is said to have more spine. In order to strike consistently, a group of arrows must be similarly spined. \"Center-shot\" bows, in which the arrow passes through the central vertical axis of the bow riser, may obtain consistent results from arrows with a wide range of spines. However, most traditional bows are not center-shot and the arrow has to deflect around the handle in the archer's paradox; such bows tend to give most consistent results with a narrower range of arrow spine that allows the arrow to deflect correctly around the bow. Higher draw-weight bows will generally require stiffer arrows, with more spine (less flexibility) to give the correct amount of flex when shot.\n\n\n==== GPI rating ====\nThe weight of an arrow shaft can be expressed in GPI (Grains Per Inch). The length of a shaft in inches multiplied by its GPI rating gives the weight of the shaft in grains. For example, a shaft that is 30 inches long and has a GPI of 9.5 weighs 285 grains, or about 18 grams. This does not include the other elements of a finished arrow, so a complete arrow will be heavier than the shaft alone.\n\n\n==== Footed arrows ====\nSometimes a shaft will be made of two different types of wood fastened together, resulting in what is known as a footed arrow. Known by some as the finest of wood arrows, footed arrows were used both by early Europeans and Native Americans. Footed arrows will typically consist of a short length of hardwood near the head of the arrow, with the remainder of the shaft consisting of softwood. By reinforcing the area most likely to break, the arrow is more likely to survive impact, while maintaining overall flexibility and lighter weight.\n\n\n=== Arrowhead ===\n\nThe arrowhead or projectile point is the primary functional part of the arrow, and plays the largest role in determining its purpose. Some arrows may simply use a sharpened tip of the solid shaft, but it is far more common for separate arrowheads to be made, usually from metal, horn, or some other hard material. Arrowheads are usually separated by function:\nBodkin points are short, rigid points with a small cross-section. They were made of unhardened iron and may have been used for better or longer flight, or for cheaper production. It has been mistakenly suggested that the bodkin came into its own as a means of penetrating armour, but research has found no hardened bodkin points, so it is likely that it was first designed either to extend range or as a cheaper and simpler alternative to the broadhead. In a modern test, a direct hit from a hard steel bodkin point penetrated Damascus chain armour. However, archery was not effective against plate armour, which became available to knights of fairly modest means by the late 14th century.\nBlunts are unsharpened arrowheads occasionally used for types of target shooting, for shooting at stumps or other targets of opportunity, or hunting small game when the goal is to stun the target without penetration. Blunts are commonly made of metal or hard rubber. They may stun, and occasionally, the arrow shaft may penetrate the head and the target; safety is still important with blunt arrows.\nJudo points have spring wires extending sideways from the tip. These catch on grass and debris to prevent the arrow from being lost in the vegetation. Used for practice and for small game.\nBroadheads were used for war and are still used for hunting. Medieval broadheads could be made from steel, sometimes with hardened edges. They usually have two to four sharp blades that cause massive bleeding in the victim. Their function is to deliver a wide cutting edge so as to kill as quickly as possible by cleanly cutting major blood vessels, and cause further trauma on removal. They are expensive, damage most targets, and are usually not used for practice.\nThere are two main types of broadheads used by hunters: The fixed-blade and the mechanical types. While the fixed-blade broadhead keeps its blades rigid and unmovable on the broadhead at all times, the mechanical broadhead deploys its blades upon contact with the target, its blades swinging out to wound the target. The mechanical head flies better because it is more streamlined, but has less penetration as it uses some of the kinetic energy in the arrow to deploy its blades.\nField tips are similar to target points and have a distinct shoulder, so that missed outdoor shots do not become as stuck in obstacles such as tree stumps. They are also used for shooting practice by hunters, by offering similar flight characteristics and weights as broadheads, without getting lodged in target materials and causing excessive damage upon removal.\nTarget points are bullet-shaped with a conical point, designed to penetrate target butts easily without causing excessive damage to them.\nSafety arrows are designed to be used in various forms of reenactment combat, to reduce the risk when shot at people. These arrows may have heads that are very wide or padded. In combination with bows of restricted draw weight and draw length, these heads may reduce to acceptable levels the risks of shooting arrows at suitably armoured people. The parameters will vary depending on the specific rules being used and on the levels of risk felt acceptable to the participants. For instance, SCA combat rules require a padded head at least 1\u200a1\u20444\" in diameter, with bows not exceeding 28 inches (710 mm) and 50 lb (23 kg) of draw for use against well-armoured individuals.\nArrowheads may be attached to the shaft with a cap, a socketed tang, or inserted into a split in the shaft and held by a process called hafting. Points attached with caps are simply slid snugly over the end of the shaft, or may be held on with hot glue. Split-shaft construction involves splitting the arrow shaft lengthwise, inserting the arrowhead, and securing it using a ferrule, sinew, or wire.\n\n\n=== Fletchings ===\n\nFletchings are found at the back of the arrow and act as airfoils to provide a small amount of force used to stabilize the flight of the arrow. They are designed to keep the arrow pointed in the direction of travel by strongly damping down any tendency to pitch or yaw. Some cultures, for example most in New Guinea, did not use fletching on their arrows. Also, arrows without fletching (called bare shaft) are used for training purposes, because they make certain errors by the archer more visible.\nFletchings are traditionally made from feathers (often from a goose or turkey) bound to the arrow's shaft, but are now often made of plastic (known as \"vanes\"). Historically, some arrows used for the proofing of armour used copper vanes. Flight archers may use razor blades for fletching, in order to reduce air resistance. With conventional three-feather fletching, one feather, called the \"cock\" feather, is at a right angle to the nock, and is normally nocked so that it will not contact the bow when the arrow is shot. Four-feather fletching is usually symmetrical and there is no preferred orientation for the nock; this makes nocking the arrow slightly easier.\nArtisans who make arrows by hand are known as \"fletchers,\" a word related to the French word for arrow, fl\u00e8che. This is the same derivation as the verb \"fletch\", meaning to provide an arrow with its feathers. Glue and/or thread are the main traditional methods of attaching fletchings. A \"fletching jig\" is often used in modern times, to hold the fletchings in exactly the right orientation on the shaft while the glue hardens.\nWhenever natural fletching is used, the feathers on any one arrow must come from the same wing of the bird. The most common being the right-wing flight feathers of turkeys. The slight cupping of natural feathers requires them to be fletched with a right-twist for right wing, a left-twist for left wing. This rotation, through a combination of gyroscopic stabilization and increased drag on the rear of the arrow, helps the arrow to fly straight away. Artificial helical fletchings have the same effect. Most arrows will have three fletches, but some have four or even more. Fletchings generally range from two to six inches (152 mm) in length; flight arrows intended to travel the maximum possible distance typically have very low fletching, while hunting arrows with broadheads require long and high fletching to stabilize them against the aerodynamic effect of the head. Fletchings may also be cut in different ways, the two most common being parabolic (i.e. a smooth curved shape) and shield (i.e. shaped as one-half of a very narrow shield) cut.\nIn modern archery with screw-in points, right-hand rotation is generally preferred as it makes the points self-tighten. In traditional archery, some archers prefer a left rotation because it gets the hard (and sharp) quill of the feather farther away from the arrow-shelf and the shooter's hand.\nA flu-flu is a form of fletching, normally made by using long sections of full length feathers taken from a turkey, in most cases six or more sections are used rather than the traditional three. Alternatively two long feathers can be spiraled around the end of the arrow shaft. The extra fletching generates more drag and slows the arrow down rapidly after a short distance, about 30 m or so.\nFlu-Flu arrows are often used for hunting birds, or for children's archery, and can also be used to play Flu-Flu Golf.\n\n\n=== Nocks ===\nThe nock is a notch in the rearmost end of the arrow. It serves to keep the arrow in place on the string as the bow is being drawn. Nocks may be simple slots cut in the back of the arrow, or separate pieces made from wood, plastic, or horn that are then attached to the end of the arrow. Modern nocks, and traditional Turkish nocks, are often constructed so as to curve around the string or even pinch it slightly, so that the arrow is unlikely to slip off. In English it is common to say \"nock an arrow\" when one readies a shot.\nIn Arab archery, there was the description of the use of \"nockless arrows\". In shooting at the enemies, either the Turks or the Persians, it was seen that the enemies would pick up the expended arrows and shoot them back at the Arabs. So they developed \"nockless arrows\", which would be useless to their foes. The bowstring would have a small ring that is tied onto the string at the proper point where the nock would normally be placed. The end of the arrow, rather than being slit for a nock, would be sharpened like an arrowhead, then the rear end of the arrow would be slipped into this ring and drawn and released as usual. Then the enemy could collect all the arrows it wanted, but they would be useless to them in shooting back. A piece of battle advice was to have several rings tied onto the bowstring in case one broke.\n\n\n== See also ==\nArchery\nArrow poison\nBowfishing\nEarly thermal weapons\nFire arrows\nFlechette\nFlu-Flu Arrow\nQuarrel\nSignal arrow\nSwiss arrow\n\n\n== Notes ==\n\n\n== External links ==\nWhat's the Point?: Identifying Flint Artifacts (OPLIN)\nDr. Ashby's reports on broadhead penetration\nTypes of Bows and Arrows\nCarbon Arrows Explained in Detail", 
                "titleUrl": "https://en.wikipedia.org/wiki/Arrow", 
                "title": "Arrow"
            }, 
            {
                "snippet": "billion.   Graphite has a layered, planar structure. The individual layers are called graphene. In each layer, the carbon atoms are arranged in a honeycomb", 
                "pageCategories": "All articles with unsourced statements\nArt materials\nArticles with unsourced statements from April 2012\nCS1 maint: Multiple names: authors list\nCommons category with local link same as on Wikidata\nDry lubricants\nElectrical conductors\nGraphite\nHexagonal minerals\nNative element minerals", 
                "pageContent": "Graphite (pronunciation: /\u02c8\u0261r\u00e6fa\u026at/), archaically referred to as plumbago, is a crystalline form of carbon, a semimetal, a native element mineral, and one of the allotropes of carbon. Graphite is the most stable form of carbon under standard conditions. Therefore, it is used in thermochemistry as the standard state for defining the heat of formation of carbon compounds. Graphite may be considered the highest grade of coal, just above anthracite and alternatively called meta-anthracite, although it is not normally used as fuel because it is difficult to ignite.\n\n\n== Types and varieties ==\nThere are three principal types of natural graphite, each occurring in different types of ore deposit:\nCrystalline flake graphite (or flake graphite for short) occurs as isolated, flat, plate-like particles with hexagonal edges if unbroken and when broken the edges can be irregular or angular;\nAmorphous graphite: very fine flake graphite is sometimes called amorphous in the trade;\nLump graphite (also called vein graphite) occurs in fissure veins or fractures and appears as massive platy intergrowths of fibrous or acicular crystalline aggregates, and is probably hydrothermal in origin.\nHighly ordered pyrolytic graphite or more correctly highly oriented pyrolytic graphite (HOPG) refers to graphite with an angular spread between the graphite sheets of less than 1\u00b0.\nThe name \"graphite fiber\" is also sometimes used to refer to carbon fiber or carbon fiber-reinforced polymer.\n\n\n== Occurrence ==\n\nGraphite occurs in metamorphic rocks as a result of the reduction of sedimentary carbon compounds during metamorphism. It also occurs in igneous rocks and in meteorites. Minerals associated with graphite include quartz, calcite, micas and tourmaline. In meteorites it occurs with troilite and silicate minerals. Small graphitic crystals in meteoritic iron are called cliftonite.\nAccording to the United States Geological Survey (USGS), world production of natural graphite in 2012 was 1,100,000 tonnes, of which the following major exporters are: China (750 kt), India (150 kt), Brazil (75 kt), North Korea (30 kt) and Canada (26 kt). Graphite is not mined in the United States, but U.S. production of synthetic graphite in 2010 was 134 kt valued at $1.07 billion.\n\n\n== Properties ==\n\n\n=== Structure ===\nGraphite has a layered, planar structure. The individual layers are called graphene. In each layer, the carbon atoms are arranged in a honeycomb lattice with separation of 0.142 nm, and the distance between planes is 0.335 nm. Atoms in the plane are bonded covalently, with only three of the four potential bonding sites satisfied. The fourth electron is free to migrate in the plane, making graphite electrically conductive. However, it does not conduct in a direction at right angles to the plane. Bonding between layers is via weak van der Waals bonds, which allows layers of graphite to be easily separated, or to slide past each other.\nThe two known forms of graphite, alpha (hexagonal) and beta (rhombohedral), have very similar physical properties, except the graphene layers stack slightly differently. The alpha graphite may be either flat or buckled. The alpha form can be converted to the beta form through mechanical treatment and the beta form reverts to the alpha form when it is heated above 1300 \u00b0C.\n\n\n=== Other properties ===\n\nThe acoustic and thermal properties of graphite are highly anisotropic, since phonons propagate quickly along the tightly-bound planes, but are slower to travel from one plane to another. Graphite's high thermal stability and electrical conductivity facilitate its widespread use as electrodes and refractories in high temperature material processing applications. However, in oxygen containing atmospheres graphite readily oxidizes to form CO2 at temperatures of 700 \u00b0C and above.\n\nGraphite is an electric conductor, consequently, useful in such applications as arc lamp electrodes. It can conduct electricity due to the vast electron delocalization within the carbon layers (a phenomenon called aromaticity). These valence electrons are free to move, so are able to conduct electricity. However, the electricity is primarily conducted within the plane of the layers. The conductive properties of powdered graphite allows its use as pressure sensor in carbon microphones.\nGraphite and graphite powder are valued in industrial applications for their self-lubricating and dry lubricating properties. There is a common belief that graphite's lubricating properties are solely due to the loose interlamellar coupling between sheets in the structure. However, it has been shown that in a vacuum environment (such as in technologies for use in space), graphite degrades as a lubricant, due to the hypoxic conditions This observation led to the hypothesis that the lubrication is due to the presence of fluids between the layers, such as air and water, which are naturally adsorbed from the environment. This hypothesis has been refuted by studies showing that air and water are not absorbed. Recent studies suggest that an effect called superlubricity can also account for graphite's lubricating properties. The use of graphite is limited by its tendency to facilitate pitting corrosion in some stainless steel, and to promote galvanic corrosion between dissimilar metals (due to its electrical conductivity). It is also corrosive to aluminium in the presence of moisture. For this reason, the US Air Force banned its use as a lubricant in aluminium aircraft, and discouraged its use in aluminium-containing automatic weapons. Even graphite pencil marks on aluminium parts may facilitate corrosion. Another high-temperature lubricant, hexagonal boron nitride, has the same molecular structure as graphite. It is sometimes called white graphite, due to its similar properties.\nWhen a large number of crystallographic defects bind these planes together, graphite loses its lubrication properties and becomes what is known as pyrolytic graphite. It is also highly anisotropic, and diamagnetic, thus it will float in mid-air above a strong magnet. If it is made in a fluidized bed at 1000\u20131300 \u00b0C then it is isotropic turbostratic, and is used in blood contacting devices like mechanical heart valves and is called pyrolytic carbon, and is not diamagnetic. Pyrolytic graphite, and pyrolytic carbon are often confused but are very different materials.\nNatural and crystalline graphites are not often used in pure form as structural materials, due to their shear-planes, brittleness and inconsistent mechanical properties.\n\n\n== History of natural graphite use ==\nIn the 4th millennium B.C., during the Neolithic Age in southeastern Europe, the Mari\u0163a culture used graphite in a ceramic paint for decorating pottery.\nSome time before 1565 (some sources say as early as 1500), an enormous deposit of graphite was discovered on the approach to Grey Knotts from the hamlet of Seathwaite in Borrowdale parish, Cumbria, England, which the locals found very useful for marking sheep. During the reign of Elizabeth I (1533\u20131603), Borrowdale graphite was used as a refractory material to line moulds for cannonballs, resulting in rounder, smoother balls that could be fired farther, contributing to the strength of the English navy. This particular deposit of graphite was extremely pure and soft, and could easily be broken into sticks. Because of its military importance, this unique mine and its production were strictly controlled by the Crown.\n\n\n=== Other names ===\nHistorically, graphite was called black lead or plumbago. Plumbago was commonly used in its massive mineral form. Both of these names arise from confusion with the similar-appearing lead ores, particularly galena. The Latin word for lead, plumbum, gave its name to the English term for this grey metallic-sheened mineral and even to the leadworts or plumbagos, plants with flowers that resemble this colour.\nThe term black lead usually refers to a powdered or processed graphite, matte black in color.\nAbraham Gottlob Werner coined the name graphite (\"writing stone\") in 1789. He attempted to clear up the confusion between molybdena, plumbago and blacklead after Carl Wilhelm Scheele in 1778 proved that there are at least three different minerals. Scheele's analysis showed that the chemical compounds molybdenum sulfide (molybdenite), lead(II) sulfide (galena) and graphite were three different soft black minerals.\n\n\n== Uses of natural graphite ==\nNatural graphite is mostly consumed for refractories, batteries, steelmaking, expanded graphite, brake linings, foundry facings and lubricants. Graphene, which occurs naturally in graphite, has unique physical properties and is among the strongest substances known. However, the process of separating it from graphite will require more technological development.\n\n\n=== Refractories ===\nThis end-use began before 1900 with the graphite crucible used to hold molten metal; this is now a minor part of refractories. In the mid-1980s, the carbon-magnesite brick became important, and a bit later the alumina-graphite shape. Currently the order of importance is alumina-graphite shapes, carbon-magnesite brick, monolithics (gunning and ramming mixes), and then crucibles.\nCrucibles began using very large flake graphite, and carbon-magnesite brick requiring not quite so large flake graphite; for these and others there is now much more flexibility in size of flake required, and amorphous graphite is no longer restricted to low-end refractories. Alumina-graphite shapes are used as continuous casting ware, such as nozzles and troughs, to convey the molten steel from ladle to mold, and carbon magnesite bricks line steel converters and electric arc furnaces to withstand extreme temperatures. Graphite Blocks are also used in parts of blast furnace linings where the high thermal conductivity of the graphite is critical. High-purity monolithics are often used as a continuous furnace lining instead of the carbon-magnesite bricks.\nThe US and European refractories industry had a crisis in 2000\u20132003, with an indifferent market for steel and a declining refractory consumption per tonne of steel underlying firm buyouts and many plant closures. Many of the plant closures resulted from the acquisition of Harbison-Walker Refractories by RHI AG and some plants had their equipment auctioned off. Since much of the lost capacity was for carbon-magnesite brick, graphite consumption within refractories area moved towards alumina-graphite shapes and monolithics, and away from the brick. The major source of carbon-magnesite brick is now imports from China. Almost all of the above refractories are used to make steel and account for 75% of refractory consumption; the rest is used by a variety of industries, such as cement.\nAccording to the USGS, US natural graphite consumption in refractories was 12,500 tonnes in 2010.\n\n\n=== Batteries ===\nThe use of graphite in batteries has been increasing in the last 30 years. Natural and synthetic graphite are used to construct the anode of all major battery technologies. The lithium-ion battery utilizes roughly twice the amount of graphite than lithium carbonate.\nThe demand for batteries, primarily nickel-metal-hydride and lithium-ion batteries, has caused a growth in graphite demand in the late 1980s and early 1990s. This growth was driven by portable electronics, such as portable CD players and power tools. Laptops, mobile phones, tablet, and smartphone products have increased the demand for batteries. Electric vehicle batteries are anticipated to increase graphite demand. As an example, a lithium-ion battery in a fully electric Nissan Leaf contains nearly 40 kg of graphite.\n\n\n=== Steelmaking ===\nNatural graphite in this end use mostly goes into carbon raising in molten steel, although it can be used to lubricate the dies used to extrude hot steel. Supplying carbon raisers is very competitive, therefore subject to cut-throat pricing from alternatives such as synthetic graphite powder, petroleum coke, and other forms of carbon. A carbon raiser is added to increase the carbon content of the steel to the specified level. An estimate based on USGS US graphite consumption statistics indicates that 10,500 tonnes were used in this fashion in 2005.\n\n\n=== Brake linings ===\nNatural amorphous and fine flake graphite are used in brake linings or brake shoes for heavier (nonautomotive) vehicles, and became important with the need to substitute for asbestos. This use has been important for quite some time, but nonasbestos organic (NAO) compositions are beginning to reduce graphite's market share. A brake-lining industry shake-out with some plant closures has not been beneficial, nor has an indifferent automotive market. According to the USGS, US natural graphite consumption in brake linings was 6,510 tonnes in 2005.\n\n\n=== Foundry facings and lubricants ===\nA foundry facing mold wash is a water-based paint of amorphous or fine flake graphite. Painting the inside of a mold with it and letting it dry leaves a fine graphite coat that will ease separation of the object cast after the hot metal has cooled. Graphite lubricants are specialty items for use at very high or very low temperatures, as forging die lubricant, an antiseize agent, a gear lubricant for mining machinery, and to lubricate locks. Having low-grit graphite, or even better no-grit graphite (ultra high purity), is highly desirable. It can be used as a dry powder, in water or oil, or as colloidal graphite (a permanent suspension in a liquid). An estimate based on USGS graphite consumption statistics indicates that 2,200 tonnes was used in this fashion in 2005.\n\n\n=== Pencils ===\n\nThe ability to leave marks on paper and other objects gave graphite its name, given in 1789 by German mineralogist Abraham Gottlob Werner. It stems from graphein, meaning to write/draw in Ancient Greek.\nFrom the 16th Century, pencils were made with leads of English natural graphite, but modern pencil lead is most commonly a mix of powdered graphite and clay; it was invented by Nicolas-Jacques Cont\u00e9 in 1795. It is chemically unrelated to the metal lead, whose ores had a similar appearance, hence the continuation of the name. Plumbago is another older term for natural graphite used for drawing, typically as a lump of the mineral without a wood casing. The term plumbago drawing is normally restricted to 17th and 18th century works, mostly portraits.\nToday, pencils are still a small but significant market for natural graphite. Around 7% of the 1.1 million tonnes produced in 2011 was used to make pencils. Low-quality amorphous graphite is used and sourced mainly from China.\n\n\n=== Other uses ===\nNatural graphite has found uses in zinc-carbon batteries, in electric motor brushes, and various specialized applications. Graphite of various hardness or softness results in different qualities and tones when used as an artistic medium. Railroads would often mix powdered graphite with waste oil or linseed oil to create a heat resistant protective coating for the exposed portions of a steam locomotive's boiler, such as the smokebox or lower part of the firebox.\n\n\n=== Expanded graphite ===\nExpanded graphite is made by immersing natural flake graphite in a bath of chromic acid, then concentrated sulfuric acid, which forces the crystal lattice planes apart, thus expanding the graphite. The expanded graphite can be used to make graphite foil or used directly as \"hot top\" compound to insulate molten metal in a ladle or red-hot steel ingots and decrease heat loss, or as firestops fitted around a fire door or in sheet metal collars surrounding plastic pipe (during a fire, the graphite expands and chars to resist fire penetration and spread), or to make high-performance gasket material for high-temperature use. After being made into graphite foil, the foil is machined and assembled into the bipolar plates in fuel cells. The foil is made into heat sinks for laptop computers which keeps them cool while saving weight, and is made into a foil laminate that can be used in valve packings or made into gaskets. Old-style packings are now a minor member of this grouping: fine flake graphite in oils or greases for uses requiring heat resistance. A GAN estimate of current US natural graphite consumption in this end use is 7,500 tonnes.\n\n\n=== Intercalated graphite ===\n\nGraphite forms intercalation compounds with some metals and small molecules. In these compounds, the host molecule or atom gets \"sandwiched\" between the graphite layers, resulting in a type of compounds with variable stoichiometry. A prominent example of an intercalation compound is potassium graphite, denoted by the formula KC8. Graphite intercalation compounds are superconductors. The highest transition temperature (by June 2009) Tc = 11.5 K is achieved in CaC6, and it further increases under applied pressure (15.1 K at 8 GPa).\n\n\n== Uses of synthetic graphite ==\n\n\n=== Invention of a process to produce synthetic graphite ===\nA process to make synthetic graphite was invented accidentally by Edward Goodrich Acheson (1856\u20131931). In the mid-1890s, Acheson discovered that overheating carborundum, which he is also credited with discovering, produced almost pure graphite. While studying the effects of high temperature on carborundum, he had found that silicon vaporizes at about 4,150 \u00b0C (7,500 \u00b0F), leaving the carbon behind in graphitic carbon. This graphite was another major discovery for him, and it became extremely valuable and helpful as a lubricant.\nIn 1896 Acheson received a patent for his method of synthesizing graphite, and in 1897 started commercial production. The Acheson Graphite Co. was formed in 1899. In 1928 this company was merged with National Carbon Company (now GrafTech International). Acheson also developed a variety of colloidal graphite products including Oildag and Aquadag. These were later manufactured by the Acheson Colloids Co. (now Acheson Industries, a unit of Henkel AG).\n\n\n=== Scientific research ===\nHighly oriented pyrolytic graphite (HOPG) is the highest-quality synthetic form of graphite. It is used in scientific research, in particular, as a length standard for scanner calibration of scanning probe microscope.\n\n\n=== Electrodes ===\nGraphite electrodes carry the electricity that melts scrap iron and steel, and sometimes direct-reduced iron (DRI), in electric arc furnaces, which are the vast majority of steel furnaces. They are made from petroleum coke after it is mixed with coal tar pitch. They are then extruded and shaped, baked to carbonize the binder (pitch), and finally graphitized by heating it to temperatures approaching 3000 \u00b0C, at which the carbon atoms arrange into graphite. They can vary in size up to 11 feet long and 30 inches in diameter. An increasing proportion of global steel is made using electric arc furnaces, and the electric arc furnace itself is getting more efficient, making more steel per tonne of electrode. An estimate based on USGS data indicates that graphite electrode consumption was 197,000 tonnes in 2005.\nElectrolytic aluminium smelting also uses graphitic carbon electrodes. On a much smaller scale, synthetic graphite electrodes are used in electrical discharge machining (EDM), commonly to make injection molds for plastics.\n\n\n=== Powder and scrap ===\nThe powder is made by heating powdered petroleum coke above the temperature of graphitization, sometimes with minor modifications. The graphite scrap comes from pieces of unusable electrode material (in the manufacturing stage or after use) and lathe turnings, usually after crushing and sizing. Most synthetic graphite powder goes to carbon raising in steel (competing with natural graphite), with some used in batteries and brake linings. According to the USGS, US synthetic graphite powder and scrap production was 95,000 tonnes in 2001 (latest data).\n\n\n=== Neutron moderator ===\n\nSpecial grades of synthetic graphite also find use as a matrix and neutron moderator within nuclear reactors. Its low neutron cross-section also recommends it for use in proposed fusion reactors. Care must be taken that reactor-grade graphite is free of neutron absorbing materials such as boron, widely used as the seed electrode in commercial graphite deposition systems\u2014this caused the failure of the Germans' World War II graphite-based nuclear reactors. Since they could not isolate the difficulty they were forced to use far more expensive heavy water moderators. Graphite used for nuclear reactors is often referred to as nuclear graphite.\n\n\n=== Other uses ===\nGraphite (carbon) fiber and carbon nanotubes are also used in carbon fiber reinforced plastics, and in heat-resistant composites such as reinforced carbon-carbon (RCC). Commercial structures made from carbon fiber graphite composites include fishing rods, golf club shafts, bicycle frames, sports car body panels, the fuselage of the Boeing 787 Dreamliner and pool cue sticks and have been successfully employed in reinforced concrete, The mechanical properties of carbon fiber graphite-reinforced plastic composites and grey cast iron are strongly influenced by the role of graphite in these materials. In this context, the term \"(100%) graphite\" is often loosely used to refer to a pure mixture of carbon reinforcement and resin, while the term \"composite\" is used for composite materials with additional ingredients.\nModern smokeless powder is coated in graphite to prevent the buildup of static charge.\nGraphite has been used in at least three radar absorbent materials. It was mixed with rubber in Sumpf and Schornsteinfeger, which were used on U-boat snorkels to reduce their radar cross section. It was also used in tiles on early F-117 Nighthawk (1983)s.\n\n\n== Graphite mining, beneficiation, and milling ==\n\nGraphite is mined by both open pit and underground methods. Graphite usually needs beneficiation. This may be carried out by hand-picking the pieces of gangue (rock) and hand-screening the product or by crushing the rock and floating out the graphite. Beneficiation by flotation encounters the difficulty that graphite is very soft and \"marks\" (coats) the particles of gangue. This makes the \"marked\" gangue particles float off with the graphite, yielding impure concentrate. There are two ways of obtaining a commercial concentrate or product: repeated regrinding and floating (up to seven times) to purify the concentrate, or by acid leaching (dissolving) the gangue with hydrofluoric acid (for a silicate gangue) or hydrochloric acid (for a carbonate gangue).\nIn milling, the incoming graphite products and concentrates can be ground before being classified (sized or screened), with the coarser flake size fractions (below 8 mesh, 8\u201320 mesh, 20\u201350 mesh) carefully preserved, and then the carbon contents are determined. Some standard blends can be prepared from the different fractions, each with a certain flake size distribution and carbon content. Custom blends can also be made for individual customers who want a certain flake size distribution and carbon content. If flake size is unimportant, the concentrate can be ground more freely. Typical end products include a fine powder for use as a slurry in oil drilling and coatings for foundry molds, carbon raiser in the steel industry (Synthetic graphite powder and powdered petroleum coke can also be used as carbon raiser). Environmental impacts from graphite mills consist of air pollution including fine particulate exposure of workers and also soil contamination from powder spillages leading to heavy metal contamination of soil.\n\n\n=== Occupational safety ===\nPeople can be exposed to graphite in the workplace by breathing it in, skin contact, and eye contact.\n\n\n==== United States ====\nThe Occupational Safety and Health Administration (OSHA) has set the legal limit (permissible exposure limit) for graphite exposure in the workplace as a time weighted average (TWA) of 15 million particles per cubic foot (1.5 mg/m3) over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of TWA 2.5 mg/m3 respirable dust over an 8-hour workday. At levels of 1250 mg/m3, graphite is immediately dangerous to life and health.\n\n\n== Graphite recycling ==\nThe most common way of recycling graphite occurs when synthetic graphite electrodes are either manufactured and pieces are cut off or lathe turnings are discarded, or the electrode (or other) are used all the way down to the electrode holder. A new electrode replaces the old one, but a sizeable piece of the old electrode remains. This is crushed and sized, and the resulting graphite powder is mostly used to raise the carbon content of molten steel. Graphite-containing refractories are sometimes also recycled, but often not because of their graphite: the largest-volume items, such as carbon-magnesite bricks that contain only 15\u201325% graphite, usually contain too little graphite. However, some recycled carbon-magnesite brick is used as the basis for furnace-repair materials, and also crushed carbon-magnesite brick is used in slag conditioners. While crucibles have a high graphite content, the volume of crucibles used and then recycled is very small.\nA high-quality flake graphite product that closely resembles natural flake graphite can be made from steelmaking kish. Kish is a large-volume near-molten waste skimmed from the molten iron feed to a basic oxygen furnace, and consists of a mix of graphite (precipitated out of the supersaturated iron), lime-rich slag, and some iron. The iron is recycled on site, leaving a mixture of graphite and slag. The best recovery process uses hydraulic classification (which utilizes a flow of water to separate minerals by specific gravity: graphite is light and settles nearly last) to get a 70% graphite rough concentrate. Leaching this concentrate with hydrochloric acid gives a 95% graphite product with a flake size ranging from 10 mesh down.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nC.Michael Hogan; Marc Papineau; et al. (December 18, 1989). Phase I Environmental Site Assessment, Asbury Graphite Mill, 2426\u20132500 Kirkham Street, Oakland, California, Earth Metrics report 10292.001 (Report). \nKlein, Cornelis; Cornelius S. Hurlbut, Jr. (1985). Manual of Mineralogy: after Dana (20th ed.). ISBN 0-471-80580-7. \nTaylor, Harold A. (2000). Graphite. Financial Times Executive Commodity Reports. London: Mining Journal Books ltd. ISBN 1-84083-332-7. \nTaylor, Harold A. (2005). Graphite. Industrial Minerals and Rocks (7th ed.). Littleton, CO: AIME-Society of Mining Engineers. ISBN 0-87335-233-5. \n\n\n== External links ==\nBattery Grade Graphite\nGraphite at Minerals.net\nMineral galleries\nMineral & Exploration \u2013 Map of World Graphite Mines and Producers 2012\nMindat w/ locations\ngiant covalent structures\nThe Graphite Page\nVideo lecture on the properties of graphite by Prof. M. Heggie, University of Sussex\nCDC \u2013 NIOSH Pocket Guide to Chemical Hazards", 
                "titleUrl": "https://en.wikipedia.org/wiki/Graphite", 
                "title": "Graphite"
            }, 
            {
                "snippet": "rigger options.  V1 - It is constructed of multiple layers of intermediate modulus, unidirectional carbon fiber and Nomex Honeycomb cured at 250 degrees.", 
                "pageCategories": "Rowing equipment manufacturers", 
                "pageContent": "Vespoli USA is a manufacturer of racing shells for rowing. It was founded by former Georgetown University rower, Olympian, and World Rowing Champion Mike Vespoli in 1980. It is located on the Quinnipiac River in New Haven, Connecticut, United States.\n\n\n== History ==\nThe company\u2019s origins date back to the late 1970s when Mike Vespoli was the freshman rowing coach at Yale University. It was during a trip to the Henley Royal Regatta in 1977 that he saw the forefather of the shell that changed his life. The now defunct British firm, Carbocraft Ltd, was building shells that sandwiched honeycomb between composite fibers. \"I saw a very advanced shell, realizing, in America at the time, we were rowing in either traditionally built wooden shells or a wooden shell frame with fiberglass skin over it,\" Vespoli said. He began to augment a modest coaching salary by becoming Carbocraft's American sales representative. In their day, Carbocrafts were exceptional boats, and the sport of rowing owes Carbocraft for the breakthroughs the company made in carbon-honeycomb shells.\nAfter three years as Carbocraft's agent, Mike Vespoli realized importing boats led to unacceptable shortfalls in service. Customers were dissatisfied with fluctuating boat prices due to unpredictable exchange rates, an unreliable supply of spare parts, and uncertain delivery dates. In the summer of 1980, he decided to leave coaching to become a boat builder. By the fall, Mike had secured the rights to manufacture Carbocraft shells in the United States and opened a 4,000 sq ft (370 m2). shop in Hamden, CT with his father and two employees, Peter Smith, a plant foreman from England, and Ed Kloman, an oarsman turned business executive. In the beginning Vespoli kept the basic Carbocraft design, which came to be known as the \u201cA\u201d hull model but he improved the construction techniques. Vespoli's new shells were well accepted and soon he talked the city of New Haven into selling him land for a modern boat-building plant.\nThe end of the decade was a busy time for Vespoli. In 1986 Vespoli moved production to its current location in New Haven. In 1988, Vespoli initiated the most comprehensive rowing shell research ever. Utilizing the talents of America\u2019s Cup winning naval architect Bruce Nelson and hydrodynamics\u2019 expert, Dr. Carl Scragg, they were the first to simulate shell movement in a tow tank with \u201cunsteady\u201d surge and pitch. Thus the \u201cD\u201d hull model was born. The \"D\" hull which became wildly accepted and praised during the 1990s had a very inauspicious start. Some contest that the United States Heavyweight Eight might have won the gold medal at the 1988 Seoul Olympics, or probably would have won the silver, instead of taking the bronze behind West Germany and the Soviet Union, if they had chosen to use the new \u201cD\u201d hull rather than the familiar 'A' hull, called Copenhagen Gold, which the crew preferred.\nIn the first half of this decade, Vespoli partnered with naval architect, Manolo Ruiz DeElvira, the principal designer of Alinghi, winner of the 2003 and 2007 America\u2019s Cup. Manolo used to the most advanced hull design programs in the world to design Vespoli\u2019s next generation of hulls, the \u201cE\u201d Hull. The \"E\" hull features increased stability and lower wetted surface [the dominant factor in slowing a rowing boat]. This model offers both wing and side mount rigger options.\n\n\n== Models ==\nV1 - It is constructed of multiple layers of intermediate modulus, unidirectional carbon fiber and Nomex Honeycomb cured at 250 degrees. The V1 model features Vespoli\u2019s \u2018rhino skin\u2019 (ultra-dense carbon layers) providing maximum impact resistance and stiffness.\nMillennium-2 First \"Ribless\" Vespoli model. Increased stiffness and cockpit room.\nMillennium - discontinued\nUltralite II - Upgraded version of the Ultralite model. Features 100% 'Unidirectional' carbon fiber.\nUltralite - discontinued\nChallenger - discontinued\nRacer - discontinued\nPerformer - Fiberglass laminate hull construction. Very tough, resilient.\n\n\n== Current models ==\nV1 (4+/x and 8+)\nM2 (4+/x and 8+)\nUltralite II (4+/x and 8+)\nPerformer(2x/-, 4+/x and 8+)\nMatrix (1x and 2x/-)\n\n\n== References ==\n\n\n== External links ==\nVespoli Official Site", 
                "titleUrl": "https://en.wikipedia.org/wiki/Vespoli", 
                "title": "Vespoli"
            }, 
            {
                "snippet": "different layers of steel are made visible during the polishing due to one or both of two reasons: 1.) Either the layers have a variation in carbon content", 
                "pageCategories": "All articles needing additional references\nAll articles that may contain original research\nAll articles with unsourced statements\nArticles containing Japanese-language text\nArticles needing additional references from March 2011\nArticles needing additional references from November 2010\nArticles that may contain original research from September 2007\nArticles with unsourced statements from October 2013\nArticles with unsourced statements from October 2016\nArticles with unsourced statements from September 2012", 
                "pageContent": "Japanese swordsmithing is the labour-intensive bladesmithing process developed in Japan for forging traditionally made bladed weapons (nihonto) including katana, wakizashi, tant\u014d, yari, naginata, nagamaki, tachi, uchigatana, nodachi, \u014ddachi, kodachi, and ya (arrow).\nJapanese sword blades were often forged with different profiles, different blade thicknesses, and varying amounts of grind. Wakizashi and tant\u014d were not simply scaled-down katana; they were often forged without ridge (hira-zukuri) or other such forms which were very rare on katana.\n\n\n== Traditional methods ==\n\n\n=== Steel production ===\nThe steel used is known as tamahagane (\u7389\u92fc:\u305f\u307e\u306f\u304c\u306d), or \"jewel steel\" (tama - ball or jewel, hagane - steel). Tamahagane is produced from iron sand, a source of iron ore, and mainly used to make Samurai swords, such as the katana, and some tools.\n\nThe smelting process used is different from the modern mass production of steel. A clay vessel about 4 ft (1.2 m) tall, 12 ft (3.7 m) long, and 4 ft (1.2 m) wide is constructed. This is known as a tatara. After the clay tub has set, it is fired until dry. A charcoal fire is started from soft pine charcoal. Then the smelter will wait for the fire to reach the correct temperature. At that point he will direct the addition of iron sand known as satetsu. This will be layered in with more charcoal and more iron sand over the next 72 hours. Four or five people need to constantly work on this process. It takes about a week to build the tatara and complete the iron conversion to steel. Because the charcoal cannot exceed the melting point of iron, the steel is not able to become fully molten, and this allows both high and low carbon material to be created and separated once cooled. When complete, the Tatara is broken to remove the steel bloom, known as a kera. At the end of the process the tatara will have consumed about 10 short tons (9.1 t) of satetsu and 12 short tons (11 t) of charcoal leaving about 2.5 short tons (2.3 t) of kera, from which less than a ton of tamahagane can be produced. A single kera can typically be worth hundreds of thousands of dollars, making it many times more expensive than modern steels.\n\nThe swordsmiths will carefully break the kera apart, and separate the various carbon steels. The lowest carbon steel is called hocho-tetsu (soft iron), which is used for the shingane (translated as \"core-steel\") of the blade. The high carbon steel (tamahagane) and higher carbon pig iron, called nabe-gane, will then be forged in alternating layers, using very intricate methods to form the very hard steel for the edge, called hagane (edge steel), and the springy metal jacket, called kawagane (skin steel). The most useful process is the folding, where the metals are forge welded, folded, and welded again, as many as 16 times. The folding removes impurities and helps even out the carbon content, while the alternating layers combine hardness with ductility to greatly enhance the toughness. Currently, tamahagane is only made three or four times a year by Nittoho and Hitachi Metals during winter in a wood building and is only sold to the master swordsmiths to use once it is made.\n\n\n=== Construction ===\n\nThe forging of a Japanese blade typically took many days or weeks, and was considered a sacred art, traditionally accompanied by a large panoply of Shinto religious rituals. As with many complex endeavors, rather than a single craftsman, several artists were involved. There was a smith to forge the rough shape, often a second smith (apprentice) to fold the metal, a specialist polisher, and even a specialist for the edge itself. Often, there were sheath, hilt, and handguard specialists as well.\n\n\n==== Forging ====\n\nThe steel bloom, or kera, that is produced in the tatara contains steel that varies greatly in carbon content, ranging from wrought iron to pig iron. Three types of steel are chosen for the blade; a very low carbon steel called hocho-tetsu is used for the core of the blade (shingane). The high carbon steel (tamahagane), and the remelted pig iron (cast iron or nabe-gane), are combined to form the outer skin of the blade (kawagane). Only about 1/3 of the kera produces steel that is suitable for sword production.\nThe best known part of the manufacturing process is the folding of the steel, where the swords are made by repeatedly heating, hammering and folding the metal. The process of folding metal to improve strength and remove impurities is frequently attributed to specific Japanese smiths in legend.\nIn traditional Japanese sword making, the low-carbon iron is folded several times by itself, to purify it. This produces the soft metal to be used for the core of the blade. The high-carbon steel and the higher-carbon cast-iron are then forged in alternating layers. The cast-iron is heated, quenched in water, and then broken into small pieces to help free it from slag. The steel is then forged into a single plate, and the pieces of cast-iron are piled on top, and the whole thing is forge welded into a single block, which is called the age-kitae process. The block is then elongated, cut, folded, and forge welded again. The steel can be folded transversely, (from front to back), or longitudinally, (from side to side). Often both folding directions are used to produce the desired grain pattern. This process, called the shita-kitae, is repeated from 8 to as many as 16 times. After 20 foldings, (220, or about a million individual layers), there is too much diffusion in the carbon content. The steel becomes almost homogeneous in this respect, and the act of folding no longer gives any benefit to the steel. Depending on the amount of carbon introduced, this process forms either the very hard steel for the edge (hagane), or the slightly less hardenable spring steel (kawagane) which is often used for the sides and the back.\n\nDuring the last few foldings, the steel may be forged into several thin plates, stacked, and forge welded into a brick. The grain of the steel is carefully positioned between adjacent layers, with the exact configuration dependent on the part of the blade for which the steel will be used.\nBetween each heating and folding, the steel is coated in a mixture of clay, water and straw-ash to protect it from oxidation and carburization. This clay provides a highly reducing environment. At around 1,650 \u00b0F (900 \u00b0C), the heat and water from the clay promote the formation of a wustite layer, which is a type of iron oxide formed in the absence of oxygen. In this reducing environment, the silicon in the clay reacts with wustite to form fayalite and, at around 2,190 \u00b0F (1,200 \u00b0C), the fayalite will become a liquid. This liquid acts as a flux, attracting impurities, and pulls out the impurities as it is squeezed from between the layers. This leaves a very pure surface which, in turn, helps facilitate the forge-welding process. Due to the loss of impurities, slag, and iron in the form of sparks during the hammering, by the end of forging the steel may be reduced to as little as 1/10 of its initial weight. This practice became popular due to the use of highly impure metals, stemming from the low temperature yielded in the smelting at that time and place. The folding did several things:\nIt provided alternating layers of differing hardenability. During quenching, the high carbon layers achieve greater hardness than the medium carbon layers. The hardness of the high carbon steels combine with the ductility of the low carbon steels to form the property of toughness.\nIt eliminated any voids in the metal.\nIt homogenized the metal, spreading the elements (such as carbon) evenly throughout - increasing the effective strength by decreasing the number of potential weak points.\nIt burned off many impurities, helping to overcome the poor quality of the raw Japanese steel.\nIt created up to 65000 layers, by continuously decarburizing the surface and bringing it into the blade's interior, which gives the swords their grain (for comparison see pattern welding).\nGenerally, swords were created with the grain of the blade (hada) running down the blade like the grain on a plank of wood. Straight grains were called masame-hada, wood-like grain itame, wood-burl grain mokume, and concentric wavy grain (an uncommon feature seen almost exclusively in the Gassan school) ayasugi-hada. The difference between the first three grains is that of cutting a tree along the grain, at an angle, and perpendicular to its direction of growth (mokume-gane) respectively, the angle causing the \"stretched\" pattern.\n\n\n==== Assembly ====\n\nIn addition to folding the steel, high quality Japanese swords are also composed of various distinct sections of different types of steel. This manufacturing technique uses different types of steel in different parts of the sword to accentuate the desired characteristics in various parts of the sword beyond the level offered by differential heat treatment.\nThe vast majority of modern katana and wakizashi are the maru type (sometimes also called muku) which is the most basic, with the entire sword being composed of a single steel. The kobuse type is made using two steels, which are called hagane (edge steel) and shingane (core steel). Honsanmai and shihozume types add the third steel, called kawagane (skin steel). There are almost an infinite number of ways the steel could be assembled, which often varied considerably from smith to smith. Sometimes the edge-steel is \"drawn out\" (hammered into a bar), bent into a 'U' shaped trough, and the very soft core steel is inserted into the harder piece. Then they are forge welded together and hammered into the basic shape of the sword. By the end of the process, the two pieces of steel are fused together, but retain their differences in hardenability. The more complex types of construction are typically only found in antique weapons, with the vast majority of modern weapons being composed of a single section, or at most two or three sections.\nAnother way is to assemble the different pieces into a block, forge weld it together, and then draw out the steel into a sword so that the correct steel ends up in the desired place. This method is often used for the complex models, which allow for parrying without fear of damaging the side of the blade. To make honsanmai or shihozume types, pieces of hard steel are added to the outside of the blade in a similar fashion. The shihozume and soshu types are quite rare, but added a rear support.\n\n\n==== Geometry (shape and form) ====\n\nDuring the Bronze Age, the swords found in Japan were very similar in shape to those found in continental Asia, i.e., China or Korea, and the Japanese adopted the Chinese convention for sword terminology along with metallurgy and swordmaking technology, classifying swords into the (either straight or curved) single-edged variety called t\u014d \u5200 and the (straight) double-edged variety called ken \u5263. There is some small overlap in that there were some double-edged curved swords such as Tulwars or Scimitars which were called T\u014d, because the curvature meant that the \"front\" edge was used in the overwhelming majority of instances.\nOver time, however, the curved single-edged sword became so dominant a style in Japan that tou and ken came to be used interchangeably to refer to swords in Japan and by others to refer to Japanese swords. For example, the Japanese typically refer to Japanese swords as \u65e5\u672c\u5200 nihont\u014d (\"Japanese tou\" i.e. \"Japanese (single-edged) blade\"), while the character ken \u5263 is used in such terms as kendo and kenjutsu. Modern formal usage often uses both characters in referring to a collection of swords, for example, in naming the The Japanese Sword Museum \u65e5\u672c\u7f8e\u8853\u5200\u5263\u535a\u7269\u9928.\nThe prototype of the Japanese sword was the chokut\u014d \u76f4\u5200, or \"straight (single-edged) sword\", a design that can be fairly described as a Japanese sword without any curvature, with a handle that is usually only a few inches long and therefore suitable for single-handed use only, with a sword guard that is prominent only on the front (where the edge is pointed) and back sides and sometimes only on the front side of the sword blade, and with a ring pommel. This design was moderately common in China and Korea during the Warring States and Han Dynasties, fading from popularity and disappearing during the Tang Dynasty. A number of such swords have been excavated in Japan from graves dating back to the kofun period.\nAs the chokut\u014d evolved into the Japanese sword as it is known today, it acquired its characteristic curvature and Japanese style fittings, including the long handle making it suitable for either one-handed or two-handed use, the non-protruding pommel, and a handguard that protruded from the sword in all directions, i.e., that is not a cross piece or a guard for the edge or edge and back sides of the blade only but a guard intended to protect the hand on all sides of the blade. The shape of the Japanese tsuba evolved in parallel with Japanese swordsmithing and Japanese swordsmanship. As Japanese swordsmiths acquired the ability to achieve an extremely hard edge, Japanese swordsmanship evolved to protect the edge against chipping, notching, and breakage by parrying with the sides or backs of swords, avoiding edge-to-edge contact. This in turn resulted in the need to protect the sword hand from a sliding blade in parries on the sides and backs, i.e., parts of the blade other than the edge side, forming the rationale behind the Japanese styled tsuba, which protrudes from the blade in every direction.\nThis style of parrying in Japanese swordsmanship has also resulted in some antique swords that have been used in battle exhibiting notches on the sides or backs of blades.\n\n\n==== Heat treating ====\n\nHaving a single edge provides certain advantages; one being that the rest of the sword can be used to reinforce and support the edge. The Japanese style of sword-making takes full advantage of this. When forging is complete, the steel is not quenched in the conventional European fashion (i.e.: uniformly throughout the blade). Steel\u2019s exact flex and strength vary dramatically with heat treating. If steel cools quickly it becomes martensite, which is very hard but brittle. Slower and it becomes pearlite, which bends easily and does not hold an edge. To maximize both the cutting edge and the resilience of the sword spine, a technique of differential heat-treatment is used. In this specific process, referred to as differential hardening or differential quenching, the sword is painted with layers of clay before heating, providing a thin layer or none at all on the edge of the sword, ensuring quick cooling to maximize the hardening for the edge. A thicker layer of clay is applied to the rest of the blade, causing slower cooling. This creates softer, more resilient steel, allowing the blade to absorb shock without breaking. This process is sometimes erroneously called differential tempering but this is actually an entirely different form of heat treatment.\nTo produce a difference in hardness, the steel is cooled at different rates by controlling the thickness of the insulating layer. By carefully controlling the heating and cooling speeds of different parts of the blade, Japanese swordsmiths were able to produce a blade that had a softer body and a hard edge. This process also has two side effects that have come to characterize Japanese swords: 1.) It causes the blade to curve and 2.) It produces a visible boundary between the hard and soft steel. When quenched, the uninsulated edge contracts, causing the sword to first bend towards the edge. However, the edge cannot contract fully before the martensite forms, because the rest of the sword remains hot and in a thermally expanded state. Because of the insulation, the sword spine remains hot and pliable for several seconds, but then contracts much more than the edge, causing the sword to bend away from the edge, which aids the smith in establishing the curvature of the blade. Also, the differentiated hardness and the methods of polishing the steel can result in the hamon \u5203\u7d0b (frequently translated as \"tempering line\" but better translated as \"hardening pattern\"). The hamon is the visible outline of the yakiba (hardened portion) and is used as a factor to judge both the quality and beauty of the finished blade. The various hamon patterns result from the manner in which the clay is applied. They can also act as an indicator of the style of sword-making, and sometimes as a signature for the individual smith. The differences in the hardenability of steels may be enhanced near the hamon, revealing layers or even different parts of the blade, such as the intersection between an edge made from edge-steel and sides made from skin-steel.\nIf the thickness of the coating on the edge is balanced just right with the temperature of the water, the proper hardness can be produced without the need for tempering. However, in most cases, the edge will end up being too hard, so tempering the entire blade evenly for a short time is usually required to bring the hardness down to a more suitable point. The ideal hardness is usually between HRc58\u201360 on the Rockwell hardness scale. Tempering is performed by heating the entire blade evenly to around 400 \u00b0F (204 \u00b0C), reducing the hardness in the martensite and turning it into a form of tempered martensite. The pearlite, on the other hand, does not respond to tempering, and does not change in hardness.\n\n\n=== Metallurgy ===\n\nTamahagane, as a raw material, is a highly impure metal. Formed in a bloomery process, the kera of sponge iron begins as an inhomogeneous mixture of wrought iron, steels, and pig iron. The pig iron contains more than 2% carbon. The tamahagane has about 1 to 1.5% carbon while the hocho iron contains about 0.2%. Steel that has a carbon content between tamahagane and hocho iron is called bu-kera, which is often resmelted with the pig iron to make saga-hagane, containing roughly 0.7% carbon. Most of the bu-kera, hocho iron and saga-hagane will be sold for making other items, like tools and knives, and only the best pieces of tamahagane, hocho iron, and pig iron are used for swordsmithing.\nThe various metals are also filled with slag, phosphorus and other impurities. Separation of the various metals from the kera was traditionally performed by breaking it apart with small hammers dropped from a certain height, and then examining the fractures, in a process similar to the modern Charpy impact test. The nature of the fractures are different for different types of steel. The tamahagane, in particular, contains pearlite, which produces a characteristic pearlescent-sheen on the crystals.\nDuring the folding process, most of the impurities are removed from the steel, continuously refining the steel while forging. By the end of forging, the steel produced was among the purest steel-alloys of the ancient world. Due to the continuous heating the steel tends to decarburize, so a good quantity of carbon is either extracted from the steel as carbon dioxide or redistributed more evenly through diffusion, leaving a nearly eutectoid composition (containing 0.77 to 0.8% carbon). The edge-steel itself will generally end up with a composition that ranges from eutectoid to slightly hypoeutectoid (containing a carbon content under the eutectoid composition), giving enough hardenability without sacrificing ductility The skin-steel generally has slightly less carbon, often in the range of 0.5%. The core-steel, however, remains nearly pure iron, responding very little to heat treatment. Cyril Stanley Smith, a professor of metallurgical history from MIT, performed an analysis of four different swords, each from a different century, determining the composition of the surface of the blades:\nIn 1993, Jerzy Piaskowski performed an analysis of a katana of the kobuse type by cutting the sword in half and taking a cross section. The analysis revealed a carbon content ranging from 0.6 to 0.8% carbon at the surface, but only 0.2% at the core.\nThe steel in even the ancient swords may have sometimes come from whatever steel was available at the time. Due to its rarity in the ancient world, steel was usually recycled, so broken tools, nails and cookwear often provided a ready supply of steel. Even steel looted from enemies in combat was often valued for its use in swordsmithing.\nAccording to Smith, the different layers of steel are made visible during the polishing due to one or both of two reasons: 1.) Either the layers have a variation in carbon content, or 2.) they have variation in the content of slag inclusions. When the variation is due to slag inclusions by themselves, there will not be a noticeable effect near the hamon, where the yakiba meets the hira. Likewise, there will be no appreciable difference in the local hardness of the individual layers. A difference in slag inclusions generally appear as layers that are somewhat pitted while the adjacent layers are not. In one of the first metallurgical studies, Professor Kuni-ichi Tawara suggests that layers of high slag may have been added for practical as well as decorative reasons. Although slag has a weakening effect on the metal, layers of high slag may have been added to diffuse vibration and dampen recoil, allowing easier use without a significant loss in toughness.\nHowever, when the patterns occur from a difference in carbon content, there will be distinct indications of this near the hamon, because the steel with higher hardenability will become martensite beyond the hamon while the adjacent layers will turn into pearlite. This leaves a distinct pattern of bright nioi, which appear as bright streaks or lines that follow the layers a short distance away from the hamon and into the hira, giving the hamon a wispy or misty appearance. The patterns were most likely revealed during the polishing operation by using a method similar to lapping, without bringing the steel to a full polish, although sometimes chemical reactions with the polishing compounds may have also been used to provide a level of etching. The differences in hardness primarily appear as a difference in the microscopic scratches left on the surface. The harder metal produces shallower scratches, so it diffuses the reflected light, while the softer metal has deeper, longer scratches, appearing either shiny or dark depending on the viewing angle.\n\n\n==== Metallography ====\n\nMetallurgy did not arise as a science until the early 20th century. Before this, metallography was the primary method used for studying metals. Metallography is the study of the patterns in metals, the nature of fractures, and the microscopic crystal-formations. However, neither metallography as a science nor the crystal theory of metals emerged until the invention of the microscope. The ancient swordsmiths had no knowledge of metallurgy, nor did they understand the relationship between carbon and iron. Everything was typically learned by a process of trial-and-error, apprenticeship, and, as sword-making technology was often a closely guarded secret, some espionage. Prior to the 14th century, very little attention was paid to the patterns in the blade as an aesthetic quality. However, the Japanese smiths often prided themselves on their understanding of the internal macro-structure of metals.\nIn Japan, steel-making technology was imported from China, most likely through Korea. The steel used in the Chinese swords, called chi-kang (combined steel), was similar to pattern welding, and edges of it were often forge welded to a back of soft iron, or jou thieh. In trying to copy the Chinese method, the ancient smiths paid much attention to the various properties of steel, and worked to combine them to produce an internal macro-structure that would provide a similar combination of hardness and toughness. Like all trial-and-error, each swordsmith often attempted to produce an internal structure that was superior to swords of their predecessors, or even ones that were better than their own previous designs. The harder metals provided strength, like \"bones\" within the steel, whereas the softer metal provided ductility, allowing the swords to bend before breaking. In ancient times, the Japanese smiths would often display these inhomogeneities in the steel, especially on fittings like the guard or pommel, creating rough, natural surfaces by letting the steel rust or by pickling it in acid, making the internal structure part of the entire aesthetic of the weapon.\nIn later times, this effect was often imitated by partially mixing various metals like copper together with the steel, forming mokume (wood-eye) patterns, although this was unsuitable for the blade itself. After the 14th century, more attention began to be paid to the patterns in the blade as an aesthetic quality. Intentionally decorative forging-techniques were often employed, such as hammering dents in certain locations, which served only to create a wood-eye pattern when the sword was filed and polished into shape, or by intentionally forging-in layers of high slag content. By the 17th century, decorative hardening methods were often being used to increase the beauty of the blade, by shaping the clay. Hamons with trees, flowers, pill boxes, or other shapes became common during this era. By the 19th century, the decorative hamons were often being combined with decorative folding-techniques to create entire landscape-portraits, often portraying specific islands or scenery, crashing waves in the ocean, and misty mountain-peaks.\n\n\n=== Decoration ===\n\nAlmost all blades are decorated, although not all blades are decorated on the visible part of the blade. Once the blade is cool, and the mud is scraped off, the blade has designs and grooves cut into it. One of the most important markings on the sword is performed here: the file markings. These are cut into the tang (nakago), or the hilt-section of the blade, during shaping, where they will be covered by a tsuka or hilt later. The tang is never supposed to be cleaned: doing this can cut the value of the sword in half or more. The purpose is to show how well the blade steel ages. A number of different types of file markings are used, including horizontal, slanted, and checked, known as ichi-monji, ko-sujikai, sujikai, \u014d-sujikai, katte-agari, shinogi-kiri-sujikai, taka-no-ha, and gyaku-taka-no-ha. A grid of marks, from raking the file diagonally both ways across the tang, is called higaki, whereas specialized \"full dress\" file marks are called kesho-yasuri. Lastly, if the blade is very old, it may have been shaved instead of filed. This is called sensuki. While ornamental, these file marks also serve the purpose of providing an uneven surface which bites well into the hilt which fits over it, and is made from wood. It is this pressure fit for the most part that holds the hilt in place, while the mekugi pin serves as a secondary method and a safety.\nSome other marks on the blade are aesthetic: signatures and dedications written in kanji and engravings depicting gods, dragons, or other acceptable beings, called horimono. Some are more practical. The so-called \"blood groove\" or fuller does not in actuality allow blood to flow more freely from cuts made with the sword, but is simply to reduce the weight of the sword while keeping structural integrity and strength. Grooves come in wide (bo-hi), twin narrow (futasuji-hi), twin wide and narrow (bo-hi ni tsure-hi), short (koshi-hi), twin short (gomabushi), twin long with joined tips (shobu-hi), twin long with irregular breaks (kuichigai-hi), and halberd-style (naginata-hi).\n\n\n=== Polishing ===\n\nWhen the rough blade is completed, the swordsmith turns the blade over to a polisher called a togishi, whose job it is to refine the shape of a blade and improve its aesthetic value. The entire process takes considerable time, in some cases easily up to several weeks. Early polishers used three types of stone, whereas a modern polisher generally uses seven. The modern high level of polish was not normally done before around 1600, since greater emphasis was placed on function over form. The polishing process almost always takes longer than even crafting, and a good polish can greatly improve the beauty of a blade, while a bad one can ruin the best of blades. More importantly, inexperienced polishers can permanently ruin a blade by badly disrupting its geometry or wearing down too much steel, both of which effectively destroy the sword's monetary, historic, artistic, and functional value.\n\n\n=== Mountings ===\n\nIn Japanese, the scabbard for a katana is referred to as a saya, and the handguard piece, often intricately designed as an individual work of art \u2014 especially in later years of the Edo period \u2014 was called the tsuba. Other aspects of the mountings (koshirae), such as the menuki (decorative grip swells), habaki (blade collar and scabbard wedge), fuchi and kashira (handle collar and cap), kozuka (small utility knife handle), kogai (decorative skewer-like implement), saya lacquer, and tsuka-ito (professional handle wrap, also named emaki), received similar levels of artistry.\nAfter the blade is finished it is passed on to a mountings-maker, or sayashi (literally \"Sheath Maker\" but referring to those who make fittings in general). Sword mountings vary in their exact nature depending on the era, but generally consist of the same general idea, with the variation being in the components used and in the wrapping style. The obvious part of the hilt consists of a metal or wooden grip called a tsuka, which can also be used to refer to the entire hilt. The hand guard, or tsuba, on Japanese swords (except for certain 20th century sabers which emulate Western navies') is small and round, made of metal, and often very ornate. (See koshirae.)\nThere is a pommel at the base known as a kashira, and there is often a decoration under the braided wrappings called a menuki. A bamboo peg called a mekugi is slipped through the tsuka and through the tang of the blade, using the hole called a mekugi-ana (\"peg hole\") drilled in it. This anchors the blade securely into the hilt. To anchor the blade securely into the sheath it will soon have, the blade acquires a collar, or habaki, which extends an inch or so past the hand guard and keeps the blade from rattling.\nThe sheaths themselves are not an easy task. There are two types of sheaths, both of which require exacting work to create. One is the shirasaya, which is generally made of wood and considered the \"resting\" sheath, used as a storage sheath. The other sheath is the more decorative or battle-worthy sheath which is usually called either a jindachi-zukuri, if suspended from the obi (belt) by straps (tachi-style), or a buke-zukuri sheath if thrust through the obi (katana-style). Other types of mounting include the ky\u016b-gunt\u014d, shin-gunt\u014d, and kai-gunt\u014d types for the twentieth-century military.\n\n\n== Modern swordsmithing ==\nTraditional swords are still made in Japan and occasionally elsewhere; they are termed \"shinsakuto\" or \"shinken\" (true sword), and can be very expensive. These are not considered reproductions as they are made by traditional techniques and from traditional materials. Swordsmiths in Japan are licensed; acquiring this license requires a long apprenticeship. Outside Japan there are a couple of smiths working by traditional or mostly traditional techniques, and occasional short courses taught in Japanese swordsmithing. The only Japanese-licensed swordsmith outside Japan was Keith Austin (art-name Nobuhira or Nobuyoshi), who died in 1997.\nA very large number of low-quality reproduction katana and wakizashi are available; their prices usually range between $10 to about $200. These cheap blades are Japanese in shape only\u2014they are usually machine made and machine sharpened, and minimally hardened or heat-treated. The hamon pattern (if any) on the blade is applied by scuffing, etching or otherwise marking the surface, without any difference in hardness or temper of the edge. The metal used to make low-quality blades is mostly cheap stainless steel, and typically is much harder and more brittle than true katana. Finally, cheap reproduction Japanese swords usually have fancy designs on them since they are just for show. Better-quality reproduction katana typically range from $200 to about $1000 (though some can go easily above two thousand for quality production blades, folded and often traditionally constructed and with a proper polish ), and high-quality or custom-made reproductions can go up to $15,000\u2013$50,000. These blades are made to be used for cutting, and are usually heat-treated. High-quality reproductions made from carbon steel will often have a differential hardness or temper similar to traditionally made swords, and will show a hamon; they won't show a hada (grain), since they're not often made from folded steel.\nA wide range of steels are used in reproductions, ranging from carbon steels such as 1020, 1040, 1060, 1070, 1095, and 5160, stainless steels such as 400, 420, 440, to high-end specialty steels such as L6 and D2. Most cheap reproductions are made from inexpensive stainless steels such as 440A (often just termed \"440\"). With a normal Rockwell hardness of 56 and up to 60, stainless steel is much harder than the back of a differentially hardened katana (HR50), and is therefore much more prone to breaking, especially when used to make long blades. Stainless steel is also much softer at the edge (a traditional katana is usually more than HR60 at the edge). Furthermore, cheap swords designed as wall-hanging or sword rack decorations often also have a \"rat-tail\" tang, which is a thin, usually threaded bolt of metal welded onto the blade at the hilt area. These are a major weak point and often break at the weld, resulting in an extremely dangerous and unreliable sword.\nSome modern swordsmiths have made high quality reproduction swords using the traditional method, including one Japanese swordsmith who began manufacturing swords in Thailand using traditional methods, and various American and Chinese manufacturers. These however will always be different from Japanese swords made in Japan, as it is illegal to export the tamahagane jewel steel as such without it having been made into value-added products first. Nevertheless, some manufacturers have made differentially tempered swords folded in the traditional method available for relatively little money (often one to three thousand dollars), and differentially tempered, non-folded steel swords for several hundred. Some practicing martial artists prefer modern swords, whether of this type or made in Japan by Japanese craftsmen, because many of them cater to martial arts demonstrations by designing \"extra light\" swords which can be maneuvered relatively faster for longer periods of time, or swords specifically designed to perform well at cutting practice targets, with thinner blades and either razor-like flat-ground edges or even a hollow ground edges.\n\n\n== Commercial folded steel swords ==\nIn recent years, as the public has become more aware of the Japanese style of sword making, many companies have begun to offer laminated steel swords, typically marketing them as \"damascus\" swords, which usually command higher prices than their non-folded equivalents. Many people are willing to pay a premium for such blades in the belief that any laminated blade will be superior in performance and quality to any non-folded blade, but in fact it is just the reverse\u2014a low quality laminated sword may be more likely to contain metallurgical flaws than a sword made from a single piece of steel that came off the line in a modern steel plant, and any flaws would significantly increase the likelihood of breakage at the moment of contact, making the use of such a sword potentially highly hazardous to the wielder and any bystanders, as any breakage at the moment of contact will result in sharpened metal flying at unpredictable directions with the force of the blow. Pattern welding offers a slight advantage in heat treatment, in that the higher carbon steel layers will harden, while the mild steel will not.\nSword manufacturers marketing these swords will also often attempt to enhance the appearance of the layers by using comparatively few layers (thus leaving them thicker and more easily visible), using an acid wash to blacken the layers that are less corrosion resistant, or some combination of these techniques, resulting in a blade with extremely prominent laminations. Where the acid wash technique is used, the blade will be various shades of gray and black and the hamon is merely etched and grit-blasted afterwards, not a temper line. All swords made by the traditional Japanese method, regardless of the quality or assembly type, results in a bright and shiny blade upon completion, and therefore any blade that is black or gray in color when new absolutely cannot have been made in the traditional manner of the Japanese swordsmiths.\nCommercial \"folded\" steel swords are made by layering multiple sheets, usually of two different steels, and then welding them by hot-rolling the entire stack. The completed stack is then rolled thinner and cut to blanks the size needed for each blade. These blanks may then be drop-forged or hand hammered. The blanks are then ground down to form the edges, exposing the folds. It is comparatively easy and efficient (in the sense that less of the sheet tends to be lost during the stamping process) to produce a billet this way.\nReplica swords, varying and copying the major styles, have an active market world-wide. Replicas usually do not have a sharp blade but the point is quite sharp and easily used as a stabbing weapon. Replicas sell to tourists in Tokyo for about US$100 to $250. Export is no problem, but some nations limit imports even of replicas.\n\n\n== Notable swordsmiths ==\nAmakuni legendary swordsmith who supposedly created the first single-edged longsword with curvature along the edge in the Yamato Province around 700 AD\nAkitsugu Amata (1927\u20132013)\nHikoshiro Sadamune (1298\u20131349)\nKanenobu (17th century)\nKenz\u014d Kotani (1909\u20132003)\nMasamune (c. 1264 \u2013 1343)\nMuramasa (16th century)\nNagasone Kotetsu (c. 1597 \u2013 1678)\nOkubo Kazuhira (born 1943)\nShint\u014dgo Kunimitsu (13th century)\nMasamine Sumitani (1921\u20131998)\n\n\n== See also ==\nTatara (furnace)\nPattern welding\nMaraging steel for fencing blades - highly breakage resistant, very good for pointed weapons, not good for edged\n\n\n== References ==\n\n\n== External links ==\nConstruction of the Shinken in the Modern Age\nJapanese Sword Polishing Techniques\nThe Tale of the Tatara by Hitachi Metals\nJapanese Sword History\nThe Japan Times - Weapons of Wonder\nWhat is Tamahagane ?\nTamahagane \u2013 Unique Metal for Unique Japanese Swords", 
                "titleUrl": "https://en.wikipedia.org/wiki/Japanese_swordsmithing", 
                "title": "Japanese swordsmithing"
            }, 
            {
                "snippet": "the ocean's deeper, more carbon rich layers as dead soft tissue or in shells as calcium carbonate. It circulates in this layer for long periods of time", 
                "pageCategories": "Articles with inconsistent citation formats\nBiogeochemical cycle\nBiogeography\nCarbon\nChemical oceanography\nCommons category with local link different than on Wikidata\nGeochemistry\nNumerical climate and weather models\nPhotosynthesis\nSoil biology", 
                "pageContent": "The carbon cycle is the biogeochemical cycle by which carbon is exchanged among the biosphere, pedosphere, geosphere, hydrosphere, and atmosphere of the Earth. Along with the nitrogen cycle and the water cycle, the carbon cycle comprises a sequence of events that are key to making the Earth capable of sustaining life; it describes the movement of carbon as it is recycled and reused throughout the biosphere, including carbon sinks.\nThe global carbon budget is the balance of the exchanges (incomes and losses) of carbon between the carbon reservoirs or between one specific loop (e.g., atmosphere <-> biosphere) of the carbon cycle. An examination of the carbon budget of a pool or reservoir can provide information about whether the pool or reservoir is functioning as a source or sink for carbon dioxide. The carbon cycle was initially discovered by Joseph Priestley and Antoine Lavoisier, and popularized by Humphry Davy.\n\n\n== Global climate ==\nCarbon-based molecules are crucial for life on Earth, because it is the main component of biological compounds. Carbon is also a major component of many minerals. Carbon also exists in various forms in the atmosphere. Carbon dioxide (CO2) is partly responsible for the greenhouse effect and is the most important human-contributed greenhouse gas.\nIn the past two centuries, human activities have seriously altered the global carbon cycle, most significantly in the atmosphere. Although carbon dioxide levels have changed naturally over the past several thousand years, human emissions of carbon dioxide into the atmosphere exceed natural fluctuations. Changes in the amount of atmospheric CO2 are considerably altering weather patterns and indirectly influencing oceanic chemistry. Current carbon dioxide levels in the atmosphere exceed measurements from the last 420,000 years and levels are rising faster than ever recorded, making it of critical importance to better understand how the carbon cycle works and what its effects are on the global climate.\n\n\n== Main components ==\nThe global carbon cycle is now usually divided into the following major reservoirs of carbon interconnected by pathways of exchange:\nThe atmosphere\nThe terrestrial biosphere\nThe oceans, including dissolved inorganic carbon and living and non-living marine biota\nThe sediments, including fossil fuels, fresh water systems and non-living organic material.\nThe Earth's interior, carbon from the Earth's mantle and crust. These carbon stores interact with the other components through geological processes\nThe carbon exchanges between reservoirs occur as the result of various chemical, physical, geological, and biological processes. The ocean contains the largest active pool of carbon near the surface of the Earth. The natural flows of carbon between the atmosphere, ocean, terrestrial ecosystems, and sediments is fairly balanced, so that carbon levels would be roughly stable without human influence.\n\n\n=== Atmosphere ===\n\nCarbon in the Earth's atmosphere exists in two main forms: carbon dioxide and methane. Both of these gases absorb and retain heat in the atmosphere and are partially responsible for the greenhouse effect. Methane produces a large greenhouse effect per volume as compared to carbon dioxide, but it exists in much lower concentrations and is more short-lived than carbon dioxide, making carbon dioxide the more important greenhouse gas of the two.\nCarbon dioxide leaves the atmosphere through photosynthesis, thus entering the terrestrial and oceanic biospheres. Carbon dioxide also dissolves directly from the atmosphere into bodies of water (oceans, lakes, etc.), as well as dissolving in precipitation as raindrops fall through the atmosphere. When dissolved in water, carbon dioxide reacts with water molecules and forms carbonic acid, which contributes to ocean acidity. It can then be absorbed by rocks through weathering. It also can acidify other surfaces it touches or be washed into the ocean.\nHuman activities over the past two centuries have significantly increased the amount of carbon in the atmosphere, mainly in the form of carbon dioxide, both by modifying ecosystems' ability to extract carbon dioxide from the atmosphere and by emitting it directly, e.g., by burning fossil fuels and manufacturing concrete.\n\n\n=== Terrestrial biosphere ===\n\nThe terrestrial biosphere includes the organic carbon in all land-living organisms, both alive and dead, as well as carbon stored in soils. About 500 gigatons of carbon are stored above ground in plants and other living organisms, while soil holds approximately 1,500 gigatons of carbon. Most carbon in the terrestrial biosphere is organic carbon, while about a third of soil carbon is stored in inorganic forms, such as calcium carbonate. Organic carbon is a major component of all organisms living on earth. Autotrophs extract it from the air in the form of carbon dioxide, converting it into organic carbon, while heterotrophs receive carbon by consuming other organisms.\nBecause carbon uptake in the terrestrial biosphere is dependent on biotic factors, it follows a diurnal and seasonal cycle. In CO2 measurements, this feature is apparent in the Keeling curve. It is strongest in the northern hemisphere, because this hemisphere has more land mass than the southern hemisphere and thus more room for ecosystems to absorb and emit carbon.\nCarbon leaves the terrestrial biosphere in several ways and on different time scales. The combustion or respiration of organic carbon releases it rapidly into the atmosphere. It can also be exported into the oceans through rivers or remain sequestered in soils in the form of inert carbon. Carbon stored in soil can remain there for up to thousands of years before being washed into rivers by erosion or released into the atmosphere through soil respiration. Between 1989 and 2008 soil respiration increased by about 0.1% per year. In 2008, the global total of CO2 released from the soil reached roughly 98 billion tonnes, about 10 times more carbon than humans are now putting into the atmosphere each year by burning fossil fuel. There are a few plausible explanations for this trend, but the most likely explanation is that increasing temperatures have increased rates of decomposition of soil organic matter, which has increased the flow of CO2. The length of carbon sequestering in soil is dependent on local climatic conditions and thus changes in the course of climate change. From pre-industrial era to 2010, the terrestrial biosphere represented a net source of atmospheric CO2 prior to 1940, switching subsequently to a net sink.\n\n\n=== Oceans ===\n\nOceans contain the greatest quantity of actively cycled carbon in this world and are second only to the lithosphere in the amount of carbon they store. The oceans' surface layer holds large amounts of dissolved inorganic carbon that is exchanged rapidly with the atmosphere. The deep layer's concentration of dissolved inorganic carbon (DIC) is about 15% higher than that of the surface layer. DIC is stored in the deep layer for much longer periods of time. Thermohaline circulation exchanges carbon between these two layers.\nCarbon enters the ocean mainly through the dissolution of atmospheric carbon dioxide, which is converted into carbonate. It can also enter the oceans through rivers as dissolved organic carbon. It is converted by organisms into organic carbon through photosynthesis and can either be exchanged throughout the food chain or precipitated into the ocean's deeper, more carbon rich layers as dead soft tissue or in shells as calcium carbonate. It circulates in this layer for long periods of time before either being deposited as sediment or, eventually, returned to the surface waters through thermohaline circulation.\nOceanic absorption of CO2 is one of the most important forms of carbon sequestering limiting the human-caused rise of carbon dioxide in the atmosphere. However, this process is limited by a number of factors. Because the rate of CO2 dissolution in the ocean is dependent on the weathering of rocks and this process takes place slower than current rates of human greenhouse gas emissions, ocean CO2 uptake will decrease in the future. CO2 absorption also makes water more acidic, which affects ocean biosystems. The projected rate of increasing oceanic acidity could slow the biological precipitation of calcium carbonates, thus decreasing the ocean's capacity to absorb carbon dioxide.\n\n\n=== Geological carbon cycle ===\nThe geologic component of the carbon cycle operates slowly in comparison to the other parts of the global carbon cycle. It is one of the most important determinants of the amount of carbon in the atmosphere, and thus of global temperatures.\nMost of the earth's carbon is stored inertly in the earth's lithosphere. Much of the carbon stored in the earth's mantle was stored there when the earth formed. Some of it was deposited in the form of organic carbon from the biosphere. Of the carbon stored in the geosphere, about 80% is limestone and its derivatives, which form from the sedimentation of calcium carbonate stored in the shells of marine organisms. The remaining 20% is stored as kerogens formed through the sedimentation and burial of terrestrial organisms under high heat and pressure. Organic carbon stored in the geosphere can remain there for millions of years.\nCarbon can leave the geosphere in several ways. Carbon dioxide is released during the metamorphosis of carbonate rocks when they are subducted into the earth's mantle. This carbon dioxide can be released into the atmosphere and ocean through volcanoes and hotspots. It can also be removed by humans through the direct extraction of kerogens in the form of fossil fuels. After extraction, fossil fuels are burned to release energy, thus emitting the carbon they store into the atmosphere.\n\n\n=== Human influence ===\n\nSince the industrial revolution, human activity has modified the carbon cycle by changing its component's functions and directly adding carbon to the atmosphere.\nThe largest human impact on the carbon cycle is through direct emissions from burning fossil fuels, which transfers carbon from the geosphere into the atmosphere. The rest of this increase is caused mostly by changes in land-use, particularly deforestation.\nAnother direct human impact on the carbon cycle is the chemical process of calcination of limestone for clinker production, which releases CO2. Clinker is an industrial precursor of cement.\nHumans also influence the carbon cycle indirectly by changing the terrestrial and oceanic biosphere. Over the past several centuries, direct and indirect human-caused land use and land cover change (LUCC) has led to the loss of biodiversity, which lowers ecosystems' resilience to environmental stresses and decreases their ability to remove carbon from the atmosphere. More directly, it often leads to the release of carbon from terrestrial ecosystems into the atmosphere. Deforestation for agricultural purposes removes forests, which hold large amounts of carbon, and replaces them, generally with agricultural or urban areas. Both of these replacement land cover types store comparatively small amounts of carbon, so that the net product of the process is that more carbon stays in the atmosphere.\nOther human-caused changes to the environment change ecosystems' productivity and their ability to remove carbon from the atmosphere. Air pollution, for example, damages plants and soils, while many agricultural and land use practices lead to higher erosion rates, washing carbon out of soils and decreasing plant productivity.\nHumans also affect the oceanic carbon cycle. Current trends in climate change lead to higher ocean temperatures, thus modifying ecosystems. Also, acid rain and polluted runoff from agriculture and industry change the ocean's chemical composition. Such changes can have dramatic effects on highly sensitive ecosystems such as coral reefs, thus limiting the ocean's ability to absorb carbon from the atmosphere on a regional scale and reducing oceanic biodiversity globally.\nArctic methane emissions indirectly caused by anthropogenic global warming also affect the carbon cycle, and contribute to further warming in what is known as climate change feedback.\nOn 12 November 2015, NASA scientists reported that human-made carbon dioxide (CO2) continues to increase above levels not seen in hundreds of thousands of years: currently, about half of the carbon dioxide released from the burning of fossil fuels remains in the atmosphere and is not absorbed by vegetation and the oceans.\n\n\n== See also ==\nBiochar\nCalvin cycle\nCarbon cycle re-balancing\nCarbon dioxide in Earth's atmosphere\nCarbon footprint\nDeficit irrigation\nNitrogen cycle\nOcean acidification\nPermafrost carbon cycle\nSnowball Earth and the \"Slow carbon cycle\"\nSoil plant atmosphere continuum\n\n\n== References ==\n\n\n== Further reading ==\nThe Carbon Cycle, updated primer by NASA Earth Observatory, 2011\nAppenzeller, Tim (2004). \"The case of the missing carbon\". National Geographic Magazine.  \u2013 article about the missing carbon sink\nBolin, Bert; Degens, E. T.; Kempe, S.; Ketner, P. (1979). The global carbon cycle. Chichester ; New York: Published on behalf of the Scientific Committee on Problems of the Environment (SCOPE) of the International Council of Scientific Unions (ICSU) by Wiley. ISBN 0-471-99710-2. Retrieved 2008-07-08. \nHoughton, R. A. (2005). \"The contemporary carbon cycle\". In William H Schlesinger (editor). Biogeochemistry. Amsterdam: Elsevier Science. pp. 473\u2013513. ISBN 0-08-044642-6. \nJanzen, H. H. (2004). \"Carbon cycling in earth systems\u2014a soil science perspective\". Agriculture, Ecosystems & Environment. 104 (3): 399\u2013417. doi:10.1016/j.agee.2004.01.040. \nMillero, Frank J. (2005). Chemical Oceanography (3 ed.). CRC Press. ISBN 0-8493-2280-4. \nSundquist, Eric; Broecker, Wallace S., eds. (1985). The Carbon Cycle and Atmospheric CO2: Natural variations Archean to Present. Geophysical Monographs Series. American Geophysical Union. \n\n\n== External links ==\nCarbon Cycle Science Program \u2013 an interagency partnership.\nNOAA's Carbon Cycle Greenhouse Gases Group\nGlobal Carbon Project \u2013 initiative of the Earth System Science Partnership\nUNEP \u2013 The present carbon cycle \u2013 Climate Change carbon levels and flows\nNASA's Orbiting Carbon Observatory\nCarboSchools, a European website with many resources to study carbon cycle in secondary schools.\nCarbon and Climate, an educational website with a carbon cycle applet for modeling your own projection.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Carbon_cycle", 
                "title": "Carbon cycle"
            }, 
            {
                "snippet": "that covered the region's slopes were burned off, leaving a tell-tale carbon layer, and replaced by the fire-tolerant, and fire-prone, Mediterranean scrub", 
                "pageCategories": "2nd-millennium BC disestablishments in Europe\n3rd-millennium BC establishments in Europe\nArchaeological cultures of Western Europe\nArchaeological type sites\nBronze Age Spain\nCommons category with local link same as on Wikidata\nCoordinates on Wikidata", 
                "pageContent": "El Argar is the type site of an Early Bronze Age culture called the Argaric culture, which flourished from the town of Antas, in what is now the province of Almer\u00eda in southeastern Spain, between c. 2200 BC and 1550 BC. The Argaric culture was characterised by its early adoption of bronze, which briefly allowed this tribe local dominance over other, copper age peoples. El Argar also developed sophisticated pottery and ceramic techniques, which they traded with other Mediterranean tribes.\nThe center of this civilization is displaced to the north and its extension and influence is clearly greater than that of its ancestor. Their mining and metallurgy were quite advanced, with bronze, silver and gold being mined and worked for weapons and jewelry.\nPollen analysis in a peat deposit in the Canada del Gitano basin high in the Sierra de Baza suggests that the Argaric exhausted precious natural resources, helping bring about its own ruin. The deciduous oak forest that covered the region's slopes were burned off, leaving a tell-tale carbon layer, and replaced by the fire-tolerant, and fire-prone, Mediterranean scrub familiar under the names garrigue and maquis.\n\n\n== Extension ==\n\nThe civilization of El Argar extended to all the province of Almer\u00eda, north onto the central Meseta, to most of the land of (Murcia) and westwards into the provinces of Granada and Jaen.\nIts cultural and possibly political influence was much wider, clearly influencing eastern and southwestern Iberia (Algarve), and possibly other regions as well.\nSome authors have suggested that El Argar was a unified state.\n\n\n=== Main Argaric towns ===\nEl Argar: irregularly shaped (280 x 90 m).\nFuente Vermeja: small fortified site, 3 km north of El Argar\nLugarico Viejo: larger town very close to Fuente Vermeja.\nPuntarr\u00f3n Chico: in the top of a small hill, near Beniaj\u00e1n (Murcia)\nIfre (Murcia): on a rocky elevation.\nZapata (Murcia): 4 km. west of Ifre, fortified.\nCabezo Redondo (Villena, Alicante): one of the biggest settlements, on a rocky elevation next to an old lagoon and salt evaporation pond.\nGatas (4 km west of Moj\u00e1car, Almer\u00eda): fortified town on a hill with remarcable water canalizations.\nEl Oficio (9 km north of Villaricos, Almer\u00eda): atop of a well defended hill, strongly fortified, especially towards the sea.\nCerro de las Vi\u00f1as, Coy, Spain\nFuente \u00c1lamo (7 km north of Cuevas de Almazora, Almer\u00eda): the citadel is atop a hill, while the houses are terraced in its southern slope.\nAlmizaraque (Almer\u00eda): a town dating to Los Millares civilization.\nCerro de la Virgen de Orce (Granada).\nCerro de la Encina (Monachil, Granada).\nCuesta del Negro (Purullena, Granada).\n\n\n== Periodization ==\nThe culture of El Argar has traditionally been divided in two phases, named A and B.\n\n\n=== El Argar A ===\nThis phase started in the 18th century BC, with the earliest calibrated C-14 dates pointing to the first half or this century:\n1785 BC (+/- 55 years) in the transitional Late Chalcolithic-Early Bronze of Cerro de la Virgen de Orce, a peripheral site.\n1730 BC (+/- 70 years) in Fuente \u00c1lamo for El Argar A2, with six undated A1 layers under it.\n1700 BC in Cuesta del Negro (another peripheral site) with clear Argarian materials in its lower layer.\n\n\n=== El Argar B ===\nThis phase begins in the 16th century BC. The main C-14 date is that of 1550 BC (+/- 70 years) in Fuente \u00c1lamo for the upper layer of El Argar B2 (with four layers underneath the lowest B phase). Other stratigraphic dates are somewhat more recent but are not confirmed by C-14.\n\n\n=== Post-Argarian phase ===\nEl Argar B ends in the 14th or 13th century BC, giving way to a less homogeneous post-Argarian culture. Again Fuente \u00c1lamo gives the best C-14 dating with 1330 BC (+/- 70 years).\n\n\n=== Recent trends ===\nMany more C-14 dates have been published since the beginning of the 21st century. In recent publications, at least 260 such dates are cited altogether. There's now a widespread consensus that the emergence of El Argar can be dated at 2200 cal BC, although its end is still somewhat disputed. Various opinions place the end of El Argar at 15th-14th centuries.\n\n\n== Material culture ==\n\n\n=== Metallurgy ===\nEl Argar is the center of the Early and Middle Bronze Age in Iberia. Metallurgy of bronze and pseudo-bronze (alloyed with arsenic instead of tin). Weapons are the main metallurgic product: knives, halberds, swords, spear and arrow points, and big axes of curved edge are all abundant, not just in the Argaric area but also elsewhere in Iberia. Silver is also exploited, while gold, which had been abundant in the Chalcolithic period, becomes less common.\n\n\n=== Glass beads ===\nA meaningful element are the glass beads (of blue, green and white colors) that are found in this culture and which have been related with similar findings in Egypt (Amarna), Mycenaean Greece (dated in the 14th century BC), the British Wessex culture (dated c. 1400 BC) and some sites in France. Nevertheless, some of these beads are already found in chalcolithic contexts (site of La Pastora) which has brought some to speculate on an earlier date for the introduction of this material in southeast Iberia (late 3rd millennium BC).\n\n\n=== Other manufactured goods ===\n\nPottery undergoes important changes, almost totally abandoning decoration and with new types.\nTextile manufacture seems important, working specially with wool and flax. Basket-making also seems to have been important, showing greater extent and diversification than in previous periods.\n\n\n== Funerary customs ==\nThe collective burial tradition typical of European Megalithic Culture is abandoned in favor of individual burials. The tholos is abandoned in favour of small cists, either under the homes or outside. This trend seems to come from the Eastern Mediterranean, most likely from Mycenaean Greece (skipping Sicily and Italy, where the collective burial tradition remains for some time yet).\nFrom the Argarian civilization, these new burial customs will gradually and irregularly extend to the rest of Iberia.\nIn the phase B of this civilization, burial in pithoi (large jars) becomes most frequent (see: Jar-burials). Again this custom (that never reached beyond the Argarian circle) seems to come from Greece, where it was used after. ca 2000 BC.\n\n\n== Related cultures ==\nLos Millares: its antecessor culture.\nBronze of Levante: extending by the Land of Valencia: with smaller towns but very related to El Argar.\nMotillas (La Mancha): what would seem a military march of these proto-Iberian peoples.\nCogotas culture was influenced by El Argar.\nSouth-Western Iberian Bronze circle.\nMycenaean Greece: some cultural exchanges across the Mediterranean are very clear, with Argarians adopting Greek funerary customs (individual burials, first in cist and then in pithos), while Greeks also import the Iberian tholos for the same purpose.\nNuragic civilization: Cultural exchange and probably influenced the Nuragic people with their tholos.\n\n\n== See also ==\nVila Nova de Sao Pedro\nPrehistoric Iberia\n\n\n== Notes ==\n\n\n== Bibliography ==\nF. Jord\u00e1 Cerd\u00e1 et al. History of Spain 1: Prehistory. Gredos ed. 1986. ISBN 84-249-1015-X\n\n\n== External links ==\n Media related to El Argar at Wikimedia Commons", 
                "titleUrl": "https://en.wikipedia.org/wiki/El_Argar", 
                "title": "El Argar"
            }, 
            {
                "snippet": "ethylbenzene, the potassium-promoted iron oxide catalyst is coated with a carbon layer as the active phase. In another early example, a variety of substituted", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from July 2010\nCatalysis", 
                "pageContent": "Carbocatalysis is a form of catalysis that uses heterogeneous carbon materials for the transformation or synthesis of organic or inorganic substrates. The catalysts are characterized by their high surface areas, surface functionality, and large, aromatic basal planes. Carbocatalysis can be distinguishable from supported catalysis (such as palladium on carbon) in that no metal is present, or if metals are present they are not the active species.\nAs of 2010, the mechanisms of reactivity are not well understood.\nOne of the most common examples of carbocatalysis is the oxidative dehydrogenation of ethylbenzene to styrene discovered in the 1970s. Also in the industrial process of (non-oxidative) dehydrogenation of ethylbenzene, the potassium-promoted iron oxide catalyst is coated with a carbon layer as the active phase. In another early example, a variety of substituted nitrobenzenes were reduced to the corresponding aniline using hydrazine and graphite as the catalyst.\nThe discovery of nanostructured carbon allotropes such as carbon nanotubes, fullerenes, or graphene promoted further developments. Oxidized carbon nanotubes were used to dehydrogenate n-butane to 1-butene, and to selectively oxidize acrolein to acrylic acid. Fullerenes were used in the catalytic reduction of nitrobenzene to aniline in the presence of H2. Graphene oxide was used as a carbocatalyst to facilitate the oxidation of alcohols to the corresponding aldehydes/ketones (shown in the picture), the hydration of alkynes, and the oxidation of alkenes.\n\n\n== References ==\n\n\n== External links ==\nCarbon Materials for Catalysis", 
                "titleUrl": "https://en.wikipedia.org/wiki/Carbocatalysis", 
                "title": "Carbocatalysis"
            }
        ], 
        "phraseCharStart": "827"
    }, 
    {
        "phraseCharEnd": "846", 
        "phraseIndex": "T17", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "ITER", 
        "wikiSearchResults": [
            {
                "snippet": "43\u00b042\u203217.84\u2033N 5\u00b046\u20329.1\u2033E\ufeff / \ufeff43.7049556\u00b0N 5.769194\u00b0E\ufeff / 43.7049556; 5.769194 ITER (International Thermonuclear Experimental Reactor, and is also Latin for", 
                "pageCategories": "All articles with dead external links\nAll articles with unsourced statements\nArticles with French-language external links\nArticles with dead external links from August 2012\nArticles with unsourced statements from April 2008\nArticles with unsourced statements from December 2015\nArticles with unsourced statements from February 2007\nArticles with unsourced statements from March 2016\nBuildings and structures in Bouches-du-Rh\u00f4ne\nCommons category with local link same as on Wikidata", 
                "pageContent": "ITER (International Thermonuclear Experimental Reactor, and is also Latin for \"the way\") is an international nuclear fusion research and engineering megaproject, which will be the world's largest magnetic confinement plasma physics experiment. It is an experimental tokamak nuclear fusion reactor that is being built next to the Cadarache facility in Saint-Paul-l\u00e8s-Durance, south of France.\nThe ITER project aims to make the long-awaited transition from experimental studies of plasma physics to full-scale electricity-producing fusion power stations. The ITER fusion reactor has been designed to produce 500 megawatts of output power for several seconds while needing 50 megawatts to operate. Thereby the machine aims to demonstrate the principle of producing more energy from the fusion process than is used to initiate it, something that has not yet been achieved in any fusion reactor.\nThe project is funded and run by seven member entities\u2014the European Union, India, Japan, China, Russia, South Korea, and the United States. The EU, as host party for the ITER complex, is contributing about 45 percent of the cost, with the other six parties contributing approximately 9 percent each.\nConstruction of the ITER Tokamak complex started in 2013 and the building costs are now over US$14 billion as of June 2015. The facility is expected to finish its construction phase in 2019 and will start commissioning the reactor that same year and initiate plasma experiments in 2020 with full deuterium\u2013tritium fusion experiments starting in 2027. If ITER becomes operational, it will become the largest magnetic confinement plasma physics experiment in use, surpassing the Joint European Torus. The first commercial demonstration fusion power station, named DEMO, is proposed to follow on from the ITER project.\n\n\n== Background ==\nFusion power has the potential to provide sufficient energy to satisfy mounting demand, and to do so sustainably, with a relatively small impact on the environment.\nNuclear fusion has many potential attractions. Firstly, its hydrogen isotope fuels are relatively abundant \u2013 one of the necessary isotopes, deuterium, can be extracted from seawater, while the other fuel, tritium, would be bred from a lithium blanket using neutrons produced in the fusion reaction itself. Furthermore, a fusion reactor would produce virtually no CO2 or atmospheric pollutants, and its other radioactive waste products would be very short-lived compared to those produced by conventional nuclear reactors.\nOn 21 November 2006, the seven participants formally agreed to fund the creation of a nuclear fusion reactor. The program is anticipated to last for 30 years \u2013 10 for construction, and 20 of operation. ITER was originally expected to cost approximately \u20ac5billion, but the rising price of raw materials and changes to the initial design have seen that amount almost triple to \u20ac13billion. The reactor is expected to take 10 years to build with completion scheduled for 2019. Site preparation has begun in Cadarache, France, and procurement of large components has started.\nITER is designed to produce approximately 500 MW of fusion power sustained for up to 1,000 seconds (compared to JET's peak of 16 MW for less than a second) by the fusion of about 0.5 g of deuterium/tritium mixture in its approximately 840 m3 reactor chamber. Although ITER is expected to produce (in the form of heat) 10 times more energy than the amount consumed to heat up the plasma to fusion temperatures, the generated heat will not be used to generate any electricity.\nITER was originally an acronym for International Thermonuclear Experimental Reactor, but that title was eventually dropped due to the negative popular connotations of the word \"thermonuclear\", especially when used in conjunction with \"experimental\". \"Iter\" also means \"journey\", \"direction\" or \"way\" in Latin, reflecting ITER's potential role in harnessing nuclear fusion as a peaceful power source.\n\n\n== Organization history ==\n\nITER began in 1985 as a Reagan\u2013Gorbachev initiative with the equal participation of the Soviet Union, European Union (through European Atomic Energy Community), the United States, and Japan through the 1988\u20131998 initial design phases. Preparations for the first Gorbachev-Reagan Summit showed that there were no tangible agreements in the works for the summit.\nOne energy research project, however, was being considered quietly by two physicists, Alvin Trivelpiece and Evgeny Velikhov. The project involved collaboration on the next phase of magnetic fusion research \u2014 the construction of a demonstration model. At the time, magnetic fusion research was ongoing in Japan, Europe, the Soviet Union and the US. Velikhov and Trivelpiece believed that taking the next step in fusion research would be beyond the budget of any of the key nations and that collaboration would be useful internationally.\nA major bureaucratic fight erupted in the US government over the project. One argument against collaboration was that the Soviets would use it to steal US technology and know-how. A second was symbolic \u2014 the Soviet physicist Andrei Sakharov was in internal exile and the US was pushing the Soviet Union on its human rights record. The United States National Security Council convened a meeting under the direction of William Flynn Martin that resulted in a consensus that the US should go forward with the project.\nMartin and Velikhov concluded the agreement that was agreed at the summit and announced in the last paragraph of this historic summit meeting, \"... The two leaders emphasized the potential importance of the work aimed at utilizing controlled thermonuclear fusion for peaceful purposes and, in this connection, advocated the widest practicable development of international cooperation in obtaining this source of energy, which is essentially inexhaustible, for the benefit for all mankind.\"\nConceptual and engineering design phases carried out under the auspices of the IAEA led to an acceptable, detailed design in 2001, underpinned by US$650 million worth of research and development by the \"ITER Parties\" to establish its practical feasibility. These parties, namely EU, Japan, Russian Federation (replacing the Soviet Union), and United States (which opted out of the project in 1999 and returned in 2003), were joined in negotiations by China, South Korea, and Canada (who then terminated its participation at the end of 2003). India officially became part of ITER on December 2005.\nOn 28 June 2005, it was officially announced that ITER would be built in the European Union in Southern France. The negotiations that led to the decision ended in a compromise between the EU and Japan, in that Japan was promised 20% of the research staff on the French location of ITER, as well as the head of the administrative body of ITER. In addition, another research facility for the project will be built in Japan, and the European Union has agreed to contribute about 50% of the costs of this institution.\nOn 21 November 2006, an international consortium signed a formal agreement to build the reactor. On 24 September 2007, the People's Republic of China became the seventh party to deposit the ITER Agreement to the IAEA. Finally, on 24 October 2007, the ITER Agreement entered into force and the ITER Organization legally came into existence.\n\n\n== Objectives ==\nITER's mission is to demonstrate the feasibility of fusion power, and prove that it can work without negative impact. Specifically, the project aims:\nTo momentarily produce ten times more thermal energy from fusion heating than is supplied by auxiliary heating (a Q value equals 10).\nTo produce a steady-state plasma with a Q value greater than 5. (Q = 1 is breakeven.)\nTo maintain a fusion pulse for up to 8 minutes.\nTo ignite a \"burning\" (self-sustaining) plasma. (i.e. 'ignition' see Lawson criterion)\nTo develop technologies and processes needed for a fusion power station \u2014 including superconducting magnets and remote handling (maintenance by robot).\nTo verify tritium breeding concepts.\nTo refine neutron shield/heat conversion technology (most of the energy in the D+T fusion reaction is released in the form of fast neutrons).\n\n\n== Timeline and current status ==\nIn 1978, the EC, Japan, USA, and USSR joined in the International Tokamak Reactor (INTOR) Workshop, under the auspices of the International Atomic Energy Agency (IAEA), to assess the readiness of magnetic fusion to move forward to the experimental power reactor (EPR) stage, to identify the additional R&D that must be undertaken, and to define the characteristics of such an EPR by means of a conceptual design.\nHundreds of fusion scientists and engineers in each participating country took part in a detailed assessment of the then present status of the tokamak confinement concept vis-a-vis the requirements of an EPR, identified the required R&D by early 1980, and produced a conceptual design by mid-1981.\nTimeline:\n1985. At the Geneva summit meeting in 1985, Mikhail Gorbachev suggested to Ronald Reagan that the two countries jointly undertake the construction of a tokamak EPR as proposed by the INTOR Workshop. The ITER project was initiated in 1988.\n1988. Conceptual design activities ran from 1988 to 1990.\n1992. Engineering design activities started.\n1998. In June, the 'Final design' from the Engineering Design Activities was approved.\n2001. In July, the \"cost-cutting\" 'ITER-FEAT' design was agreed.\n2006. The ITER project was formally agreed to and funded with a cost estimate of \u20ac10 billion ($12.8 billion) projecting the start of construction in 2008 and completion a decade later.\n2007. In September, fourteen major design changes were agreed to the 2001 design.\n2013. The project had run into many delays and budget overruns. The facility is not expected to begin operations at the schedule initially anticipated.\n2014. In February, The New Yorker published the ITER Management Assessment report, listing 11 essential recommendations, for example: \"Create a Project Culture\", \"Instill a Nuclear Safety Culture\", \"Develop a realistic ITER Project Schedule\" and \"Simplify and Reduce the IO Bureaucracy\". The USA considered withdrawal, but is still participating in ITER.\n2015. In November a project review concludes that the schedule may need extending by at least six years; (e.g. first plasma in 2026).\n2016. Atomic Energy Organization of Iran completed the preliminary work for Iran to join ITER\n\n\n== Reactor overview ==\n\nWhen deuterium and tritium fuse, two nuclei come together to form a helium nucleus (an alpha particle), and a high-energy neutron.\n2\n1D + 3\n1T \u2192 4\n2He + 1\n0n + 6988281983061712000\u266017.6 MeV\nWhile nearly all stable isotopes lighter on the periodic table than iron-56 and nickel-62, which have the highest binding energy per nucleon, will fuse with some other isotope and release energy, deuterium and tritium are by far the most attractive for energy generation as they require the lowest activation energy (thus lowest temperature) to do so, while producing among the most energy per unit weight.\nAll proto- and mid-life stars radiate enormous amounts of energy generated by fusion processes. Mass for mass, the deuterium\u2013tritium fusion process releases roughly three times as much energy as uranium-235 fission, and millions of times more energy than a chemical reaction such as the burning of coal. It is the goal of a fusion power station to harness this energy to produce electricity.\nActivation energies for fusion reactions are generally high because the protons in each nucleus will tend to strongly repel one another, as they each have the same positive charge. A heuristic for estimating reaction rates is that nuclei must be able to get within 100 femtometer (1 \u00d7 10\u221213 meter) of each other, where the nuclei are increasingly likely to undergo quantum tunneling past the electrostatic barrier and the turning point where the strong nuclear force and the electrostatic force are equally balanced, allowing them to fuse. In ITER, this distance of approach is made possible by high temperatures and magnetic confinement. High temperatures give the nuclei enough energy to overcome their electrostatic repulsion (see Maxwell\u2013Boltzmann distribution). For deuterium and tritium, the optimal reaction rates occur at temperatures on the order of 100,000,000 K. The plasma is heated to a high temperature by ohmic heating (running a current through the plasma). Additional heating is applied using neutral beam injection (which cross magnetic field lines without a net deflection and will not cause a large electromagnetic disruption) and radio frequency (RF) or microwave heating.\nAt such high temperatures, particles have a large kinetic energy, and hence velocity. If unconfined, the particles will rapidly escape, taking the energy with them, cooling the plasma to the point where net energy is no longer produced. A successful reactor would need to contain the particles in a small enough volume for a long enough time for much of the plasma to fuse. In ITER and many other magnetic confinement reactors, the plasma, a gas of charged particles, is confined using magnetic fields. A charged particle moving through a magnetic field experiences a force perpendicular to the direction of travel, resulting in centripetal acceleration, thereby confining it to move in a circle or helix around the lines of magnetic flux.\nA solid confinement vessel is also needed, both to shield the magnets and other equipment from high temperatures and energetic photons and particles, and to maintain a near-vacuum for the plasma to populate. The containment vessel is subjected to a barrage of very energetic particles, where electrons, ions, photons, alpha particles, and neutrons constantly bombard it and degrade the structure. The material must be designed to endure this environment so that a power station would be economical. Tests of such materials will be carried out both at ITER and at IFMIF (International Fusion Materials Irradiation Facility).\nOnce fusion has begun, high energy neutrons will radiate from the reactive regions of the plasma, crossing magnetic field lines easily due to charge neutrality (see neutron flux). Since it is the neutrons that receive the majority of the energy, they will be ITER's primary source of energy output. Ideally, alpha particles will expend their energy in the plasma, further heating it.\nBeyond the inner wall of the containment vessel one of several test blanket modules will be placed. These are designed to slow and absorb neutrons in a reliable and efficient manner, limiting damage to the rest of the structure, and breeding tritium for fuel from lithium and the incoming neutrons. Energy absorbed from the fast neutrons is extracted and passed into the primary coolant. This heat energy would then be used to power an electricity-generating turbine in a real power station; in ITER this generating system is not of scientific interest, so instead the heat will be extracted and disposed of.\n\n\n== Technical design ==\n\n\n=== Vacuum vessel ===\nThe vacuum vessel is the central part of the ITER machine: a double walled steel container in which the plasma is contained by means of magnetic fields.\nThe ITER vacuum vessel will be twice as large and 16 times as heavy as any previously manufactured fusion vessel: each of the nine torus shaped sectors will weigh between 390 and 430 tonnes. When all the shielding and port structures are included, this adds up to a total of 5,116 tonnes. Its external diameter will measure 19.4 metres (64 ft), the internal 6.5 metres (21 ft). Once assembled, the whole structure will be 11.3 metres (37 ft) high.\nThe primary function of the vacuum vessel is to provide a hermetically sealed plasma container. Its main components are the main vessel, the port structures and the supporting system. The main vessel is a double walled structure with poloidal and toroidal stiffening ribs between 60-millimetre-thick (2.4 in) shells to reinforce the vessel structure. These ribs also form the flow passages for the cooling water. The space between the double walls will be filled with shield structures made of stainless steel. The inner surfaces of the vessel will act as the interface with breeder modules containing the breeder blanket component. These modules will provide shielding from the high-energy neutrons produced by the fusion reactions and some will also be used for tritium breeding concepts.\nThe vacuum vessel has 18 upper, 17 equatorial and 9 lower ports that will be used for remote handling operations, diagnostic systems, neutral beam injections and vacuum pumping.\n\n\n=== Breeder blanket ===\nOwing to very limited terrestrial resources of tritium, a key component of the ITER reactor design is the breeder blanket. This component, located adjacent to the vacuum vessel, serves to produce tritium through reaction of 6Li isotopes with high energy neutrons from the plasma. Concepts for the breeder blanket include helium cooled lithium lead (HCLL) and helium cooled pebble bed (HCPB) methods. Test blanket modules based on both concepts will be tested in ITER and will share a common box geometry. Materials for use as breeder pebbles in the HCPB concept include lithium metatitanate and lithium orthosilicate. Requirements of breeder materials include good tritium production and extraction, mechanical stability and low activation levels.\n\n\n=== Magnet system ===\nThe central solenoid coil will use superconducting niobium-tin to carry 46 kA and produce a field of up to 13.5 teslas. The 18 toroidal field coils will also use niobium-tin. At their maximum field strength of 11.8 teslas, they will be able to store 41 gigajoules. They have been tested at a record 80 kA. Other lower field ITER magnets (PF and CC) will use niobium-titanium for their superconducting elements.\n\n\n=== Additional heating ===\nThere will be three types of external heating in ITER:\nTwo Heating Neutral Beam injectors (HNB), each providing about 17MW to the burning plasma, with the possibility to add a third one. The requirements in terms of deuterium beam energy (1MeV), total current (40A) and beam pulse duration (up to 1h). The prototype is being built at the a Neutral Beam Test Facility (NBTF) prototype is being constructed in Padova\nIon Cyclotron Resonance Heating (ICRH)\nElectron Cyclotron Resonance Heating (ECRH)\n\n\n=== Cryostat ===\nThe cryostat is a large 3,800-tonne stainless steel structure surrounding the vacuum vessel and the superconducting magnets, in order to provide a super-cool vacuum environment. Its thickness ranging from 50 to 250 mm will allow it to withstand the atmospheric pressure on the area of a volume of 8,500 cubic meters. The total of 54 modules of the cryostat will be engineered, procured, manufactured, and installed by Larsen & Toubro Heavy Engineering.\n\n\n=== Cooling systems ===\nThe ITER tokamak will use three interconnected cooling systems. Most of the heat will be removed by a primary water cooling loop, itself cooled by water through a heat exchanger within the tokamak building's secondary confinement. The secondary cooling loop will be cooled by a larger complex, comprising a cooling tower, a 5 km pipeline supplying water from Canal de Provence, and basins that allow cooling water to be cooled and tested for chemical contamination and tritium before being released into the Durance River. This system will need to dissipate an average power of 450 MW during the tokamak's operation. A liquid nitrogen system will provide a further 1,300 kW of cooling to 80 kelvins, and a liquid helium system will provide 75 kW of cooling to 4.5 K. The liquid helium system will be designed, manufactured, installed and commissioned by Air Liquide.\n\n\n== Location ==\n\nThe process of selecting a location for ITER was long and drawn out. The most likely sites were Cadarache in Provence-Alpes-C\u00f4te-d'Azur, France, and Rokkasho, Aomori, Japan. Additionally, Canada announced a bid for the site in Clarington in May 2001, but withdrew from the race in 2003. Spain also offered a site at Vandell\u00f2s on 17 April 2002, but the EU decided to concentrate its support solely behind the French site in late November 2003. From this point on, the choice was between France and Japan. On 3 May 2005, the EU and Japan agreed to a process which would settle their dispute by July.\nAt the final meeting in Moscow on 28 June 2005, the participating parties agreed to construct ITER at Cadarache in Provence-Alpes-C\u00f4te-d'Azur, France. Construction of the ITER complex began in 2007, while assembly of the tokamak itself is scheduled to begin in 2015.\nFusion for Energy, the EU agency in charge of the European contribution to the project, is located in Barcelona, Spain. Fusion for Energy (F4E) is the European Union's Joint Undertaking for ITER and the Development of Fusion Energy. According to the agency's website:\n\n\"F4E is responsible for providing Europe's contribution to ITER, the world's largest scientific partnership that aims to demonstrate fusion as a viable and sustainable source of energy. [...] F4E also supports fusion research and development initiatives [...]\"\n\nThe ITER Neutral Beam Test Facility aimed at developing and optimizing the neutral beam injector prototype, is being constructed in Padova. It will be the only ITER facility out of the site in Cadarache.\n\n\n== Participants ==\n\nCurrently there are seven parties participating in the ITER program: the European Union (through the legally distinct organisation EURATOM), India, Japan, China, Russia, South Korea, and the United States. Canada was previously a full member, but has since pulled out due to a lack of funding from the federal government. The lack of funding also resulted in Canada withdrawing from its bid for the ITER site in 2003. The host member of the ITER project, and hence the member contributing most of the costs, is the EU.\nIn 2007, it was announced that participants in the ITER will consider Kazakhstan's offer to join the program and in March 2009, Switzerland, an associate member of EURATOM since 1979, also ratified the country's accession to the European Domestic Agency Fusion for Energy as a third country member.\nITER's work is supervised by the ITER Council, which has the authority to appoint senior staff, amend regulations, decide on budgeting issues, and allow additional states or organizations to participate in ITER. The present Chairman of the ITER Council is Dr Hideyuki Takatsu \nParticipating countries\n\n\n== Funding ==\nAs of 2016, the total price of constructing the experiment is expected to be in excess of \u20ac20 billion, an increase of \u20ac4.6 billion of its 2010 estimate, and of \u20ac9.6 billion from the 2009 estimate. Prior to that, the proposed costs for ITER were \u20ac5 billion for the construction and \u20ac5 billion for maintenance and the research connected with it during its 35-year lifetime. At the June 2005 conference in Moscow the participating members of the ITER cooperation agreed on the following division of funding contributions: 45% by the hosting member, the European Union, and the rest split between the non-hosting members \u2013 China, India, Japan, South Korea, the Russian Federation and the USA. During the operation and deactivation phases, Euratom will contribute to 34% of the total costs.\nAlthough Japan's financial contribution as a non-hosting member is one-eleventh of the total, the EU agreed to grant it a special status so that Japan will provide for two-elevenths of the research staff at Cadarache and be awarded two-elevenths of the construction contracts, while the European Union's staff and construction components contributions will be cut from five-elevenths to four-elevenths.\nIt was reported in December 2010 that the European Parliament had refused to approve a plan by member states to reallocate \u20ac1.4 billion from the budget to cover a shortfall in ITER building costs in 2012\u201313. The closure of the 2010 budget required this financing plan to be revised, and the European Commission (EC) was forced to put forward an ITER budgetary resolution proposal in 2011.\nThe U.S. withdrew from the ITER consortium in 2000. In 2006, Congress voted to rejoin, and again contribute financially. In June 2015, it appeared that the U.S. Senate might vote to stop the scheduled U.S. contribution of $150 million in the 2015\u20132016 fiscal year.\n\n\n== Criticism ==\n\nA technical concern is that the 14 MeV neutrons produced by the fusion reactions will damage the materials from which the reactor is built. Research is in progress to determine whether and how reactor walls can be designed to last long enough to make a commercial power station economically viable in the presence of the intense neutron bombardment. The damage is primarily caused by high energy neutrons knocking atoms out of their normal position in the crystal lattice. A related problem for a future commercial fusion power station is that the neutron bombardment will induce radioactivity in the reactor material itself. Maintaining and decommissioning a commercial reactor may thus be difficult and expensive. Another problem is that superconducting magnets are damaged by neutron fluxes. A new special research facility, IFMIF, is planned to investigate this problem.\nAnother source of concern comes from the recent tokamak parameters database interpolation which says that power load on tokamak divertors will be five times the expected value for ITER and much more for actual electricity-generating reactors. Given that the projected power load on the ITER divertor is already very high, these new findings mean that new divertor designs should be urgently tested. However, the corresponding test facility (ADX) still has not received any funding.\nA number of fusion researchers working on non-tokamak systems, such as Robert Bussard and Eric Lerner, have been critical of ITER for diverting funding from what they believe could be a potentially more viable and/or cost-effective path to fusion power, such as the polywell reactor. Many critics accuse ITER researchers of being unwilling to face up to the technical and economic potential problems posed by Tokamak fusion schemes. The expected cost of ITER has risen from $5 billion USD to $20 billion USD, and the timeline for operation at full power was moved from the original estimate of 2016 to 2027.\nA French association including about 700 anti-nuclear groups, Sortir du nucl\u00e9aire (Get Out of Nuclear Energy), claimed that ITER was a hazard because scientists did not yet know how to manipulate the high-energy deuterium and tritium hydrogen isotopes used in the fusion process.\nRebecca Harms, Green/EFA member of the European Parliament's Committee on Industry, Research and Energy, said: \"In the next 50 years, nuclear fusion will neither tackle climate change nor guarantee the security of our energy supply.\" Arguing that the EU's energy research should be focused elsewhere, she said: \"The Green/EFA group demands that these funds be spent instead on energy research that is relevant to the future. A major focus should now be put on renewable sources of energy.\" French Green party lawmaker No\u00ebl Mam\u00e8re claims that more concrete efforts to fight present-day global warming will be neglected as a result of ITER: \"This is not good news for the fight against the greenhouse effect because we're going to put ten billion euros towards a project that has a term of 30\u201350 years when we're not even sure it will be effective.\"\nITER is not designed to produce electricity, but made as a proof of concept reactor for the later DEMO project.\n\n\n=== Responses to criticism ===\nProponents believe that much of the ITER criticism is misleading and inaccurate, in particular the allegations of the experiment's \"inherent danger.\" The stated goals for a commercial fusion power station design are that the amount of radioactive waste produced should be hundreds of times less than that of a fission reactor, and that it should produce no long-lived radioactive waste, and that it is impossible for any such reactor to undergo a large-scale runaway chain reaction. A direct contact of the plasma with ITER inner walls would contaminate it, causing it to cool immediately and stop the fusion process. In addition, the amount of fuel contained in a fusion reactor chamber (one half gram of deuterium/tritium fuel) is only sufficient to sustain the fusion burn pulse from minutes up to an hour at most, whereas a fission reactor usually contains several years' worth of fuel. Moreover, some detritiation systems will be implemented, so that at a fuel cycle inventory level of about 2 kg, ITER will eventually need to recycle large amounts of tritium and at turnovers orders of magnitude higher than any preceding tritium facility worldwide.\nIn the case of an accident (or sabotage), it is expected that a fusion reactor might release far less radioactive pollution than would an ordinary fission nuclear station. Furthermore, ITER's type of fusion power has little in common with nuclear weapons technology, and does not produce the fissile materials necessary for the construction of a weapon. Proponents note that large-scale fusion power would be able to produce reliable electricity on demand, and with virtually zero pollution (no gaseous CO2, SO2, or NOx by-products are produced).\nAccording to researchers at a demonstration reactor in Japan, a fusion generator should be feasible in the 2030s and no later than the 2050s. Japan is pursuing its own research program with several operational facilities that are exploring several fusion paths.\nIn the United States alone, electricity accounts for US$210 billion in annual sales. Asia's electricity sector attracted US$93 billion in private investment between 1990 and 1999. These figures take into account only current prices. Proponents of ITER contend that an investment in research now should be viewed as an attempt to earn a far greater future return. Also, worldwide investment of less than US$1 billion per year into ITER is not incompatible with concurrent research into other methods of power generation, which in 2007 totaled US$16.9 billion.\nSupporters of ITER emphasize that the only way to test ideas for withstanding the intense neutron flux is to experimentally subject materials to that flux, which is one of the primary missions of ITER and the IFMIF, and both facilities will be vitally important to that effort. The purpose of ITER is to explore the scientific and engineering questions that surround potential fusion power stations. It is nearly impossible to acquire satisfactory data for the properties of materials expected to be subject to an intense neutron flux, and burning plasmas are expected to have quite different properties from externally heated plasmas. Supporters contend that the answer to these questions requires the ITER experiment, especially in the light of the monumental potential benefits.\nFurthermore, the main line of research via tokamaks has been developed to the point that it is now possible to undertake the penultimate step in magnetic confinement plasma physics research with a self-sustained reaction. In the tokamak research program, recent advances devoted to controlling the configuration of the plasma have led to the achievement of substantially improved energy and pressure confinement, which reduces the projected cost of electricity from such reactors by a factor of two to a value only about 50% more than the projected cost of electricity from advanced light-water reactors. In addition, progress in the development of advanced, low activation structural materials supports the promise of environmentally benign fusion reactors and research into alternate confinement concepts is yielding the promise of future improvements in confinement. Finally, supporters contend that other potential replacements to the fossil fuels have environmental issues of their own. Solar, wind, and hydroelectric power all have a relatively low power output per square kilometer compared to ITER's successor DEMO which, at 2,000 MW, would have an energy density that exceeds even large fission power stations.\n\n\n== Similar projects ==\nPrecursors to ITER were JET and Tore Supra. Other planned and proposed fusion reactors include DEMO, Wendelstein 7-X, NIF, HiPER, and MAST, as well as CFETR (China Fusion Engineering Test Reactor), a 200 MW tokamak.\n\n\n== See also ==\n\nTokamak\nITER Neutral Beam Test Facility, the facility dedicated to the development of the ITER neutral beam injector prototype\nFusion for Energy, the Domestic Agency in charge of managing EU contributions to the ITER project\nInternational Fusion Materials Irradiation Facility, proposed, construction not started\nJT-60/JT-60SA\nEAST (Experimental Advanced Superconducting Tokamak)\nNational Ignition Facility, inertial confinement using lasers\nNuclear power in France\nWendelstein 7-X (German experimental fusion reactor) - a stellarator\nFusenet, European Fusion Education Network, 2008-2013\n\n\n== References ==\n\n\n== External links ==\nOfficial website\nThe New Yorker, Mar. 3 2014, Star in a Bottle, by Raffi Khatchadourian\nArchival material collected by Prof. McCray relating to ITER\u2019s early phase (1979\u20131989) can be consulted at the Historical Archives of the European Union in Florence\n\"Way to New Energy\" video (23:24) at YouTube, by RT, on May 6, 2014.\nThe roles of the Host and the non-Host for the ITER Project. June 2005 The broader approach agreement with Japan.\nFusion Electricity - A roadmap to the realisation of fusion energy EFDA 2012 - 8 missions, ITER, project plan with dependancies, ...", 
                "titleUrl": "https://en.wikipedia.org/wiki/ITER", 
                "title": "ITER"
            }, 
            {
                "snippet": "Iter-pi\u0161a, inscribed in cuneiform as i-te-er-pi/pi4-\u0161a and meaning \"Her command is surpassing\", ca. 1769\u20131767 BC (short chronology) or ca. 1833\u20131831 BC", 
                "pageCategories": "18th-century BC rulers\nBabylonian kings", 
                "pageContent": "Iter-pi\u0161a, inscribed in cuneiform as i-te-er-pi/pi4-\u0161a and meaning \"Her command is surpassing\", ca. 1769\u20131767 BC (short chronology) or ca. 1833\u20131831 BC (middle chronology), was the 12th king of Isin during the Old Babylonian period. The Sumerian King List tells us that \"the divine Iter-pi\u0161a ruled for 4 years.\" The Ur-Isin King List which was written in the 4th year of the reign of Damiq-ili\u0161u gives a reign of just 3 years.\n\n\n== Biography ==\nHe was a contemporary of Warad-Sin (ca. 1770 BC to 1758 BC) the king of Larsa, whose brother and successor, Rim-Sin I would eventually come to overthrow the dynasty, ending the cities' bitter rivalry around 40 years later. He is only known from Kings lists and year-name date formulae.\nA letter from Iter-pi\u0161a to a deity was excavated in a scribal school, \"House F,\" in Nippur during the 1951\u201352 dig season. The scribal school had operated during the 1740s, early in the reign of king Samsu-iluna and the piece had become a belle letter.\n\n\n== External links ==\nIter-pi\u0161a year-names at CDLI, but note the tablet reference BM 85384 in year-name (b) is incorrect.\n\n\n== Inscriptions ==\n\n\n== Notes ==\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Iter-pisha", 
                "title": "Iter-pisha"
            }, 
            {
                "snippet": "European Union (EU) organisation responsible for Europe\u2019s contribution to ITER, the world\u2019s largest scientific partnership aiming to demonstrate fusion", 
                "pageCategories": "Agencies of the European Union\nAll articles needing additional references\nArticles needing additional references from October 2015\nEuropean Atomic Energy Community\nWikipedia articles with possible conflicts of interest from October 2015", 
                "pageContent": "Fusion for Energy (F4E) is the European Union (EU) organisation responsible for Europe\u2019s contribution to ITER, the world\u2019s largest scientific partnership aiming to demonstrate fusion as a viable and sustainable source of energy. The organisation \u2013 formally known as the European Joint Undertaking for ITER and the Development of Fusion Energy \u2013was created under article 45 of the Euratom Treaty by the decision of the Council of the European Union on 27 March 2007 for a period of 35 years.\nF4E counts 400 members of staff and its offices are located in Barcelona, in Spain. One of its main tasks is to work together with European industry and research organisations to develop and provide a wide range of high technology components for the ITER project. The European Union is the host party for the ITER project. Its contribution amounts to 45%, while the other six parties have an in-kind contribution of approximately 9% each. Since 2008, F4E has been collaborating with at least 250 companies and more than 50 R&D organisations.\n\n\n== Mission and governance ==\nF4E\u2019s primary mission is to manage the European contribution to the ITER project; therefore it provides financial funds, which mostly come from the European Community budget. Among other tasks, F4E oversees the preparation of the ITER construction site in Saint-Paul-l\u00e8s-Durance, in France. F4E is formed by Euratom (represented by the European Commission), the Member States of the European Union and Switzerland, which participates as a third country. To ensure the overall supervision of its activities, the members sit on a governing board, which has a wide range of responsibilities including appointing the director.\n\n\n== Fusion energy ==\nFusion is the process which powers the sun, producing energy by fusing together light atoms such as hydrogen at extremely high pressures and temperatures. Fusion reactors use two forms of hydrogen, deuterium and tritium, as fuel.\nThe benefits of fusion energy are that it is an inherently safe process and it does not create greenhouse gases or long-lasting radioactive waste.\n\n\n== The ITER project ==\nITER, meaning \u201cthe way\u201d in Latin, is an international experiment aiming to demonstrate the scientific and technical feasibility of fusion as an energy source. The machine is being constructed in Saint-Paul-l\u00e8s-Durance in the South of France and is funded by seven parties: China, the European Union, India, Japan, Russia, South Korea and the United States. Collectively, the parties taking part in the ITER project represent over one half of the world\u2019s population and 80% of the global GDP.\n\n\n== The Broader Approach activities ==\nThe Broader Approach (BA) activities are three research projects carried out under an agreement between the European Atomic Energy Community (Euratom) and Japan, which contribute equally financially. They are meant to complement the ITER project and accelerate the development of fusion energy through R&D by cooperating on a number of projects of mutual interest.\nThis agreement entered into force on 1 June 2007 and runs for at least 10 years. The Broader Approach consists of three main projects located in Japan: the Satellite Tokamak Programme project JT-60SA (super advanced), the International Fusion Materials Irradiation Facility - Engineering Validation and Engineering Design Activities (IFMIF/EVEDA) and the International Fusion Energy Research Centre (IFERC).\n\n\n== The DEMO project ==\nF4E also aims to contribute to DEMO (Demonstration Power Plant). This experiment is supposed to generate significant amounts of electricity over extended periods and will be self-sufficient in tritium, one of the necessary gases to create fusion. The first commercial fusion electricity power plants are set to be established following DEMO, which is set to be larger in size than ITER and to produce significantly larger fusion power over long periods: a continuous production of up to 500 megawatts of electricity.\n\n\n== Management difficulties ==\nA report by the consultancy Ernst & Young published in 2013 by the European Parliament's Budgetary Control Committee found that F4E has suffered from significant management difficulties. According to the report, \"the organisation faced a series of internal problems that have only been gradually addressed, notably an organisational structure ill-adapted for project-oriented activities.\" From 2010, a host of reforms were undertaken within F4E, including a reshuffling and reorientation of the governance and management structures, as well as a cost-savings programme.\n\n\n== See also ==\nITER\nFusenet\nFusion power\nEuratom\n\n\n== References ==\n\n\n== External links ==\nFusion for Energy, the agency's home page.\nFusion for Energy: Understanding Fusion\nEuratom/fusion, the Fusion page of the EURATOM\n[1], the Broader Approach agreement", 
                "titleUrl": "https://en.wikipedia.org/wiki/Fusion_for_Energy", 
                "title": "Fusion for Energy"
            }, 
            {
                "snippet": "build upon the ITER experimental nuclear fusion reactor. The objectives of DEMO are usually understood to lie somewhere between those of ITER and a \"first", 
                "pageCategories": "All articles containing potentially dated statements\nAll articles with unsourced statements\nArticles containing potentially dated statements from 2016\nArticles with unsourced statements from August 2011\nFusion power\nInterlanguage link template link number\nProposed fusion reactors\nProposed nuclear power stations\nTokamaks\nUse dmy dates from August 2011", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/DEMO", 
                "title": "DEMO"
            }, 
            {
                "snippet": "Iter.Viator is the debut solo album of ex-Peccatum member, (and wife of Ihsahn) Ihriel. Translated from Latin \"Iter Viator\" literally means \"road traveler\"", 
                "pageCategories": "2002 debut albums\nArticles with album ratings that need to be turned into prose\nArticles with hAudio microformats", 
                "pageContent": "Iter.Viator is the debut solo album of ex-Peccatum member, (and wife of Ihsahn) Ihriel. Translated from Latin \"Iter Viator\" literally means \"road traveler\".\n\n\n== Track listing ==\nAll Songs Written & Arranged By Ihriel.\n\"Chasm Blue\" \u2013 1:44\n\"Sanies\" \u2013 7:03\n\"Beautiful As Torment\" \u2013 6:38\n\"Death Salutes Atropos\" \u2013 5:27\n\"The Nudity Of Light\" \u2013 3:26\n\"Odie Et Amo\" \u2013 7:14\n\"In The Throws Of Guilt\" \u2013 11:00\n\n\n== Personnel ==\n\n\n=== Star of Ash ===\nHeidi S. Tveitan: Vocals, Keyboards, programming\n\n\n=== Additional Personnel ===\nVegard Sverre Tveitan: Guitar on all songs except \"Sanies\", bass on tracks 3\u20137, vocals on 4 & 7.\nEinar Solberg: Vocals on track 3.\nJostein Thomassen: Guitar on track 3 & 6.\nKnut Aalefj\u00e6r: Drums & percussion on tracks 3\u20136.\nKenneth Lia Solberg: Guitar on tracks 4 & 6.\nKris G. Rygg: Vocals on tracks 5 & 7.\nThe Star Of Ash Choir on track 7: Kaia Lia (conductor), Sanne Anundsk\u00e5s, Astrid Marie Lia, Inger Bronken, Elisabeth Lia, Marit B\u00f8e, Heidi S. Tveitan, P\u00e5l Solberg, Knut Bendik Breistein, Einar Solberg, Vegard Tveitan, Kenneth Lia Solberg\n\n\n== Production ==\nProduced By Heidi S. Tveitan, V. Tveitan, Tore Ylwizaker & Kris G. Rygg\nRecorded, Engineered & Mixed By Kristoffer G. Rygg, Tore Ylwizaker & Ihriel\nMastered By Tom Kvalsvoll\n\n\n== External links ==\n\"Iter.Viator\" at discogs", 
                "titleUrl": "https://en.wikipedia.org/wiki/Iter.Viator", 
                "title": "Iter.Viator"
            }, 
            {
                "snippet": "fusion energy which will be pertinent to the ITER fusion project as part of that country's contribution to the ITER effort. The project was approved in 1995", 
                "pageCategories": "Interlanguage link template link number\nTokamaks", 
                "pageContent": "The KSTAR, or Korea Superconducting Tokamak Advanced Research is a magnetic fusion device being built at the National Fusion Research Institute in Daejeon, South Korea. It is intended to study aspects of magnetic fusion energy which will be pertinent to the ITER fusion project as part of that country's contribution to the ITER effort. The project was approved in 1995 but construction was delayed by the East Asian financial crisis which weakened the South Korean economy considerably; however the construction phase of the project was completed on September 14, 2007. First plasma occurred on July 15, 2008. or more likely on June 30 2008.\nKSTAR will be one of the first research tokamaks in the world to feature fully superconducting magnets, which again will be of great relevance to ITER as this will also use SC magnets. The KSTAR magnet system consists of 16 niobium-tin direct current toroidal field magnets, 10 niobium-tin alternating current poloidal field magnets and 4 niobium-titanium alternating current poloidal field magnets. It is planned that the reactor will study plasma pulses of up to 20 seconds duration until 2011, when it will be upgraded to study pulses of up to 300 seconds duration. The reactor vessel will have a major radius of 1.8 m, a minor radius of 0.5 m, a maximum toroidal field of 3.5 tesla, and a maximum plasma current of 2 megaampere. As with other tokamaks, heating and current drive will be initiated using neutral beam injection, ion cyclotron resonance heating (ICRH), radio frequency heating and electron cyclotron resonance heating (ECRH). Initial heating power will be 8 megawatt from neutral beam injection upgradeable to 24 MW, 6 MW from ICRH upgradeable to 12 MW, and at present undetermined heating power from ECRH and RF heating. The experiment will use both hydrogen and deuterium fuels but not the deuterium-tritium mix which will be studied in ITER.\nIn 2012, it succeeded in maintaining high-temperature plasma (about 50 million degrees Celsius) for 17 seconds.\n\n\n== Timeline ==\nThe design was based on Tokomak Physics Experiment which was based on Compact Ignition Tokamak design - See Robert J. Goldston.\n1995 - Started Project KSTAR\n1997 - JET of EU emits 17 MW energy from itself.\n1998 - JT-60U went beyond energy junction successfully, and acknowledged possibility of commercialization of nuclear fusion.\n2006 - Life span of 3 Fusion Reactors (JT-60U, JET, and DIII-D) are terminated.\n2007, September - KSTAR's major devices are constructed.\n2008, July - First plasma occurred. Maintenance time: 0.865 seconds, Temperature: 2\u00d7106 K\n2009 - Maintained 320,000A plasma for 3.6 seconds.\n2010, November - First H-mode plasma run.\n2011 - Maintained high-temperature plasma for 5.2 seconds, Temperature: ~50\u00d7106 K, successfully fully deterred ELM (Edge-Localized Mode), first ever in the World.\n2012 - Maintained high-temperature plasma for 17 seconds, Temperature: 50\u00d7106 K\n2013 - Maintained high-temperature plasma for 20 seconds, Temperature: 50\u00d7106 K\n2014 - Maintained high-temperature plasma for 48 seconds, and successfully fully deterred ELM for 5 seconds.\n\n\n== References ==\n\n\n== External links ==\nKSTAR homepage\nEnglish KSTAR homepage\nKSTAR parameters re ITER and other tokamaks\n\nKSTAR Project Status PDF (undated - seems to be 2001. Includes slide-13 construction schedule to end 2004 and slide-16 operation from 2005 with upgrade planned 2010-11.)\nKSTAR Assembly Status, October 2006 PDF\nStatus and Result of the KSTAR Upgrade for the 2010\u2019s Campaign\nKSTAR ICRF transmission line system upgrade for load resilient operation. Jan 2013", 
                "titleUrl": "https://en.wikipedia.org/wiki/KSTAR", 
                "title": "KSTAR"
            }, 
            {
                "snippet": "journey from Rome to Brundisium. It is thus also known as the Iter Brundisium or Iter ad Brundisium. Alluding to a famous satire in which Horace\u2019s poetic", 
                "pageCategories": "1st-century BC Latin books\nAll articles lacking in-text citations\nArticles containing Latin-language text\nArticles lacking in-text citations from October 2012\nPoetry by Horace\nSatirical works", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Satires_(Horace)", 
                "title": "Satires (Horace)"
            }, 
            {
                "snippet": "series of experiments. Compared to the ITER international project, IGNITOR would be much smaller and cheaper: the ITER reactor is 19,000 tons[vague] in weight", 
                "pageCategories": "All Wikipedia articles needing clarification\nArticles with Italian-language external links\nFusion power\nInterlanguage link template link number\nNuclear research institutes\nResearch projects\nTokamaks\nWikipedia articles needing clarification from January 2010", 
                "pageContent": "IGNITOR is the Italian name for a nuclear research project of magnetic confinement fusion, developed by ENEA Laboratories in Frascati. Construction (in Russia) is not complete.\nThe project theory is based on ignited plasma in tokamak. Started in 1977 by Prof. Bruno Coppi of MIT, IGNITOR is based on the 1970s Alcator machine at MIT which pioneered the high magnetic field approach to plasma magnetic confinement, continued with the Alcator C/C-Mod at MIT and the FT/FTU series of experiments.\nCompared to the ITER international project, IGNITOR would be much smaller and cheaper: the ITER reactor is 19,000 tons in weight while the IGNITOR is only 500 tons in weight. IGNITOR is designed to produce approximately 100 MW of fusion power (and ITER to produce ~500 MW fusion power).\n\n\n== Development ==\nAt a meeting with the scientific attach\u00e9s of the European embassies in Moscow in early February 2010 Mikhail Kovalchuk, Director of the Kurchatov Institute, announced that an initiative aimed at developing a fast paced joint research programme in nuclear fusion research was strongly supported by the Governments of Russia and Italy.\nThe original proposal had been initiated earlier by Evgeny Velikhov (President of the Kurchatov Institute) and Bruno Coppi (Head of the High Energy Plasmas Undertaking, MIT) during the early developments of the Alcator C-Mod programme at MIT, where well known scientists of the Kurchatov Institute made key contributions to experiments that identified the unique confinement and purity properties of the high density plasmas produced by the high field Alcator machine. In effects this investigated, for the first time, physical processes leading to attain self-sustained fusion burning plasmas.\nThe collaboration with the Kurchatov Institute is directed at the construction of the Ignitor machine, the first experiment proposed to achieve ignition conditions by nuclear fusion reactions on the basis of existing knowledge of plasma physics and available technologies. Ignitor is part of the line of research on high magnetic field, experiments producing high density plasmas that began with the Alcator and the Frascati Torus programs at MIT and in Italy, respectively. It remains, at the world level, the only experiment capable of reaching ignition by the magnetic field confinement approach. However, several fusion scientists have contested the claim made for IGNITOR that it is a bigger step towards fusion power than the international ITER project.\nAccording to existing plans, Ignitor will be installed at the Triniti site at Troitsk near Moscow that has facilities which can be upgraded to house and operate the machine. This site will become open and made to be easily accessible to scientists of all nations. The management of the relevant research programme will involve Italy and Russia only to facilitate the success of the enterprise. The proponents have suggested that the US become an Associate Member of this effort with a similar arrangement to that made with CERN for its participation in the LHC (Large Hadron Collider) Programme.\nThe goal to produce meaningful fusion reactors in a reasonable time leads to pursuing the achievement of ignition conditions in the near term in order to understand the plasma physical regimes needed for a net power producing reactor. In addition, an objective other than ignition that can be envisioned for the relatively near term is that of high flux neutron sources for material testing involving compact, high density fusion machines. This has been one of the incentives that have led the Ignitor Project to adopt magnesium diboride (MgB2) superconducting cables in the machine design, a first in fusion research. Accordingly, the largest coils (about 5 m diameter) of the machine will be made entirely of MgB2 cables.\nIn the context of the Italy-Russia summit meeting held in Milan on 26 April 2010 the agreement to proceed with the proposed joint Ignitor program has been signed. The participants, from the Russian side, have included the Prime Minister Vladimir Putin, the Deputy Prime Minister Igor Sechin, the Energy Minister Sergei Shmatko, and the Vice Minister of Education and Research Sergey Mazurenko. Participants from the Italian side have included Prime Minister Silvio Berlusconi, the Foreign Affairs Advisor to the Prime Minister Valentino Valentini (who had a key role in forging the agreement on the Ignitor program), and the Minister of Education and Research Mariastella Gelmini who, together with Sergey Mazurenko, signed the agreement in the presence of the two Prime Ministers.\n\n\n== Progress on construction ==\nSome components have been built in Italy.\n\n\n== See also ==\nList of plasma (physics) articles\n\n\n== External links ==\nIGNITOR website\nFact sheet says \"Construction on the reactor is projected to be complete in 2014\"\n\n(English) IGNITOR technical specs on ENEA Laboratories in Frascati\n(Italian) Paolo Detragiache, Technical presentation of the project\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/IGNITOR", 
                "title": "IGNITOR"
            }, 
            {
                "snippet": "cross-section. Pioneering experiments followed, including for example the ITER-relevant tests of magnetic field correction with saddle coils for RMPs experiment", 
                "pageCategories": "All NPOV disputes\nCommons category without a link on Wikidata\nFusion power\nNPOV disputes from October 2016\nTokamaks", 
                "pageContent": "Tokamak COMPASS (COMPact ASSembly)[1][2] is the main experimental facility of Tokamak department of Institute of Plasma Physics[3] of the Academy of Sciences of the Czech Republic since 2006. It was designed in the 1980s in the British Culham Science Centre as a flexible research facility dedicated mostly to plasma physics studies in circular and D shaped plasmas.\nThe first plasma in COMPASS \"broke down\" in 1989 in a C-shaped vacuum vessel, i.e., in a simpler vessel with a circular cross-section. Pioneering experiments followed, including for example the ITER-relevant tests of magnetic field correction with saddle coils for RMPs experiment (Resonant magnetic perturbations) or experiments with non-inductive current drive in plasma.\nThe operation of tokamak restarted with a D-shaped vacuum vessel in 1992. The operation mode with high plasma confinement (H-mode) was achieved, which represents a reference operation (\"standard scenario\") for the ITER tokamak. The COMPASS tokamak with its size (major radius 0.6 m and height of the vessel approx. 0.7 m) ranks to smaller tokamaks capable of the H-mode operation. Importantly, due to its size and shape the COMPASS plasmas correspond to one tenth (in the linear scale) of the ITER plasmas. At present, besides COMPASS there are only two operational tokamaks in Europe with ITER-like configuration capable of regime with the high plasma confinement. It is the Joint European Torus (JET) and the German tokamak ASDEX Upgrade (Institut f\u00fcr Plasmaphysik, Garching, Germany). JET is the biggest experimental device of this type in the world.\nIn 2002, British scientists started alternative research on larger, spherical tokamak MAST. Operation of COMPASS was discontinued due to insufficient resources for operation of both tokamaks, however, the research program foreseen for the latter tokamak was not concluded. Due to its important and not completely realised opportunities - and, in particular, due to its direct relevance to the ITER project - the facility was offered for free by the European Commission and UKAEA to the Institute of Plasma Physics AS CR in Prague in autumn 2004.\nThe Prague institute has been coordinating research in thermonuclear fusion in the Czech Republic in the framework of EURATOM since 1999. Team of physicists from the institute has a long-time experience in this field of research including operation of a small tokamak CASTOR. The European Commission has declared that the institute is fully competent to operate the tokamak COMPASS. \n\n\n== Parameters of the tokamak COMPASS ==\nMovie: COMPASS discharge using fast - visible camera: [4]\n\n\n== References ==\n\n\n== See also ==\nList of fusion experiments\nELM (Edge Localized Mode)\nBall-pen probe\nLangmuir probe\nThomson scattering\nResonant magnetic perturbations\n\n\n== External links ==\nMagnetic fusion in the Czech Republic\nDiagnostic system on COMPASS", 
                "titleUrl": "https://en.wikipedia.org/wiki/COMPASS_tokamak", 
                "title": "COMPASS tokamak"
            }, 
            {
                "snippet": "Britain and Roman roads in Britain      The British section is known as the Iter Britanniarum, and can be described as the 'road map' of Roman Britain. There", 
                "pageCategories": "2nd-century Latin books\nArticles containing Latin-language text\nGeography of England\nGeography of Wales\nLatin prose texts\nMaps\nNerva\u2013Antonine dynasty\nRoman Britain\nRoman itineraries", 
                "pageContent": "The Antonine Itinerary (Latin: Itinerarium Antonini Augusti, lit. \"The Itinerary of the Emperor Antoninus\") is a famous itinerarium, a register of the stations and distances along various roads. Seemingly based on official documents, possibly from a survey carried out under Augustus, it describes the roads of the Roman Empire. Owing to the scarcity of other extant records of this type, it is a valuable historical record. Almost nothing is known of its date or author. Scholars consider it likely that the original edition was prepared at the beginning of the 3rd century: although it is traditionally ascribed to the patronage of the 2nd-century Antoninus Pius, the oldest extant copy has been assigned to the time of Diocletian and the most likely imperial patron\u2014if the work had one\u2014would have been Caracalla.\n\n\n== Iter Britanniarum ==\n\nThe British section is known as the Iter Britanniarum, and can be described as the 'road map' of Roman Britain. There are 15 such itineraries in the document applying to different geographic areas.\nThe itinerary measures distances in Roman miles, where 1,000 Roman paces equals one Roman mile. A Roman pace was two steps, left plus right. Roman paces were not everywhere the same, and conversion to modern units is imprecise, but 1 Roman mile approximately equals 4,690 feet, or 1430 m.\n\n\n=== Examples ===\nBelow is the original Latin ablative forms for sites along route 13, followed by a translation with a possible (but not necessarily authoritative) name for the modern sites. A transcriber omitted an entry, so that the total number of paces does not equal the sum of paces between locations.\nBelow is the original Latin for route 14 followed by a translation with a possible (but not necessarily authoritative) name for the modern site.\n\n\n=== A confounding factor ===\nDe Situ Britanniae (made available c.\u20091749, published 1757) was a forgery that provided much spurious information on Roman Britain, including \"itineraries\" that overlapped the legitimate Antonine Itineraries, sometimes with contradicting information. Its authenticity was not seriously challenged until 1845, and it was still cited as an authoritative source until the late nineteenth century. By then, its false data had infected almost every account of ancient British history, and been adopted into the Ordnance Survey maps, as General Roy and his successors believed it to be a legitimate source of information, on a par with the Antonine Itineraries. While the document is no longer cited, since its authenticity became indefensible, its data has not been systematically removed from past and present works.\nSome authors, such as Thomas Reynolds, without challenging the authenticity of the forgery, took care to note its discrepancies and challenge the quality of its information. This was not always so, even after the forgery was debunked.\nGonzalo Arias (died 2008) proposed that some of the distance anomalies in the British section of the Antonine Itinerary resulted from the loss of Latin grammatical endings, as these had marked junctions heading towards places, as distinct from the places themselves. However, Arias may not have taken account of earlier work indicating that distances were measured between the edges of administrative areas of named settlements as opposed to centre-to-centre, thereby explaining supposed distance shortfalls and providing additional useful data on the approximate sizes of such areas.\n\n\n== Hispania ==\n\n\n== Citations ==\n\n\n== Bibliography ==\n\n\n== External links ==\nThe Antonine Itinerary: Iter Britanniarum - The British Section\nAnalysis of the Itinerary\nItinerarium Antonini Augusti (the Balkanic roads) at SOLTDM.COM\nRoman Roads in Britain", 
                "titleUrl": "https://en.wikipedia.org/wiki/Antonine_Itinerary", 
                "title": "Antonine Itinerary"
            }
        ], 
        "phraseCharStart": "842"
    }, 
    {
        "phraseCharEnd": "876", 
        "phraseIndex": "T18", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "carbon materials", 
        "wikiSearchResults": [
            {
                "snippet": "Amorphous carbon is free, reactive carbon that does not have any crystalline structure (also called diamond-like carbon). Amorphous carbon materials may be", 
                "pageCategories": "Carbon forms\nWikipedia articles needing rewrite from December 2012", 
                "pageContent": "Amorphous carbon is free, reactive carbon that does not have any crystalline structure (also called diamond-like carbon). Amorphous carbon materials may be stabilized by terminating dangling-\u03c0 bonds with hydrogen. As with other amorphous solids, some short-range order can be observed. Amorphous carbon is often abbreviated to aC for general amorphous carbon, aC:H or HAC for hydrogenated amorphous carbon, or to ta-C for tetrahedral amorphous carbon.\n\n\n== In mineralogy ==\nIn mineralogy, amorphous carbon is the name used for coal, soot, carbide-derived carbon, and other impure forms of carbon that are neither graphite nor diamond. In a crystallographic sense, however, the materials are not truly amorphous but rather polycrystalline materials of graphite or diamond within an amorphous carbon matrix. Commercial carbon also usually contains significant quantities of other elements, which may also form crystalline impurities.\n\n\n== In modern science ==\nWith the development of modern thin film deposition and growth techniques in the latter half of the 20th century, such as chemical vapour deposition, sputter deposition, and cathodic arc deposition, it became possible to fabricate truly amorphous carbon materials.\nTrue amorphous carbon has localized \u03c0 electrons (as opposed to the aromatic \u03c0 bonds in graphite), and its bonds form with lengths and distances that are inconsistent with any other allotrope of carbon. It also contains a high concentration of dangling bonds; these cause deviations in interatomic spacing (as measured using diffraction) of more than 5% as well as noticeable variation in bond angle.\nThe properties of amorphous carbon films vary depending on the parameters used during deposition. The primary method for characterizing amorphous carbon is through the ratio of sp2 to sp3 hybridized bonds present in the material. Graphite consists purely of sp2 hybridized bonds, whereas diamond consists purely of sp3 hybridized bonds. Materials that are high in sp3 hybridized bonds are referred to as tetrahedral amorphous carbon, owing to the tetrahedral shape formed by sp3 hybridized bonds, or as diamond-like carbon (owing to the similarity of many physical properties to those of diamond).\nExperimentally, sp2 to sp3 ratios can be determined by comparing the relative intensities of various spectroscopic peaks (including EELS, XPS, and Raman spectroscopy) to those expected for graphite or diamond. In theoretical works, the sp2 to sp3 ratios are often obtained by counting the number of carbon atoms with three bonded neighbors versus those with four bonded neighbors. (This technique requires deciding on a somewhat arbitrary metric for determining whether neighboring atoms are considered bonded or not, and is therefore merely used as an indication of the relative sp2-sp3 ratio.)\nAlthough the characterization of amorphous carbon materials by the sp2-sp3 ratio may seem to indicate a one-dimensional range of properties between graphite and diamond, this is most definitely not the case. Research is currently ongoing into ways to characterize and expand on the range of properties offered by amorphous carbon materials.\nAll practical forms of hydrogenated carbon (e.g. smoke, chimney soot, mined coal such as bitumen and anthracite) contain large amounts of polycyclic aromatic hydrocarbon tars, and are therefore almost certainly carcinogenic.\n\n\n== See also ==\nGlassy carbon\nDiamond-like carbon\nCarbon black\nSoot\nCarbon\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Amorphous_carbon", 
                "title": "Amorphous carbon"
            }, 
            {
                "snippet": "The Butterfly blades use carbon materials such as Tamasu Carbon (TamCa) 5000, or Uniaxial Light Carbon (ULC), fiber materials such as Arylate (AL) or Zylon", 
                "pageCategories": "1950 establishments in Japan\nArticles containing Japanese-language text\nCompanies based in Tokyo\nCompanies established in 1950", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Tamasu_(corporation)", 
                "title": "Tamasu (corporation)"
            }, 
            {
                "snippet": "Carbide-derived carbon (CDC), also known as tunable nanoporous carbon, is the common term for carbon materials derived from carbide precursors, such as", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from July 2015\nCapacitors\nCarbon forms\nNanomaterials", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Carbide-derived_carbon", 
                "title": "Carbide-derived carbon"
            }, 
            {
                "snippet": "visited the University of Cambridge during 1979 to perform studies on carbon materials. He worked for the Research Development Corporation of Japan from 1982", 
                "pageCategories": "1939 births\nCS1 Norwegian-language sources (no)\nFellows of the Royal Microscopical Society\nForeign Members of the Chinese Academy of Sciences\nJapanese physicists\nLiving people\nMembers of the Norwegian Academy of Science and Letters\nMembers of the United States National Academy of Sciences\nMicroscopists\nNanotechnologists", 
                "pageContent": "Sumio Iijima (\u98ef\u5cf6 \u6f84\u7537 Iijima Sumio, born May 2, 1939) is a Japanese physicist, often cited as the inventor of carbon nanotubes. Although carbon nanotubes had been observed prior to his \"invention\", Iijima's 1991 paper generated unprecedented interest in the carbon nanostructures and has since fueled intense research in the area of nanotechnology. For this and other work Sumio Iijima was awarded, together with Louis Brus, the inaugural Kavli Prize for nanoscience in 2008.\nBorn in Saitama Prefecture in 1939, Iijima graduated with a Bachelor of Engineering degree in 1963 from the University of Electro-Communications, Tokyo. He received a Master's degree in 1965 and completed his Ph.D. in solid-state physics in 1968, both at Tohoku University in Sendai.\nBetween 1970 and 1982 he performed research with crystalline materials and high-resolution electron microscopy at Arizona State University. He visited the University of Cambridge during 1979 to perform studies on carbon materials.\nHe worked for the Research Development Corporation of Japan from 1982 to 1987, studying ultra-fine particles, after which he joined NEC Corporation where he is still employed. He discovered carbon nanotubes in 1991 while working with NEC. He is also a University Professor at Meijo University since 1999. Furthermore, he is the Honorary AIST Fellow of the National Institute of Advanced Industrial Science and Technology, Distinguished Invited University Professor of Nagoya University.\nHe was awarded the Benjamin Franklin Medal in Physics in 2002, \"for the discovery and elucidation of the atomic structure and helical character of multi-wall and single-wall carbon nanotubes, which have had an enormous impact on the rapidly growing condensed matter and materials science field of nanoscale science and electronics.\"\nHe is a foreign associate of National Academy of Sciences, foreign member of the Norwegian Academy of Science and Letters. Also, He is a Member of the Japan Academy.\n\n\n== Research Fields ==\nNano Science, Crystallography, Electron Microscopy, Solid-State Physics, Materials Science\n\n\n== Professional Record ==\n1968 - 1974: Research Associate, Research Institute for Scientific Measurements, Tohoku University, Sendai\n1970 - 1977: Research Associate, Department of Physics, Arizona State University, Tempe, Arizona\n1977 - 1982: Senior Research Associate, Center for Solid State Science, Arizona State University, Tempe, Arizona\n1979: Visiting Senior Scientist, Department of Metallurgy and Materials Science, University of Cambridge, Cambridge\n1982 - 1987: Group Leader, ERATO Program, Research Development Corporation of Japan, Nagoya\n1987 \u2013 Present: Senior Research Fellow, NEC Corporation, Tsukuba (Joined NEC in 1987 as Senior Principal Researcher)\n1998 - 2002: Research Director, JST/ICORP \"Nanotubulites\" Project Tsukuba and Nagoya\n1999 \u2013 Present: University Professor, Meijo University, Nagoya\n2001 \u2013 2015: Director, Nanotube Research Center, National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba\n2005 \u2013 2012: Dean, SKKU Advanced Institute of Nanotechnology (SAINT, http://saint.skku.edu), Sungkyunkwan University, Suwon, Korea.\n2006 \u2013 2009: Project Reader, NEDO \u201cCarbon Nanotube Capacitor Development Project\u201d\n2007 \u2013 Present: Distinguished Invited University Professor of Nagoya University, Nagoya\n2008 \u2013 2012: Distinguished Invited Chair Professor for World Class University (WCU) Program, Sungkyunkwan University, Suwon, Korea.\n2015 \u2013 Present: Honorary AIST Fellow, National Institute of Advanced Industrial Science and Technology (AIST)\n\n\n== Academy ==\n2007: Foreign Associate, The National Academy of Sciences\n2009: Foreign Member, The Norwegian Academy of Science and Letters.\n2010: Member, The Japan Academy\n2011: Foreign Fellow, Chinese Academy of Science\n\n\n== Honors ==\n2000: Fellow, The American Physical Society\n2001: Honorary Fellowship, Royal Microscopical Society\n2002: Honorary Doctor, University of Antwerp\n2002: Honorary Member, The Crystallographic Society of Japan\n2003: Honorary Doctor, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne(EPFL)\n2004: Honorary Member, The Japanese Society of Microscopy\n2005: Honorary Professor, Xi\u2019an Jiaotong University\n2005: Honorary Professor, Peking University\n2007: Fellow, The Japan Society of Applied Physics\n2009: Fellow, The Microscopy Society of America\n2009: Honorary Member, The Chemical Society of Japan\n2009: Honorary Professor, Tsinghua University\n2009: Distinguished Professor, The University of Electro-Communications\n2010: Honorary Professor, Zhejiang University\n2010: Honorary Professor, Southeast University\n2014: Honorary Doctor, Aalto University\n\n\n== Major Awards ==\n1976: Bertram Eugene Warren Diffraction Physics Award, (The American Crystallography Society)\n1985: Nishina Memorial Award, (The Nishina Memorial Foundation)\n1996: Asahi Prize, (The Asahi Shinbun Cultural Foundation)\n2002: Agilent EuroPhysics Prize, (European Physical Society)\n2002: James C. McGroddy Prize for New Materials, (American Physical Society)\n2002: Benjamin Franklin Medal in Physics, (The Franklin Institute)\n2002: Japan Academy Award and Imperial Award, (The Japan Academy)\n2003: Person of Cultural Merit\n2007: Gregori Aminoff Prize in crystallography 2007, (Royal Swedish Academy of Sciences)\n2007: Fujihara Award, (The Fujihara Foundation of Science)\n2007: Balzan Prize for Nanoscience, (The International Balzan Prize Foundation)\n2008: The Kavli Prize Nanoscience 2008 (The Kavli Foundation)\n2008: The Prince of Asturias Award for Technical Scientific Research 2008, (The Prince of Asturias Foundation)\n2009: Order of Culture\nand others\n\n\n== References ==\n\n\n== External links ==\n\"About Myself\" - NEC's page about Dr. Sumio Iijima\n\"Nanotubulites\" - about Dr. Sumio Iijima\nNanotubes: The Materials of the 21st Century - video presentation by Sumio Iijima\nArizona State University story on Kavli Prize", 
                "titleUrl": "https://en.wikipedia.org/wiki/Sumio_Iijima", 
                "title": "Sumio Iijima"
            }, 
            {
                "snippet": "Cambridge. He is a leading specialist in the theory of amorphous carbon and related materials.   Robertson received his Bachelor of Arts and Doctor of Philosophy", 
                "pageCategories": "1950 births\nAll articles with unsourced statements\nAlumni of the University of Cambridge\nArticles with unsourced statements from May 2015\nBritish electrical engineers\nEnglish physicists\nFellow Members of the IEEE\nFellows of the American Physical Society\nFellows of the Royal Society\nLiving people", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/John_Robertson_(physicist)", 
                "title": "John Robertson (physicist)"
            }, 
            {
                "snippet": "increasing nozzle ratio. As a rule, their modern design assumes use of carbon-carbon materials without regenerative cooling. Along with sliding expanders, stationary", 
                "pageCategories": "Articles with Russian-language external links\nNozzles\nRocket propulsion\nSpacecraft propulsion", 
                "pageContent": "Nozzle extension \u2014 nozzle expander of reaction/rocket engine. The application of nozzle extensions improves efficiency of rocket engines in vacuum by increasing nozzle ratio. As a rule, their modern design assumes use of carbon-carbon materials without regenerative cooling. Along with sliding expanders, stationary nozzle extensions have some applications too.\n\n\n== Description ==\n\nAs of 2009, the search for various schemes to achieve higher area ratios for rocket nozzles remains an active field of research and patenting. Generally, modern application of these designs can be divided into engines, which start their work at sea level and finish it at vacuum conditions, and engines, which perform all their operations in a vacuum.\n\n\n=== \"Air-to-vacuum\" engines ===\nFor first stage rocket engines, the engine works with nozzle extension in disposed position during the first minutes of flight and expands it at some predefined level of air pressure. This scheme assumes the outer skirt of the bell is extended while the engine is functioning and its installation to working position happens in the upper layers of the atmosphere. It excludes problems with flow separation at sea level and increases efficiency of the engine in vacuum. For example, application of nozzle extension for liquid rocket engine NK-33 improves the value of specific impulse up to 15-20 sec for near-space conditions. Therefore, this scheme adjusts the system to ambient conditions along the trajectory or, in other words, allows altitude compensation.\n\n\n=== \"Vacuum\" engines ===\nRocket engines of upper stages perform all their operations in space and therefore in a vacuum. In order to achieve maximum efficiency for this class of engines they need high area ratios. This makes the nozzles a very sizable part of the engine, which must be completely enclosed below the nose cone of a rocket. The payload fairing and supporting constructions must endure all stresses and loads during launch and flight. Consequently, the use of an outer expandable skirt in this case allows the size of the upper stage and payload fairing to be minimized, which in turn decreases the total mass of the nose cone. For these reasons, nozzle extensions are used for rocket engines RL-10 and RD-58.\n\n\n== See also ==\nRocket engine nozzle\nDe Laval nozzle\nStepped nozzle\nF-1\nNK-33\n\n\n== References ==\n\n\n== External links ==\n(Russian) Surprises of \"Engines-2000\", News of cosmonautics, April 2000\n(Russian) Patent of NPO Iskra, Patent department\n(Russian) The research of possible options for construction of liquid rocket engine with changeable nozzle ratio, Magazine \"Engine\"\n(Russian) Casing for fire, Magazine \"Engine\"\nVulcain-2 Cryogenic Engine Passes First Test with New Nozzle Extension, European Space Agency", 
                "titleUrl": "https://en.wikipedia.org/wiki/Nozzle_extension", 
                "title": "Nozzle extension"
            }, 
            {
                "snippet": "Materials used to absorb other materials due to their high affinity for doing so. Examples include: In composting, dry (brown, high-carbon) materials", 
                "pageCategories": "All articles lacking sources\nAll stub articles\nArticles lacking sources from December 2009\nDesiccants\nMaterials stubs\nNatural materials\nSynthetic materials", 
                "pageContent": "A sorbent is a material used to absorb or adsorb liquids or gases. Examples include:\nA material similar to molecular sieve material, which acts by adsorption (attracting molecules to its surface). It has a large internal surface area and good thermal conductivity. It is typically supplied in pellets of 1 mm to 2 mm diameter and roughly 5 mm length or as grains of the order 1 mm. Occasionally as beads up to 5 mm diameter. They are typically made from aluminium oxide with a porous structure.\nMaterials used to absorb other materials due to their high affinity for doing so. Examples include:\nIn composting, dry (brown, high-carbon) materials absorb many odoriferous chemicals, and these chemicals help to decompose these sorbents.\nA sponge absorbs many times its own weight in water.\nA polypropylene fiber mat may be used to absorb oil.\nA cellulose fiber product may be used to absorb oil.\nThe granular gel material in a baby diaper will absorb several times its original weight in urine.\nPolymethylsiloxane polyhydrate (PMSPH) is a gelly-like polymeric organosilicon compound. PMSPG is an sorbent designed for binding toxic substances of different nature, pathogens and metabolites in the gastrointestinal tract and their excretion.\nDesiccants absorb water, drying out (desiccating) the surrounding materials.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Sorbent", 
                "title": "Sorbent"
            }, 
            {
                "snippet": "oscillators. Carbon nanotubes and graphene's physical strength allows carbon based materials to meet higher stress demands, when common materials would normally", 
                "pageCategories": "All articles with unsourced statements\nApplied sciences\nArticles with unsourced statements from April 2016\nEmerging technologies\nNanoelectronics", 
                "pageContent": "Nanoelectromechanical systems (NEMS) are a class of devices integrating electrical and mechanical functionality on the nanoscale. NEMS form the logical next miniaturization step from so-called microelectromechanical systems, or MEMS devices. NEMS typically integrate transistor-like nanoelectronics with mechanical actuators, pumps, or motors, and may thereby form physical, biological, and chemical sensors. The name derives from typical device dimensions in the nanometer range, leading to low mass, high mechanical resonance frequencies, potentially large quantum mechanical effects such as zero point motion, and a high surface-to-volume ratio useful for surface-based sensing mechanisms. Uses include accelerometers, or detectors of chemical substances in the air.\n\n\n== Overview ==\nAs noted by Richard Feynman in his famous talk in 1959, \"There's Plenty of Room at the Bottom,\" there are many potential applications of machines at smaller and smaller sizes; by building and controlling devices at smaller scales, all technology benefits. Among the expected benefits include greater efficiencies and reduced size, decreased power consumption and lower costs of production in electromechanical systems.\nIn 2000, the first very-large-scale integration (VLSI) NEMS device was demonstrated by researchers at IBM. Its premise was an array of AFM tips which can heat/sense a deformable substrate in order to function as a memory device. Further devices have been described by Stefan de Haan. In 2007, the International Technical Roadmap for Semiconductors (ITRS) contains NEMS Memory as a new entry for the Emerging Research Devices section.\n\n\n== Atomic force microscopy ==\nA key application of NEMS is atomic force microscope tips. The increased sensitivity achieved by NEMS leads to smaller and more efficient sensors to detect stresses, vibrations, forces at the atomic level, and chemical signals. AFM tips and other detection at the nanoscale rely heavily on NEMS.\n\n\n== Approaches to miniaturization ==\nTwo complementary approaches to fabrication of NEMS can be found. The top-down approach uses the traditional microfabrication methods, i.e. optical and electron beam lithography, to manufacture devices. While being limited by the resolution of these methods, it allows a large degree of control over the resulting structures. Typically, devices are fabricated from metallic thin films or etched semiconductor layers.\nBottom-up approaches, in contrast, use the chemical properties of single molecules to cause single-molecule components to self-organize or self-assemble into some useful conformation, or rely on positional assembly. These approaches utilize the concepts of molecular self-assembly and/or molecular recognition. This allows fabrication of much smaller structures, albeit often at the cost of limited control of the fabrication process.\nA combination of these approaches may also be used, in which nanoscale molecules are integrated into a top-down framework. One such example is the carbon nanotube nanomotor.\n\n\n== Materials ==\n\n\n=== Carbon allotropes ===\nMany of the commonly used materials for NEMS technology have been carbon based, specifically diamond, carbon nanotubes and graphene. This is mainly because of the useful properties of carbon based materials which directly meet the needs of NEMS. The mechanical properties of carbon (such as large Young's modulus) are fundamental to the stability of NEMS while the metallic and semiconductor conductivities of carbon based materials allow them to function as transistors.\nBoth graphene and diamond exhibit high Young's modulus, low density, low friction, exceedingly low mechanical dissipation, and large surface area. The low friction of CNTs, allow practically frictionless bearings and has thus been a huge motivation towards practical applications of CNTs as constitutive elements in NEMS, such as nanomotors, switches, and high-frequency oscillators. Carbon nanotubes and graphene's physical strength allows carbon based materials to meet higher stress demands, when common materials would normally fail and thus further support their use as a major materials in NEMS technological development.\nAlong with the mechanical benefits of carbon based materials, the electrical properties of carbon nanotubes and graphene allow it to be used in many electrical components of NEMS. Nanotransistors have been developed for both carbon nanotubes as well as graphene. Transistors are one of the basic building blocks for all electronic devices, so by effectively developing usable transistors, carbon nanotubes and graphene are both very crucial to NEMS.\n\n\n==== Metallic carbon nanotubes ====\n\nCarbon nanotubes (CNTs) are allotropes of carbon with a cylindrical nanostructure. They can be considered a rolled up graphene. When rolled at specific and discrete (\"chiral\") angles, and the combination of the rolling angle and radius decides whether the nanotube has a bandgap (semiconductoring) or no bandgap (metallic).\nMetallic carbon nanotubes have also been proposed for nanoelectronic interconnects since they can carry high current densities. This is a useful property as wires to transfer current are another basic building block of any electrical system. Carbon nanotubes have specifically found so much use in NEMS that methods have already been discovered to connect suspended carbon nanotubes to other nanostructures. This allows carbon nanotubes to form complicated nanoelectric systems. Because carbon based products can be properly controlled and act as interconnects as well as transistors, they serve as a fundamental material in the electrical components of NEMS.\n\n\n==== Difficulties ====\nDespite all of the useful properties of carbon nanotubes and graphene for NEMS technology, both of these products face several hindrances to their implementation. One of the main problems is carbon\u2019s response to real life environments. Carbon nanotubes exhibit a large change in electronic properties when exposed to oxygen. Similarly, other changes to the electronic and mechanical attributes of carbon based materials must fully be explored before their implementation, especially because of their high surface area which can easily react with surrounding environments. Carbon nanotubes were also found to have varying conductivities, being either metallic or semiconducting depending on their helicity when processed. Because of this, special treatment must be given to the nanotubes during processing to assure that all of the nanotubes have appropriate conductivities. Graphene also has complicated electric conductivity properties compared to traditional semiconductors because it lacks an energy band gap and essentially changes all the rules for how electrons move through a graphene based device. This means that traditional constructions of electronic devices will likely not work and completely new architectures must be designed for these new electronic devices.\n\n\n== Simulations ==\nComputer simulations have long been important counterparts to experimental studies of NEMS devices. Through continuum mechanics and molecular dynamics (MD), important behaviors of NEMS devices can be predicted via computational modeling before engaging in experiments. Additionally, combining continuum and MD techniques enables engineers to efficiently analyze the stability of NEMS devices without resorting to ultra-fine meshes and time-intensive simulations. Simulations have other advantages as well: they do not require the time and expertise associated with fabricating NEMS devices; they can effectively predict the interrelated roles of various electromechanical effects; and parametric studies can be conducted fairly readily as compared with experimental approaches. For example, computational studies have predicted the charge distributions and \u201cpull-in\u201d electromechanical responses of NEMS devices. Using simulations to predict mechanical and electrical behavior of these devices can help optimize NEMS device design parameters.\n\n\n== Future of NEMS ==\nKey hurdles currently preventing the commercial application of many NEMS devices include low-yields and high device quality variability. Before NEMS devices can actually be implemented, reasonable integrations of carbon based products must be created. A recent step in that direction has been demonstrated for diamond, achieving a processing level comparable to that of silicon. The focus is currently shifting from experimental work towards practical applications and device structures that will implement and profit from such novel devices. The next challenge to overcome involves understanding all of the properties of these carbon-based tools, and using the properties to make efficient and durable NEMS with low failure rates.\nCarbon-based materials have served as prime materials for NEMS use, because of their exceptional mechanical and electrical properties.\nThe global market of NEMS is projected to reach $108.88 million by 2022 \n\n\n== Applications ==\nNanoelectromechanical relay\nNanoelectromechanical systems mass spectrometer\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Nanoelectromechanical_systems", 
                "title": "Nanoelectromechanical systems"
            }, 
            {
                "snippet": "first to novel carbon materials such as fullerenes (C60), graphene and carbon nanotubes. After discovering how to mass-produce carbon nanotubes, he and", 
                "pageCategories": "1954 births\nCommons category with page title same as on Wikidata\nLiving people\nMembers of the French Academy of Sciences\nNanotechnologists\nNorwegian expatriates in France\nNorwegian physical chemists\nOberlin College alumni\nPierre and Marie Curie University alumni\nUniversity of Strasbourg faculty", 
                "pageContent": "Thomas Ebbesen (born 30 January 1954 in Oslo) is a physical chemist and professor at the University of Strasbourg in France, known for his pioneering work in nanoscience and received the Kavli Prize in Nanoscience \u201cfor transformative contributions to the field of nano-optics that have broken long-held beliefs about the limitations of the resolution limits of optical microscopy and imaging\u201d, together with Stefan Hell, and Sir John Pendry in 2014.\nThomas Ebbesen received his bachelors from Oberlin College, and a PhD from Pierre and Marie Curie University in Paris in the field of photo-physical chemistry. He then worked at the Notre Dame Radiation Laboratory before joining the NEC Fundamental Research Laboratories in Japan in 1988 where his research shifted first to novel carbon materials such as fullerenes (C60), graphene and carbon nanotubes. After discovering how to mass-produce carbon nanotubes, he and his colleagues measured many of their unique features such as their mechanical and wetting properties. For his pioneering and extensive contribution to the field of carbon nanotubes, he shared the 2001 Agilent Europhysics Prize with Sumio Iijima, Cees Dekker and Paul McEuen.\nWhile working at NEC, Ebbesen discovered a major new optical phenomenon. He found that, contrary to the then accepted theory, it was possible to transmit light extremely efficiently through subwavelength holes milled in opaque metal films under certain conditions. The phenomenon, known as extraordinary optical transmission, involves surface plasmons. It has raised fundamental questions and is finding applications in broad variety of areas from chemistry to opto-electronics. Ebbesen has received several awards for the discovery of the extraordinary optical transmission such as the 2005 France Telecom Prize of the French Academy of Sciences and the 2009 Quantum Electronics and Optics Prize of the European Physical Society.\nHis current research is focused on the physics and chemistry of light-matter interactions at the nanoscale.\nIn 1999, Thomas Ebbesen joined ISIS founded by Jean-Marie Lehn at the University of Strasbourg, which he headed from 2004 to 2012. He is the director of the International Center for Frontier Research in Chemistry. and the University of Strasbourg Institute for Advanced Study. He is a member of the Institut Universitaire de France, the Norwegian Academy of Science and Letters, the French Academy of Science and the Royal Flemish Academy of Belgium for Sciences and the Arts.\nHe is married to the pianist Masako Hayashi-Ebbesen. They have two daughters.\n\n\n== Awards ==\nNEC Research Prize 1992\nRanders Prize 2001\nAgilent Europhysics Prize 2001\nPrix France Telecom 2005\nTomassoni Prize 2009\nScola Physica Romana Medal 2009\nQuantum Electronics and Optics Prize 2009\nDr. Scient. H.C., University of Southern Denmark 2009\nKavli Prize in Nanoscience 2014\nPrix Special of the French Physics Society, 2014\nHonorary Doctorate, Oberlin College, USA 2015\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Thomas_Ebbesen", 
                "title": "Thomas Ebbesen"
            }, 
            {
                "snippet": "natural diamond requires very specific conditions\u2014exposure of carbon-bearing materials to high pressure, ranging approximately between 45 and 60 kilobars", 
                "pageCategories": "Abrasives\nAll Wikipedia articles written in American English\nArticles containing video clips\nCubic minerals\nDiamond\nEconomic geology\nFeatured articles\nGroup IV semiconductors\nImpact event minerals\nLuminescent minerals", 
                "pageContent": "Diamond (pronunciation: /\u02c8da\u026a\u0259m\u0259nd/ or /\u02c8da\u026am\u0259nd/) is a metastable allotrope of carbon, where the carbon atoms are arranged in a variation of the face-centered cubic crystal structure called a diamond lattice. Diamond is less stable than graphite, but the conversion rate from diamond to graphite is negligible at standard conditions. Diamond is renowned as a material with superlative physical qualities, most of which originate from the strong covalent bonding between its atoms. In particular, diamond has the highest hardness and thermal conductivity of any bulk material. Those properties determine the major industrial application of diamond in cutting and polishing tools and the scientific applications in diamond knives and diamond anvil cells.\nBecause of its extremely rigid lattice, it can be contaminated by very few types of impurities, such as boron and nitrogen. Small amounts of defects or impurities (about one per million of lattice atoms) color diamond blue (boron), yellow (nitrogen), brown (lattice defects), green (radiation exposure), purple, pink, orange or red. Diamond also has relatively high optical dispersion (ability to disperse light of different colors).\nMost natural diamonds are formed at high temperature and pressure at depths of 140 to 190 kilometers (87 to 118 mi) in the Earth's mantle. Carbon-containing minerals provide the carbon source, and the growth occurs over periods from 1 billion to 3.3 billion years (25% to 75% of the age of the Earth). Diamonds are brought close to the Earth's surface through deep volcanic eruptions by magma, which cools into igneous rocks known as kimberlites and lamproites. Diamonds can also be produced synthetically in a HPHT method which approximately simulates the conditions in the Earth's mantle. An alternative, and completely different growth technique is chemical vapor deposition (CVD). Several non-diamond materials, which include cubic zirconia and silicon carbide and are often called diamond simulants, resemble diamond in appearance and many properties. Special gemological techniques have been developed to distinguish natural diamonds, synthetic diamonds, and diamond simulants. The word is from the ancient Greek \u1f00\u03b4\u03ac\u03bc\u03b1\u03c2 \u2013 ad\u00e1mas \"unbreakable\".\n\n\n== History ==\n\nThe name diamond is derived from the ancient Greek \u03b1\u03b4\u03ac\u03bc\u03b1\u03c2 (ad\u00e1mas), \"proper\", \"unalterable\", \"unbreakable\", \"untamed\", from \u1f00- (a-), \"un-\" + \u03b4\u03b1\u03bc\u03ac\u03c9 (dam\u00e1\u014d), \"I overpower\", \"I tame\". Diamonds are thought to have been first recognized and mined in India, where significant alluvial deposits of the stone could be found many centuries ago along the rivers Penner, Krishna and Godavari. Diamonds have been known in India for at least 3,000 years but most likely 6,000 years.\nDiamonds have been treasured as gemstones since their use as religious icons in ancient India. Their usage in engraving tools also dates to early human history. The popularity of diamonds has risen since the 19th century because of increased supply, improved cutting and polishing techniques, growth in the world economy, and innovative and successful advertising campaigns.\nIn 1772, Antoine Lavoisier used a lens to concentrate the rays of the sun on a diamond in an atmosphere of oxygen, and showed that the only product of the combustion was carbon dioxide, proving that diamond is composed of carbon. Later in 1797, Smithson Tennant repeated and expanded that experiment. By demonstrating that burning diamond and graphite releases the same amount of gas, he established the chemical equivalence of these substances.\nThe most familiar uses of diamonds today are as gemstones used for adornment, a use which dates back into antiquity, and as industrial abrasives for cutting hard materials. The dispersion of white light into spectral colors is the primary gemological characteristic of gem diamonds. In the 20th century, experts in gemology developed methods of grading diamonds and other gemstones based on the characteristics most important to their value as a gem. Four characteristics, known informally as the four Cs, are now commonly used as the basic descriptors of diamonds: these are carat (its weight), cut (quality of the cut is graded according to proportions, symmetry and polish), color (how close to white or colorless; for fancy diamonds how intense is its hue), and clarity (how free is it from inclusions). A large, flawless diamond is known as a paragon.\n\n\n=== Natural history ===\nThe formation of natural diamond requires very specific conditions\u2014exposure of carbon-bearing materials to high pressure, ranging approximately between 45 and 60 kilobars (4.5 and 6 GPa), but at a comparatively low temperature range between approximately 900 and 1,300 \u00b0C (1,650 and 2,370 \u00b0F). These conditions are met in two places on Earth; in the lithospheric mantle below relatively stable continental plates, and at the site of a meteorite strike.\n\n\n==== Formation in cratons ====\n\nThe conditions for diamond formation to happen in the lithospheric mantle occur at considerable depth corresponding to the requirements of temperature and pressure. These depths are estimated between 140 and 190 kilometers (87 and 118 mi) though occasionally diamonds have crystallized at depths about 300 km (190 mi). The rate at which temperature changes with increasing depth into the Earth varies greatly in different parts of the Earth. In particular, under oceanic plates the temperature rises more quickly with depth, beyond the range required for diamond formation at the depth required. The correct combination of temperature and pressure is only found in the thick, ancient, and stable parts of continental plates where regions of lithosphere known as cratons exist. Long residence in the cratonic lithosphere allows diamond crystals to grow larger.\nThrough studies of carbon isotope ratios (similar to the methodology used in carbon dating, except with the stable isotopes C-12 and C-13), it has been shown that the carbon found in diamonds comes from both inorganic and organic sources. Some diamonds, known as harzburgitic, are formed from inorganic carbon originally found deep in the Earth's mantle. In contrast, eclogitic diamonds contain organic carbon from organic detritus that has been pushed down from the surface of the Earth's crust through subduction (see plate tectonics) before transforming into diamond. These two different source of carbon have measurably different 13C:12C ratios. Diamonds that have come to the Earth's surface are generally quite old, ranging from under 1 billion to 3.3 billion years old. This is 22% to 73% of the age of the Earth.\nDiamonds occur most often as euhedral or rounded octahedra and twinned octahedra known as macles. As diamond's crystal structure has a cubic arrangement of the atoms, they have many facets that belong to a cube, octahedron, rhombicosidodecahedron, tetrakis hexahedron or disdyakis dodecahedron. The crystals can have rounded off and unexpressive edges and can be elongated. Sometimes they are found grown together or form double \"twinned\" crystals at the surfaces of the octahedron. These different shapes and habits of some diamonds result from differing external circumstances. Diamonds (especially those with rounded crystal faces) are commonly found coated in nyf, an opaque gum-like skin.\n\n\n==== Transport from mantle ====\n\nDiamond-bearing rock is carried from the mantle to the Earth's surface by deep-origin volcanic eruptions. The magma for such a volcano must originate at a depth where diamonds can be formed\u2014150 km (93 mi) or more (three times or more the depth of source magma for most volcanoes). This is a relatively rare occurrence. These typically small surface volcanic craters extend downward in formations known as volcanic pipes. The pipes contain material that was transported toward the surface by volcanic action, but was not ejected before the volcanic activity ceased. During eruption these pipes are open to the surface, resulting in open circulation; many xenoliths of surface rock and even wood and fossils are found in volcanic pipes. Diamond-bearing volcanic pipes are closely related to the oldest, coolest regions of continental crust (cratons). This is because cratons are very thick, and their lithospheric mantle extends to great enough depth that diamonds are stable. Not all pipes contain diamonds, and even fewer contain enough diamonds to make mining economically viable.\nThe magma in volcanic pipes is usually one of two characteristic types, which cool into igneous rock known as either kimberlite or lamproite. The magma itself does not contain diamond; instead, it acts as an elevator that carries deep-formed rocks (xenoliths), minerals (xenocrysts), and fluids upward. These rocks are characteristically rich in magnesium-bearing olivine, pyroxene, and amphibole minerals which are often altered to serpentine by heat and fluids during and after eruption. Certain indicator minerals typically occur within diamantiferous kimberlites and are used as mineralogical tracers by prospectors, who follow the indicator trail back to the volcanic pipe which may contain diamonds. These minerals are rich in chromium (Cr) or titanium (Ti), elements which impart bright colors to the minerals. The most common indicator minerals are chromium garnets (usually bright red chromium-pyrope, and occasionally green ugrandite-series garnets), eclogitic garnets, orange titanium-pyrope, red high-chromium spinels, dark chromite, bright green chromium-diopside, glassy green olivine, black picroilmenite, and magnetite. Kimberlite deposits are known as blue ground for the deeper serpentinized part of the deposits, or as yellow ground for the near surface smectite clay and carbonate weathered and oxidized portion.\nOnce diamonds have been transported to the surface by magma in a volcanic pipe, they may erode out and be distributed over a large area. A volcanic pipe containing diamonds is known as a primary source of diamonds. Secondary sources of diamonds include all areas where a significant number of diamonds have been eroded out of their kimberlite or lamproite matrix, and accumulated because of water or wind action. These include alluvial deposits and deposits along existing and ancient shorelines, where loose diamonds tend to accumulate because of their size and density. Diamonds have also rarely been found in deposits left behind by glaciers (notably in Wisconsin and Indiana); in contrast to alluvial deposits, glacial deposits are minor and are therefore not viable commercial sources of diamond.\n\n\n==== Space diamonds ====\n\nNot all diamonds found on Earth originated on Earth. Primitive interstellar meteorites were found to contain carbon possibly in the form of diamond. A type of diamond called carbonado that is found in South America and Africa may have been deposited there via an asteroid impact (not formed from the impact) about 3 billion years ago. These diamonds may have formed in the intrastellar environment, but as of 2008, there was no scientific consensus on how carbonado diamonds originated.\nDiamonds can also form under other naturally occurring high-pressure conditions. Very small diamonds of micrometer and nanometer sizes, known as microdiamonds or nanodiamonds respectively, have been found in meteorite impact craters. Such impact events create shock zones of high pressure and temperature suitable for diamond formation. Impact-type microdiamonds can be used as an indicator of ancient impact craters. Popigai crater in Russia may have the world's largest diamond deposit, estimated at trillions of carats, and formed by an asteroid impact.\nScientific evidence indicates that white dwarf stars have a core of crystallized carbon and oxygen nuclei. The largest of these found in the universe so far, BPM 37093, is located 50 light-years (4.7\u00d71014 km) away in the constellation Centaurus. A news release from the Harvard-Smithsonian Center for Astrophysics described the 2,500-mile (4,000 km)-wide stellar core as a diamond.\n\n\n== Material properties ==\n\nA diamond is a transparent crystal of tetrahedrally bonded carbon atoms in a covalent network lattice (sp3) that crystallizes into the diamond lattice which is a variation of the face centered cubic structure. Diamonds have been adapted for many uses because of the material's exceptional physical characteristics. Most notable are its extreme hardness and thermal conductivity (900\u20137003232000000000000\u26602320 W\u00b7m\u22121\u00b7K\u22121), as well as wide bandgap and high optical dispersion. Above 7003197315000000000\u26601700 \u00b0C (7003197300000000000\u26601973 K / 7003224592777777777\u26603583 \u00b0F) in vacuum or oxygen-free atmosphere, diamond converts to graphite; in air, transformation starts at ~7002973150000000000\u2660700 \u00b0C. Diamond's ignition point is 720 \u2013 7003107315000000000\u2660800 \u00b0C in oxygen and 850 \u2013 7003127315000000000\u26601000 \u00b0C in air. Naturally occurring diamonds have a density ranging from 3.15\u20137003353000000000000\u26603.53 g/cm3, with pure diamond close to 7003352000000000000\u26603.52 g/cm3. The chemical bonds that hold the carbon atoms in diamonds together are weaker than those in graphite. In diamonds, the bonds form an inflexible three-dimensional lattice, whereas in graphite, the atoms are tightly bonded into sheets, which can slide easily over one another, making the overall structure weaker. In a diamond, each carbon atom is surrounded by neighboring four carbon atoms forming a tetrahedral shaped unit.\n\n\n=== Hardness ===\nDiamond is the hardest known natural material on both the Vickers and the Mohs scale. Diamond's hardness has been known since antiquity, and is the source of its name.\nDiamond hardness depends on its purity, crystalline perfection and orientation: hardness is higher for flawless, pure crystals oriented to the <111> direction (along the longest diagonal of the cubic diamond lattice). Therefore, whereas it might be possible to scratch some diamonds with other materials, such as boron nitride, the hardest diamonds can only be scratched by other diamonds and nanocrystalline diamond aggregates.\nThe hardness of diamond contributes to its suitability as a gemstone. Because it can only be scratched by other diamonds, it maintains its polish extremely well. Unlike many other gems, it is well-suited to daily wear because of its resistance to scratching\u2014perhaps contributing to its popularity as the preferred gem in engagement or wedding rings, which are often worn every day.\n\nThe hardest natural diamonds mostly originate from the Copeton and Bingara fields located in the New England area in New South Wales, Australia. These diamonds are generally small, perfect to semiperfect octahedra, and are used to polish other diamonds. Their hardness is associated with the crystal growth form, which is single-stage crystal growth. Most other diamonds show more evidence of multiple growth stages, which produce inclusions, flaws, and defect planes in the crystal lattice, all of which affect their hardness. It is possible to treat regular diamonds under a combination of high pressure and high temperature to produce diamonds that are harder than the diamonds used in hardness gauges.\nSomewhat related to hardness is another mechanical property toughness, which is a material's ability to resist breakage from forceful impact. The toughness of natural diamond has been measured as 7.5\u201310 MPa\u00b7m1/2. This value is good compared to other ceramic materials, but poor compared to most engineering materials such as engineering alloys, which typically exhibit toughnesses over 100 MPa\u00b7m1/2. As with any material, the macroscopic geometry of a diamond contributes to its resistance to breakage. Diamond has a cleavage plane and is therefore more fragile in some orientations than others. Diamond cutters use this attribute to cleave some stones, prior to faceting. \"Impact toughness\" is one of the main indexes to measure the quality of synthetic industrial diamonds.\n\n\n=== Pressure resistance ===\nUsed in so-called diamond anvil experiments to create high-pressure environments, diamonds are able to withstand crushing pressures in excess of 600 gigapascals (6 million atmospheres).\n\n\n=== Electrical conductivity ===\nOther specialized applications also exist or are being developed, including use as semiconductors: some blue diamonds are natural semiconductors, in contrast to most diamonds, which are excellent electrical insulators. The conductivity and blue color originate from boron impurity. Boron substitutes for carbon atoms in the diamond lattice, donating a hole into the valence band.\nSubstantial conductivity is commonly observed in nominally undoped diamond grown by chemical vapor deposition. This conductivity is associated with hydrogen-related species adsorbed at the surface, and it can be removed by annealing or other surface treatments.\n\n\n=== Surface property ===\nDiamonds are naturally lipophilic and hydrophobic, which means the diamonds' surface cannot be wet by water but can be easily wet and stuck by oil. This property can be utilized to extract diamonds using oil when making synthetic diamonds. However, when diamond surfaces are chemically modified with certain ions, they are expected to become so hydrophilic that they can stabilize multiple layers of water ice at human body temperature.\nThe surface of diamonds is partially oxidized. The oxidized surface can be reduced by heat treatment under hydrogen flow. That is to say, this heat treatment partially removes oxygen-containing functional groups. But diamonds (sp3C) are unstable against high temperature (above about 400 \u00b0C (752 \u00b0F) ) under atmospheric pressure. The structure gradually changes into sp2C above this temperature. Thus, diamonds should be reduced under this temperature.\n\n\n=== Chemical stability ===\nDiamonds are not very reactive. Under room temperature diamonds do not react with any chemical reagents including strong acids and bases. A diamond's surface can only be oxidized at temperatures above about 850 \u00b0C (1,560 \u00b0F) in air. Diamond also reacts with fluorine gas above about 700 \u00b0C (1,292 \u00b0F).\n\n\n=== Color ===\n\nDiamond has a wide bandgap of 6981881197067849999\u26605.5 eV corresponding to the deep ultraviolet wavelength of 225 nanometers. This means pure diamond should transmit visible light and appear as a clear colorless crystal. Colors in diamond originate from lattice defects and impurities. The diamond crystal lattice is exceptionally strong and only atoms of nitrogen, boron and hydrogen can be introduced into diamond during the growth at significant concentrations (up to atomic percents). Transition metals nickel and cobalt, which are commonly used for growth of synthetic diamond by high-pressure high-temperature techniques, have been detected in diamond as individual atoms; the maximum concentration is 0.01% for nickel and even less for cobalt. Virtually any element can be introduced to diamond by ion implantation.\nNitrogen is by far the most common impurity found in gem diamonds and is responsible for the yellow and brown color in diamonds. Boron is responsible for the blue color. Color in diamond has two additional sources: irradiation (usually by alpha particles), that causes the color in green diamonds; and plastic deformation of the diamond crystal lattice. Plastic deformation is the cause of color in some brown and perhaps pink and red diamonds. In order of rarity, yellow diamond is followed by brown, colorless, then by blue, green, black, pink, orange, purple, and red. \"Black\", or Carbonado, diamonds are not truly black, but rather contain numerous dark inclusions that give the gems their dark appearance. Colored diamonds contain impurities or structural defects that cause the coloration, while pure or nearly pure diamonds are transparent and colorless. Most diamond impurities replace a carbon atom in the crystal lattice, known as a carbon flaw. The most common impurity, nitrogen, causes a slight to intense yellow coloration depending upon the type and concentration of nitrogen present. The Gemological Institute of America (GIA) classifies low saturation yellow and brown diamonds as diamonds in the normal color range, and applies a grading scale from \"D\" (colorless) to \"Z\" (light yellow). Diamonds of a different color, such as blue, are called fancy colored diamonds, and fall under a different grading scale.\nIn 2008, the Wittelsbach Diamond, a 35.56-carat (7.112 g) blue diamond once belonging to the King of Spain, fetched over US$24 million at a Christie's auction. In May 2009, a 7.03-carat (1.406 g) blue diamond fetched the highest price per carat ever paid for a diamond when it was sold at auction for 10.5 million Swiss francs (6.97 million euro or US$9.5 million at the time). That record was however beaten the same year: a 5-carat (1.0 g) vivid pink diamond was sold for $10.8 million in Hong Kong on December 1, 2009.\n\n\n=== Identification ===\nDiamonds can be identified by their high thermal conductivity. Their high refractive index is also indicative, but other materials have similar refractivity. Diamonds cut glass, but this does not positively identify a diamond because other materials, such as quartz, also lie above glass on the Mohs scale and can also cut it. Diamonds can scratch other diamonds, but this can result in damage to one or both stones. Hardness tests are infrequently used in practical gemology because of their potentially destructive nature. The extreme hardness and high value of diamond means that gems are typically polished slowly using painstaking traditional techniques and greater attention to detail than is the case with most other gemstones; these tend to result in extremely flat, highly polished facets with exceptionally sharp facet edges. Diamonds also possess an extremely high refractive index and fairly high dispersion. Taken together, these factors affect the overall appearance of a polished diamond and most diamantaires still rely upon skilled use of a loupe (magnifying glass) to identify diamonds 'by eye'.\n\n\n== Industry ==\n\nThe diamond industry can be separated into two distinct categories: one dealing with gem-grade diamonds and another for industrial-grade diamonds. Both markets value diamonds differently.\n\n\n=== Gem-grade diamonds ===\n\nA large trade in gem-grade diamonds exists. Although most gem-grade diamonds are sold newly polished, there is a well-established market for resale of polished diamonds (e.g. pawnbroking, auctions, second-hand jewelry stores, diamantaires, bourses, etc.). One hallmark of the trade in gem-quality diamonds is its remarkable concentration: wholesale trade and diamond cutting is limited to just a few locations; in 2003, 92% of the world's diamonds were cut and polished in Surat, India. Other important centers of diamond cutting and trading are the Antwerp diamond district in Belgium, where the International Gemological Institute is based, London, the Diamond District in New York City, the Diamond Exchange District in Tel Aviv, and Amsterdam. One contributory factor is the geological nature of diamond deposits: several large primary kimberlite-pipe mines each account for significant portions of market share (such as the Jwaneng mine in Botswana, which is a single large-pit mine that can produce between 12,500,000 carats (2,500 kg) to 15,000,000 carats (3,000 kg) of diamonds per year). Secondary alluvial diamond deposits, on the other hand, tend to be fragmented amongst many different operators because they can be dispersed over many hundreds of square kilometers (e.g., alluvial deposits in Brazil).\nThe production and distribution of diamonds is largely consolidated in the hands of a few key players, and concentrated in traditional diamond trading centers, the most important being Antwerp, where 80% of all rough diamonds, 50% of all cut diamonds and more than 50% of all rough, cut and industrial diamonds combined are handled. This makes Antwerp a de facto \"world diamond capital\". The city of Antwerp also hosts the Antwerpsche Diamantkring, created in 1929 to become the first and biggest diamond bourse dedicated to rough diamonds. Another important diamond center is New York City, where almost 80% of the world's diamonds are sold, including auction sales.\nThe De Beers company, as the world's largest diamond mining company, holds a dominant position in the industry, and has done so since soon after its founding in 1888 by the British imperialist Cecil Rhodes. De Beers is currently the world's largest operator of diamond production facilities (mines) and distribution channels for gem-quality diamonds. The Diamond Trading Company (DTC) is a subsidiary of De Beers and markets rough diamonds from De Beers-operated mines. De Beers and its subsidiaries own mines that produce some 40% of annual world diamond production. For most of the 20th century over 80% of the world's rough diamonds passed through De Beers, but by 2001\u20132009 the figure had decreased to around 45%, and by 2013 the company's market share had further decreased to around 38% in value terms and even less by volume. De Beers sold off the vast majority of its diamond stockpile in the late 1990s \u2013 early 2000s and the remainder largely represents working stock (diamonds that are being sorted before sale). This was well documented in the press but remains little known to the general public.\nAs a part of reducing its influence, De Beers withdrew from purchasing diamonds on the open market in 1999 and ceased, at the end of 2008, purchasing Russian diamonds mined by the largest Russian diamond company Alrosa. As of January 2011, De Beers states that it only sells diamonds from the following four countries: Botswana, Namibia, South Africa and Canada. Alrosa had to suspend their sales in October 2008 due to the global energy crisis, but the company reported that it had resumed selling rough diamonds on the open market by October 2009. Apart from Alrosa, other important diamond mining companies include BHP Billiton, which is the world's largest mining company; Rio Tinto Group, the owner of Argyle (100%), Diavik (60%), and Murowa (78%) diamond mines; and Petra Diamonds, the owner of several major diamond mines in Africa.\n\nFurther down the supply chain, members of The World Federation of Diamond Bourses (WFDB) act as a medium for wholesale diamond exchange, trading both polished and rough diamonds. The WFDB consists of independent diamond bourses in major cutting centers such as Tel Aviv, Antwerp, Johannesburg and other cities across the USA, Europe and Asia. In 2000, the WFDB and The International Diamond Manufacturers Association established the World Diamond Council to prevent the trading of diamonds used to fund war and inhumane acts. WFDB's additional activities include sponsoring the World Diamond Congress every two years, as well as the establishment of the International Diamond Council (IDC) to oversee diamond grading.\nOnce purchased by Sightholders (which is a trademark term referring to the companies that have a three-year supply contract with DTC), diamonds are cut and polished in preparation for sale as gemstones ('industrial' stones are regarded as a by-product of the gemstone market; they are used for abrasives). The cutting and polishing of rough diamonds is a specialized skill that is concentrated in a limited number of locations worldwide. Traditional diamond cutting centers are Antwerp, Amsterdam, Johannesburg, New York City, and Tel Aviv. Recently, diamond cutting centers have been established in China, India, Thailand, Namibia and Botswana. Cutting centers with lower cost of labor, notably Surat in Gujarat, India, handle a larger number of smaller carat diamonds, while smaller quantities of larger or more valuable diamonds are more likely to be handled in Europe or North America. The recent expansion of this industry in India, employing low cost labor, has allowed smaller diamonds to be prepared as gems in greater quantities than was previously economically feasible.\nDiamonds which have been prepared as gemstones are sold on diamond exchanges called bourses. There are 28 registered diamond bourses in the world. Bourses are the final tightly controlled step in the diamond supply chain; wholesalers and even retailers are able to buy relatively small lots of diamonds at the bourses, after which they are prepared for final sale to the consumer. Diamonds can be sold already set in jewelry, or sold unset (\"loose\"). According to the Rio Tinto Group, in 2002 the diamonds produced and released to the market were valued at US$9 billion as rough diamonds, US$14 billion after being cut and polished, US$28 billion in wholesale diamond jewelry, and US$57 billion in retail sales.\n\n\n==== Cutting ====\n\nMined rough diamonds are converted into gems through a multi-step process called \"cutting\". Diamonds are extremely hard, but also brittle and can be split up by a single blow. Therefore, diamond cutting is traditionally considered as a delicate procedure requiring skills, scientific knowledge, tools and experience. Its final goal is to produce a faceted jewel where the specific angles between the facets would optimize the diamond luster, that is dispersion of white light, whereas the number and area of facets would determine the weight of the final product. The weight reduction upon cutting is significant and can be of the order of 50%. Several possible shapes are considered, but the final decision is often determined not only by scientific, but also practical considerations. For example, the diamond might be intended for display or for wear, in a ring or a necklace, singled or surrounded by other gems of certain color and shape. Some of them may be considered as classical, such as round, pear, marquise, oval, hearts and arrows diamonds, etc. Some of them are special, produced by certain companies, for example, Phoenix, Cushion, Sole Mio diamonds, etc.\nThe most time-consuming part of the cutting is the preliminary analysis of the rough stone. It needs to address a large number of issues, bears much responsibility, and therefore can last years in case of unique diamonds. The following issues are considered:\nThe hardness of diamond and its ability to cleave strongly depend on the crystal orientation. Therefore, the crystallographic structure of the diamond to be cut is analyzed using X-ray diffraction to choose the optimal cutting directions.\nMost diamonds contain visible non-diamond inclusions and crystal flaws. The cutter has to decide which flaws are to be removed by the cutting and which could be kept.\nThe diamond can be split by a single, well calculated blow of a hammer to a pointed tool, which is quick, but risky. Alternatively, it can be cut with a diamond saw, which is a more reliable but tedious procedure.\nAfter initial cutting, the diamond is shaped in numerous stages of polishing. Unlike cutting, which is a responsible but quick operation, polishing removes material by gradual erosion and is extremely time consuming. The associated technique is well developed; it is considered as a routine and can be performed by technicians. After polishing, the diamond is reexamined for possible flaws, either remaining or induced by the process. Those flaws are concealed through various diamond enhancement techniques, such as repolishing, crack filling, or clever arrangement of the stone in the jewelry. Remaining non-diamond inclusions are removed through laser drilling and filling of the voids produced.\n\n\n==== Marketing ====\nMarketing has significantly affected the image of diamond as a valuable commodity.\nN. W. Ayer & Son, the advertising firm retained by De Beers in the mid-20th century, succeeded in reviving the American diamond market. And the firm created new markets in countries where no diamond tradition had existed before. N. W. Ayer's marketing included product placement, advertising focused on the diamond product itself rather than the De Beers brand, and associations with celebrities and royalty. Without advertising the De Beers brand, De Beers was advertising its competitors' diamond products as well, but this was not a concern as De Beers dominated the diamond market throughout the 20th century. De Beers' market share dipped temporarily to 2nd place in the global market below Alrosa in the aftermath of the global economic crisis of 2008, down to less than 29% in terms of carats mined, rather than sold. The campaign lasted for decades but was effectively discontinued by early 2011. De Beers still advertises diamonds, but the advertising now mostly promotes its own brands, or licensed product lines, rather than completely \"generic\" diamond products. The campaign was perhaps best captured by the slogan \"a diamond is forever\". This slogan is now being used by De Beers Diamond Jewelers, a jewelry firm which is a 50%/50% joint venture between the De Beers mining company and LVMH, the luxury goods conglomerate.\nBrown-colored diamonds constituted a significant part of the diamond production, and were predominantly used for industrial purposes. They were seen as worthless for jewelry (not even being assessed on the diamond color scale). After the development of Argyle diamond mine in Australia in 1986, and marketing, brown diamonds have become acceptable gems. The change was mostly due to the numbers: the Argyle mine, with its 35,000,000 carats (7,000 kg) of diamonds per year, makes about one-third of global production of natural diamonds; 80% of Argyle diamonds are brown.\n\n\n=== Industrial-grade diamonds ===\n\nIndustrial diamonds are valued mostly for their hardness and thermal conductivity, making many of the gemological characteristics of diamonds, such as the 4 Cs, irrelevant for most applications. 80% of mined diamonds (equal to about 135,000,000 carats (27,000 kg) annually), are unsuitable for use as gemstones, and used industrially. In addition to mined diamonds, synthetic diamonds found industrial applications almost immediately after their invention in the 1950s; another 570,000,000 carats (114,000 kg) of synthetic diamond is produced annually for industrial use (in 2004; in 2014 it's 4,500,000,000 carats (900,000 kg), 90% of which is produced in China). Approximately 90% of diamond grinding grit is currently of synthetic origin.\nThe boundary between gem-quality diamonds and industrial diamonds is poorly defined and partly depends on market conditions (for example, if demand for polished diamonds is high, some lower-grade stones will be polished into low-quality or small gemstones rather than being sold for industrial use). Within the category of industrial diamonds, there is a sub-category comprising the lowest-quality, mostly opaque stones, which are known as bort.\nIndustrial use of diamonds has historically been associated with their hardness, which makes diamond the ideal material for cutting and grinding tools. As the hardest known naturally occurring material, diamond can be used to polish, cut, or wear away any material, including other diamonds. Common industrial applications of this property include diamond-tipped drill bits and saws, and the use of diamond powder as an abrasive. Less expensive industrial-grade diamonds, known as bort, with more flaws and poorer color than gems, are used for such purposes. Diamond is not suitable for machining ferrous alloys at high speeds, as carbon is soluble in iron at the high temperatures created by high-speed machining, leading to greatly increased wear on diamond tools compared to alternatives.\nSpecialized applications include use in laboratories as containment for high pressure experiments (see diamond anvil cell), high-performance bearings, and limited use in specialized windows. With the continuing advances being made in the production of synthetic diamonds, future applications are becoming feasible. The high thermal conductivity of diamond makes it suitable as a heat sink for integrated circuits in electronics.\n\n\n=== Mining ===\n\nApproximately 130,000,000 carats (26,000 kg) of diamonds are mined annually, with a total value of nearly US$9 billion, and about 100,000 kg (220,000 lb) are synthesized annually.\nRoughly 49% of diamonds originate from Central and Southern Africa, although significant sources of the mineral have been discovered in Canada, India, Russia, Brazil, and Australia. They are mined from kimberlite and lamproite volcanic pipes, which can bring diamond crystals, originating from deep within the Earth where high pressures and temperatures enable them to form, to the surface. The mining and distribution of natural diamonds are subjects of frequent controversy such as concerns over the sale of blood diamonds or conflict diamonds by African paramilitary groups. The diamond supply chain is controlled by a limited number of powerful businesses, and is also highly concentrated in a small number of locations around the world.\nOnly a very small fraction of the diamond ore consists of actual diamonds. The ore is crushed, during which care is required not to destroy larger diamonds, and then sorted by density. Today, diamonds are located in the diamond-rich density fraction with the help of X-ray fluorescence, after which the final sorting steps are done by hand. Before the use of X-rays became commonplace, the separation was done with grease belts; diamonds have a stronger tendency to stick to grease than the other minerals in the ore.\n\nHistorically, diamonds were found only in alluvial deposits in Guntur and Krishna district of the Krishna River delta in Southern India. India led the world in diamond production from the time of their discovery in approximately the 9th century BC to the mid-18th century AD, but the commercial potential of these sources had been exhausted by the late 18th century and at that time India was eclipsed by Brazil where the first non-Indian diamonds were found in 1725. Currently, one of the most prominent Indian mines is located at Panna.\nDiamond extraction from primary deposits (kimberlites and lamproites) started in the 1870s after the discovery of the Diamond Fields in South Africa. Production has increased over time and now an accumulated total of 4,500,000,000 carats (900,000 kg) have been mined since that date. Twenty percent of that amount has been mined in the last five years, and during the last 10 years, nine new mines have started production; four more are waiting to be opened soon. Most of these mines are located in Canada, Zimbabwe, Angola, and one in Russia.\nIn the U.S., diamonds have been found in Arkansas, Colorado, Wyoming, and Montana. In 2004, the discovery of a microscopic diamond in the U.S. led to the January 2008 bulk-sampling of kimberlite pipes in a remote part of Montana. The Crater of Diamonds State Park in Arkansas is open to the public, and is the only mine in the world where members of the public can dig for diamonds.\nToday, most commercially viable diamond deposits are in Russia (mostly in Sakha Republic, for example Mir pipe and Udachnaya pipe), Botswana, Australia (Northern and Western Australia) and the Democratic Republic of the Congo. In 2005, Russia produced almost one-fifth of the global diamond output, according to the British Geological Survey. Australia boasts the richest diamantiferous pipe, with production from the Argyle diamond mine reaching peak levels of 42 metric tons per year in the 1990s. There are also commercial deposits being actively mined in the Northwest Territories of Canada and Brazil. Diamond prospectors continue to search the globe for diamond-bearing kimberlite and lamproite pipes.\n\n\n=== Political issues ===\n\nIn some of the more politically unstable central African and west African countries, revolutionary groups have taken control of diamond mines, using proceeds from diamond sales to finance their operations. Diamonds sold through this process are known as conflict diamonds or blood diamonds. Major diamond trading corporations continue to fund and fuel these conflicts by doing business with armed groups.\nIn response to public concerns that their diamond purchases were contributing to war and human rights abuses in central and western Africa, the United Nations, the diamond industry and diamond-trading nations introduced the Kimberley Process in 2002. The Kimberley Process aims to ensure that conflict diamonds do not become intermixed with the diamonds not controlled by such rebel groups. This is done by requiring diamond-producing countries to provide proof that the money they make from selling the diamonds is not used to fund criminal or revolutionary activities. Although the Kimberley Process has been moderately successful in limiting the number of conflict diamonds entering the market, some still find their way in. According to the International Diamond Manufacturers Association, conflict diamonds constitute 2\u20133% of all diamonds traded. Two major flaws still hinder the effectiveness of the Kimberley Process: (1) the relative ease of smuggling diamonds across African borders, and (2) the violent nature of diamond mining in nations that are not in a technical state of war and whose diamonds are therefore considered \"clean\".\nThe Canadian Government has set up a body known as the Canadian Diamond Code of Conduct to help authenticate Canadian diamonds. This is a stringent tracking system of diamonds and helps protect the \"conflict free\" label of Canadian diamonds.\n\n\n== Synthetics, simulants, and enhancements ==\n\n\n=== Synthetics ===\n\nSynthetic diamonds are diamonds manufactured in a laboratory, as opposed to diamonds mined from the Earth. The gemological and industrial uses of diamond have created a large demand for rough stones. This demand has been satisfied in large part by synthetic diamonds, which have been manufactured by various processes for more than half a century. However, in recent years it has become possible to produce gem-quality synthetic diamonds of significant size. It is possible to make colorless synthetic gemstones that, on a molecular level, are identical to natural stones and so visually similar that only a gemologist with special equipment can tell the difference.\nThe majority of commercially available synthetic diamonds are yellow and are produced by so-called high-pressure high-temperature (HPHT) processes. The yellow color is caused by nitrogen impurities. Other colors may also be reproduced such as blue, green or pink, which are a result of the addition of boron or from irradiation after synthesis.\n\nAnother popular method of growing synthetic diamond is chemical vapor deposition (CVD). The growth occurs under low pressure (below atmospheric pressure). It involves feeding a mixture of gases (typically 1 to 99 methane to hydrogen) into a chamber and splitting them to chemically active radicals in a plasma ignited by microwaves, hot filament, arc discharge, welding torch or laser. This method is mostly used for coatings, but can also produce single crystals several millimeters in size (see picture).\nAs of 2010, nearly all 5,000 million carats (1,000 tonnes) of synthetic diamonds produced per year are for industrial use. Around 50% of the 133 million carats of natural diamonds mined per year end up in industrial use. Mining companies' expenses average $40 to $60 per carat for natural colorless diamonds, while synthetic manufacturers' expenses average $2,500 per carat for synthetic, gem-quality colorless diamonds. However, a purchaser is more likely to encounter a synthetic when looking for a fancy-colored diamond because nearly all synthetic diamonds are fancy-colored, while only 0.01% of natural diamonds are.\n\n\n=== Simulants ===\n\nA diamond simulant is a non-diamond material that is used to simulate the appearance of a diamond, and may be referred to as diamante. Cubic zirconia is the most common. The gemstone moissanite (silicon carbide) can be treated as a diamond simulant, though more costly to produce than cubic zirconia. Both are produced synthetically.\n\n\n=== Enhancements ===\n\nDiamond enhancements are specific treatments performed on natural or synthetic diamonds (usually those already cut and polished into a gem), which are designed to better the gemological characteristics of the stone in one or more ways. These include laser drilling to remove inclusions, application of sealants to fill cracks, treatments to improve a white diamond's color grade, and treatments to give fancy color to a white diamond.\nCoatings are increasingly used to give a diamond simulant such as cubic zirconia a more \"diamond-like\" appearance. One such substance is diamond-like carbon\u2014an amorphous carbonaceous material that has some physical properties similar to those of the diamond. Advertising suggests that such a coating would transfer some of these diamond-like properties to the coated stone, hence enhancing the diamond simulant. Techniques such as Raman spectroscopy should easily identify such a treatment.\n\n\n=== Identification ===\nEarly diamond identification tests included a scratch test relying on the superior hardness of diamond. This test is destructive, as a diamond can scratch another diamond, and is rarely used nowadays. Instead, diamond identification relies on its superior thermal conductivity. Electronic thermal probes are widely used in the gemological centers to separate diamonds from their imitations. These probes consist of a pair of battery-powered thermistors mounted in a fine copper tip. One thermistor functions as a heating device while the other measures the temperature of the copper tip: if the stone being tested is a diamond, it will conduct the tip's thermal energy rapidly enough to produce a measurable temperature drop. This test takes about 2\u20133 seconds.\nWhereas the thermal probe can separate diamonds from most of their simulants, distinguishing between various types of diamond, for example synthetic or natural, irradiated or non-irradiated, etc., requires more advanced, optical techniques. Those techniques are also used for some diamonds simulants, such as silicon carbide, which pass the thermal conductivity test. Optical techniques can distinguish between natural diamonds and synthetic diamonds. They can also identify the vast majority of treated natural diamonds. \"Perfect\" crystals (at the atomic lattice level) have never been found, so both natural and synthetic diamonds always possess characteristic imperfections, arising from the circumstances of their crystal growth, that allow them to be distinguished from each other.\nLaboratories use techniques such as spectroscopy, microscopy and luminescence under shortwave ultraviolet light to determine a diamond's origin. They also use specially made instruments to aid them in the identification process. Two screening instruments are the DiamondSure and the DiamondView, both produced by the DTC and marketed by the GIA.\nSeveral methods for identifying synthetic diamonds can be performed, depending on the method of production and the color of the diamond. CVD diamonds can usually be identified by an orange fluorescence. D-J colored diamonds can be screened through the Swiss Gemmological Institute's Diamond Spotter. Stones in the D-Z color range can be examined through the DiamondSure UV/visible spectrometer, a tool developed by De Beers. Similarly, natural diamonds usually have minor imperfections and flaws, such as inclusions of foreign material, that are not seen in synthetic diamonds.\nScreening devices based on diamond type detection can be used to make a distinction between diamonds that are certainly natural and diamonds that are potentially synthetic. Those potentially synthetic diamonds require more investigation in a specialized lab. Examples of commercial screening devices are D-Screen (WTOCD / HRD Antwerp) and Alpha Diamond Analyzer (Bruker / HRD Antwerp).\n\n\n== Stolen diamonds ==\nOccasionally large thefts of diamonds take place. In February 2013 armed robbers carried out a raid at Brussels Airport and escaped with gems estimated to be worth $50m (\u00a332m; 37m euros). The gang broke through a perimeter fence and raided the cargo hold of a Swiss-bound plane. The gang have since been arrested and large amounts of cash and diamonds recovered.\nThe identification of stolen diamonds presents a set of difficult problems. Rough diamonds will have a distinctive shape depending on whether their source is a mine or from an alluvial environment such as a beach or river - alluvial diamonds have smoother surfaces than those that have been mined. Determining the provenance of cut and polished stones is much more complex.\nThe Kimberley Process was developed to monitor the trade in rough diamonds and prevent their being used to fund violence. Before exporting, rough diamonds are certificated by the government of the country of origin. Some countries, such as Venezuela, are not party to the agreement. The Kimberley Process does not apply to local sales of rough diamonds within a country.\nDiamonds may be etched by laser with marks invisible to the naked eye. Lazare Kaplan, a US-based company, developed this method. However, whatever is marked on a diamond can readily be removed.\n\n\n== See also ==\n\nList of diamonds\nList of largest rough diamonds\n\nList of minerals\nDiamonds on Jupiter and Saturn\nLonsdaleite\n\n\n== References ==\n\n\n== Books ==\nC. Even-Zohar (2007). From Mine to Mistress: Corporate Strategies and Government Policies in the International Diamond Industry (2nd ed.). Mining Journal Press. \nG. Davies (1994). Properties and growth of diamond. INSPEC. ISBN 0-85296-875-2. \nM. O'Donoghue, M (2006). Gems. Elsevier. ISBN 0-7506-5856-8. \nM. O'Donoghue and L. Joyner (2003). Identification of gemstones. Great Britain: Butterworth-Heinemann. ISBN 0-7506-5512-7. \nA. Feldman and L.H. Robins (1991). Applications of Diamond Films and Related Materials. Elsevier. \nJ.E. Field (1979). The Properties of Diamond. London: Academic Press. ISBN 0-12-255350-0. \nJ.E. Field (1992). The Properties of Natural and Synthetic Diamond. London: Academic Press. ISBN 0-12-255352-7. \nW. Hershey (1940). The Book of Diamonds. Hearthside Press New York. ISBN 1-4179-7715-9. \nS. Koizumi, C.E. Nebel and M. Nesladek (2008). Physics and Applications of CVD Diamond. Wiley VCH. ISBN 3-527-40801-0. \nL.S. Pan and D.R. Kani (1995). Diamond: Electronic Properties and Applications. Kluwer Academic Publishers. ISBN 0-7923-9524-7. \nPagel-Theisen, Verena (2001). Diamond Grading ABC: the Manual. Antwerp: Rubin & Son. ISBN 3-9800434-6-0. \nR.L. Radovic, P.M. Walker and P.A. Thrower (1965). Chemistry and physics of carbon: a series of advances. New York: Marcel Dekker. ISBN 0-8247-0987-X. \nM. Tolkowsky (1919). Diamond Design: A Study of the Reflection and Refraction of Light in a Diamond. London: E. & F.N. Spon. \nR.W. Wise (2003). Secrets of the Gem Trade: The Connoisseur's Guide to Precious Gemstones. Brunswick House Press. \nA.M. Zaitsev (2001). Optical Properties of Diamond: A Data Handbook. Springer. ISBN 3-540-66582-X. \n\n\n== External links ==\nProperties of diamond: Ioffe database\n\"A Contribution to the Understanding of Blue Fluorescence on the Appearance of Diamonds\". (2007) Gemological Institute of America (GIA)\nTyson, Peter (November 2000). \"Diamonds in the Sky\". Retrieved March 10, 2005.\nHave You Ever Tried to Sell a Diamond?", 
                "titleUrl": "https://en.wikipedia.org/wiki/Diamond", 
                "title": "Diamond"
            }
        ], 
        "phraseCharStart": "860"
    }, 
    {
        "phraseCharEnd": "892", 
        "phraseIndex": "T19", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "divertor", 
        "wikiSearchResults": [
            {
                "snippet": "divertor was proposed as a solution to this problem. Operating on the same principle as a mass spectrometer, the plasma passes through the divertor region", 
                "pageCategories": "All stub articles\nEnergy stubs\nFusion power", 
                "pageContent": "In nuclear fusion power research, a divertor is a device within a tokamak that allows the online removal of waste material from the plasma while the reactor is still operating. This allows control over the buildup of fusion products in the fuel, and removes impurities in the plasma that have entered into it from the vessel lining.\nThe divertor was initially introduced during the earliest studies of fusion power systems in the 1950s. It was realized early on that successful fusion would result in heavier ions being created and left in the fuel (the so-called \"fusion ash\"). These impurities were responsible for the loss of heat, and caused other effects that made it more difficult to keep the reaction going. The divertor was proposed as a solution to this problem. Operating on the same principle as a mass spectrometer, the plasma passes through the divertor region where heavier ions are flung out of the fuel mass by centrifugal force, colliding with some sort of absorber material, and depositing its energy as heat. Initially considered to be a device required for operational reactors, few early designs included a divertor.\nWhen early long-shot reactors started to appear in the 1970s, a serious practical problem emerged. No matter how tightly constrained, plasma continued to leak out of the main confinement area, striking the walls of the reactor core and causing all sorts of problems. A major concern was sputtering in reactors with higher power and particle flux density, which caused ions of the vacuum chamber's metal walls to flow into the fuel and to cool it.\nDuring the 1980s it became common for reactors to include a feature known as the limiter, which is a small piece of material that projects a short distance into the outer edge of the main plasma confinement area. Ions from the fuel that are travelling outwards strike the limiter, thereby protecting the walls of the chamber from this damage. However, the problems with material being deposited into the fuel remained; the limiter simply changed where that material was coming from.\nThis led to the re-emergence of the divertor, as a device for protecting the reactor itself. In these designs, magnets pull the lower edge of the plasma to create a small region where the outer edge of the plasma, the \"Scrape-Off Layer\" (SOL), hits a limiter-like plate. The divertor improves on the limiter in several ways, but mainly because modern reactors try to create plasmas with D-shaped cross-sections (\"elongation\" and \"triangularity\") so the lower edge of the D is a natural location for the divertor. In modern examples the plates are replaced by lithium metal, which better captures the ions and causes less cooling when it enters the plasma. \nIn ITER and the latest configuration of Joint European Torus, the lowest region of the torus is configured as a divertor, while Alcator C-Mod was built with divertor channels at both top and bottom.\nA tokamak featuring a divertor is known as a divertor tokamak or divertor configuration tokamak. In this configuration, the particles escape through a magnetic \"gap\" (separatrix), which allows the energy absorbing part of the divertor to be placed outside the plasma. The divertor configuration also makes it easier to obtain a more stable \"H-mode\" of operation. The plasma facing material in the divertor faces significantly different stresses compared to the majority of the first wall.\n\n\n== See also ==\nNuclear fission\n\n\n== References ==\n\n\n== Further reading ==\nSnowflake and the multiple divertor concepts. March 2016\n\n\n== External links ==\nLimiters\nDivertors", 
                "titleUrl": "https://en.wikipedia.org/wiki/Divertor", 
                "title": "Divertor"
            }, 
            {
                "snippet": "The ULS (UMIST Linear System) is a gas target divertor simulator located on the former UMIST campus of the University of Manchester. It enables physicists", 
                "pageCategories": "Plasma physics\nUniversity of Manchester", 
                "pageContent": "The ULS (UMIST Linear System) is a gas target divertor simulator located on the former UMIST campus of the University of Manchester. It enables physicists to study the recombination processes of a detached plasma in a hydrogen target chamber.\nResearch on detached plasma and on its recombination modes is of primary importance in order to design an appropriate divertor region in a future nuclear fusion power plant, where huge amounts of energy will be deposited by the fast-moving particles generated in the main reactor. The major goal of the ULS as for many other linear divertor simulators, is to reproduce the same temperature and density conditions of the SOL (Scrape Off Layer) of a tokamak in a linear environment and therefore to make easier the study of its properties.\nIn the past few years, ULS has been used with great insight to analyze the molecular activation and the electron-ion recombination modes, and to determine the conditions for their activation: diffusive processes have also been considered. However, research on these subjects is still ongoing and our understanding of the elementary processes involved in a detached plasma is still far from being satisfactory.\n\n\n== Further reading ==\nKay, Michael J. (1998) A Study of Plasma Attenuation and Recombination in the Gas Target Chamber of a Divertor Simulator. UMIST Ph.D. thesis\nMihailjcic, B.; et al. (2007). \"Spatially resolved spectroscopy of detached recombining plasmas in the University of Manchester Linear System divertor simulator\". Physics of Plasmas. White Rose Research Online. Retrieved 1 October 2010.", 
                "titleUrl": "https://en.wikipedia.org/wiki/UMIST_linear_system", 
                "title": "UMIST linear system"
            }, 
            {
                "snippet": "ASDEX Upgrade (Axially Symmetric Divertor Experiment) is a divertor tokamak, that went into operation at the Max-Planck-Institut f\u00fcr Plasmaphysik, Garching", 
                "pageCategories": "All articles to be expanded\nArticles to be expanded from December 2015\nArticles using small message boxes\nInterlanguage link template link number\nTokamaks\nWikipedia articles with GND identifiers", 
                "pageContent": "ASDEX Upgrade (Axially Symmetric Divertor Experiment) is a divertor tokamak, that went into operation at the Max-Planck-Institut f\u00fcr Plasmaphysik, Garching in 1991. At present, it is Germany's second largest fusion experiment after stellarator Wendelstein 7X.\n\n\n== Overview ==\nTo make experiments under reactor-like conditions possible, essential plasma properties, particularly the plasma density and pressure and the wall load, have been adapted in ASDEX Upgrade to the conditions that will be present in a future fusion power plant.\nASDEX Upgrade is, compared to other international tokamaks, a midsize tokamak experiment. It began operation in 1991 and it succeeds the ASDEX experiment, which was in operation from 1980 until 1990.\nOne innovative feature of the ASDEX Upgrade experiment is its all-tungsten first wall; tungsten is a good choice for the first wall of a tokamak because of its very high melting point (over 3000 degrees Celsius) which enables it to stand up to the very high heat fluxes emanating from the hot plasma at the heart of the tokamak; however there are also problems associated with a tungsten first wall, such as tungsten's tendency to ionise at high temperatures, \"polluting\" the plasma and diluting the deuterium-tritium fuel mix. Furthermore, as a high Z material, radiation from fully ionized tungsten in the plasma is several orders of magnitude higher than that of other proposed first wall components such as carbon fibre composites (CFCs) or beryllium. This result allows for far less tungsten to \"contaminate\" a proposed break-even plasma. ASDEX Upgrade will examine ways to overcome this problem, in preparation for the construction of ITER's first wall.\nThe experiment has an overall radius of 5 metres, a gross weight of 800 metric tons, a maximum magnetic field strength of 3.1 tesla, a maximum plasma current of 1.6 megaampere and maximum heating power of up to 27 megawatt. Plasma heating and current drive in the ASDEX Upgrade is derived from several sources, namely 1 MW of ohmic heating, 20 MW of neutral beam injection, 6 MW via ion cyclotron resonance heating (ICRH) at frequencies between 30 and 120 megahertz, and 2 x 2 MW of electron cyclotron resonance heating (ECRH) at 140 gigahertz. It has 16 toroidal field coils and 12 poloidal field coils.\nThree large flywheel generators feed the 580 MVA pulsed power supply system for the magnetic confinement and plasma heating.\n\n\n== ASDEX ==\nThe original configuration was ? It went into operation in 1980. It shutdown in 1990 for a major upgrade.\n\n\n== See also ==\nList of fusion experiments\nEdge-localized mode\nBall-pen probe\n\n\n== External links ==\nASDEX Upgrade homepage\nASDEX Upgrade technical specs\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/ASDEX_Upgrade", 
                "title": "ASDEX Upgrade"
            }, 
            {
                "snippet": "system A mechanism for magnetically limiting a plasma, and hence for controlling the nuclear fusion in a tokamak; see divertor  separator (disambiguation)", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Separatrix", 
                "title": "Separatrix"
            }, 
            {
                "snippet": "Prior to October 1995, START had no rapid terminations. In October 1995, divertor coils were installed and images showed the plasma would interact with the", 
                "pageCategories": "All stub articles\nInterlanguage link template link number\nNuclear and atomic physics stubs\nTokamaks", 
                "pageContent": "The Small Tight Aspect Ratio Tokamak, or START was a nuclear fusion experiment that used magnetic confinement to hold plasma. START was the first full-sized machine to use the spherical tokamak design, which aimed to greatly reduce the aspect ratio of the traditional tokamak design.\nThe experiment began at the Culham Science Centre in the United Kingdom in 1990 and was retired in 1998. It was built as a low cost design, largely using parts already available to the START team. The START experiment revolutionized the tokamak by changing the previous toroidal shape into a tighter, almost spherical, doughnut shape. The new shape increased efficiency by reducing the cost over the conventional design, while the field required to maintain a stable plasma was a factor of 10 less.\nThe main components that comprised START included the support structure, pulse transformer, vacuum tank, toroidal and poloidal field coils, and a limiter. The support structure positioned and supported the vacuum tank which also shared the same spherical center as the large pulse transformer. The main role of the pulse transformer was to provide the current for the toroidal field coils which was supplied through fifteen irons cores that were spirally wound from a .03 millimeter iron strip. The toroidal field coil was a central conductor made of copper on the axis of the vacuum tank, and was attached to the vacuum tank through copper limbs covered by insulated clamps. START had six poloidal field coils within the vacuum tank and were encased in 3 millimeter stainless steel cases. The poloidal coils were supported from the base of the tank and each could be adjusted as necessary. The vacuum tank was the primary vessel where experiments take place; it was cylindrical in shape and was divided into three sections. The tank offered numerous ports for the attachment of pumps and diagnostics. A graphite limiter was positioned around the central stainless steel tube and this provided a simple way to measure the innermost edge of the plasma during experiments.\nIn order to successfully heat experiments in a spherical tokamak, physicists performed neutral beam injection. This involved interjecting hydrogen into hydrogen or deuterium plasmas, providing effective heating of both ions and electrons. Although the atoms were injected with no net electrostatic charge, as the beam passed through the plasma, the atoms were ionized as they bounced off the ions already in the plasma. Consequently, because the magnetic field inside the torus was circular, these fast ions were confined to the background plasma. The background plasma slowed down the confined fast ions, in a similar way to how air resistance slows down a baseball. The energy transfer from the fast ions to the plasma increased the overall plasma temperature. The neutral beam injector used in START was on loan from Oak Ridge National Laboratory.\nThe magneto-hydro-dynamic limit (MHD) was an operational limit of tokamaks, with START being no exception. The START team would test the MHD using forty-six sets of Mirnov coils at different heights on the center column of START. Plasmas being formed by compression within START limited the fluctuation of the MHD.\nPrior to October 1995, START had no rapid terminations. In October 1995, divertor coils were installed and images showed the plasma would interact with the coils before disruptions occurred. These suspicions were further strengthened when the divertor coils were moved closer to the plasma in December 1996, which resulted in a higher frequency of disruptions.\nThe characteristics of plasma within START were also measured. Typical plasma within START had an aspect ratio A=1.3, elongation k=1.8, and a temperature of 400 eV.\nA number of experiments reached 32 percent beta with START, where the previous world record for beta in a tokamak was 12.6 percent. Factors that contributed to the significantly higher beta number include better vacuum conditions, a more powerful neutral beam injection, a lower toroidal field, a higher plasma pressure, and a lower magnetic pressure.\nIn March 1998, the START experiment finished and has since been disassembled and transferred to the ENEA research laboratory at Frascati, Italy. The START team began the Mega Ampere Spherical Tokamak Experiment or MAST in 1999 which operated in the Culham Science Centre, UK until 2013.\n\n\n== References ==\n\n\n== External links ==\nMAST (START follow on experiment)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Small_Tight_Aspect_Ratio_Tokamak", 
                "title": "Small Tight Aspect Ratio Tokamak"
            }, 
            {
                "snippet": "Espa\u00f1a ASD Simplified Technical English ASDEX Upgrade (Axially Symmetric Divertor Experiment) Alliance of Socialists and Democrats for Europe  ASD (disambiguation)", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/ASDE", 
                "title": "ASDE"
            }, 
            {
                "snippet": "specialized tiles and active cooling. The advanced, so-called double null divertor plasma configuration has to be maintained through efficient feedback control", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from December 2015\nAtomic and nuclear energy research in India\nFusion Research\nInterlanguage link template link number\nNuclear technology in India\nPlasma physics\nTokamaks", 
                "pageContent": "SST-1 (steady state superconducting tokamak) is a plasma confinement experimental device in the Institute for Plasma Research (IPR), an autonomous research institute under Department of Atomic Energy, India. It belongs to a new generation of tokamaks with the major objective being steady state operation of an advanced configuration ('D' Shaped) plasma. It has been designed as a medium-sized tokamak with superconducting magnets.\nThe SST-1 project will increase India's stronghold in a selected group of countries who are capable of conceptualizing and making a fully functional fusion based reactor device. The SST-1 System is housed in Institute for Plasma Research, Gandhinagar. The SST-1 mission has been chaired by eminent Indian plasma physicists like Prof. Y.C. Saxena, Dr. Chenna Reddy, and is headed by Dr. Subrata Pradhan.\nNext stage of the SST-1 mission, the SST-2, dubbed as 'DEMO', has already been initiated.\n\n\n== History ==\nThe first talks about SST Mission started in 1994. The technical details and mechanical drawings of the system were finalized in 2001. The machine was fabricated by 2005. Godrej-Boyce Pvt. Ltd. played a crucial role in fabrication of the SST-1 coils. The assembly of SST-1 convinced the top brass of Indian bureaucracy to give a green flag to the claim of Indian physicists to join the ITER program [See Info Box]. On 17 August 2005, PM Sayeed, then India's power minister informed the Rajya Sabha about India's claim to join ITER.  A team from ITER, France visited the SST-1 mission control housed in Institute for Plasma Research to see the advances Indian scientists had made. Finally on 6 December 2005, India was officially accepted as a full partner of the ITER project.  To improve and modify some of the components, the SST-1 machine was subsequently disassembled. The improved version of the machine was completely assembled by January 2012.\nIt was fully commissioned in 2013. And by 2015, produces repeatable plasma discharges up to ~ 500 ms with plasma currents in excess of 75000 A at a central field of 1.5 T. \"SST-1 is also the only tokamak in the world with superconducting toroidal field magnets operating in two-phase helium instead of supercritical helium in a cryo-stable manner, thereby demonstrating reduced cold helium consumption. \"\nAs of Dec 2015 it is having upgrades including to the plasma facing components to allow longer pulses.\n\n\n== Objectives ==\nTraditionally the tokamaks have operated with a `transformer' action- with plasma acting as a secondary, thus having the vital `self-generated' magnetic field on top of the `externally generated' (toroidal and equilibrium) fields. This is a pretty good scheme in which creation, current-drive and heating are neatly integrated and remained a choice of the fusion community for many years until the stage came to heat the plasma to multi-keV temperatures. Heating was then accomplished separately by radio frequency (RF) waves and/or energetic neutral beam injection (NBI).\nSubsequently, excellent control got established on tokamak plasma performance by controlling the plasma-wall interaction processes at the plasma boundary so the plasma duration was limited primarily by the `transformer pulse length'. However, for relevance to future power reactors it is essential to operate these devices in a steady state mode. The very idea of steady state operation presents a series of physics and technology challenges. For example, the excellent plasma performance which was accomplished earlier, was with the surrounding material wall acting as a good 'pump' of particles, a fact which may not be true in steady state.\nSo one has to try and accomplish an equally good performance in presence of a possibly `saturated' wall. Secondly, a host of engineering and technical considerations spring up. The magnets must be superconducting type, otherwise the power dissipation in conventional (resistive) types can reach uneconomical levels. They have to be specially designed to remain superconducting in spite of their proximity to the other `warm' objects (like vacuum vessel etc.). The heat and particle exhaust must be handled in steady state with specialized tiles and active cooling. The advanced, so-called double null divertor plasma configuration has to be maintained through efficient feedback control avoiding plasma disruptions over long discharge durations.\n\n\n== Tokamak parameters ==\n\n\n== Plasma diagnostics on SST-1 ==\nSST-1 will feature many new plasma diagnostic devices, many of which are being used for the first time in fusion research in India. Some of the novel plasma diagnostics devices incorporated in SST-1 are:\nFast Scanning Langmuir probe system\nGas Puff Imaging Diagnostics\nBolometer for imaging Divertor radiations\nAlmost all of the diagnostic devices installed on SST-1 are indigenous and are designed and developed by Diagnostics Group of Institute for Plasma Research. This group is the only group working on plasma diagnostics and related technologies in Indian Subcontinent.\n\n\n== Key people ==\nDr.Subrata Pradhan - Head\n\n\n== SST-2 ==\nThe next stage of SST mission, the SST-2 fusion reactor, dubbed as 'DEMO' among Indian scientific circles has already been conceived. A group of eminent scientists from Institute for Plasma Research under the leadership of Dr. R. Srinivasan is working towards making of a full-fledged fusion reactor capable of producing electricity. Many new features like D-T plasma, Test Blanket Module, Biological shielding and an improved divertor will be incorporated in SST-2. SST-2 will also be built in the Indian state of Gujarat. The land acquisition and other basic formalities have been completed for the same.\n\n\n=== Similar projects ===\nOther designs of fusion reactor are DEMO, Wendelstein 7-X, NIF, HiPER, JET (precursor to ITER), and MAST.\n\n\n== See also ==\n\nADITYA (tokamak)\nMegaproject\nFusion for Energy\nNuclear power in India\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/SST-1_(tokamak)", 
                "title": "SST-1 (tokamak)"
            }, 
            {
                "snippet": "controlling plasma instabilities through real-time diagnostics Materials for diverters and plasma facing components Operation with \u03b2N = 2 and confinement factor", 
                "pageCategories": "All articles needing coordinates\nAll articles with dead external links\nAll articles with unsourced statements\nAnhui articles missing geocoordinate data\nArticles containing Chinese-language text\nArticles with dead external links from February 2016\nArticles with unsourced statements from February 2016\nBuildings and structures in Hefei\nInterlanguage link template link number\nNuclear energy in China", 
                "pageContent": "The Experimental Advanced Superconducting Tokamak (EAST), internal designation HT-7U) is an experimental superconducting tokamak magnetic fusion energy reactor in Hefei, China. The Hefei-based Institute of Plasma Physics is conducting the experiment for the Chinese Academy of Sciences. It has operated since 2006. It was later put under control of Hefei Institutes of Physical Science.\nIt is the first tokamak to employ superconducting toroidal and poloidal magnets. It aims for plasma pulses of up to 1000 seconds.\n\n\n== History ==\nThe project was proposed in 1996 and approved in 1998. According to a 2003 schedule, buildings and site facilities were to be constructed by 2003. Tokamak assembly was to take place from 2003 through 2005.\nConstruction was completed in March 2006 and on September 28, 2006, \"first plasma\" was achieved.\nThe reactor is an improvement over China's first superconducting tokamak device, dubbed HT-7, built by the Institute of Plasma Physics in partnership with Russia in the early 1990s.\nAccording to official reports, the project's budget is CNY \u00a5300 million (approx. USD $37 million), some 1/15 to 1/20 the cost of a comparable reactor built in other countries.\n\n\n== Operations and results ==\nOn September 28, 2006, \"first plasma\" was achieved.\nBy Jan 2007 \"the reactor created a plasma lasting nearly five seconds and generating an electrical current of 500 kilo amperes\".\nBy May 2015 it was reporting 1 MA currents, and H-mode for 6.4 seconds.\nIn February 2016, a plasma pulse was maintained for a record 102 seconds at approximately 50 million Kelvin. Plasma current of 400kA and a density of about 2.4 x 1019/m3 with slowly increasing temperature.\n\n\n== Physics objectives ==\nChina is a member of the ITER consortium, and EAST is a testbed for ITER technologies.\nEAST was designed to test:\nSuperconducting Niobium-titanium poloidal field magnets, making it the first tokamak with superconducting toroidal and poloidal magnets\nNon-inductive current drive\nPulses of up to 102 seconds with 0.5 MA plasma current\nSchemes for controlling plasma instabilities through real-time diagnostics\nMaterials for diverters and plasma facing components\nOperation with \u03b2N = 2 and confinement factor H89 > 2\n\n\n== Tokamak parameters ==\n\n\n== References ==\n\n\n== External links ==\nChinese Academy of Sciences Institute of Plasma Physics - EAST\nPeople's Daily article\nXinhua article Mar 1 2006 - Note that EAST is not the \"world's first experimental nuclear fusion device\".\nXinhua article Mar 24, 2006 Nuke fusion reactor completes test\nMainichi Daily News article Jun 2, 2006", 
                "titleUrl": "https://en.wikipedia.org/wiki/Experimental_Advanced_Superconducting_Tokamak", 
                "title": "Experimental Advanced Superconducting Tokamak"
            }, 
            {
                "snippet": "Hydrogen recombination modes are of vital importance in the development of divertor regions for tokamak reactors. In fact they will provide a good way for", 
                "pageCategories": "All articles lacking sources\nAll stub articles\nArticles lacking sources from October 2008\nPhysics stubs\nPlasma physics", 
                "pageContent": "Plasma recombination is a process by which positive ions of a plasma capture a free (energetic) electron and combine with electrons or negative ions to form new neutral atoms (gas). Recombination is an exothermic reaction, meaning heat releasing.\nRecombination usually take place in the whole volume of a plasma (volume recombination), although in some cases it is confined to some special region of it. Each kind of reaction is called a recombining mode and their individual rates are strongly affected by the properties of the plasma such as its energy (heat), density of each species, pressure and temperature of the surrounding environment. An everyday example of rapid plasma recombination occurs when a fluorescent lamp is switched off. The low-density plasma in the lamp (which generates the light by bombardment of the fluorescent coating on the inside of the glass wall) recombines in a fraction of a second after the plasma-generating electric field is removed by switching off the electric power source.\nHydrogen recombination modes are of vital importance in the development of divertor regions for tokamak reactors. In fact they will provide a good way for extracting the energy produced in the core of the plasma. At the present time, it is believed that the most likely plasma losses observed in the recombining region are due to two different modes: electron ion recombination (EIR) and molecular activated recombination (MAR).", 
                "titleUrl": "https://en.wikipedia.org/wiki/Plasma_recombination", 
                "title": "Plasma recombination"
            }, 
            {
                "snippet": "beam injector. by 2016 it was upgraded/enhanced to run with a 'snowflake' divertor    TCV Auxiliary Heating.  TCV - Development of new plasma shapes. May", 
                "pageCategories": "All articles needing additional references\nArticles needing additional references from March 2016\nFusion power\nInterlanguage link template link number\nNuclear fusion\nNuclear research institutes\nResearch projects\nTokamaks\n\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne", 
                "pageContent": "The Tokamak \u00e0 configuration variable (TCV, literally \"variable configuration tokamak\") is a research fusion reactor of the \u00c9cole polytechnique f\u00e9d\u00e9rale de Lausanne. Its distinguishing feature over other tokamaks is that its torus section is three times higher than wide. This allows studying several shapes of plasmas, which is particularly relevant since the shape of the plasma has links to the performance of the reactor. The TCV was set up in November 1992.\n\n\n== Characteristics ==\nPlasma height: 1.40 metres\nMinor radius: 0.25 metre\nMajor radius: 0.88 metre\nPlasma current: 1.2 megaamperes\nPlasma life span: 2 seconds maximum\nToroidal magnetic field: 1.43 teslas\nAdditional heating power: 4.5 megawatts\n\n\n== Main studies ==\nConfinement studies\nconfinement as a function of the shape of the plasma (triangular, square or elongated)\nImprovement of the confinement of the core\n\nStudies on vertically elongated plasmas\nStudies with ECRH and ECCD (electron cyclotron resonance heating and electron cyclotron current drive)\nBy 2012 it had 16 poloidal plasma shaping coils and could achieve a variety of field configurations and plasma shapes.\n\n\n== History ==\n1976: First proposal for an elongated tokamak by the \"New Swiss Association\"\n1985: Second proposal, with a more elongated tokamak\n1986: Acceptance of the TCV proposal (Tokamak \u00e0 Configuration Variable)\n1992: First plasma discharge\n1997: World record of plasma elongation (see plasma shaping)\nby August 2015 it had had a 19-month shutdown/upgrade to install its first neutral beam injector.\nby 2016 it was upgraded/enhanced to run with a 'snowflake' divertor\n\n\n== References ==\n\n\n== External links ==\nTCV official site\nTCV Technical data as of Oct 2012", 
                "titleUrl": "https://en.wikipedia.org/wiki/Tokamak_\u00e0_configuration_variable", 
                "title": "Tokamak \u00e0 configuration variable"
            }
        ], 
        "phraseCharStart": "884"
    }, 
    {
        "phraseCharEnd": "1034", 
        "phraseIndex": "T20", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "nuclear fusion devices", 
        "wikiSearchResults": [
            {
                "snippet": "a magnetically confined nuclear fusion device, at the Massachusetts Institute of Technology (MIT) Plasma Science and Fusion Center (PSFC). It is the", 
                "pageCategories": "All Wikipedia articles in need of updating\nInterlanguage link template link number\nMassachusetts Institute of Technology\nOfficial website different in Wikidata and Wikipedia\nPlasma physics\nTokamaks\nVague or ambiguous time from October 2015\nWikipedia articles in need of updating from October 2015\nWikipedia articles needing clarification from October 2015", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Alcator_C-Mod", 
                "title": "Alcator C-Mod"
            }, 
            {
                "snippet": "may refer to: Reversed-Field eXperiment, a reversed field pinch nuclear fusion device RFX Interactive, a French videogame company J P Hunt Air Carriers", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/RFX", 
                "title": "RFX"
            }, 
            {
                "snippet": "Fort Wayne, Indiana. In later life, Farnsworth invented a small nuclear fusion device, the Farnsworth\u2013Hirsch fusor, or simply \"fusor\", employing inertial", 
                "pageCategories": "1906 births\n1971 deaths\nAll articles needing additional references\nAll articles with unsourced statements\nAmerican Latter Day Saints\nAmerican inventors\nArticles needing additional references from March 2013\nArticles with hCards\nArticles with unsourced statements from December 2009\nArticles with unsourced statements from May 2013", 
                "pageContent": "Philo Taylor Farnsworth (August 19, 1906 \u2013 March 11, 1971) was an American inventor and television pioneer. He made many contributions that were crucial to the early development of all-electronic television. He is perhaps best known for his 1927 invention of the first fully functional all-electronic image pickup device (video camera tube), the \"image dissector\", as well as the first fully functional and complete all-electronic television system. He was also the first person to demonstrate such a system to the public. Farnsworth developed a television system complete with receiver and camera, which he produced commercially in the form of the Farnsworth Television and Radio Corporation, from 1938 to 1951, in Fort Wayne, Indiana.\nIn later life, Farnsworth invented a small nuclear fusion device, the Farnsworth\u2013Hirsch fusor, or simply \"fusor\", employing inertial electrostatic confinement (IEC). Although not a practical device for generating nuclear energy, the fusor serves as a viable source of neutrons. The design of this device has been the acknowledged inspiration for other fusion approaches including the Polywell reactor concept in terms of a general approach to fusion design. Farnsworth held 300 patents, mostly in radio and television.\n\n\n== Early life ==\nPhilo T. Farnsworth was born August 19, 1906, the eldest of five children of Lewis Edwin Farnsworth and Serena Amanda Bastian, an LDS couple then living in a small log cabin built by Lewis's father in a place called Indian Creek near Beaver, Utah. In 1918, the family moved to a relative's 240-acre ranch near Rigby, Idaho, where Lewis supplemented his farming income by hauling freight with his horse-drawn wagon. Philo was excited to find his new home was wired for electricity, with a Delco generator providing power for lighting and farm machinery. He was a quick student in mechanical and electrical technology, repairing the troublesome generator, and upon finding a burned out electric motor among some items discarded by the previous tenants, proceeding to rewind the armature and convert his mother's hand-powered washing machine into an electric-powered one. Philo developed an early interest in electronics after his first telephone conversation with an out-of-state relative and the discovery of a large cache of technology magazines in the attic of the family\u2019s new home, and won a $25 first prize in a pulp-magazine contest for inventing a magnetized car lock.\nFarnsworth excelled in chemistry and physics at Rigby High School. He asked his high school science teacher, Justin Tolman, for advice about an electronic television system he was contemplating. He provided the teacher with sketches and diagrams covering several blackboards to show how it might be accomplished electronically. He asked his teacher if he should go ahead with his ideas, and he was encouraged to do so. One of the drawings he did on a blackboard for his chemistry teacher was recalled and reproduced for a patent interference case between Farnsworth and Radio Corporation of America (RCA). In 1923, the Farnsworths moved to Provo, Utah, and Farnsworth attended Brigham Young High School beginning that fall. His father died of pneumonia in January 1924, at age 58, and Farnsworth, as eldest son, assumed responsibility for sustaining the family while still attending high school and graduating in June 1924. He went on to attend Brigham Young University that year, and to earn Junior Radio-Trician certification from the National Radio Institute, adding a full certification in 1925. While attending college, he met Provo High School student Elma \u201cPem\u201d Gardner, (February 25, 1908 \u2013 April 27, 2006), whom he would later marry.\nLater in 1924, Farnsworth applied to the United States Naval Academy in Annapolis, Maryland, where he was recruited after he earned the nation's second highest score on academy tests. However, he was already thinking ahead to his television projects; and upon learning that the government would own his patents if he stayed in the military, he sought and received an honorable discharge within months, under a provision in which the eldest child in a fatherless family could be excused from military service in order to provide for his family. He returned to Provo and enrolled again at Brigham Young University, where he was allowed to take advanced science classes.\nPhilo worked while his sister Agnes, the elder of the two sisters, took charge of the family home and the second-floor boarding house (with the help of a cousin then living with the family). The Farnsworths later moved into half of a duplex, with family friends the Gardners moving into the other side when it became vacant. Philo developed a close friendship with Pem Gardner's brother, Cliff Gardner, who shared Farnsworth's interest in electronics. The two moved to Salt Lake City to start a radio repair business.\nThe business failed, and Gardner returned to Provo. Farnsworth remained in Salt Lake City, and through enrollment in a University of Utah job-placement service became acquainted with Leslie Gorrell and George Everson, a pair of San Francisco philanthropists who were then conducting a Salt Lake City Community Chest fundraising campaign.\nThey agreed to fund Farnsworth's early television research with an initial $6,000 in backing, and set up a laboratory in Los Angeles for Farnsworth to carry out his experiments. Before relocating to California, Farnsworth married Pem Gardner Farnsworth (February 25, 1908 \u2013 April 27, 2006), on May 27, 1926, and the two traveled to the West Coast in a Pullman coach.\n\n\n== Career ==\n\nA few months after arriving in California, Farnsworth was prepared to show his models and drawings to a patent attorney who was nationally recognized as an authority on electrophysics. Everson and Gorrell agreed that Farnsworth should apply for patents for his designs, a decision which proved crucial in later disputes with RCA. Most television systems in use at the time used image scanning devices (\"rasterizers\") employing rotating \"Nipkow disks\" comprising lenses arranged in spiral patterns such that they swept across an image in a succession of short arcs while focusing the light they captured on photosensitive elements, thus producing a varying electrical signal corresponding to the variations in light intensity. Farnsworth recognized the limitations of the mechanical systems, and that an all-electronic scanning system could produce a superior image for transmission to a receiving device.\nOn September 7, 1927, Farnsworth's image dissector camera tube transmitted its first image, a simple straight line, to a receiver in another room of his laboratory at 202 Green Street in San Francisco. Pem Farnsworth recalled in 1985 that her husband broke the stunned silence of his lab assistants by saying, \"There you are \u2014 electronic television!\" The source of the image was a glass slide, backlit by an arc lamp. An extremely bright source was required because of the low light sensitivity of the design. By 1928, Farnsworth had developed the system sufficiently to hold a demonstration for the press. His backers had demanded to know when they would see dollars from the invention; so the first image shown was, appropriately, a dollar sign. In 1929, the design was further improved by elimination of a motor-generator; so the television system now had no mechanical parts. That year Farnsworth transmitted the first live human images using his television system, including a three and a half-inch image of his wife Pem.\nMany inventors had built electromechanical television systems before Farnsworth's seminal contribution, but Farnsworth designed and built the world's first working all-electronic television system, employing electronic scanning in both the pickup and display devices. He first demonstrated his system to the press on September 3, 1928, and to the public at the Franklin Institute in Philadelphia on August 25, 1934.\nIn 1930, Vladimir Zworykin, who had been developing his own all-electronic television system at Westinghouse in Pittsburgh since 1923, but which he had never been able to make work or satisfactorily demonstrate to his superiors, was recruited by RCA to lead its television development department. Before leaving his old employer, Zworykin visited Farnsworth's laboratory and was sufficiently impressed with the performance of the Image Dissector that he reportedly had his team at Westinghouse make several copies of the device for experimentation. But Zworykin later abandoned research on the Image Dissector, which at the time required extremely bright illumination of its subjects to be effective, and turned his attention to what would become the Iconoscope. In a 1970s series of videotaped interviews, Zworykin recalled that, \"Farnsworth was closer to this thing you're using now [i.e., a video camera] than anybody, because he used the cathode-ray tube for transmission. But, Farnsworth didn't have the mosaic [of discrete light elements], he didn't have storage. Therefore, [picture] definition was very low.... But he was very proud, and he stuck to his method.\" Contrary to Zworykin's statement, Farnsworth's patent #2,087,683 for the Image Dissector (filed April 26, 1933) features a \"low velocity\" method of electron scanning, and describes \"discrete particles\" whose \"potential\" is manipulated and \"saturated\" to varying degrees depending on their velocity. Farnsworth's patent numbers 2,140,695 and 2,233,888 are for a \"charge storage dissector\" and \"charge storage amplifier,\" respectively.\nIn 1931, David Sarnoff of RCA offered to buy Farnsworth's patents for US$100,000, with the stipulation that he become an employee of RCA, but Farnsworth refused. In June of that year, Farnsworth joined the Philco company and moved to Philadelphia along with his wife and two children. RCA would later file an interference suit against Farnsworth, claiming Zworykin's 1923 patent had priority over Farnsworth's design, despite the fact it could present no evidence that Zworykin had actually produced a functioning transmitter tube before 1931. Farnsworth had lost two interference claims to Zworykin in 1928, but this time he prevailed and the U.S. Patent Office rendered a decision in 1934 awarding priority of the invention of the image dissector to Farnsworth. RCA lost a subsequent appeal, but litigation over a variety of issues continued for several years with Sarnoff finally agreeing to pay Farnsworth royalties. Zworykin received a patent in 1928 for a color transmission version of his 1923 patent application; he also divided his original application in 1931, receiving a patent in 1935, while a second one was eventually issued in 1938 by the Court of Appeals on a non-Farnsworth-related interference case, and over the objection of the Patent Office.\nIn 1932, while in England to raise money for his legal battles with RCA, Farnsworth met with John Logie Baird, a Scottish inventor who had given the world's first public demonstration of a working television system in London in 1926, using an electro-mechanical imaging system, and who was seeking to develop electronic television receivers. Baird demonstrated his mechanical system for Farnsworth. Baird's company directors pursued a merger with Farnsworth, paying $50,000 to supply electronic television equipment and provide access to Farnsworth patents. Baird and Farnsworth competed with EMI for the U.K. standard television system, but EMI merged with the Marconi Company in 1934, gaining access to the RCA Iconoscope patents. After trials of both systems, the BBC committee chose the Marconi-EMI system, which was by then virtually identical to RCA's system. The image dissector scanned well but had poor light sensitivity compared to the Marconi-EMI Iconoscopes, dubbed \"Emitrons.\"\nIn March 1932, Philco denied Farnsworth time to travel to Utah to bury his young son Kenny, placing a strain on Farnsworth's marriage, and possibly marking the beginning of his struggle with depression. In May 1933, Philco severed its relationship with Farnsworth because, said Everson, \"it [had] become apparent that Philo's aim at establishing a broad patent structure through research [was] not identical with the production program of Philco.\" In Everson's view the decision was mutual and amicable. Philco set up shop at 127 East Mermaid Lane in Philadelphia, and In 1934 held the first public exhibition of his device at the Franklin Institute in that city.\nAfter sailing to Europe in 1934, Farnsworth secured an agreement with Goerz-Bosch-Fernseh in Germany. Some image dissector cameras were used to broadcast the 1936 Olympic Games in Berlin.\nFarnsworth returned to his laboratory, and by 1936 his company was regularly transmitting entertainment programs on an experimental basis. That same year, while working with University of Pennsylvania biologists, Farnsworth developed a process to sterilize milk using radio waves. He also invented a fog-penetrating beam for ships and airplanes.\nIn 1936 he attracted the attention of Collier's Weekly, which described his work in glowing terms. \"One of those amazing facts of modern life that just don't seem possible \u2013 namely, electrically scanned television that seems destined to reach your home next year, was largely given to the world by a nineteen-year-old boy from Utah ... Today, barely thirty years old he is setting the specialized world of science on its ears.\"\nIn 1938, Farnsworth established the Farnsworth Television and Radio Corporation in Fort Wayne, Indiana, with E. A. Nicholas as president and himself as director of research. In September 1939, after a more than decade-long legal battle, RCA finally conceded to a multi-year licensing agreement concerning Farnsworth's 1927 patent for television totaling $1 million. RCA was then free, after showcasing electronic television at New York World's Fair on April 20, 1939, to sell electronic television cameras to the public.\nFarnsworth Television and Radio Corporation was purchased by International Telephone and Telegraph (ITT) in 1951. During his time at ITT, Farnsworth worked in a basement laboratory known as \"the cave\" on Pontiac Street in Fort Wayne. From there he introduced a number of breakthrough concepts, including a defense early warning signal, submarine detection devices, radar calibration equipment and an infrared telescope. \"Philo was a very deep person \u2013 tough to engage in conversation, because he was always thinking about what he could do next,\" said Art Resler, an ITT photographer who documented Farnsworth\u2019s work in pictures. One of Farnsworth's most significant contributions at ITT was the PPI Projector, an enhancement on the iconic \"circular sweep\" radar display, which allowed safe air traffic control from the ground. This system developed in the 1950s was the forerunner of today\u2019s air traffic control systems.\nIn addition to his electronics research, ITT management agreed to nominally fund Farnsworth's nuclear fusion research. He and staff members invented and refined a series of fusion reaction tubes called \"fusors.\" For scientific reasons unknown to Farnsworth and his staff, the necessary reactions lasted no longer than thirty seconds. In December 1965, ITT came under pressure from its board of directors to terminate the expensive project and sell the Farnsworth subsidiary. It was only due to the urging of president Harold Geneen that the 1966 budget was accepted, extending ITT's fusion research for an additional year. The stress associated with this managerial ultimatum, however, caused Farnsworth to suffer a relapse. A year later he was terminated and eventually allowed medical retirement.\nIn the spring of 1967, Farnsworth and his family moved back to Utah to continue his fusion research at Brigham Young University, which presented him with an honorary doctorate. The university also offered him office space and an underground concrete bunker for the project. Realizing the fusion lab was to be dismantled at ITT, Farnsworth invited staff members to accompany him to Salt Lake City, as team members in Philo T. Farnsworth Associates (PTFA). By late 1968, the associates began holding regular business meetings and PTFA was underway. Although a contract with the National Aeronautics and Space Administration (NASA) was promptly secured, and more possibilities were within reach, financing stalled for the $24,000 in monthly expenses required to cover salaries and equipment rental.\nBy Christmas 1970, PTFA had failed to secure the necessary financing, and the Farnsworths had sold all their own ITT stock and cashed in Philo's life insurance policy to maintain organizational stability. The underwriter had failed to provide the financial backing that was to have supported the organization during its critical first year. The banks called in all outstanding loans, repossession notices were placed on anything not previously sold, and the Internal Revenue Service put a lock on the laboratory door until delinquent taxes were paid. In January 1971, PTFA disbanded. Farnsworth had begun abusing alcohol in his later years, and as a consequence he became seriously ill with pneumonia, and died on March 11, 1971.\nFarnsworth's wife Elma Gardner \"Pem\" Farnsworth fought for decades after his death to assure his place in history. Farnsworth always gave her equal credit for creating television, saying, \"my wife and I started this TV.\" She died on April 27, 2006, at age 98. The inventor and wife were survived by two sons, Russell (then living in New York City), and Kent (then living in Fort Wayne, Indiana).\nIn 1999, Time magazine included Farnsworth in the \"Time 100: The Most Important People of the Century\".\n\n\n== Inventions ==\n\n\n=== Electronic television ===\nFarnsworth worked out the principle of the image dissector in the summer of 1921, not long before his fifteenth birthday, and demonstrated the first working version on September 7, 1927, having turned 21 the previous August. A farm boy, his inspiration for scanning an image as series of lines came from the back-and-forth motion used to plow a field. In the course of a patent interference suit brought by RCA in 1934 and decided in February 1935, his high school chemistry teacher, Justin Tolman, produced a sketch he had made of a blackboard drawing Farnsworth had shown him in spring 1922. Farnsworth won the suit; RCA appealed the decision in 1936 and lost. Although Farnsworth was paid royalties by RCA, he never became wealthy. The video camera tube that evolved from the combined work of Farnsworth, Zworykin and many others was used in all television cameras until the late 20th century, when alternate technologies such as charge-coupled devices started to appear.\nFarnsworth also developed the \"image oscillite\", a cathode ray tube that displayed the images captured by the image dissector.\nFarnsworth called his device an image dissector because it converted individual elements of the image into electricity one at a time. He replaced the spinning disks with caesium, an element that emits electrons when exposed to light.\n\n\n=== Fusor ===\nThe Farnsworth\u2013Hirsch fusor is an apparatus designed by Farnsworth to create nuclear fusion. Unlike most controlled fusion systems, which slowly heat a magnetically confined plasma, the fusor injects high-temperature ions directly into a reaction chamber, thereby avoiding a considerable amount of complexity.\nWhen the Farnsworth-Hirsch fusor was first introduced to the fusion research world in the late 1960s, the fusor was the first device that could clearly demonstrate it was producing fusion reactions at all. Hopes at the time were high that it could be quickly developed into a practical power source. However, as with other fusion experiments, development into a power source has proven difficult. Nevertheless, the fusor has since become a practical neutron source and is produced commercially for this role.\n\n\n=== Other inventions ===\nAt the time he died, Farnsworth held 300 U.S. and foreign patents. His inventions contributed to the development of radar, infra-red night vision devices, the electron microscope, the baby incubator, the gastroscope, and the astronomical telescope.\n\n\n== TV appearance ==\nAlthough he was the man responsible for its technology, Farnsworth appeared only once on a television program. On July 3, 1957, he was a mystery guest (\"Doctor X\") on the CBS quiz show I've Got A Secret. He fielded questions from the panel as they unsuccessfully tried to guess his secret (\"I invented electronic television.\"). For stumping the panel, he received $80 and a carton of Winston cigarettes. Host Garry Moore then spent a few minutes discussing with Farnsworth his research on such projects as high-definition television, flat-screen receivers, and fusion power. Farnsworth said, \"There had been attempts to devise a television system using mechanical disks and rotating mirrors and vibrating mirrors \u2014 all mechanical. My contribution was to take out the moving parts and make the thing entirely electronic, and that was the concept that I had when I was just a freshman in high school in the Spring of 1921 at age 14.\" When Moore asked about others' contributions, Farnsworth agreed, \"There are literally thousands of inventions important to television. I hold something in excess of 165 American patents.\" The host then asked about his current research, and the inventor replied, \"In television, we're attempting first to make better utilization of the bandwidth, because we think we can eventually get in excess of 2000 lines instead of 525 ... and do it on an even narrower channel ... which will make for a much sharper picture. We believe in the picture-frame type of a picture, where the visual display will be just a screen. And we hope for a memory, so that the picture will be just as though it's pasted on there.\"\nA letter to the editor of the Idaho Falls Post Register disputed that Farnsworth had made only one television appearance. Roy Southwick claimed \"... I interviewed Mr. [Philo] Farnsworth back in 1953 - the first day KID-TV went on the air.\" KID-TV, which later became KIDK-TV, was then located near the Rigby area where Farnsworth grew up.\n\n\n== Memorials and legacy ==\nIn a 1996 videotaped interview by the Academy of Television Arts & Sciences, Elma Farnsworth recounts Philo's change of heart about the value of television, after seeing how it showed man walking on the moon, in real time, to millions of viewers:\nInterviewer: The image dissector was used to send shots back from the moon to earth.\nElma Farnsworth: Right.\nInterviewer: What did Phil think of that?\nElma Farnsworth: We were watching it, and, when Neil Armstrong landed on the moon, Phil turned to me and said, \"Pem, this has made it all worthwhile.\" Before then, he wasn't too sure.\nIn fiction, Farnsworth appeared in the Futurama episode \"All The Presidents' Heads\" as an ancestor of Professor Farnsworth and Philip J. Fry, and was referred to as having invented the television.\nFarnsworth and the introduction of television are significant characters in Carter Beats the Devil, a novel by Glen David Gold published in 2001 by Hyperion.\nA fictionalized representation of Farnsworth appears in Canadian writer Wayne Johnston's 1994 novel, Human Amusements. The main character in the novel appears as the protagonist in a television show that features Farnsworth as the main character. In the show, an adolescent Farnsworth invents many different devices (television among them) while being challenged at every turn by a rival inventor.\n\nIn 2006, Farnsworth was posthumously presented the Eagle Scout award when it was discovered he had earned it but had never been presented with it. The award was presented to his wife, Pem, who died four months later.\nFarnsworth was posthumously inducted into the Broadcast Pioneers of Philadelphia [1] Hall of Fame in 2006.\nA bronze statue of Farnsworth represents Utah in the National Statuary Hall Collection, located in the U.S. Capitol building. Another statue sits inside the Utah State Capitol, in Salt Lake City.\nA Pennsylvania Historical and Museum Commission marker located at 1260 E. Mermaid Lane, Wyndmoor, Pennsylvania, commemorates Farnsworth's television work there in the 1930s. The Plaque reads \"Inventor of electronic television, he led some of the first experiments in live local TV broadcasting in the late 1930s from his station W3XPF located on this site. A pioneer in electronics, Farnsworth held many patents and was inducted into the Inventors Hall of Fame.\"\nOn September 15, 1981 a plaque honoring Farnsworth as The Genius of Green Street was placed on the 202 Green Street location (37.80037N, 122.40251W) of his research laboratory in San Francisco, California by the State Department of Parks and recreation.\nThe scenic \"Farnsworth Steps\" in San Francisco lead from Willard Street (just above Parnassus) up to Edgewood Avenue.\n\nIn March 2008, the Letterman Digital Arts Center in San Francisco installed a statue of Farnsworth in front of its D building.\nA plaque honoring Farnsworth is located next to his former home in a historical district on the southwest corner of East State and St. Joseph boulevards in Fort Wayne, Indiana.\nFarnsworth's television-related work, including an original TV tube he developed, are on display at the Farnsworth TV & Pioneer Museum at 118 W. 1st S. Rigby, Idaho.\nA Farnsworth image dissector is on display at Fry's Electronics in Sunnyvale, California, along with other artifacts of the history of electronics in Silicon Valley.\nFarnsworth Peak on the northern end of the Oquirrh Mountains, approximately 18 miles (29 km) south west of Salt Lake City, Utah, is named after Philo Farnsworth. It is the location of many of the area's television and FM radio transmitters.\nThe Philo Awards named after Philo Farnsworth is an annual public-access television cable TV competition where the winners receive notice for their efforts in various categories in producing Community Media.\nSeveral buildings and streets around rural Brownfield, Maine are named for Farnsworth as he lived there for some time.\nA 1983 United States postage stamp honored Farnsworth.\nFarnsworth is one of the inventors honored with a plaque in the Walt Disney World's \"Inventor's Circle\" in Future World West in Epcot.\nThe eccentric broadcast engineer in the 1989 film UHF is named Philo in tribute to Farnsworth.\nOn the Beakman's World Season 1, Episode 10, aired Nov 14, 1992 \"Levers, Beakmania, & Television\" Paul Zaloom appears as the \"guest scientist\" Philo T. Farnsworth explaining his own invention.\nSince 2003, the Academy of Television Arts & Sciences (ATAS) has awarded the Philo T. Farnsworth Corporate Achievement Award on an irregular schedule, to companies who have significantly affected the state of television and broadcast engineering over a long period of time.\nThe Farnsworth Invention, a stage play by Aaron Sorkin which debuted in 2007 after Sorkin adapted it from his unproduced screenplay, dramatized the conflict arising from Farnsworth's invention of TV and David Sarnoff of RCA's alleged stealing of the design.\nThe 2009 SyFy television series Warehouse 13 features a video communicator affectionately called \"The Farnsworth.\" In the show's universe, Philo Farnsworth built at least five of these communicators after creating television (including a very personalised one used by him) though it's possible he made more than that. He also has a \"Farnsworth aisle\" in the Warehouse which includes not just some parts and items created by him, but some of his nuclear fusion experiments that one character claims to still be \"kicking.\" Farnsworth also makes an appearance in an episode in a flashback set in 1944 during Season 2.\nOn January 10, 2011, Farnsworth was inducted by Mayor Gavin Newsom into the newly established San Francisco Hall of Fame, in the science and technology category.\nIn the video game Trenched, renamed as Iron Brigade, the main antagonist is a character named Vladamir Farnsworth, who created mechanical enemies known as \"Tubes\" which spread a deadly broadcast. This character name is alluding to Philo Farnsworth and Vladimir K. Zworykin, who invented the iconoscope.\nHe was inducted into the Television Academy Hall of Fame in 2013.\nHe is recognized in the Hall of Fame of the Indiana Broadcast Pioneers which notes that in addition to his inventive accomplishments his company owned and operated WGL radio in Fort Wayne. Indiana.\n\n\n== Fort Wayne factory razing, residence history ==\n\nIn 2010, the former Farnsworth factory in Fort Wayne, Indiana, was razed, eliminating the \"cave,\" where many of Farnsworth's inventions were first created, and where its radio and television receivers and transmitters, television tubes, and radio-phonographs were mass-produced under the Farnsworth, Capehart, and Panamuse trade names. The facility was located at 3702 E. Pontiac St.\nAlso that year, additional Farnsworth factory artifacts were added to the Fort Wayne History Center's collection, including a radio-phonograph and three table-top radios from the 1940s, as well as advertising and product materials from the 1930s to the 1950s.\nFarnsworth's Fort Wayne residence from 1948-1967, then the former Philo T. Farnsworth Television Museum, stands on the northwest corner of E. State and E. St Joseph Boulevards. The residence is recognized by an Indiana state historical marker and was listed on the National Register of Historic Places in 2013.\n\n\n== Marion, Indiana factory ==\nIn addition to Fort Wayne, Farnsworth operated a factory in Marion, Indiana, that made shortwave radios used by American combat soldiers in World War II. Acquired by RCA after the war, the facility was located at 3301 S. Adams St.\n\n\n== Patents ==\n\n\n== References ==\n\n\n== Further reading ==\n\n\n== External links ==\nOfficial Homepage: \u201cPhilo. T Farnsworth Archives\u201d (managed by Farnsworth heirs)\nNational Inventors Hall of Fame profile\nPhilo Farnsworth photo archive\nRigby, Idaho: Birthplace of Television (Jefferson County Historical Society and Museum)\nThe Boy Who Invented Television; by Paul Schatzkin\n1939 Farnsworth Article (from the Fort Wayne News-Sentinel)\nPhilo Farnsworth's Gravesite\nThe Farnsworth Invention on Broadway\nArchive of American Television oral history interview with Philo Farnsworth's widow, Elma \"Pem\" Farnsworth\\\nVideo of Farnsworth on Television's \"I've Got a Secret\" on YouTube\nBooknotes interview with Daniel Stashower on The Boy Genius and the Mogul: The Untold Story of Television, July 21, 2002.\nTranscript, Big Dreams Small Screen, American Experience (PBS) 1997\nBroadcast Pioneers of Philadelphia website\nPhilo T. Farnsworth papers and audio, Archives West, Orbis Cascade Alliance. Archived from the original on February 4, 2016.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Philo_Farnsworth", 
                "title": "Philo Farnsworth"
            }, 
            {
                "snippet": "EAST is not the \"world's first experimental nuclear fusion device\". Xinhua article Mar 24, 2006 Nuke fusion reactor completes test Mainichi Daily News", 
                "pageCategories": "All articles needing coordinates\nAll articles with dead external links\nAll articles with unsourced statements\nAnhui articles missing geocoordinate data\nArticles containing Chinese-language text\nArticles with dead external links from February 2016\nArticles with unsourced statements from February 2016\nBuildings and structures in Hefei\nInterlanguage link template link number\nNuclear energy in China", 
                "pageContent": "The Experimental Advanced Superconducting Tokamak (EAST), internal designation HT-7U) is an experimental superconducting tokamak magnetic fusion energy reactor in Hefei, China. The Hefei-based Institute of Plasma Physics is conducting the experiment for the Chinese Academy of Sciences. It has operated since 2006. It was later put under control of Hefei Institutes of Physical Science.\nIt is the first tokamak to employ superconducting toroidal and poloidal magnets. It aims for plasma pulses of up to 1000 seconds.\n\n\n== History ==\nThe project was proposed in 1996 and approved in 1998. According to a 2003 schedule, buildings and site facilities were to be constructed by 2003. Tokamak assembly was to take place from 2003 through 2005.\nConstruction was completed in March 2006 and on September 28, 2006, \"first plasma\" was achieved.\nThe reactor is an improvement over China's first superconducting tokamak device, dubbed HT-7, built by the Institute of Plasma Physics in partnership with Russia in the early 1990s.\nAccording to official reports, the project's budget is CNY \u00a5300 million (approx. USD $37 million), some 1/15 to 1/20 the cost of a comparable reactor built in other countries.\n\n\n== Operations and results ==\nOn September 28, 2006, \"first plasma\" was achieved.\nBy Jan 2007 \"the reactor created a plasma lasting nearly five seconds and generating an electrical current of 500 kilo amperes\".\nBy May 2015 it was reporting 1 MA currents, and H-mode for 6.4 seconds.\nIn February 2016, a plasma pulse was maintained for a record 102 seconds at approximately 50 million Kelvin. Plasma current of 400kA and a density of about 2.4 x 1019/m3 with slowly increasing temperature.\n\n\n== Physics objectives ==\nChina is a member of the ITER consortium, and EAST is a testbed for ITER technologies.\nEAST was designed to test:\nSuperconducting Niobium-titanium poloidal field magnets, making it the first tokamak with superconducting toroidal and poloidal magnets\nNon-inductive current drive\nPulses of up to 102 seconds with 0.5 MA plasma current\nSchemes for controlling plasma instabilities through real-time diagnostics\nMaterials for diverters and plasma facing components\nOperation with \u03b2N = 2 and confinement factor H89 > 2\n\n\n== Tokamak parameters ==\n\n\n== References ==\n\n\n== External links ==\nChinese Academy of Sciences Institute of Plasma Physics - EAST\nPeople's Daily article\nXinhua article Mar 1 2006 - Note that EAST is not the \"world's first experimental nuclear fusion device\".\nXinhua article Mar 24, 2006 Nuke fusion reactor completes test\nMainichi Daily News article Jun 2, 2006", 
                "titleUrl": "https://en.wikipedia.org/wiki/Experimental_Advanced_Superconducting_Tokamak", 
                "title": "Experimental Advanced Superconducting Tokamak"
            }, 
            {
                "snippet": "science behind nuclear fusion. For its use in producing energy, see fusion power.           In nuclear physics, nuclear fusion is a nuclear reaction in which", 
                "pageCategories": "All articles with dead external links\nAll articles with unsourced statements\nArticles with Wayback Machine links\nArticles with dead external links from April 2014\nArticles with unsourced statements from April 2010\nArticles with unsourced statements from August 2015\nArticles with unsourced statements from November 2015\nArticles with unsourced statements from October 2014\nConcepts in physics\nEnergy conversion", 
                "pageContent": "In nuclear physics, nuclear fusion is a nuclear reaction in which two or more atomic nuclei come close enough to form one or more different atomic nuclei and subatomic particles (neutrons and/or protons). The difference in mass between the products and reactants is manifested as the release of large amounts of energy. This difference in mass arises due to the difference in atomic \"binding energy\" between the atomic nuclei before and after the reaction. Fusion is the process that powers active or \"main sequence\" stars, or other high magnitude stars.\nThe fusion process that produces a nucleon with a mass less than iron-56 (which, along with nickel-62, will yield a net energy release. Iron has the largest binding energy per nucleon) generally releases energy (an exothermic process), while a fusion producing nuclei heavier than iron will result in energy retained by the resulting nucleons and the reaction is endothermic. The opposite is true for the reverse process, nuclear fission. This means that the lighter elements, such as hydrogen and helium, are in general more fusable; while the heavier elements, such as uranium and plutonium, are more fissionable. The extreme astrophysical event of a supernova can produce enough energy to fuse nuclei into elements heavier than iron.\nFollowing the discovery of quantum tunneling by physicist Friedrich Hund, in 1929 Robert Atkinson and Fritz Houtermans used the measured masses of light elements to predict that large amounts of energy could be released by fusing small nuclei. Building upon the nuclear transmutation experiments by Ernest Rutherford, carried out several years earlier, the laboratory fusion of hydrogen isotopes was first accomplished by Mark Oliphant in 1932. During the remainder of that decade the steps of the main cycle of nuclear fusion in stars were worked out by Hans Bethe. Research into fusion for military purposes began in the early 1940s as part of the Manhattan Project. Fusion was accomplished in 1951 with the Greenhouse Item nuclear test. Nuclear fusion on a large scale in an explosion was first carried out on November 1, 1952, in the Ivy Mike hydrogen bomb test.\nResearch into developing controlled thermonuclear fusion for civil purposes also began in earnest in the 1950s, and it continues to this day.\n\n\n== Process ==\n\nThe origin of the energy released in fusion of light elements is due to interplay of two opposing forces, the nuclear force which combines together protons and neutrons, and the Coulomb force, which causes protons to repel each other. The protons are positively charged and repel each other but they nonetheless stick together, demonstrating the existence of another force referred to as nuclear attraction. This force, called the strong nuclear force, overcomes electric repulsion at very close range. The effect of this force is not observed outside the nucleus, hence the force is called a short-range force. The same force also pulls the nucleons (neutrons and protons) together allowing ordinary matter to exist. Light nuclei (or nuclei smaller than iron and nickel), are sufficiently small and proton-poor allowing the nuclear force to overcome the repulsive Coulomb force. This is because the nucleus is sufficiently small that all nucleons feel the short-range attractive force at least as strongly as they feel the infinite-range Coulomb repulsion. Building up these nuclei from lighter nuclei by fusion thus releases the extra energy from the net attraction of these particles. For larger nuclei, however, no energy is released, since the nuclear force is short-range and cannot continue to act across still larger atomic nuclei. Thus, energy is no longer released when such nuclei are made by fusion; instead, energy is required as input to such processes.\nFusion reactions create the light elements that power the stars and produce virtually all elements in a process called nucleosynthesis. The fusion of lighter elements in stars releases energy and the mass that always accompanies it. For example, in the fusion of two hydrogen nuclei to form helium, 0.7% of the mass is carried away from the system in the form of kinetic energy of an alpha particle or other forms of energy, such as electromagnetic radiation.\nResearch into controlled fusion, with the aim of producing fusion power for the production of electricity, has been conducted for over 60 years. It has been accompanied by extreme scientific and technological difficulties, but has resulted in progress. At present, controlled fusion reactions have been unable to produce break-even (self-sustaining) controlled fusion. Workable designs for a reactor that theoretically will deliver ten times more fusion energy than the amount needed to heat plasma to the required temperatures are in development (see ITER). The ITER facility is expected to finish its construction phase in 2019. It will start commissioning the reactor that same year and initiate plasma experiments in 2020, but is not expected to begin full deuterium-tritium fusion until 2027.\nIt takes considerable energy to force nuclei to fuse, even those of the lightest element, hydrogen. This is because all nuclei have a positive charge due to their protons, and as like charges repel, nuclei strongly resist being pushed close together. Accelerated to high speeds, they can overcome this electrostatic repulsion and be forced close enough such that the attractive nuclear force is stronger than the repulsive force. As the strong force grows very rapidly once beyond that critical distance, the fusing nucleons \"fall\" into one another and result is fusion and net energy produced. The fusion of lighter nuclei, which creates a heavier nucleus and often a free neutron or proton, generally releases more energy than it takes to force the nuclei together; this is an exothermic process that can produce self-sustaining reactions. The US National Ignition Facility, which uses laser-driven inertial confinement fusion, was designed with a goal of break-even fusion.\nThe first large-scale laser target experiments were performed in June 2009 and ignition experiments began in early 2011.\nEnergy released in most nuclear reactions are much larger than in chemical reactions, because the binding energy that holds a nucleus together is far greater than the energy that holds electrons to a nucleus. For example, the ionization energy gained by adding an electron to a hydrogen nucleus is 6982217896002232000\u266013.6 eV\u2014less than one-millionth of the 6988281983061712000\u266017.6 MeV released in the deuterium\u2013tritium (D\u2013T) reaction shown in the diagram to the right. The complete conversion of one gram of matter would release 9\u00d71013 joules of energy. Fusion reactions have an energy density many times greater than nuclear fission; the reactions produce far greater energy per unit of mass even though individual fission reactions are generally much more energetic than individual fusion ones, which are themselves millions of times more energetic than chemical reactions. Only direct conversion of mass into energy, such as that caused by the annihilatory collision of matter and antimatter, is more energetic per unit of mass than nuclear fusion.\n\n\n== Nuclear fusion in stars ==\n\nThe most important fusion process in nature is the one that powers stars, stellar nucleosynthesis. In the 20th century, it was realized that the energy released from nuclear fusion reactions accounted for the longevity of the Sun and other stars as a source of heat and light. The fusion of nuclei in a star, starting from its initial hydrogen and helium abundance, provides that energy and synthesizes new nuclei as a byproduct of the fusion process. The prime energy producer in the Sun is the fusion of hydrogen to form helium, which occurs at a solar-core temperature of 14 million kelvin. The net result is the fusion of four protons into one alpha particle, with the release of two positrons, two neutrinos (which changes two of the protons into neutrons), and energy. Different reaction chains are involved, depending on the mass of the star. For stars the size of the sun or smaller, the proton-proton chain dominates. In heavier stars, the CNO cycle is more important.\nAs a star uses up a substantial fraction of its hydrogen, it begins to synthesize heavier elements. However the heaviest elements are synthesized by fusion that occurs as a more massive star undergoes a violent supernova at the end of its life, a process known as supernova nucleosynthesis.\n\n\n== Requirements ==\nDetails and supporting references on the material in this section can be found in textbooks on nuclear physics or nuclear fusion.\nA substantial energy barrier of electrostatic forces must be overcome before fusion can occur. At large distances, two naked nuclei repel one another because of the repulsive electrostatic force between their positively charged protons. If two nuclei can be brought close enough together, however, the electrostatic repulsion can be overcome by the quantum effect in which nuclei can tunnel through columb forces.\nWhen a nucleon such as a proton or neutron is added to a nucleus, the nuclear force attracts it to all the other nucleons of the nucleus (if the atom is small enough), but primarily to its immediate neighbours due to the short range of the force. The nucleons in the interior of a nucleus have more neighboring nucleons than those on the surface. Since smaller nuclei have a larger surface area-to-volume ratio, the binding energy per nucleon due to the nuclear force generally increases with the size of the nucleus but approaches a limiting value corresponding to that of a nucleus with a diameter of about four nucleons. It is important to keep in mind that nucleons are quantum objects. So, for example, since two neutrons in a nucleus are identical to each other, the goal of distinguishing one from the other, such as which one is in the interior and which is on the surface, is in fact meaningless, and the inclusion of quantum mechanics is therefore necessary for proper calculations.\nThe electrostatic force, on the other hand, is an inverse-square force, so a proton added to a nucleus will feel an electrostatic repulsion from all the other protons in the nucleus. The electrostatic energy per nucleon due to the electrostatic force thus increases without limit as nuclei atomic number grows.\n\nThe net result of the opposing electrostatic and strong nuclear forces is that the binding energy per nucleon generally increases with increasing size, up to the elements iron and nickel, and then decreases for heavier nuclei. Eventually, the binding energy becomes negative and very heavy nuclei (all with more than 208 nucleons, corresponding to a diameter of about 6 nucleons) are not stable. The four most tightly bound nuclei, in decreasing order of binding energy per nucleon, are 62Ni, 58Fe, 56Fe, and 60Ni. Even though the nickel isotope, 62Ni, is more stable, the iron isotope 56Fe is an order of magnitude more common. This is due to the fact that there is no easy way for stars to create 62Ni through the alpha process.\nAn exception to this general trend is the helium-4 nucleus, whose binding energy is higher than that of lithium, the next heaviest element. This is because protons and neutrons are fermions, which according to the Pauli exclusion principle cannot exist in the same nucleus in exactly the same state. Each proton or neutron energy state in a nucleus can accommodate both a spin up particle and a spin down particle. Helium-4 has an anomalously large binding energy because its nucleus consists of two protons and two neutrons, so all four of its nucleons can be in the ground state. Any additional nucleons would have to go into higher energy states. Indeed, the helium-4 nucleus is so tightly bound that it is commonly treated as a single particle in nuclear physics, namely, the alpha particle.\nThe situation is similar if two nuclei are brought together. As they approach each other, all the protons in one nucleus repel all the protons in the other. Not until the two nuclei actually come close enough for long enough can the strong nuclear force take over (by way of tunneling). Consequently, even when the final energy state is lower, there is a large energy barrier that must first be overcome. It is called the Coulomb barrier.\nThe Coulomb barrier is smallest for isotopes of hydrogen, as their nuclei contain only a single positive charge. A diproton is not stable, so neutrons must also be involved, ideally in such a way that a helium nucleus, with its extremely tight binding, is one of the products.\nUsing deuterium-tritium fuel, the resulting energy barrier is about 0.1 MeV. In comparison, the energy needed to remove an electron from hydrogen is 13.6 eV, about 7500 times less energy. The (intermediate) result of the fusion is an unstable 5He nucleus, which immediately ejects a neutron with 14.1 MeV. The recoil energy of the remaining 4He nucleus is 3.5 MeV, so the total energy liberated is 17.6 MeV. This is many times more than what was needed to overcome the energy barrier.\n\nThe reaction cross section \u03c3 is a measure of the probability of a fusion reaction as a function of the relative velocity of the two reactant nuclei. If the reactants have a distribution of velocities, e.g. a thermal distribution, then it is useful to perform an average over the distributions of the product of cross section and velocity. This average is called the 'reactivity', denoted <\u03c3v>. The reaction rate (fusions per volume per time) is <\u03c3v> times the product of the reactant number densities:\n\n  \n    \n      \n        f\n        =\n        \n          n\n          \n            1\n          \n        \n        \n          n\n          \n            2\n          \n        \n        \u27e8\n        \u03c3\n        v\n        \u27e9\n        .\n      \n    \n    {\\displaystyle f=n_{1}n_{2}\\langle \\sigma v\\rangle .}\n  \nIf a species of nuclei is reacting with itself, such as the DD reaction, then the product \n  \n    \n      \n        \n          n\n          \n            1\n          \n        \n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle n_{1}n_{2}}\n   must be replaced by \n  \n    \n      \n        (\n        1\n        \n          /\n        \n        2\n        )\n        \n          n\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle (1/2)n^{2}}\n  .\n\n  \n    \n      \n        \u27e8\n        \u03c3\n        v\n        \u27e9\n      \n    \n    {\\displaystyle \\langle \\sigma v\\rangle }\n   increases from virtually zero at room temperatures up to meaningful magnitudes at temperatures of 10\u2013100 keV. At these temperatures, well above typical ionization energies (13.6 eV in the hydrogen case), the fusion reactants exist in a plasma state.\nThe significance of \n  \n    \n      \n        \u27e8\n        \u03c3\n        v\n        \u27e9\n      \n    \n    {\\displaystyle \\langle \\sigma v\\rangle }\n   as a function of temperature in a device with a particular energy confinement time is found by considering the Lawson criterion. This is an extremely challenging barrier to overcome on Earth, which explains why fusion research has taken many years to reach the current high state of technical prowess.\n\n\n== Methods for achieving fusion ==\n\n\n=== Thermonuclear fusion ===\n\nIf the matter is sufficiently heated (hence being plasma), the fusion reaction may occur due to collisions with extreme thermal kinetic energies of the particles. In the form of thermonuclear weapons, thermonuclear fusion is the only fusion technique so far to yield undeniably large amounts of useful fusion energy. Usable amounts of thermonuclear fusion energy released in a controlled manner have yet to be achieved. In nature, this is what produces energy in stars through stellar nucleosynthesis.\n\n\n=== Inertial confinement fusion ===\n\nInertial confinement fusion (ICF) is a type of fusion energy research that attempts to initiate nuclear fusion reactions by heating and compressing a fuel target, typically in the form of a pellet that most often contains a mixture of deuterium and tritium.\n\n\n=== Inertial electrostatic confinement ===\n\nInertial electrostatic confinement is a set of devices that use an electric field to heat ions to fusion conditions. The most well known is the fusor. Starting in 1999, a number of amateurs have been able to do amateur fusion using these homemade devices. Other IEC devices include: the Polywell, MIX POPS and Marble concepts.\n\n\n=== Beam-beam or beam-target fusion ===\nIf the energy to initiate the reaction comes from accelerating one of the nuclei, the process is called beam-target fusion; if both nuclei are accelerated, it is beam-beam fusion.\nAccelerator-based light-ion fusion is a technique using particle accelerators to achieve particle kinetic energies sufficient to induce light-ion fusion reactions. Accelerating light ions is relatively easy, and can be done in an efficient manner\u2014requiring only a vacuum tube, a pair of electrodes, and a high-voltage transformer; fusion can be observed with as little as 10 kV between the electrodes. The key problem with accelerator-based fusion (and with cold targets in general) is that fusion cross sections are many orders of magnitude lower than Coulomb interaction cross sections. Therefore, the vast majority of ions expending their energy emitting bremsstrahlung radiation and the ionization of atoms of the target. Devices referred to as sealed-tube neutron generators are particularly relevant to this discussion. These small devices are miniature particle accelerators filled with deuterium and tritium gas in an arrangement that allows ions of those nuclei to be accelerated against hydride targets, also containing deuterium and tritium, where fusion takes place, releasing a flux of neutrons. Hundreds of neutron generators are produced annually for use in the petroleum industry where they are used in measurement equipment for locating and mapping oil reserves.\n\n\n=== Muon-catalyzed fusion ===\nMuon-catalyzed fusion is a well-established and reproducible fusion process that occurs at ordinary temperatures. It was studied in detail by Steven Jones in the early 1980s. Net energy production from this reaction cannot occur because of the high energy required to create muons, their short 2.2 \u00b5s half-life, and the high chance that a muon will bind to the new alpha particle and thus stop catalyzing fusion.\n\n\n=== Other principles ===\n\nSome other confinement principles have been investigated.\nAntimatter-initialized fusion uses small amounts of antimatter to trigger a tiny fusion explosion. This has been studied primarily in the context of making nuclear pulse propulsion, and pure fusion bombs feasible. This is not near becoming a practical power source, due to the cost of manufacturing antimatter alone.\nPyroelectric fusion was reported in April 2005 by a team at UCLA. The scientists used a pyroelectric crystal heated from \u221234 to 7 \u00b0C (\u221229 to 45 \u00b0F), combined with a tungsten needle to produce an electric field of about 25 gigavolts per meter to ionize and accelerate deuterium nuclei into an erbium deuteride target. At the estimated energy levels, the D-D fusion reaction may occur, producing helium-3 and a 2.45 MeV neutron. Although it makes a useful neutron generator, the apparatus is not intended for power generation since it requires far more energy than it produces.\nHybrid nuclear fusion-fission (hybrid nuclear power) is a proposed means of generating power by use of a combination of nuclear fusion and fission processes. The concept dates to the 1950s, and was briefly advocated by Hans Bethe during the 1970s, but largely remained unexplored until a revival of interest in 2009, due to the delays in the realization of pure fusion. Project PACER, carried out at Los Alamos National Laboratory (LANL) in the mid-1970s, explored the possibility of a fusion power system that would involve exploding small hydrogen bombs (fusion bombs) inside an underground cavity. As an energy source, the system is the only fusion power system that could be demonstrated to work using existing technology. However it would also require a large, continuous supply of nuclear bombs, making the economics of such a system rather questionable.\n\n\n== Important reactions ==\n\n\n=== Astrophysical reaction chains ===\nAt the temperatures and densities in stellar cores the rates of fusion reactions are notoriously slow. For example, at solar core temperature (T \u2248 15 MK) and density (160 g/cm3), the energy release rate is only 276 \u03bcW/cm3\u2014about a quarter of the volumetric rate at which a resting human body generates heat. Thus, reproduction of stellar core conditions in a lab for nuclear fusion power production is completely impractical. Because nuclear reaction rates depend on density as well as temperature and most fusion scemes operate at relatively low densities, those methods are strongly dependent on higher temperatures. The fusion rate as a function of temperature (exp(\u2212E/kT)), leads to the need to achieve temperatures in terrestrial reactors 10\u2013100 times higher temperatures and in stellar interiors: T \u2248 0.1\u20131.0\u00d7109 K.\n\n\n=== Criteria and candidates for terrestrial reactions ===\n\nIn artificial fusion, the primary fuel is not constrained to be protons and higher temperatures can be used, so reactions with larger cross-sections are chosen. Another concern is the production of neutrons, which activate the reactor structure radiologically, but also have the advantages of allowing volumetric extraction of the fusion energy and tritium breeding. Reactions that release no neutrons are referred to as aneutronic.\nTo be a useful energy source, a fusion reaction must satisfy several criteria. It must:\nBe exothermic\nThis limits the reactants to the low Z (number of protons) side of the curve of binding energy. It also makes helium 4He the most common product because of its extraordinarily tight binding, although 3He and 3H also show up.\nInvolve low atomic number (Z) nuclei\nThis is because the electrostatic repulsion must be overcome before the nuclei are close enough to fuse.\nHave two reactants\nAt anything less than stellar densities, three body collisions are too improbable. In inertial confinement, both stellar densities and temperatures are exceeded to compensate for the shortcomings of the third parameter of the Lawson criterion, ICF's very short confinement time.\nHave two or more products\nThis allows simultaneous conservation of energy and momentum without relying on the electromagnetic force.\nConserve both protons and neutrons\nThe cross sections for the weak interaction are too small.\nFew reactions meet these criteria. The following are those with the largest cross sections:\n\nFor reactions with two products, the energy is divided between them in inverse proportion to their masses, as shown. In most reactions with three products, the distribution of energy varies. For reactions that can result in more than one set of products, the branching ratios are given.\nSome reaction candidates can be eliminated at once. The D-6Li reaction has no advantage compared to p+-11\n5B because it is roughly as difficult to burn but produces substantially more neutrons through 2\n1D-2\n1D side reactions. There is also a p+-7\n3Li reaction, but the cross section is far too low, except possibly when Ti > 1 MeV, but at such high temperatures an endothermic, direct neutron-producing reaction also becomes very significant. Finally there is also a p+-9\n4Be reaction, which is not only difficult to burn, but 9\n4Be can be easily induced to split into two alpha particles and a neutron.\nIn addition to the fusion reactions, the following reactions with neutrons are important in order to \"breed\" tritium in \"dry\" fusion bombs and some proposed fusion reactors:\n\nThe latter of the two equations was unknown when the U.S. conducted the Castle Bravo fusion bomb test in 1954. Being just the second fusion bomb ever tested (and the first to use lithium), the designers of the Castle Bravo \"Shrimp\" had understood the usefulness of Lithium-6 in tritium production, but had failed to recognize that Lithium-7 fission would greatly increase the yield of the bomb. While Li-7 has a small neutron cross-section for low neutron energies, it has a higher cross section above 5 MeV. Li-7 also undergoes a chain reaction due to its release of a neutron after fissioning. The 15 Mt yield was 150% greater than the predicted 6 Mt and caused casualties from the fallout generated.\nTo evaluate the usefulness of these reactions, in addition to the reactants, the products, and the energy released, one needs to know something about the cross section. Any given fusion device has a maximum plasma pressure it can sustain, and an economical device would always operate near this maximum. Given this pressure, the largest fusion output is obtained when the temperature is chosen so that <\u03c3v>/T2 is a maximum. This is also the temperature at which the value of the triple product nT\u03c4 required for ignition is a minimum, since that required value is inversely proportional to <\u03c3v>/T2 (see Lawson criterion). (A plasma is \"ignited\" if the fusion reactions produce enough power to maintain the temperature without external heating.) This optimum temperature and the value of <\u03c3v>/T2 at that temperature is given for a few of these reactions in the following table.\nNote that many of the reactions form chains. For instance, a reactor fueled with 3\n1T and 3\n2He creates some 2\n1D, which is then possible to use in the 2\n1D-3\n2He reaction if the energies are \"right\". An elegant idea is to combine the reactions (8) and (9). The 3\n2He from reaction (8) can react with 6\n3Li in reaction (9) before completely thermalizing. This produces an energetic proton, which in turn undergoes reaction (8) before thermalizing. Detailed analysis shows that this idea would not work well, but it is a good example of a case where the usual assumption of a Maxwellian plasma is not appropriate.\n\n\n=== Neutronicity, confinement requirement, and power density ===\n\nAny of the reactions above can in principle be the basis of fusion power production. In addition to the temperature and cross section discussed above, we must consider the total energy of the fusion products Efus, the energy of the charged fusion products Ech, and the atomic number Z of the non-hydrogenic reactant.\nSpecification of the 2\n1D-2\n1D reaction entails some difficulties, though. To begin with, one must average over the two branches (2i) and (2ii). More difficult is to decide how to treat the 3\n1T and 3\n2He products. 3\n1T burns so well in a deuterium plasma that it is almost impossible to extract from the plasma. The 2\n1D-3\n2He reaction is optimized at a much higher temperature, so the burnup at the optimum 2\n1D-2\n1D temperature may be low, so it seems reasonable to assume the 3\n1T but not the 3\n2He gets burned up and adds its energy to the net reaction. Thus the total reaction would be the sum of (2i), (2ii), and (1):\n5 2\n1D \u2192 4\n2He + 2 n0 + 3\n2He + p+, Efus = 4.03+17.6+3.27 = 24.9 MeV, Ech = 4.03+3.5+0.82 = 8.35 MeV.\nWe count the 2\n1D-2\n1D fusion energy per D-D reaction (not per pair of deuterium atoms) as Efus = (4.03 MeV + 17.6 MeV)\u00d750% + (3.27 MeV)\u00d750% = 12.5 MeV and the energy in charged particles as Ech = (4.03 MeV + 3.5 MeV)\u00d750% + (0.82 MeV)\u00d750% = 4.2 MeV. (Note: if the tritium ion reacts with a deuteron while it still has a large kinetic energy, then the kinetic energy of the helium-4 produced may be quite different from 3.5 MeV, so this calculation of energy in charged particles is only approximate.)\nAnother unique aspect of the 2\n1D-2\n1D reaction is that there is only one reactant, which must be taken into account when calculating the reaction rate.\nWith this choice, we tabulate parameters for four of the most important reactions\nThe last column is the neutronicity of the reaction, the fraction of the fusion energy released as neutrons. This is an important indicator of the magnitude of the problems associated with neutrons like radiation damage, biological shielding, remote handling, and safety. For the first two reactions it is calculated as (Efus-Ech)/Efus. For the last two reactions, where this calculation would give zero, the values quoted are rough estimates based on side reactions that produce neutrons in a plasma in thermal equilibrium.\nOf course, the reactants should also be mixed in the optimal proportions. This is the case when each reactant ion plus its associated electrons accounts for half the pressure. Assuming that the total pressure is fixed, this means that density of the non-hydrogenic ion is smaller than that of the hydrogenic ion by a factor 2/(Z+1). Therefore, the rate for these reactions is reduced by the same factor, on top of any differences in the values of <\u03c3v>/T2. On the other hand, because the 2\n1D-2\n1D reaction has only one reactant, its rate is twice as high as when the fuel is divided between two different hydrogenic species, thus creating a more efficient reaction.\nThus there is a \"penalty\" of (2/(Z+1)) for non-hydrogenic fuels arising from the fact that they require more electrons, which take up pressure without participating in the fusion reaction. (It is usually a good assumption that the electron temperature will be nearly equal to the ion temperature. Some authors, however discuss the possibility that the electrons could be maintained substantially colder than the ions. In such a case, known as a \"hot ion mode\", the \"penalty\" would not apply.) There is at the same time a \"bonus\" of a factor 2 for 2\n1D-2\n1D because each ion can react with any of the other ions, not just a fraction of them.\nWe can now compare these reactions in the following table.\nThe maximum value of <\u03c3v>/T2 is taken from a previous table. The \"penalty/bonus\" factor is that related to a non-hydrogenic reactant or a single-species reaction. The values in the column \"reactivity\" are found by dividing 1.24\u00d710\u221224 by the product of the second and third columns. It indicates the factor by which the other reactions occur more slowly than the 2\n1D-3\n1T reaction under comparable conditions. The column \"Lawson criterion\" weights these results with Ech and gives an indication of how much more difficult it is to achieve ignition with these reactions, relative to the difficulty for the 2\n1D-3\n1T reaction. The last column is labeled \"power density\" and weights the practical reactivity with Efus. It indicates how much lower the fusion power density of the other reactions is compared to the 2\n1D-3\n1T reaction and can be considered a measure of the economic potential.\n\n\n=== Bremsstrahlung losses in quasineutral, isotropic plasmas ===\nThe ions undergoing fusion in many systems will essentially never occur alone but will be mixed with electrons that in aggregate neutralize the ions' bulk electrical charge and form a plasma. The electrons will generally have a temperature comparable to or greater than that of the ions, so they will collide with the ions and emit x-ray radiation of 10\u201330 keV energy, a process known as Bremsstrahlung.\nThe huge size of the Sun and stars means that the x-rays produced in this process will not escape and will deposit their energy back into the plasma. They are said to be opaque to x-rays. But any terrestrial fusion reactor will be optically thin for x-rays of this energy range. X-rays are difficult to reflect but they are effectively absorbed (and converted into heat) in less than mm thickness of stainless steel (which is part of a reactor's shield). This means the bremsstrahlung process is carrying energy out of the plasma, cooling it.\nThe ratio of fusion power produced to x-ray radiation lost to walls is an important figure of merit. This ratio is generally maximized at a much higher temperature than that which maximizes the power density (see the previous subsection). The following table shows estimates of the optimum temperature and the power ratio at that temperature for several reactions.\nThe actual ratios of fusion to Bremsstrahlung power will likely be significantly lower for several reasons. For one, the calculation assumes that the energy of the fusion products is transmitted completely to the fuel ions, which then lose energy to the electrons by collisions, which in turn lose energy by Bremsstrahlung. However, because the fusion products move much faster than the fuel ions, they will give up a significant fraction of their energy directly to the electrons. Secondly, the ions in the plasma are assumed to be purely fuel ions. In practice, there will be a significant proportion of impurity ions, which will then lower the ratio. In particular, the fusion products themselves must remain in the plasma until they have given up their energy, and will remain some time after that in any proposed confinement scheme. Finally, all channels of energy loss other than Bremsstrahlung have been neglected. The last two factors are related. On theoretical and experimental grounds, particle and energy confinement seem to be closely related. In a confinement scheme that does a good job of retaining energy, fusion products will build up. If the fusion products are efficiently ejected, then energy confinement will be poor, too.\nThe temperatures maximizing the fusion power compared to the Bremsstrahlung are in every case higher than the temperature that maximizes the power density and minimizes the required value of the fusion triple product. This will not change the optimum operating point for 2\n1D-3\n1T very much because the Bremsstrahlung fraction is low, but it will push the other fuels into regimes where the power density relative to 2\n1D-3\n1T is even lower and the required confinement even more difficult to achieve. For 2\n1D-2\n1D and 2\n1D-3\n2He, Bremsstrahlung losses will be a serious, possibly prohibitive problem. For 3\n2He-3\n2He, p+-6\n3Li and p+-11\n5B the Bremsstrahlung losses appear to make a fusion reactor using these fuels with a quasineutral, isotropic plasma impossible. Some ways out of this dilemma are considered\u2014and rejected\u2014in Fundamental limitations on plasma fusion systems not in thermodynamic equilibrium by Todd Rider. This limitation does not apply to non-neutral and anisotropic plasmas; however, these have their own challenges to contend with.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\n\"What is Nuclear Fusion?\". NuclearFiles.org. \nS. Atzeni; J. Meyer-ter-Vehn (2004). \"Nuclear fusion reactions\". The Physics of Inertial Fusion (PDF). University of Oxford Press. ISBN 978-0-19-856264-1. \nG. Brumfiel (22 May 2006). \"Chaos could keep fusion under control\". Nature. doi:10.1038/news060522-2. \nR.W. Bussard (9 November 2006). \"Should Google Go Nuclear? Clean, Cheap, Nuclear Power\". Google TechTalks. Archived from the original on 26 April 2007. \nA. Wenisch; R. Kromp; D. Reinberger (November 2007). \"Science of Fiction: Is there a Future for Nuclear?\" (PDF). Austrian Institute of Ecology. \nW.J. Nuttall (September 2008). \"Fusion as an Energy Source: Challenges and Opportunities\" (PDF). Institute of Physics Report. Institute of Physics. \nM. Kikuchi, K. Lackner & M. Q. Tran (2012). Fusion Physics. International Atomic Energy Agency. p. 22. ISBN 9789201304100. \n\n\n== External links ==\nNuclearFiles.org\u2014A repository of documents related to nuclear power.\nAnnotated bibliography for nuclear fusion from the Alsos Digital Library for Nuclear Issues\n[1]-NRL Fusion Formulary\nOrganizations\nFusion for Energy website\nITER (International Thermonuclear Experimental Reactor) website\nCCFE (Culham Centre for Fusion Energy) website\nJET (Joint European Torus) website\nNaka Fusion Institute at JAEA (Japan Atomic Energy Agency) website", 
                "titleUrl": "https://en.wikipedia.org/wiki/Nuclear_fusion", 
                "title": "Nuclear fusion"
            }, 
            {
                "snippet": "Fusion power is the generation of energy by nuclear fusion. Fusion reactions are high energy reactions in which two lighter atomic nuclei fuse to form", 
                "pageCategories": "All accuracy disputes\nAll articles containing potentially dated statements\nAll articles needing additional references\nAll articles with dead external links\nAll articles with unsourced statements\nAlternative energy\nArticles containing potentially dated statements from January 2016\nArticles needing additional references from March 2016\nArticles with Wayback Machine links\nArticles with dead external links from August 2014", 
                "pageContent": "Fusion power is the generation of energy by nuclear fusion. Fusion reactions are high energy reactions in which two lighter atomic nuclei fuse to form a heavier nucleus. This major area of plasma physics research is concerned with harnessing this reaction as a source of large scale sustainable energy. There is no question of fusion's scientific feasibility, since stellar nucleosynthesis is the process in which stars transmute matter into energy emitted as radiation.\nIn almost all large scale commercial proposals, heat from neutron scattering in a controlled fusion reaction is used to operate a steam turbine that drives electrical generators, as in existing fossil fuel and nuclear fission power stations. Many different fusion concepts have come in and out of vogue over the years. The current leading designs are the tokamak and inertial confinement fusion (laser) approaches. As of January 2016, these technologies are not yet practically viable, as they are not energetically viable\u2014i.e., it currently takes more energy to initiate and contain a fusion reaction than the reaction then produces.\nThere are also smaller-scale commercial proposals relying on other means of energy transfer, mostly forms of aneutronic fusion\u2014but these are largely considered to be more remote than the large scale neutron scattering approaches.\n\n\n== Background ==\n\n\n=== Mechanism ===\nFusion reactions occur when two (or more) atomic nuclei come close enough for the strong nuclear force pulling them together to exceed the electrostatic force pushing them apart, fusing them into heavier nuclei. For nuclei lighter than iron-56, the reaction is exothermic, releasing energy. For nuclei heavier than iron-56, it is endothermic, requiring an external source of energy. Hence, nuclei smaller than iron-56 are more likely to fuse while those heavier than iron-56 are more likely to break apart.\nTo fuse, nuclei must be brought close enough together for the strong force to act, which occurs only at very short distances. The electrostatic force keeping them apart acts over long distances, so a significant amount of kinetic energy is needed to overcome this \"Coulomb barrier\" before the reaction can take place. There are several ways of doing this, including speeding up atoms in a particle accelerator, or more commonly, heating them to very high temperatures.\nOnce an atom is heated above its ionization energy, its electrons are stripped away, leaving just the bare nucleus (the ion). The result is a hot cloud of ions and the electrons formerly attached to them. This cloud is known as a plasma. Because the charges are separated, plasmas are electrically conductive and magnetically controllable. Many fusion devices take advantage of this to control the particles as they are being heated.\n\n\n=== Cross section ===\n\nA reaction's cross section, denoted \u03c3, is the measure of how likely it is that a fusion reaction will happen. It is a probability, and it depends on the velocity of the two nuclei when they strike one another. If the atoms move faster, fusion is more likely. If the atoms hit head on, fusion is more likely. Cross sections for many different fusion reactions were measured mainly in the 1970s using particle beams. A beam of ions of material A was fired at material B at different speeds, and the amount of neutrons coming off was measured. Neutrons are a key product of most fusion reactions.\nIn most cases, the nuclei are flying around in a hot cloud, with some distribution of velocities. If the plasma is thermalized, then the distribution looks like a bell curve, or maxwellian distribution. In this case, it is useful to take the average cross section over the velocity distribution. This is entered into the volumetric fusion rate:\n\n  \n    \n      \n        \n          P\n          \n            fusion\n          \n        \n        =\n        \n          n\n          \n            A\n          \n        \n        \n          n\n          \n            B\n          \n        \n        \u27e8\n        \u03c3\n        \n          v\n          \n            A\n            ,\n            B\n          \n        \n        \u27e9\n        \n          E\n          \n            fusion\n          \n        \n      \n    \n    {\\displaystyle P_{\\text{fusion}}=n_{A}n_{B}\\langle \\sigma v_{A,B}\\rangle E_{\\text{fusion}}}\n  \nwhere:\n\n  \n    \n      \n        \n          P\n          \n            fusion\n          \n        \n      \n    \n    {\\displaystyle P_{\\text{fusion}}}\n   is the energy made by fusion, per time and volume\nn is the number density of species A or B, the particles in the volume\n\n  \n    \n      \n        \u27e8\n        \u03c3\n        \n          v\n          \n            A\n            ,\n            B\n          \n        \n        \u27e9\n      \n    \n    {\\displaystyle \\langle \\sigma v_{A,B}\\rangle }\n   is the cross section of that reaction, average over all the velocities of the two species v\n\n  \n    \n      \n        \n          E\n          \n            fusion\n          \n        \n      \n    \n    {\\displaystyle E_{\\text{fusion}}}\n   is the energy released by that fusion reaction.\n\n\n=== Lawson criterion ===\nThis equation shows that energy varies with the temperature, density, speed of collision, and fuel used. This equation was central to John Lawsons' analysis of fusion power stations working with a hot plasma. Lawson assumed an energy balance, shown below.\nNet Power = Efficiency * (Fusion - Radiation Loss - Conduction Loss)\nNet Power is the net power for any fusion power station.\nEfficiency how much energy is needed to drive the device and how well it collects power.\nFusion is rate of energy generated by the fusion reactions.\nRadiation is the energy lost as light, leaving the plasma.\nConduction is the energy lost, as momentum leaves the plasma.\nPlasma clouds lose energy through conduction and radiation. Conduction is when ions, electrons or neutrals hit a surface and transfer a portion of their kinetic energy to the atoms of the surface. Radiation is when energy leaves the cloud as light. This can be in the visible, UV, IR, or X-ray light. Radiation increases as the temperature rises. To get net power from fusion, you must overcome these losses.\n\n\n=== Triple product: density, temperature, time ===\nThe Lawson criterion argues that a machine holding a hot thermalized and quasi-neutral plasma has to meet basic criteria to overcome the radiation losses, conduction losses and a power station efficiency of 30 percent. This became known as the \"triple product\": the plasma density and temperature and how long it is held in. For many years, fusion research has focused on achieving the highest triple product possible. The tendency towards maximising the triple product has led to building larger plants in order to reduce conduction and radiation losses, increase temperature, and increase plasma retention time. Building larger plants puts the structural materials further away from the centre of the plasma, which reduces conduction losses, and reduces radiation losses since more of the radiation is internally reflected. This emphasis on \n  \n    \n      \n        (\n        n\n        T\n        \u03c4\n        )\n      \n    \n    {\\displaystyle (nT\\tau )}\n   as a metric of success has hurt other considerations such as cost, size, complexity and efficiency. This has led to larger, more complicated and more expensive machines such as ITER and NIF.\n\n\n=== Plasma behavior ===\nPlasma can be made by fully ionizing a gas. Plasma is an ionized gas which conducts electricity. In bulk, it is modeled using hydrodynamics which is a combination of the Navier-Stokes equations governing fluids and Maxwell's equations governing how magnetic and electric fields behave. Fusion exploits several plasma properties, including:\nSelf-organization plasma conducts electric and magnetic fields. This means that it can self-organize. Its motions can generate fields which can, in turn, self-contain it.\nDiamagnetic plasma can generate its own internal magnetic field. This can reject an externally applied magnetic field, making it diamagnetic.\nMagnetic mirrors Plasma can be reflected when it moves from a low to high density magnetic field.\n\n\n=== Energy capture ===\nThere are several proposals for energy capture. The simplest is using a heat cycle to heat a fluid with fusion reactions. It has been proposed to use the neutrons generated by fusion to re-generate a spent fission fuel. In addition, direct energy conversion, has been developed (at LLNL in the 1980s) as a method to maintain a voltage using the products of a fusion reaction. This has demonstrated an energy capture efficiency of 48 percent.\n\n\n== Possible approaches ==\n\n\n=== Magnetic confinement fusion ===\nThe tokamak is the most well-developed and well-funded approach to fusion energy. As of April 2012 there were an estimated 215 experimental tokamaks either planned, decommissioned or currently operating (35 tokamaks), worldwide. This method races hot plasma around in a magnetically confined ring, with an internal current. When completed, ITER will be the world's largest tokamak.\nSpherical tokamak: A variation on the tokamak with a spherical shape.\nStellarator: These are twisted rings of hot plasma. The stellarator attempts to create a natural twist plasma path, using external magnets; while Tokamaks create those magnetic fields using an internal current. Stellarators were developed by Lyman Spitzer in 1950 and have four designs: Torsatron, Heliotron, Heliac and Helias. One example is Wendelstein 7-X, a German fusion device that produced its first plasma on December 10, 2015. Wendelstein 7-X, the world's largest stellarator-type fusion device, is not intended to produce energy, but will investigate the suitability of this type of device for a power station.\nLevitated Dipole Experiment (LDX): These use a solid superconducting torus. This is magnetically levitated inside the reactor chamber. The superconductor forms an axisymmetric magnetic field that contains the plasma. The LDX was developed between MIT and Columbia University after 2000 by Jay Kesner and Michael E. Mauel.\nMagnetic mirror: Developed by Richard F. Post and teams at LLNL in the 1960s. Magnetic mirrors reflected hot plasma back and forth in a line. Variations included the magnetic bottle and the biconic cusp. A series of well-funded, large, mirror machines were built by the US government in the 1970s and 1980s. Mirror research continues today.\nField-reversed configuration: This device traps plasma in a self-organized quasi-stable structure; where the particle motion makes an internal magnetic field which then traps itself.\nReversed field pinch: Here the plasma moves inside a ring. It has an internal magnetic field. As you move out from the center of this ring, the magnetic field reverses direction.\n\n\n=== Inertial confinement fusion ===\nDirect drive: In this technique, lasers directly blast a pellet of fuel. The goal is to start ignition, a fusion chain reaction. Ignition was first suggested by John Nuckolls, in 1972. Notable direct drive experiments have been conducted at the Laboratory for Laser Energetics, Laser M\u00e9gajoule and the GEKKO XII facilities. Good implosions require fuel pellets with close to a perfect shape in order to generate a symmetrical inward shock wave and to produce the high-density plasma.\nFast ignition: This method uses two laser blasts. The first blast compresses the fusion fuel, while the second high energy pulse ignites it. Experiments have been conducted at the Laboratory for Laser Energetics using the Omega and Omega EP systems and at the GEKKO XII laser at the institute for laser engineering in Osaka Japan.\nIndirect drive: In this technique, lasers blasts a structure around the pellet of fuel. This structure is known as a Hohlraum. As it disintegrates the pellet is bathed in a more uniform x-ray light, creating better compression. The largest system using this method is the National Ignition Facility.\nMagneto-inertial fusion or Magnetized Liner Inertial Fusion: This combines a laser pulse with a magnetic pinch. The pinch community refers to it as magnetized liner Inertial fusion while the ICF community refers to it as magneto-inertial fusion.\nHeavy Ion Beams There are also proposals to do inertial confinement fusion with ion beams instead of laser beams. The main difference is the mass of the beam has momentum, whereas lasers do not.\n\n\n=== Magnetic or electric pinches ===\n\nZ-Pinch: This method sends a strong current (in the z-direction) through the plasma. The current generates a magnetic field that squeezes the plasma to fusion conditions. Pinches were the first method for man-made controlled fusion. Some examples include the Dense plasma focus and the Z machine at Sandia National Laboratories.\nTheta-Pinch: This method sends a current inside a plasma, in the theta direction.\nScrew Pinch: This method combines a theta and z-pinch for improved stabilization.\n\n\n=== Inertial electrostatic confinement ===\nFusor: This method uses an electric field to heat ions to fusion conditions. The machine typically uses two spherical cages, a cathode inside the anode, inside a vacuum. These machines are not considered a viable approach to net power because of their high conduction and radiation losses. They are simple enough to build that amateurs have fused atoms using them.\nPolywell: This designs attempts to combine magnetic confinement with electrostatic fields, to avoid the conduction losses generated by the cage.\n\n\n=== Other ===\nMagnetized target fusion: This method confines hot plasma using a magnetic field and squeezes it using inertia. Examples include LANL FRX-L machine, General Fusion and the plasma liner experiment.\nUncontrolled: Fusion has been initiated by man, using uncontrolled fission explosions to ignite the so-called Hydrogen Bomb. Early proposals for fusion power included using bombs to initiate reactions.\nBeam fusion: A beam of high energy particles can be fired at another beam or target and fusion will occur. This was used in the 1970s and 1980s to study the cross sections of high energy fusion reactions.\nBubble fusion: This was a supposed fusion reaction that was supposed to occur inside extraordinarily large collapsing gas bubbles, created during acoustic liquid cavitation. This approach was discredited.\nCold fusion: This is a hypothetical type of nuclear reaction that would occur at, or near, room temperature. Cold fusion has gained a reputation as Pathological science.\nMuon-catalyzed fusion: Muons allow atoms to get much closer and thus reduce the kinetic energy required to initiate fusion. Muons require more energy to produce than can be obtained from muon-catalysed fusion, making this approach impractical for the generation of power.\nGravitational-confinement fusion (GCF) Direct Photo-Electric Conversion: Also known as Space-Based Solar Power argues that a majority of available fusion fuels exists within the sphere of the Sun where it is gravitationally confined, and an tractable way to accomplish large-scale fusion power is to build very large space-borne platforms that capture energy via photons rather than via a carnot cycle. The theoretical limits of fusion power via this means is a type-2 civilization via a Dyson Sphere.\n\n\n== Common tools ==\n\n\n=== Heating ===\nGas must be first heated to form a plasma. This then needs to be hot enough to start fusion reactions. A number of heating schemes have been explored:\nRadiofrequency Heating A radio wave is applied to the plasma, causing it to oscillate. This is basically the same concept as a microwave oven. This is also known as electron cyclotron resonance heating or Dielectric heating.\nElectrostatic Heating An electric field can do work on charged ions or electrons, heating them.\nNeutral Beam Injection An external source of hydrogen is ionized and accelerated by an electric field to form a charged beam which is shone through a source of neutral hydrogen gas towards the plasma which itself is ionized and contained in the reactor by a magnetic field. Some of the intermediate hydrogen gas is accelerated towards the plasma by collisions with the charged beam while remaining neutral: this neutral beam is thus unaffected by the magnetic field and so shines through it into the plasma. Once inside the plasma the neutral beam transmits energy to the plasma by collisions as a result of which it becomes ionized and thus contained by the magnetic field thereby both heating and refuelling the reactor in one operation. The remainder of the charged beam is diverted by magnetic fields onto cooled beam dumps.\nMagnetic Oscillations\n\n\n=== Measurement ===\nThomson Scattering Light scatters from plasma. This light can be detected and used to reconstruct the plasmas' behavior. This technique can be used to find its density and temperature. It is common in Inertial confinement fusion, Tokamaks and fusors. In ICF systems, this can be done by firing a second beam into a gold foil adjacent to the target. This makes x-rays that scatter or traverse the plasma. In Tokamaks, this can be done using mirrors and detectors to reflect light across a plane (two dimensions) or in a line (one dimension).\nLangmuir probe This is a metal object placed in a plasma. A potential is applied to it, giving it a positive or negative voltage against the surrounding plasma. The metal collects charged particles, drawing a current. As the voltage changes, the current changes. This makes a IV Curve. The IV-curve can be used to determine the local plasma density, potential and temperature.\nGeiger counter Deuterium or tritium fusion produces neutrons. Geiger counters record the rate of neutron production, so they are an essential tool for demonstrating success.\nFlux loop A loop of wire is inserted into the magnetic field. As the field passes through the loop, a current is made. The current is measured and used to find the total magnetic flux through that loop. This has been used on the National Compact Stellarator Experiment, the polywell and the LDX machines.\nX-ray detector All plasma loses energy by emitting light. This covers the whole spectrum: visible, IR, UV, and X-rays. This occurs anytime a particle changes speed, for any reason. If the reason is deflection by a magnetic field, the radiation is Cyclotron radiation at low speeds and Synchrotron radiation at high speeds. If the reason is deflection by another particle, plasma radiates X-rays, known as Bremsstrahlung radiation. X-rays are termed in both hard and soft, based on their energy.\n\n\n=== Power production ===\nSteam turbines It has been proposed  that steam turbines be used to convert the heat from the fusion chamber into electricity. The heat is transferred into a working fluid that turns into steam, driving electric generators.\nNeutron blankets Deuterium and tritium fusion generates neutrons. This varies by technique (NIF has a record of 3E14 neutrons per second while a typical fusor produces 1E5\u20131E9 neutrons per second). It has been proposed to use these neutrons as a way to regenerate spent fission fuel  or as a way to breed tritium from a liquid lithium blanket.\nDirect conversion This is a method where the kinetic energy of a particle is converted into voltage. It was first suggested by Richard F. Post in conjunction with magnetic mirrors, in the late sixties. It has also been suggested for Field-Reversed Configurations. The process takes the plasma, expands it, and converts a large fraction of the random energy of the fusion products into directed motion. The particles are then collected on electrodes at various large electrical potentials. This method has demonstrated an experimental efficiency of 48 percent.\n\n\n== Confinement ==\n\nConfinement refers to all the conditions necessary to keep a plasma dense and hot long enough to undergo fusion. Here are some general principles.\nEquilibrium: The forces acting on the plasma must be balanced for containment. One exception is inertial confinement, where the relevant physics must occur faster than the disassembly time.\nStability: The plasma must be so constructed so that disturbances will not lead to the plasma disassembling.\nTransport or conduction: The loss of material must be sufficiently slow. The plasma carries off energy with it, so rapid loss of material will disrupt any machines power balance. Material can be lost by transport into different regions or conduction through a solid or liquid.\nTo produce self-sustaining fusion, the energy released by the reaction (or at least a fraction of it) must be used to heat new reactant nuclei and keep them hot long enough that they also undergo fusion reactions.\n\n\n=== Unconfined ===\nThe first human-made, large-scale fusion reaction was the test of the hydrogen bomb, Ivy Mike, in 1952. As part of the PACER project, it was once proposed to use hydrogen bombs as a source of power by detonating them in underground caverns and then generating electricity from the heat produced, but such a power station is unlikely ever to be constructed.\n\n\n=== Magnetic confinement ===\nAt the temperatures required for fusion, the fuel is heated to a plasma state. In this state it has a very good electrical conductivity. This opens the possibility of confining the plasma with magnetic fields. This is the case of magnetized plasma, where the magnetic fields and plasma intermix. This is generally known as magnetic confinement. The field lines put a Lorentz force on the plasma. The force works perpendicular to the magnetic fields, so one problem in magnetic confinement is preventing the plasma from leaking out the ends of the field lines. A general measure of magnetic trapping in fusion is the beta ratio:\n\n  \n    \n      \n        \u03b2\n        =\n        \n          \n            p\n            \n              p\n              \n                m\n                a\n                g\n              \n            \n          \n        \n        =\n        \n          \n            \n              n\n              \n                k\n                \n                  B\n                \n              \n              T\n            \n            \n              (\n              \n                B\n                \n                  2\n                \n              \n              \n                /\n              \n              2\n              \n                \u03bc\n                \n                  0\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\beta ={\\frac {p}{p_{mag}}}={\\frac {nk_{B}T}{(B^{2}/2\\mu _{0})}}}\n   \nThis is the ratio of the externally applied field to the internal pressure of the plasma. A value of 1 is ideal trapping. Some examples of beta vales include:\nThe START machine: 0.32\nThe Levitated dipole experiment: 0.26\nSpheromaks: \u2248 0.1, Maximum 0.2 based on Mercier limit.\nThe DIII-D machine: 0.126\nThe Gas Dynamic Trap a magnetic mirror: 0.6  for 5E-3 seconds.\nMagnetic Mirror One example of magnetic confinement is with the magnetic mirror effect. If a particle follows the field line and enters a region of higher field strength, the particles can be reflected. There are several devices that try to use this effect. The most famous was the magnetic mirror machines, which was a series of large, expensive devices built at the Lawrence Livermore National Laboratory from the 1960s to mid 1980s. Some other examples include the magnetic bottles and Biconic cusp. Because the mirror machines were straight, they had some advantages over a ring shape. First, mirrors were easier to construct and maintain and second direct conversion energy capture, was easier to implement. As the confinement achieved in experiments was poor, this approach was abandoned.\nMagnetic Loops Another example of magnetic confinement is to bend the field lines back on themselves, either in circles or more commonly in nested toroidal surfaces. The most highly developed system of this type is the tokamak, with the stellarator being next most advanced, followed by the Reversed field pinch. Compact toroids, especially the Field-Reversed Configuration and the spheromak, attempt to combine the advantages of toroidal magnetic surfaces with those of a simply connected (non-toroidal) machine, resulting in a mechanically simpler and smaller confinement area.\n\n\n=== Inertial confinement ===\nInertial confinement is the use of rapidly imploding shell to heat and confine plasma. The shell is imploded using a direct laser blast (direct drive) or a secondary x-ray blast (indirect drive) or heavy ion beams. Theoretically, fusion using lasers would be done using tiny pellets of fuel that explode several times a second. To induce the explosion, the pellet must be compressed to about 30 times solid density with energetic beams. If direct drive is used\u2014the beams are focused directly on the pellet\u2014it can in principle be very efficient, but in practice is difficult to obtain the needed uniformity. The alternative approach, indirect drive, uses beams to heat a shell, and then the shell radiates x-rays, which then implode the pellet. The beams are commonly laser beams, but heavy and light ion beams and electron beams have all been investigated.\n\n\n=== Electrostatic confinement ===\nThere are also electrostatic confinement fusion devices. These devices confine ions using electrostatic fields. The best known is the Fusor. This device has a cathode inside an anode wire cage. Positive ions fly towards the negative inner cage, and are heated by the electric field in the process. If they miss the inner cage they can collide and fuse. Ions typically hit the cathode, however, creating prohibitory high conduction losses. Also, fusion rates in fusors are very low because of competing physical effects, such as energy loss in the form of light radiation. Designs have been proposed to avoid the problems associated with the cage, by generating the field using a non-neutral cloud. These include a plasma oscillating device, a magnetically-shielded-grid a penning trap and the polywell. The technology is relatively immature, however, and many scientific and engineering questions remain.\n\n\n== History of research ==\n\n\n=== 1920s ===\nResearch into nuclear fusion started in the early part of the 20th century. In 1920 the British physicist Francis William Aston discovered that the total mass equivalent of four hydrogen atoms (two protons and two neutrons) are heavier than the total mass of one helium atom (He-4), which implied that net energy can be released by combining hydrogen atoms together to form helium, and provided the first hints of a mechanism by which stars could produce energy in the quantities being measured. Through the 1920s, Arthur Stanley Eddington became a major proponent of the proton\u2013proton chain reaction (PP reaction) as the primary system running the Sun.\n\n\n=== 1930s ===\nA theory was verified by Hans Bethe in 1939 showing that beta decay and quantum tunneling in the Sun's core might convert one of the protons into a neutron and thereby producing deuterium rather than a diproton. The deuterium would then fuse through other reactions to further increase the energy output. For this work, Bethe won the Nobel Prize in Physics.\n\n\n=== 1940s ===\nIn 1942, nuclear fusion research was subsumed into the Manhattan Project when the secrecy surrounding the field obscured by the science. The first patent related to a fusion reactor was registered in 1946 by the United Kingdom Atomic Energy Authority. The inventors were Sir George Paget Thomson and Moses Blackman. This was the first detailed examination of the Z-pinch concept.\nZ-pinch is based on the fact that plasmas are electrically conducting. Running a current through the plasma, will generate a magnetic field around the plasma. This field will, according to Lenz's law, create an inward directed force that causes the plasma to collapse inward, raising its density. Denser plasmas generate denser magnetic fields, increasing the inward force, leading to a chain reaction. If the conditions are correct, this can lead to the densities and temperatures needed for fusion. The difficulty is getting the current into the plasma, which would normally melt any sort of mechanical electrode. A solution emerges again because of the conducting nature of the plasma; by placing the plasma in the middle of an electromagnet, induction can be used to generate the current.\nStarting in 1947, two UK teams carried out small experiments and began building a series of ever-larger experiments. When the Huemul results hit the news (see below), James L. Tuck, a UK physicist working at Los Alamos, introduced the pinch concept in the US and produced a series of machines known as the Perhapsatron. The Soviet Union, unbeknownst to the West, was also building a series of similar machines. All of these devices quickly demonstrated a series of instabilities when the pinch was applied. This broke up the plasma column long before it reached the densities and temperatures required for fusion.\n\n\n=== 1950s ===\n\nThe first successful man-made fusion device was the boosted fission weapon tested in 1951 in the Greenhouse Item test. This was followed by true fusion weapons in 1952's Ivy Mike, and the first practical examples in 1954's Castle Bravo. This was uncontrolled fusion. In these devices, the energy released by the fission explosion is used to compress and heat fusion fuel, starting a fusion reaction. Fusion releases neutrons. These neutrons hit the surrounding fission fuel, causing the atoms to split apart much faster than normal fission processes\u2014almost instantly by comparison. This increases the effectiveness of bombs: normal fission weapons blow themselves apart before all their fuel is used; fusion/fission weapons do not have this practical upper limit.\nIn 1949 an expatriate German, Ronald Richter, proposed the Huemul Project in Argentina, announcing positive results in 1951. These turned out to be fake, but it prompted considerable interest in the concept as a whole. In particular, it prompted Lyman Spitzer to begin considering ways to solve some of the more obvious problems involved in confining a hot plasma, and, unaware of the z-pinch efforts, he developed a new solution to the problem known as the stellarator. Spitzer applied to the US Atomic Energy Commission for funding to build a test device. During this period, Jim Tuck who had worked with the UK teams had been introducing the z-pinch concept to his coworkers at his new job at Los Alamos National Laboratory (LANL). When he heard of Spitzer's pitch for funding, he applied to build a machine of his own, the Perhapsatron.\nSpitzer's idea won funding and he began work on the stellarator under the code name Project Matterhorn. His work led to the creation of the Princeton Plasma Physics Laboratory. Tuck returned to LANL and arranged local funding to build his machine. By this time, however, it was clear that all of the pinch machines were suffering from the same issues involving stability, and progress stalled. In 1953, Tuck and others suggested a number of solutions to the stability problems. This led to the design of a second series of pinch machines, led by the UK ZETA and Sceptre devices.\nSpitzer had planned an aggressive development project of four machines, A, B, C, and D. A and B were small research devices, C would be the prototype of a power-producing machine, and D would be the prototype of a commercial device. A worked without issue, but even by the time B was being used it was clear the stellarator was also suffering from instabilities and plasma leakage. Progress on C slowed as attempts were made to correct for these problems.\nBy the mid-1950s it was clear that the simple theoretical tools being used to calculate the performance of all fusion machines were simply not predicting their actual behavior. Machines invariably leaked their plasma from their confinement area at rates far higher than predicted. In 1954, Edward Teller held a gathering of fusion researchers at the Princeton Gun Club, near the Project Matterhorn (now known as Project Sherwood) grounds. Teller started by pointing out the problems that everyone was having, and suggested that any system where the plasma was confined within concave fields was doomed to fail. Attendees remember him saying something to the effect that the fields were like rubber bands, and they would attempt to snap back to a straight configuration whenever the power was increased, ejecting the plasma. He went on to say that it appeared the only way to confine the plasma in a stable configuration would be to use convex fields, a \"cusp\" configuration.\nWhen the meeting concluded, most of the researchers quickly turned out papers saying why Teller's concerns did not apply to their particular device. The pinch machines did not use magnetic fields in this way at all, while the mirror and stellarator seemed to have various ways out. This was soon followed by a paper by Martin David Kruskal and Martin Schwarzschild discussing pinch machines, however, which demonstrated instabilities in those devices were inherent to the design.\nThe largest \"classic\" pinch device was the ZETA, including all of these suggested upgrades, starting operations in the UK in 1957. In early 1958, John Cockcroft announced that fusion had been achieved in the ZETA, an announcement that made headlines around the world. When physicists in the US expressed concerns about the claims they were initially dismissed. US experiments soon demonstrated the same neutrons, although temperature measurements suggested these could not be from fusion reactions. The neutrons seen in the UK were later demonstrated to be from different versions of the same instability processes that plagued earlier machines. Cockcroft was forced to retract the fusion claims, and the entire field was tainted for years. ZETA ended its experiments in 1968.\nThe first controlled fusion experiment was accomplished using Scylla I at the Los Alamos National Laboratory in 1958. This was a pinch machine, with a cylinder full of deuterium. Electric current shot down the sides of the cylinder. The current made magnetic fields that compressed the plasma to 15 million degrees Celsius, squeezed the gas, fused it and produced neutrons.\nIn 1950\u20131951 I.E. Tamm and A.D. Sakharov in the Soviet Union, first discussed a tokamak-like approach. Experimental research on those designs began in 1956 at the Kurchatov Institute in Moscow by a group of Soviet scientists led by Lev Artsimovich. The tokamak essentially combined a low-power pinch device with a low-power simple stellarator. The key was to combine the fields in such a way that the particles orbited within the reactor a particular number of times, today known as the \"safety factor\". The combination of these fields dramatically improved confinement times and densities, resulting in huge improvements over existing devices.\n\n\n=== 1960s ===\nA key plasma physics text was published by Lyman Spitzer at Princeton in 1963. Spitzer took the ideal gas laws and adopted them to an ionized plasma, developing many of the fundamental equations used to model a plasma.\nLaser fusion was suggested in 1962 by scientists at Lawrence Livermore National Laboratory, shortly after the invention of the laser itself in 1960. At the time, Lasers were low power machines, but low-level research began as early as 1965. Laser fusion, formally known as inertial confinement fusion, involves imploding a target by using laser beams. There are two ways to do this: indirect drive and direct drive. In direct drive, the laser blasts a pellet of fuel. In indirect drive, the lasers blast a structure around the fuel. This makes x-rays that squeeze the fuel. Both methods compress the fuel so that fusion can take place.\nAt the 1964 World's Fair, the public was given its first demonstration of nuclear fusion. The device was a \u03b8-pinch from General Electric. This was similar to the Scylla machine developed earlier at Los Alamos.\nThe magnetic mirror was first published in 1967 by Richard F. Post and many others at the Lawrence Livermore National Laboratory. The mirror consisted of two large magnets arranged so they had strong fields within them, and a weaker, but connected, field between them. Plasma introduced in the area between the two magnets would \"bounce back\" from the stronger fields in the middle.\nThe A.D. Sakharov group constructed the first tokamaks, the most successful being the T-3 and its larger version T-4. T-4 was tested in 1968 in Novosibirsk, producing the world's first quasistationary fusion reaction. When this were first announced, the international community was highly skeptical. A British team was invited to see T-3, however, and after measuring it in depth they released their results that confirmed the Soviet claims. A burst of activity followed as many planned devices were abandoned and new tokamaks were introduced in their place \u2014 the C model stellarator, then under construction after many redesigns, was quickly converted to the Symmetrical Tokamak.\nIn his work with vacuum tubes, Philo Farnsworth observed that electric charge would accumulate in regions of the tube. Today, this effect is known as the Multipactor effect. Farnsworth reasoned that if ions were concentrated high enough they could collide and fuse. In 1962, he filed a patent on a design using a positive inner cage to concentrate plasma, in order to achieve nuclear fusion. During this time, Robert L. Hirsch joined the Farnsworth Television labs and began work on what became the fusor. Hirsch patented the design in 1966 and published the design in 1967.\n\n\n=== 1970s ===\n\nIn 1972, John Nuckolls outlined the idea of ignition. This is a fusion chain reaction. Hot helium made during fusion reheats the fuel and starts more reactions. John argued that ignition would require lasers of about 1 kJ. This turned out to be wrong. Nuckolls's paper started a major development effort. Several laser systems were built at LLNL. These included the argus, the Cyclops, the Janus, the long path, the Shiva laser and the Nova in 1984. This prompted the UK to build the Central Laser Facility in 1976.\nDuring this time, great strides in understanding the tokamak system were made. A number of improvements to the design are now part of the \"advanced tokamak\" concept, which includes non-circular plasma, internal diverters and limiters, often superconducting magnets, and operate in the so-called \"H-mode\" island of increased stability. Two other designs have also become fairly well studied; the compact tokamak is wired with the magnets on the inside of the vacuum chamber, while the spherical tokamak reduces its cross section as much as possible.\nIn 1974 a study of the ZETA results demonstrated an interesting side-effect; after an experimental run ended, the plasma would enter a short period of stability. This led to the reversed field pinch concept, which has seen some level of development since. On May 1, 1974, the KMS fusion company (founded by Kip Siegel) achieves the world's first laser induced fusion in a deuterium-tritium pellet.\nIn the mid-1970s, Project PACER, carried out at Los Alamos National Laboratory (LANL) explored the possibility of a fusion power system that would involve exploding small hydrogen bombs (fusion bombs) inside an underground cavity. As an energy source, the system is the only fusion power system that could be demonstrated to work using existing technology. It would also require a large, continuous supply of nuclear bombs, however, making the economics of such a system rather questionable.\nIn 1976, the two beam Argus laser becomes operational at livermore. In 1977, The 20 beam Shiva laser at Livermore is completed, capable of delivering 10.2 kilojoules of infrared energy on target. At a price of $25 million and a size approaching that of a football field, Shiva is the first of the megalasers. That same year, the JET project is approved by the European Commission and a site is selected.\n\n\n=== 1980s ===\n\nAs a result of advocacy, the cold war, and the 1970s energy crisis a massive magnetic mirror program was funded by the US federal government in the late 1970s and early 1980s. This program resulted in a series of large magnetic mirror devices including: 2X, Baseball I, Baseball II, the Tandem Mirror Experiment, the Tandem mirror experiment upgrade, the Mirror Fusion Test Facility and the MFTF-B. These machines were built and tested at Livermore from the late 1960s to the mid 1980s. A number of institutions collaborated on these machines, conducting experiments. These included the Institute for Advanced Study and the University of Wisconsin\u2013Madison. The last machine, the Mirror Fusion Test Facility cost 372 million dollars and was, at that time, the most expensive project in Livermore history. It opened on February 21, 1986 and was promptly shut down. The reason given was to balance the United States federal budget. This program was supported from within the Carter and early Reagan administrations by Edwin E. Kintner, a US Navy captain, under Alvin Trivelpiece.\nIn Laser fusion progressed: in 1983, the NOVETTE laser was completed. The following December 1984, the ten beam NOVA laser was finished. Five years later, NOVA would produce a maximum of 120 kilojoules of infrared light, during a nanosecond pulse. Meanwhile, efforts focused on either fast delivery or beam smoothness. Both tried to deliver the energy uniformly to implode the target. One early problem was that the light in the infrared wavelength, lost lots of energy before hitting the fuel. Breakthroughs were made at the Laboratory for Laser Energetics at the University of Rochester. Rochester scientists used frequency-tripling crystals to transform the infrared laser beams into ultraviolet beams. In 1985, Donna Strickland and G\u00e9rard Mourou invented a method to amplify lasers pulses by \"chirping\". This method changes a single wavelength into a full spectrum. The system then amplifies the laser at each wavelength and then reconstitutes the beam into one color. Chirp pulsed amplification became instrumental in building the National Ignition Facility and the Omega EP system. Most research into ICF was towards weapons research, because the implosion is relevant to nuclear weapons.\nDuring this time Los Alamos National Laboratory constructed a series of laser facilities. This included Gemini (a two beam system), Helios (eight beams), Antares (24 beams) and Aurora (96 beams). The program ended in the early nineties with a cost on the order of one billion dollars.\nIn 1987, Akira Hasegawa  noticed that in a dipolar magnetic field, fluctuations tended compress the plasma without energy loss. This effect was noticed in data taken by Voyager 2, when it encountered Uranus. This observation would become the basis for a fusion approach known as the Levitated dipole.\nIn Tokamaks, the Tore Supra was under construction over the middle of the eighties (1983 to 1988). This was a Tokamak built in Cadarache, France. In 1983, the JET was completed and first plasmas achieved. In 1985, the Japanese tokamak, JT-60 was completed. In 1988, the T-15 a Soviet tokamak was completed. It was the first industrial fusion reactor to use superconducting magnets to control the plasma. These were Helium cooled.\nIn 1989, Pons and Fleischmann submitted papers to the Journal of Electroanalytical Chemistry claiming that they had observed fusion in a room temperature device and disclosing their work in a press release. Some scientists reported excess heat, neutrons, tritium, helium and other nuclear effects in so-called cold fusion systems, which for a time gained interest as showing promise. Hopes fell when replication failures were weighed in view of several reasons cold fusion is not likely to occur, the discovery of possible sources of experimental error, and finally the discovery that Fleischmann and Pons had not actually detected nuclear reaction byproducts. By late 1989, most scientists considered cold fusion claims dead, and cold fusion subsequently gained a reputation as pathological science. However, a small community of researchers continues to investigate cold fusion claiming to replicate Fleishmann and Pons' results including nuclear reaction byproducts. Claims related to cold fusion are largely disbelieved in the mainstream scientific community. In 1989, the majority of a review panel organized by the US Department of Energy (DOE) found that the evidence for the discovery of a new nuclear process was not persuasive. A second DOE review, convened in 2004 to look at new research, reached conclusions similar to the first.\nIn 1984, Martin Peng of ORNL proposed an alternate arrangement of the magnet coils that would greatly reduce the aspect ratio while avoiding the erosion issues of the compact tokamak: a Spherical tokamak. Instead of wiring each magnet coil separately, he proposed using a single large conductor in the center, and wiring the magnets as half-rings off of this conductor. What was once a series of individual rings passing through the hole in the center of the reactor was reduced to a single post, allowing for aspect ratios as low as 1.2. The ST concept appeared to represent an enormous advance in tokamak design. However, it was being proposed during a period when US fusion research budgets were being dramatically scaled back. ORNL was provided with funds to develop a suitable central column built out of a high-strength copper alloy called \"Glidcop\". However, they were unable to secure funding to build a demonstration machine, \"STX\". Failing to build an ST at ORNL, Peng began a worldwide effort to interest other teams in the ST concept and get a test machine built. One way to do this quickly would be to convert a spheromak machine to the Spherical tokamak layout. Peng's advocacy also caught the interest of Derek Robinson, of the United Kingdom Atomic Energy Authority fusion center at Culham. Robinson was able to gather together a team and secure funding on the order of 100,000 pounds to build an experimental machine, the Small Tight Aspect Ratio Tokamak, or START. Several parts of the machine were recycled from earlier projects, while others were loaned from other labs, including a 40 keV neutral beam injector from ORNL. Construction of START began in 1990, it was assembled rapidly and started operation in January 1991.\n\n\n=== 1990s ===\n\nIn 1991 the Preliminary Tritium Experiment at the Joint European Torus in England achieved the world\u2019s first controlled release of fusion power.\nIn 1992, a major article was published in Physics Today by Robert McCory at the Laboratory for laser energetics outlying the current state of ICF and advocating for a national ignition facility. This was followed up by a major review article, from John Lindl in 1995, advocating for NIF. During this time a number of ICF subsystems were developing, including target manufacturing, cryogenic handling systems, new laser designs (notably the NIKE laser at NRL) and improved diagnostics like time of flight analyzers and Thomson scattering. This work was done at the NOVA laser system, General Atomics, Laser M\u00e9gajoule and the GEKKO XII system in Japan. Through this work and lobbying by groups like the fusion power associates and John Sethian at NRL, a vote was made in congress, authorizing funding for the NIF project in the late nineties.\nIn the early nineties, theory and experimental work regarding fusors and polywells was published. In response, Todd Rider at MIT developed general models of these devices. Rider argued that all plasma systems at thermodynamic equilibrium were fundamentally limited. In 1995, William Nevins published a criticism  arguing that the particles inside fusors and polywells would build up angular momentum, causing the dense core to degrade.\nIn 1995, the University of Wisconsin\u2013Madison built a large fusor, known as HOMER, which is still in operation. Meanwhile, Dr George H. Miley at Illinois, built a small fusor that has produced neutrons using deuterium gas  and discovered the \"star mode\" of fusor operation. The following year, the first \"US-Japan Workshop on IEC Fusion\", was conducted. At this time in Europe, an IEC device was developed as a commercial neutron source by Daimler-Chrysler and NSD Fusion.\nIn 1996, the Z-machine was upgraded and opened to the public by the US Army in August 1998 in Scientific American. The key attributes of Sandia\u2019s Z machine are its 18 million amperes and a discharge time of less than 100 nanoseconds. This generates a magnetic pulse, inside a large oil tank, this strikes an array of tungsten wires called a liner. Firing the Z-machine has become a way to test very high energy, high temperature (2 billion degrees) conditions. In 1996, the Tore Supra creates a plasma for two minutes with a current of almost 1 million amperes driven non-inductively by 2.3 MW of lower hybrid frequency waves. This is 280 MJ of injected and extracted energy. This result was possible because of the actively cooled plasma-facing components\nIn 1997, JET produced a peak of 16.1MW of fusion power (65% of heat to plasma), with fusion power of over 10MW sustained for over 0.5 sec. Its successor, the International Thermonuclear Experimental Reactor (ITER), was officially announced as part of a seven-party consortium (six countries and the EU). ITER is designed to produce ten times more fusion power than the power put into the plasma. ITER is currently under construction in Cadarache, France.\nIn the late nineties, a team at Columbia University and MIT developed the Levitated dipole a fusion device which consisted of a superconducting electromagnet, floating in a saucer shaped vacuum chamber. Plasma swirled around this donut and fused along the center axis.\n\n\n=== 2000s ===\n\nIn the March 8, 2002 issue of the peer-reviewed journal Science, Rusi P. Taleyarkhan and colleagues at the Oak Ridge National Laboratory (ORNL) reported that acoustic cavitation experiments conducted with deuterated acetone (C3D6O) showed measurements of tritium and neutron output consistent with the occurrence of fusion. Taleyarkhan was later found guilty of misconduct, the Office of Naval Research debarred him for 28 months from receiving Federal Funding, and his name was listed in the 'Excluded Parties List'.\n\"Fast ignition\" was developed in the late nineties, and was part of a push by the Laboratory for Laser Energetics for building the Omega EP system. This system was finished in 2008. Fast ignition showed such dramatic power savings that ICF appears to be a useful technique for energy production. There are even proposals to build an experimental facility dedicated to the fast ignition approach, known as HiPER.\nIn April 2005, a team from UCLA announced it had devised a way of producing fusion using a machine that \"fits on a lab bench\", using lithium tantalate to generate enough voltage to smash deuterium atoms together. The process, however, does not generate net power (see Pyroelectric fusion). Such a device would be useful in the same sort of roles as the fusor. In 2006, China's EAST test reactor is completed. This was the first tokamak to use superconducting magnets to generate both the toroidal and poloidal fields.\nIn the early 2000s, Researchers at LANL reasoned that a plasma oscillating could be at local thermodynamic equilibrium. This prompted the POPS and Penning trap designs. At this time, researchers at MIT became interested in fusors for space propulsion and powering space vehicles. Specifically, researchers developed fusors with multiple inner cages. Greg Piefer graduated from Madison and founded Phoenix Nuclear Labs, a company that developed the fusor into a neutron source for the mass production of medical isotopes. Robert Bussard began speaking openly about the Polywell in 2006. He attempted to generate interest in the research, before his death. In 2008, Taylor Wilson achieved notoriety for achieving nuclear fusion at 14, with a homemade fusor.\nIn 2009, a high-energy laser system, the National Ignition Facility (NIF), was finished in the US, which can heat hydrogen atoms to temperatures only existing in nature in the cores of stars. The new laser is expected to have the ability to produce, for the first time, more energy from controlled, inertially confined nuclear fusion than was required to initiate the reaction.\n\n\n=== 2010s ===\n\nIn 2010, NIF researchers were conducting a series of \"tuning\" shots to determine the optimal target design and laser parameters for high-energy ignition experiments with fusion fuel in the following months. Two firing tests were performed on October 31, 2010 and November 2, 2010. In early 2012, NIF director Mike Dunne expected the laser system to generate fusion with net energy gain by the end of 2012. However, it was delayed and not achieved by that date.\nInertial (laser) confinement is being developed at the United States National Ignition Facility (NIF) based at Lawrence Livermore National Laboratory in California, the French Laser M\u00e9gajoule, and the planned European Union High Power laser Energy Research (HiPER) facility. NIF reached initial operational status in 2010 and has been in the process of increasing the power and energy of its \"shots\", with fusion ignition tests to follow. A three-year goal announced in 2009 to produce net energy from fusion by 2012 was missed; in September 2013, however, the facility announced a significant milestone from an August 2013 test that produced more energy from the fusion reaction than had been provided to the fuel pellet. This was reported as the first time this had been accomplished in fusion power research. The facility reported that their next step involved improving the system to prevent the hohlraum from either breaking up asymmetrically or too soon.\nA 2012 paper demonstrated that a dense plasma focus had achieved temperatures of 1.8 billion degrees Celsius, sufficient for boron fusion, and that fusion reactions were occurring primarily within the contained plasmoid, a necessary condition for net power. The focus consists of two coaxial cylindrical electrodes made from copper or beryllium and housed in a vacuum chamber containing a low-pressure fusible gas. An electrical pulse is applied across the electrodes, heating the gas into a plasma. The current forms into a minuscule vortex along the axis of the machine, which then kinks into a cage of current with an associated magnetic field. The cage of current and magnetic-field-entrapped plasma is called a plasmoid. The acceleration of the electrons about the magnetic field lines heats the nuclei within the plasmoid to fusion temperatures.\nIn April 2014, Lawrence Livermore National Laboratory ended the Laser Inertial Fusion Energy (LIFE) program and redirected their efforts towards NIF. In August 2014, Phoenix Nuclear Labs announced the sale of a high-yield neutron generator that could sustain 5\u00d71011 deuterium fusion reactions per second over a 24-hour period. In October 2014, Lockheed Martin's Skunk Works announced the development of a high-beta fusion reactor that they hope to yield a functioning 100-megawatt prototype by 2017 and to be ready for regular operation by 2022.\nDeep-space exploration, as well as higher-velocity lower-cost space transport services in general would be enabled by this compact fusion reactor technology.\nIn January 2015, the polywell was presented at Microsoft Research.\nIn August, 2015, MIT announced a tokamak it named ARC fusion reactor design using rare-earth barium-copper oxide (REBCO) superconducting tapes to produce high-magnetic field coils that it claimed produce comparable magnetic field strength in a smaller configuration than other designs.\nIn October 2015, researchers at the Max Planck Institute of Plasma Physics completed building the largest stellarator to date, named Wendelstein 7-X. On December 10, they successfully produced the first helium plasma, and on February 3, 2016 produced the device's first Hydrogen plasma. With plasma discharges lasting up to 30 minutes, Wendelstein 7-X will try to demonstrate the essential stellarator attribute: continuous operation of a high-temperature hydrogen plasma.\n\n\n== Fuels ==\nBy firing particle beams at targets, many fusion reactions have been tested, while the fuels considered for power have all been light elements like the isotopes of hydrogen\u2014deuterium and tritium. Other reactions like the deuterium and Helium3 reaction or the Helium3 and Helium3 reactions, would require a supply of Helium3. This can either come from other nuclear reactions or from extraterrestrial sources. Finally, researchers hope to do the p-11B reaction, because it does not directly produce neutrons, though side reactions can.\n\n\n=== Deuterium, tritium ===\n\nThe easiest nuclear reaction, at the lowest energy, is:\n2\n1D + 3\n1T \u2192 4\n2He + 1\n0n\nThis reaction is common in research, industrial and military applications, usually as a convenient source of neutrons. Deuterium is a naturally occurring isotope of hydrogen and is commonly available. The large mass ratio of the hydrogen isotopes makes their separation easy compared to the difficult uranium enrichment process. Tritium is a natural isotope of hydrogen, but because it has a short half-life of 12.32 years, it is hard to find, store, produce, and is expensive. Consequently, the deuterium-tritium fuel cycle requires the breeding of tritium from lithium using one of the following reactions:\n1\n0n + 6\n3Li \u2192 3\n1T + 4\n2He\n1\n0n + 7\n3Li \u2192 3\n1T + 4\n2He + 1\n0n\nThe reactant neutron is supplied by the D-T fusion reaction shown above, and the one that has the greatest yield of energy. The reaction with 6Li is exothermic, providing a small energy gain for the reactor. The reaction with 7Li is endothermic but does not consume the neutron. At least some 7Li reactions are required to replace the neutrons lost to absorption by other elements. Most reactor designs use the naturally occurring mix of lithium isotopes.\nSeveral drawbacks are commonly attributed to D-T fusion power:\nIt produces substantial amounts of neutrons that result in the neutron activation of the reactor materials.\nOnly about 20% of the fusion energy yield appears in the form of charged particles with the remainder carried off by neutrons, which limits the extent to which direct energy conversion techniques might be applied.\nIt requires the handling of the radioisotope tritium. Similar to hydrogen, tritium is difficult to contain and may leak from reactors in some quantity. Some estimates suggest that this would represent a fairly large environmental release of radioactivity.\nThe neutron flux expected in a commercial D-T fusion reactor is about 100 times that of current fission power reactors, posing problems for material design. After a series of D-T tests at JET, the vacuum vessel was sufficiently radioactive that remote handling was required for the year following the tests.\nIn a production setting, the neutrons would be used to react with lithium in order to create more tritium. This also deposits the energy of the neutrons in the lithium, which would then be transferred to drive electrical production. The lithium neutron absorption reaction protects the outer portions of the reactor from the neutron flux. Newer designs, the advanced tokamak in particular, also use lithium inside the reactor core as a key element of the design. The plasma interacts directly with the lithium, preventing a problem known as \"recycling\". The advantage of this design was demonstrated in the Lithium Tokamak Experiment.\n\n\n=== Deuterium ===\n\nThis is the second easiest fusion reaction, fusing deuterium with itself. The reaction has two branches that occur with nearly equal probability:\n\nThis reaction is also common in research. The optimum energy to initiate this reaction is 15 keV, only slightly higher than the optimum for the D-T reaction. The first branch does not produce neutrons, but it does produce tritium, so that a D-D reactor will not be completely tritium-free, even though it does not require an input of tritium or lithium. Unless the tritons can be quickly removed, most of the tritium produced would be burned before leaving the reactor, which would reduce the handling of tritium, but would produce more neutrons, some of which are very energetic. The neutron from the second branch has an energy of only 2.45 MeV (0.393 pJ), whereas the neutron from the D-T reaction has an energy of 14.1 MeV (2.26 pJ), resulting in a wider range of isotope production and material damage. When the tritons are removed quickly while allowing the 3He to react, the fuel cycle is called \"tritium suppressed fusion\" The removed tritium decays to 3He with a 12.5 year half life. By recycling the 3He produced from the decay of tritium back into the fusion reactor, the fusion reactor does not require materials resistant to fast 14.1 MeV (2.26 pJ) neutrons.\nAssuming complete tritium burn-up, the reduction in the fraction of fusion energy carried by neutrons would be only about 18%, so that the primary advantage of the D-D fuel cycle is that tritium breeding would not be required. Other advantages are independence from scarce lithium resources and a somewhat softer neutron spectrum. The disadvantage of D-D compared to D-T is that the energy confinement time (at a given pressure) must be 30 times longer and the power produced (at a given pressure and volume) would be 68 times less .\nAssuming complete removal of tritium and recycling of 3He, only 6% of the fusion energy is carried by neutrons. The tritium-suppressed D-D fusion requires an energy confinement that is 10 times longer compared to D-T and a plasma temperature that is twice as high.\n\n\n=== Deuterium, helium 3 ===\nA second-generation approach to controlled fusion power involves combining helium-3 (3He) and deuterium (2H):\n\nThis reaction produces a helium-4 nucleus (4He) and a high-energy proton. As with the p-11B aneutronic fusion fuel cycle, most of the reaction energy is released as charged particles, reducing activation of the reactor housing and potentially allowing more efficient energy harvesting (via any of several speculative technologies). In practice, D-D side reactions produce a significant number of neutrons, resulting in p-11B being the preferred cycle for aneutronic fusion.\n\n\n=== Proton, boron 11 ===\nIf aneutronic fusion is the goal, then the most promising candidate may be the Hydrogen-1 (proton)/boron reaction, which releases alpha (helium) particles, but does not rely on neutron scattering for energy transfer.\n1H + 11B \u2192 3 4He\nUnder reasonable assumptions, side reactions will result in about 0.1% of the fusion power being carried by neutrons. At 123 keV, the optimum temperature for this reaction is nearly ten times higher than that for the pure hydrogen reactions, the energy confinement must be 500 times better than that required for the D-T reaction, and the power density will be 2500 times lower than for D-T.\nBecause the confinement properties of conventional approaches to fusion such as the tokamak and laser pellet fusion are marginal, most proposals for aneutronic fusion are based on radically different confinement concepts, such as the Polywell and the Dense Plasma Focus. Results have been extremely promising:\n\"In the October 2013 edition of Nature Communications, a research team led by Christine Labaune at \u00c9cole Polytechnique in Palaiseau, France, reported a new record fusion rate: an estimated 80 million fusion reactions during the 1.5 nanoseconds that the laser fired, which is at least 100 times more than any previous proton-boron experiment. \" \n\n\n== Material selection ==\n\n\n=== Considerations ===\nAny power station using hot plasma, is going to have plasma facing walls. In even the simplest plasma approaches, the material will get blasted with matter and energy. This leads to a minimum list of considerations, including dealing with:\nA heating and cooling cycle, up to a 10 MW/m\u00b2 thermal load.\nNeutron radiation, which over time leads to neutron activation and embrittlement.\nHigh energy ions leaving at tens to hundreds of electronvolts.\nAlpha particles leaving at millions of electronvolts.\nElectrons leaving at high energy.\nLight radiation (IR, visible, UV, X-ray).\nDepending on the approach, these effects may be higher or lower than typical fission reactors like the pressurized water reactor (PWR). One estimate put the radiation at 100 times the (PWR). Materials need to be selected or developed that can withstand these basic conditions. Depending on the approach, however, there may be other considerations such as electrical conductivity, magnetic permeability and mechanical strength. There is also a need for materials whose primary components and impurities do not result in long-lived radioactive wastes.\n\n\n=== Durability ===\nFor long term use, each atom in the wall is expected to be hit by a neutron and displaced about a hundred times before the material is replaced. High-energy neutrons will produce hydrogen and helium by way of various nuclear reactions that tends to form bubbles at grain boundaries and result in swelling, blistering or embrittlement.\n\n\n=== Selection ===\nOne can choose either a low-Z material, such as graphite or beryllium, or a high-Z material, usually tungsten with molybdenum as a second choice. Use of liquid metals (lithium, gallium, tin) has also been proposed, e.g., by injection of 1\u20135 mm thick streams flowing at 10 m/s on solid substrates.\nIf graphite is used, the gross erosion rates due to physical and chemical sputtering would be many meters per year, so one must rely on redeposition of the sputtered material. The location of the redeposition will not exactly coincide with the location of the sputtering, so one is still left with erosion rates that may be prohibitive. An even larger problem is the tritium co-deposited with the redeposited graphite. The tritium inventory in graphite layers and dust in a reactor could quickly build up to many kilograms, representing a waste of resources and a serious radiological hazard in case of an accident. The consensus of the fusion community seems to be that graphite, although a very attractive material for fusion experiments, cannot be the primary PFC material in a commercial reactor.\nThe sputtering rate of tungsten by the plasma fuel ions is orders of magnitude smaller than that of carbon, and tritium is much less incorporated into redeposited tungsten, making this a more attractive choice. On the other hand, tungsten impurities in a plasma are much more damaging than carbon impurities, and self-sputtering of tungsten can be high, so it will be necessary to ensure that the plasma in contact with the tungsten is not too hot (a few tens of eV rather than hundreds of eV). Tungsten also has disadvantages in terms of eddy currents and melting in off-normal events, as well as some radiological issues.\n\n\n== Safety and the environment ==\n\n\n=== Accident potential ===\nNuclear fusion is unlike nuclear fission: fusion requires extremely precise and controlled temperature, pressure and magnetic field parameters for any net energy to be produced. If a reactor suffers damage or loses even a small degree of required control, fusion reactions and heat generation would rapidly cease. Additionally, fusion reactors contain relatively small amounts of fuel, enough to \"burn\" for minutes, or in some cases, microseconds. Unless they are actively refueled, the reactions will quickly end. Therefore, fusion reactors are considered extremely safe.\nRunaway reactions cannot occur in a fusion reactor. The plasma is burnt at optimal conditions, and any significant change will quench the reactions. The reaction process is so delicate that this level of safety is inherent. Although the plasma in a fusion power station is expected to have a volume of 1,000 cubic metres (35,000 cu ft) or more, the plasma density is low and the total amount of fusion fuel in the vessel typically only a few grams. If the fuel supply is closed, the reaction stops within seconds. In comparison, a fission reactor is typically loaded with enough fuel for several months or years, and no additional fuel is necessary to continue the reaction. It is this large amount of fuel that gives rise to the possibility of a meltdown; nothing analogous exists in a fusion reactor.\nIn the magnetic approach, strong fields are developed in coils that are held in place mechanically by the reactor structure. Failure of this structure could release this tension and allow the magnet to \"explode\" outward. The severity of this event would be similar to any other industrial accident or an MRI machine quench/explosion, and could be effectively stopped with a containment building similar to those used in existing (fission) nuclear generators. The laser-driven inertial approach is generally lower-stress because of the increased size of the reaction chamber. Although failure of the reaction chamber is possible, simply stopping fuel delivery would prevent any sort of catastrophic failure.\nMost reactor designs rely on liquid hydrogen as both a coolant and a method for converting stray neutrons from the reaction into tritium, which is fed back into the reactor as fuel. Hydrogen is highly flammable, and in the case of a fire it is possible that the hydrogen stored on-site could be burned up and escape. In this case, the tritium contents of the hydrogen would be released into the atmosphere, posing a radiation risk. Calculations suggest that at about 1 kg the total amount of tritium and other radioactive gases in a typical power station would be so small that they would have diluted to legally acceptable limits by the time they blew as far as the station's perimeter fence.\nThe likelihood of small industrial accidents including the local release of radioactivity and injury to staff cannot be estimated yet. These would include accidental releases of lithium or tritium or mis-handling of decommissioned radioactive components of the reactor itself.\n\n\n=== Magnet quench ===\nA quench is an abnormal termination of magnet operation that occurs when part of the superconducting coil enters the normal (resistive) state. This can occur because the field inside the magnet is too large, the rate of change of field is too large (causing eddy currents and resultant heating in the copper support matrix), or a combination of the two.\nMore rarely a defect in the magnet can cause a quench. When this happens, that particular spot is subject to rapid Joule heating from the enormous current, which raises the temperature of the surrounding regions. This pushes those regions into the normal state as well, which leads to more heating in a chain reaction. The entire magnet rapidly becomes normal (this can take several seconds, depending on the size of the superconducting coil). This is accompanied by a loud bang as the energy in the magnetic field is converted to heat, and rapid boil-off of the cryogenic fluid. The abrupt decrease of current can result in kilovolt inductive voltage spikes and arcing. Permanent damage to the magnet is rare, but components can be damaged by localized heating, high voltages, or large mechanical forces.\nIn practice, magnets usually have safety devices to stop or limit the current when the beginning of a quench is detected. If a large magnet undergoes a quench, the inert vapor formed by the evaporating cryogenic fluid can present a significant asphyxiation hazard to operators by displacing breathable air.\nA large section of the superconducting magnets in CERN's Large Hadron Collider unexpectedly quenched during start-up operations in 2008, necessitating the replacement of a number of magnets. In order to mitigate against potentially destructive quenches, the superconducting magnets that form the LHC are equipped with fast-ramping heaters which are activated once a quench event is detected by the complex quench protection system. As the dipole bending magnets are connected in series, each power circuit includes 154 individual magnets, and should a quench event occur, the entire combined stored energy of these magnets must be dumped at once. This energy is transferred into dumps that are massive blocks of metal which heat up to several hundreds of degrees Celsius\u2014because of resistive heating\u2014in a matter of seconds. Although undesirable, a magnet quench is a \"fairly routine event\" during the operation of a particle accelerator.\n\n\n=== Effluents ===\nThe natural product of the fusion reaction is a small amount of helium, which is completely harmless to life. Of more concern is tritium, which, like other isotopes of hydrogen, is difficult to retain completely. During normal operation, some amount of tritium will be continually released.\nAlthough tritium is volatile and biologically active, the health risk posed by a release is much lower than that of most radioactive contaminants, because of tritium's short half-life (12.32 years) and very low decay energy (~14.95 keV), and because it does not bioaccumulate (instead being cycled out of the body as water, with a biological half-life of 7 to 14 days). Current ITER designs are investigating total containment facilities for any tritium.\n\n\n=== Waste management ===\nThe large flux of high-energy neutrons in a reactor will make the structural materials radioactive. The radioactive inventory at shut-down may be comparable to that of a fission reactor, but there are important differences.\nThe half-life of the radioisotopes produced by fusion tends to be less than those from fission, so that the inventory decreases more rapidly. Unlike fission reactors, whose waste remains radioactive for thousands of years, most of the radioactive material in a fusion reactor would be the reactor core itself, which would be dangerous for about 50 years, and low-level waste for another 100. Although this waste will be considerably more radioactive during those 50 years than fission waste, the very short half-life makes the process very attractive, as the waste management is fairly straightforward. By 500 years the material would have the same radiotoxicity as coal ash.\nAdditionally, the choice of materials used in a fusion reactor is less constrained than in a fission design, where many materials are required for their specific neutron cross-sections. This allows a fusion reactor to be designed using materials that are selected specifically to be \"low activation\", materials that do not easily become radioactive. Vanadium, for example, would become much less radioactive than stainless steel. Carbon fiber materials are also low-activation, as well as being strong and light, and are a promising area of study for laser-inertial reactors where a magnetic field is not required.\nIn general terms, fusion reactors would create far less radioactive material than a fission reactor, the material it would create is less damaging biologically, and the radioactivity \"burns off\" within a time period that is well within existing engineering capabilities for safe long-term waste storage.\n\n\n=== Nuclear proliferation ===\n\nAlthough fusion power uses nuclear technology, the overlap with nuclear weapons would be limited. A huge amount of tritium could be produced by a fusion power station; tritium is used in the trigger of hydrogen bombs and in a modern boosted fission weapon, but it can also be produced by nuclear fission. The energetic neutrons from a fusion reactor could be used to breed weapons-grade plutonium or uranium for an atomic bomb (for example by transmutation of U238 to Pu239, or Th232 to U233).\nA study conducted 2011 assessed the risk of three scenarios:\nUse in small-scale fusion station: As a result of much higher power consumption, heat dissipation and a more recognizable design compared to enrichment gas centrifuges this choice would be much easier to detect and therefore implausible.\nModifications to produce weapon-usable material in a commercial facility: The production potential is significant. But no fertile or fissile substances necessary for the production of weapon-usable materials needs to be present at a civil fusion system at all. If not shielded, a detection of these materials can be done by their characteristic gamma radiation. The underlying redesign could be detected by regular design information verifications. In the (technically more feasible) case of solid breeder blanket modules, it would be necessary for incoming components to be inspected for the presence of fertile material, otherwise plutonium for several weapons could be produced each year.\nPrioritizing a fast production of weapon-grade material regardless of secrecy: The fastest way to produce weapon usable material was seen in modifying a prior civil fusion power station. Unlike in some nuclear power stations, there is no weapon compatible material during civil use. Even without the need for covert action this modification would still take about 2 months to start the production and at least an additional week to generate a significant amount for weapon production. This was seen as enough time to detect a military use and to react with diplomatic or military means. To stop the production, a military destruction of inevitable parts of the facility leaving out the reactor itself would be sufficient. This, together with the intrinsic safety of fusion power would only bear a low risk of radioactive contamination.\nAnother study concludes that \"[..]large fusion reactors \u2013 even if not designed for fissile material breeding \u2013 could easily produce several hundred kg Pu per year with high weapon quality and very low source material requirements.\" It was emphasized that the implementation of features for intrinsic proliferation resistance might only be possible at this phase of research and development. The theoretical and computational tools needed for hydrogen bomb design are closely related to those needed for inertial confinement fusion, but have very little in common with the more scientifically developed magnetic confinement fusion.\n\n\n=== Energy source ===\nLarge-scale reactors using neutronic fuels (e.g. ITER) and thermal power production (turbine based) are most comparable to fission power from an engineering and economics viewpoint. Both fission and fusion power stations involve a relatively compact heat source powering a conventional steam turbine-based power station, while producing enough neutron radiation to make activation of the station materials problematic. The main distinction is that fusion power produces no high-level radioactive waste (though activated station materials still need to be disposed of). There are some power station ideas that may significantly lower the cost or size of such stations; however, research in these areas is nowhere near as advanced as in tokamaks.\nFusion power commonly proposes the use of deuterium, an isotope of hydrogen, as fuel and in many current designs also use lithium. Assuming a fusion energy output equal to the 1995 global power output of about 100 EJ/yr (= 1 \u00d7 1020 J/yr) and that this does not increase in the future, which is unlikely, then the known current lithium reserves would last 3000 years. Lithium from sea water would last 60 million years, however, and a more complicated fusion process using only deuterium from sea water would have fuel for 150 billion years. To put this in context, 150 billion years is close to 30 times the remaining lifespan of the sun, and more than 10 times the estimated age of the universe.\n\n\n== Economics ==\nWhile fusion power is still in early stages of development, substantial sums have been and continue to be invested in research. In the EU almost \u20ac10 billion was spent on fusion research up to the end of the 1990s, and the new ITER reactor alone is budgeted at \u20ac6.6 billion total for the timeframe between 2008 and 2020.\nIt is estimated that up to the point of possible implementation of electricity generation by nuclear fusion, R&D will need further promotion totalling around \u20ac60\u201380 billion over a period of 50 years or so (of which \u20ac20\u201330 billion within the EU) based on a report from 2002. Nuclear fusion research receives \u20ac750 million (excluding ITER funding) from the European Union, compared with \u20ac810 million for sustainable energy research, putting research into fusion power well ahead of that of any single rivaling technology. Indeed, the size of the investments and time frame of the expected results mean that fusion research is almost exclusively publicly funded, while research in other forms of energy can be done by the private sector. In spite of that, a number of start-up companies active in the field of fusion power have managed to attract private money.\n\n\n== Advantages ==\nFusion power would provide more energy for a given weight of fuel than any fuel-consuming energy source currently in use, and the fuel itself (primarily deuterium) exists abundantly in the Earth's ocean: about 1 in 6500 hydrogen atoms in seawater is deuterium. Although this may seem a low proportion (about 0.015%), because nuclear fusion reactions are so much more energetic than chemical combustion and seawater is easier to access and more plentiful than fossil fuels, fusion could potentially supply the world's energy needs for millions of years.\nDespite being technically non-renewable, fusion power has many of the benefits of renewable energy sources (such as being a long-term energy supply and emitting no greenhouse gases) as well as some of the benefits of the resource-limited energy sources as hydrocarbons and nuclear fission (without reprocessing). Like these currently dominant energy sources, fusion could provide very high power-generation density and uninterrupted power delivery (because it is not dependent on the weather, unlike wind and solar power).\nAnother aspect of fusion energy is that the cost of production does not suffer from diseconomies of scale. The cost of water and wind energy, for example, goes up as the optimal locations are developed first, while further generators must be sited in less ideal conditions. With fusion energy the production cost will not increase much even if large numbers of stations are built, because the raw resource (seawater) is abundant and widespread.\nSome problems that are expected to be an issue in this century, such as fresh water shortages, can alternatively be regarded as problems of energy supply. For example, in desalination stations, seawater can be purified through distillation or reverse osmosis. Nonetheless, these processes are energy intensive. Even if the first fusion stations are not competitive with alternative sources, fusion could still become competitive if large-scale desalination requires more power than the alternatives are able to provide.\nA scenario has been presented of the effect of the commercialization of fusion power on the future of human civilization. ITER and later Demo are envisioned to bring online the first commercial nuclear fusion energy reactor by 2050. Using this as the starting point and the history of the uptake of nuclear fission reactors as a guide, the scenario depicts a rapid take up of nuclear fusion energy starting after the middle of this century.\nFusion power could be used in interstellar space, where solar energy is not available.\n\n\n== Criticism ==\nBecause commercial fusion projects are very large and complex, and ongoing funding is a political issue, such projects usually involve cost overruns and missed deadlines. For example, the construction of the National Ignition Facility cost $5 billion and took seven years longer than expected. ITER's expected cost has gone from $5 billion to $20 billion, and the date for full power operation has been put back to 2027, from the original estimate of 2016.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\nChen, Francis (2011). An Indispensable Truth: How Fusion Power Can Save the Planet. New York: Springer. ISBN 978-1441978196\nClery, Daniel (2013). A Piece of the Sun. New York: Overlook. ISBN 978-1468304930\nDean, Stephen (2013). Search for the Ultimate Energy Source: A History of the U.S. Fusion Energy Program. New York: Springer. ISBN 978-1461460367\nMolina, Andr\u00e9s de Bustos (2013) Kinetic Simulations of Ion Transport in Fusion Devices. New York: Springer. ISBN 978-3319004211\nVoss, David (March 1, 1999). \"What Ever Happened to Cold Fusion\". Physics World. ISSN 0953-8585. Retrieved 1 May 2008. \nKruglinksi, Susan (2006-03-03). \"Whatever Happened To... Cold Fusion?\". Discover Magazine. ISSN 0274-7529. Retrieved 20 June 2008. \nChoi, Charles (2005). \"Back to Square One\". Scientific American. Retrieved 25 November 2008. \nFeder, Toni (January 2005). \"Cold Fusion Gets Chilly Encore\". Physics Today. 58: 31. Bibcode:2005PhT....58a..31F. doi:10.1063/1.1881896. \nHagelstein, Peter L.; McKubre, Michael; Nagel, David; Chubb, Talbot; Hekman, Randall (2004), New Physical Effects in Metal Deuterides (PDF), Washington: US Department of Energy, archived from the original (PDF) on January 6, 2007  (manuscript)\nU.S. Department of Energy (2004), Report of the Review of Low Energy Nuclear Reactions (PDF), Washington, DC: U.S. Department of Energy, archived from the original (PDF) on 2008-02-26, retrieved 2008-07-19 \nGoodstein, David (1994), \"Whatever happened to cold fusion?\", American Scholar, Phi Beta Kappa Society, 63 (4): 527\u2013541, ISSN 0003-0937, retrieved 2008-05-25 \nClose, Frank E. (1992), Too Hot to Handle: The Race for Cold Fusion (2 ed.), London: Penguin, ISBN 0-14-015926-6 \nBeaudette, Charles G. (2002), Excess Heat & Why Cold Fusion Research Prevailed, South Bristol, Maine: Oak Grove Press, ISBN 0-9678548-3-0 \nVan Noorden, R. (April 2007), \"Cold fusion back on the menu\", Chemistry World, ISSN 1473-7604, retrieved 2008-05-25 \nTaubes, Gary (1993). Bad Science: The Short Life and Weird Times of Cold Fusion. New York: Random House. ISBN 0-394-58456-2. \nBrowne, M. (May 3, 1989), \"Physicists Debunk Claim Of a New Kind of Fusion\", New York Times, retrieved 2008-05-25 \nAdam, David (24 March 2005), Rusbringer, Alan, ed., \"In from the cold\", The Guardian, London, retrieved 2008-05-25 \nPlatt, Charles (1998), \"What if Cold Fusion is Real?\", Wired Magazine (6.11), retrieved 2008-05-25 \nHutchinson, Alex (January 8, 2006), \"The Year in Science: Physics\", Discover Magazine (online), ISSN 0274-7529, retrieved 2008-06-20 \nAdam, David (24 March 2005), Rusbringer, Alan, ed., \"In from the cold\", The Guardian, London, retrieved 2008-05-25 \nAlfred, Randy (2009-03-23). \"March 23, 1989: Cold Fusion Gets Cold Shoulder\". Wired. Archived from the original on January 4, 2014. \n\n\n== External links ==\nUltimate Energy: Fusion Reactor Research\nFusion as an Energy Source\nU.S. Fusion Energy Science Program\nEURATOM/UKAEA Fusion Association\nITER\nEUROfusion\nA Central Site for Fusion Energy Links\nInstitute for Plasma Focus Studies\nMontage of 60 years of fusion research history", 
                "titleUrl": "https://en.wikipedia.org/wiki/Fusion_power", 
                "title": "Fusion power"
            }, 
            {
                "snippet": "Atomic Energy Organization of Iran (AEOI) have designed and built a nuclear fusion device, named IR-IECF. Iran is the 6th country with such technology.  See", 
                "pageCategories": "All articles needing additional references\nAll articles with dead external links\nAll articles with unsourced statements\nArticles containing Arabic-language text\nArticles needing additional references from August 2014\nArticles with dead external links from February 2012\nArticles with dead external links from January 2013\nArticles with dead external links from June 2016\nArticles with dead external links from October 2010\nArticles with dead external links from September 2010", 
                "pageContent": "Iran has made considerable advances in science and technology through education and training, despite international sanctions in almost all aspects of research during the past 30 years. Iran's university population swelled from 100,000 in 1979 to 2 million in 2006. 70% of its science and engineering students are women. Iran's scientific progress is reported to be the fastest in the world. Iran has made great strides in different sectors, including aerospace, nuclear science, medical development, as well as stem cell and cloning research.\nPersia was a cradle of science in earlier times, contributing to medicine, mathematics, science, and philosophy. Trying to revive the golden time of Persian science, Iran's scientists now are cautiously reaching out to the world. Many individual Iranian scientists, along with the Iranian Academy of Medical Sciences and Academy of Sciences of Iran, are involved in this revival.\n\n\n== Science in Persia ==\n\nScience in Persia evolved in two main phases separated by the arrival and widespread adoption of Islam in the region.\nReferences to scientific subjects such as natural science and mathematics occur in books written in the Pahlavi languages.\n\n\n=== Ancient technology in Persia ===\nThe Qanat (a water management system used for irrigation) originated in pre-Achaemenid Persia. The oldest and largest known qanat is in the Iranian city of Gonabad, which, after 2,700 years, still provides drinking and agricultural water to nearly 40,000 people.\nPersian philosophers and inventors may have created the first batteries (sometimes known as the Baghdad Battery) in the Parthian or Sassanid eras. Some have suggested that the batteries may have been used medicinally. Other scientists believe the batteries were used for electroplating\u2014transferring a thin layer of metal to another metal surface\u2014a technique still used today and the focus of a common classroom experiment.\nWindwheels were developed by the Babylonians ca. 1700 BC to pump water for irrigation. In the 7th century, Persian engineers in Greater Iran developed a more advanced wind-power machine, the windmill, building upon the basic model developed by the Babylonians.\n\n\n=== Mathematics ===\n\nThe 9th century mathematician Muhammad Ibn Musa-al-Kharazmi created the Logarithm table, developed algebra and expanded upon Persian and Indian arithmetic systems. His writings were translated into Latin by Gerard of Cremona under the title: De jebra et almucabola. Robert of Chester also translated it under the title Liber algebras et almucabala. The works of Kharazmi \"exercised a profound influence on the development of mathematical thought in the medieval West\".\nOther Persian scientists included Abu Abbas Fazl Hatam, the Banu Musa brothers, Farahani, Omar Ibn Farakhan, Abu Zeid Ahmad Ibn Soheil Balkhi (9th century AD), Abul Vafa Bouzjani, Abu Jaafar Khan, Bijan Ibn Rostam Kouhi, Ahmad Ibn Abdul Jalil Qomi, Bu Nasr Araghi, Abu Reyhan Birooni, the noted Iranian poet Hakim Omar Khayyam Neishaburi, Qatan Marvazi, Massoudi Ghaznavi (13th century AD), Khajeh Nassireddin Tusi, and Ghiasseddin Jamshidi Kashani.\n\n\n=== Medicine ===\n\nThe practice and study of medicine in Iran has a long and prolific history. Situated at the crossroads of the East and West, Persia was often involved in developments in ancient Greek and Indian medicine; pre- and post-Islamic Iran have been involved in medicine as well.\nFor example, the first teaching hospital where medical students methodically practiced on patients under the supervision of physicians was the Academy of Gundishapur in the Persian Empire. Some experts go so far as to claim that: \"to a very large extent, the credit for the whole hospital system must be given to Persia\".\nThe idea of xenotransplantation dates to the days of Achaemenidae (the Achaemenian dynasty), as evidenced by engravings of many mythologic chimeras still present in Persepolis.\n\nSeveral documents still exist from which the definitions and treatments of the headache in medieval Persia can be ascertained. These documents give detailed and precise clinical information on the different types of headaches. The medieval physicians listed various signs and symptoms, apparent causes, and hygienic and dietary rules for prevention of headaches. The medieval writings are both accurate and vivid, and they provide long lists of substances used in the treatment of headaches. Many of the approaches of physicians in medieval Persia are accepted today; however, still more of them could be of use to modern medicine.\nIn the 10th century work of Shahnameh, Ferdowsi describes a Caesarean section performed on Rudabeh, during which a special wine agent was prepared by a Zoroastrian priest and used to produce unconsciousness for the operation. Although largely mythical in content, the passage illustrates working knowledge of anesthesia in ancient Persia.\nLater in the 10th century, Abu Bakr Muhammad Bin Zakaria Razi is considered the founder of practical physics and the inventor of the special or net weight of matter. His student, Abu Bakr Joveini, wrote the first comprehensive medical book in the Persian language. Razi is also the inventor of alcohol.\nAfter the Islamic conquest of Iran, medicine continued to flourish with the rise of notables such as Rhazes and Haly Abbas, albeit Baghdad was the new cosmopolitan inheritor of Sassanid Jundishapur's medical academy.\nAn idea of the number of medical works composed in Persian alone may be gathered from Adolf Fonahn's Zur Quellenkunde der Persischen Medizin, published in Leipzig in 1910. The author enumerates over 400 works in the Persian language on medicine, excluding authors such as Avicenna, who wrote in Arabic. Author-historians Meyerhof, Casey Wood, and Hirschberg also have recorded the names of at least 80 oculists who contributed treatises on subjects related to ophthalmology from the beginning of 800 AD to the full flowering of Muslim medical literature in 1300 AD.\nAside from the aforementioned, two other medical works attracted great attention in medieval Europe, namely Abu Mansur Muwaffaq's Materia Medica, written around 950 AD, and the illustrated Anatomy of Mansur ibn Muhammad, written in 1396 AD.\nModern academic medicine began in Iran when Joseph Cochran established a medical college in Urmia in 1878. Cochran is often credited for founding Iran's \"first contemporary medical college\". The website of Urmia University credits Cochran for \"lowering the infant mortality rate in the region\" and for founding one of Iran's first modern hospitals (Westminster Hospital) in Urmia.\n\n\n=== Astronomy ===\n\nIn 1000 AD, Biruni wrote an astronomical encyclopaedia that discussed the possibility that the earth might rotate around the sun. This was before Tycho Brahe drew the first maps of the sky, using stylized animals to depict the constellations.\nIn the tenth century, the Persian astronomer Abd al-Rahman al-Sufi cast his eyes upwards to the awning of stars overhead and was the first to record a galaxy outside our own. Gazing at the Andromeda galaxy he called it a \"little cloud\" \u2013  an apt description of the slightly wispy appearance of our galactic neighbour.\n\n\n=== Biology ===\n\n\n=== Chemistry ===\n\nTusi believed that a body of matter is able to change but is not able to disappear entirely. He wrote \"a body of matter cannot disappear completely. It only changes its form, condition, composition, color, and other properties, and turns into a different complex or elementary matter\". Five hundred years later, Mikhail Lomonosov (1711\u20131765) and Antoine-Laurent Lavoisier (1743\u20131794) created the law of conservation of mass, setting down this same idea. However, it should be noted that Tusi argued for evolution within a firmly Islamic context\u2014he did not, like Darwin, draw materialist conclusions from his theories. Moreover, unlike Darwin, he was arguing hypothetically: he did not attempt to provide empirical data for his theories. Nonetheless his arguments, which in some ways prefigure natural selection, are still considered remarkably 'advanced' for their time.\nJaber Ibn Hayyan, the famous Iranian chemist who died in 804 at Tous in Khorasan, was the father of a number of discoveries recorded in an encyclopaedia and of many treatises covering two thousand topics, and these became the bible of European chemists of the 18th century, particularly of Lavoisier. These works had a variety of uses including tinctures and their applications in tanning and textiles; distillations of plants and flowers; the origin of perfumes; therapeutic pharmacy, and gunpowder, a powerful military instrument possessed by Islam long before the West. Jabir ibn Hayyan, is widely regarded as the founder of chemistry, inventing many of the basic processes and equipment still used by chemists today such as distillation.\n\n\n=== Physics ===\n\nAbu Ali al-Hasan ibn al-Haytham is known in the West as Alhazen, born in 965 in Persia and dying in 1039 in Egypt. He is known as the father of optics for his writings on, and experiments with, lenses, mirrors, refraction, and reflection. He correctly stated that vision results from light that is reflected into the eye by an object, not emitted by the eye itself and reflected back, as Aristotle believed. He solved the problem of finding the locus of points on a spherical mirror from which light will be reflected to an observer. From his studies of refraction, he determined that the atmosphere has a definite height and that twilight is caused by refraction of solar radiation from beneath the horizon.\nBiruni was the first scientist to formally propose that the speed of light is finite, before Galileo tried to experimentally prove this.\nKamal al-Din Al-Farisi (1267\u20131318) born in Tabriz, Iran, is known for giving the first mathematically satisfactory explanation of the rainbow, and an explication of the nature of colours that reformed the theory of Ibn al-Haytham. Al-Farisi also \"proposed a model where the ray of light from the sun was refracted twice by a water droplet, one or more reflections occurring between the two refractions.\" He verified this through extensive experimentation using a transparent sphere filled with water and a camera obscura.\n\n\n== Science in modern Iran ==\n\nConsidering the country's brain drain and its poor political relationship with the United States and some other Western countries, Iran's scientific community remains productive, even while economic sanctions make it difficult for universities to buy equipment or to send people to the United States to attend scientific meetings. Furthermore, Iran considers scientific backwardness, as one of the root causes of political and military bullying by developed countries over developing states. After the Iranian Revolution, there have been efforts by the religious scholars to assimilate Islam with modern science and this is seen by some as the reason behind the recent successes of Iran to augment its scientific output. Currently Iran aims for a national goal of self-sustainment in all scientific arenas. Many individual Iranian scientists, along with the Iranian Academy of Medical Sciences and Academy of Sciences of Iran, are involved in this revival. The Comprehensive Scientific Plan has been devised based on about 51,000 pages of documents and includes 224 scientific projects that must be implemented by the year 2025.\n\n\n=== Budget ===\n\nIran's national science budget was about $900 million in 2005 and it had not been subject to any significant increase for the previous 15 years. By early 2000, Iran allocated around 0.4% of its GDP to R&D, which ranked it \"far behind industrialized societies\" and the world average of 1.4%. By 2009 this ratio of research to GDP reached 0.87% and the set target is 2.5% to be reached by 2015. Iran's government has devoted huge amounts of funds for research on high technologies such as nanotechnology, biotechnology, stem cell research and information technology (2008). Iranian Research Organization for Science and Technology and the National Research Institute for Science Policy are two of the main institutions, depending on the Ministry of Science, Research and Technology, in charge of establishing research policies at the state level. In 2006, Iranian government wiped out the financial debts of all universities in a bid to relieve their budget constraints. According to UNESCO science report 2010, most of the research in Iran is government funded with the Iranian government providing almost 75% of all research fundings. The share of private businesses in total national R&D funding according to the same report is very low being just 14% as compared with the Turkey's 48%. The rest of approximately 11% of funding comes from higher education sector and non-profit organizations.\nIn 2009, Iranian government formulated a 15-year comprehensive national plan for science focused on higher education and strengthening the links between academia and industry in order to promote a knowledge based economy. As per the plan by year 2030, Iran's research and development spending is to be increased to 4% of GDP from 0.59% of 2006 and increasing its education spending to over 7% of GDP from the 2007 level of 5.49%.\nAs of 2016, R&D\u2019s share in the GNP is at 0.06% (where it should be 2.5% of GDP) and industry-driven R&D is almost non\u2011existent.\n\n\n=== Overview ===\n\nTheoretical and computational sciences are highly developed in Iran. Despite the limitations in funds, facilities, and international collaborations, Iranian scientists have been very productive in several experimental fields such as pharmacology, pharmaceutical chemistry, and organic and polymer chemistry. Iranian biophysicists, especially molecular biophysicists, have gained international reputations since the 1990s. High field nuclear magnetic resonance facility, microcalorimetry, circular dichroism, and instruments for single protein channel studies have been provided in Iran during the past two decades. Tissue engineering and research on biomaterials have just started to emerge in biophysics departments.\n\n\n=== Scientific collaboration ===\n\nIran annually hosts international science festivals. The International Kharazmi Festival in Basic Science and The Annual Razi Medical Sciences Research Festival promote original research in science, technology, and medicine in Iran. There is also an ongoing R&D collaboration between large state-owned companies and the universities in Iran.\nIranians welcome scientists from all over the world to Iran for a visit and participation in seminars or collaborations. Many Nobel laureates and influential scientists such as Bruce Alberts, F. Sherwood Rowland, Kurt W\u00fcthrich, Stephen Hawking, and Pierre-Gilles de Gennes visited Iran after the Iranian revolution. Some universities also hosted American and European scientists as guest lecturers during recent decades.\nIran is also an active member of COMSTECH and collaborates in its international projects. The coordinator general of COMSTECH, Dr. Atta ur Rahman has said that Iran is the leader in science and technology among Muslim countries and hoped for greater cooperation with Iran in different international technological and industrialization projects. In 2012, Iran researchers joined the International Thermonuclear Experimental Reactor (ITER), which is the world's largest and most advanced experimental fusion reactor.\nIranian and U.S. scientists have collaborated on a number of projects.\n\n\n=== Private sector ===\n\nThe 5th Development Plan (2010\u201315) requires the private sector to communicate research needs to universities so that universities would coordinate research projects in line with these needs, with sharing of expenses by both sides.\nIn Iran, only a limited number of large enterprises (such as IDRO, NIOC, NIPC, DIO, Iran Aviation Industries Organization, Iranian Space Agency, Iran Electronics Industries or Iran Khodro) have their own in-house R&D capabilities. In Iran, because of its weakness or absence, the support industry makes little contribution to the innovation/technology development activities. Supporting the development of small and medium enterprises in Iran will strengthen greatly the supplier network. A 2003 report by the United Nations Industrial Development Organization regarding small and medium-sized enterprises (SMEs) identified the following impediments to industrial development:\nLack of monitoring institutions;\nInefficient banking system;\nInsufficient research & development;\nShortage of managerial skills;\nCorruption;\nInefficient taxation;\nSocio-cultural apprehensions;\nAbsence of social learning loops;\nShortcomings in international market awareness necessary for global competition,\nCumbersome bureaucratic procedures;\nShortage of skilled labor;\nLack of intellectual property protection;\nInadequate social capital, social responsibility and socio-cultural values.\nDespite these problems, Iran has progressed in various scientific and technological fields, including petrochemical, pharmaceutical, aerospace, defense, and heavy industry. Even in the face of economic sanctions, Iran is emerging as an industrialized country.\nParallel to academic research, several companies have been founded in Iran during last few decades. For example, CinnaGen, established in 1992, is one of the pioneering biotechnology companies in the region. CinnaGen won Biotechnology Asia 2005 Innovation Awards due to its achievements and innovation in biotechnology research. In 2006, Pars\u00e9 Semiconductor Co. announced it had designed and produced a 32-bit computer microprocessor inside the country for the first time. Software companies are growing rapidly. In CeBIT 2006, ten Iranian software companies introduced their products. Iran's National Foundation for Computer Games unveiled the country's first online video game in 2010, capable of supporting up to 5,000 users at the same time.\n\n\n=== Innovation ===\n\nAs of 2004, Iran's national innovation system (NIS) had not experienced a serious entrance to the technology creation phase and mainly exploited the technologies developed by other countries (e.g. in the petrochemicals industry).\nIn 2016, Iran ranked second in the percentage of graduates in science and engineering in the Global Innovation Index. Iran also ranked fourth in tertiary education, 26 in knowledge creation, 31 in gross percentage of tertiary enrollment, 41 in general infrastructure, 48 in human capital as well as research and 51 in innovation efficiency ratio.\nIn recent years several drugmakers in Iran are gradually developing the ability to innovate, away from generic drugs production itself.\nAccording to the State Registration Organization of Deeds and Properties, a total of 9,570 national inventions was registered in Iran during 2008. Compared with the previous year, there was a 38-percent increase in the number of inventions registered by the organization.\nIran has several funds to support entrepreneurship and innovation:\nInnovation and Flourishing/Prosperity Fund of the Directorate of Science and Technology of the Presidential Office,\nNational Researchers and Industrialists Support Fund,\nNokhbegan Technology Development Institute,\nNovin Technology Development Fund,\nSharif Export Development Research and Technology Fund,\nSupport Fund of Researchers and Technologists,\nPayambar Azam (the great prophet) Scientific and Technological Award,\nStudent Entrepreneurs Support Fund,\n+6,000 private interest-free funds & 3 venture capital funds (Shenasa, Simorgh and Sarava Pars). See also: Banking in Iran.\n\n\n=== Technology parks ===\n\nAs of 2012, Iran had officially 31 science and technology parks nationwide. Furthermore, as of 2014, 36 science and technology parks hosting more than 3,650 companies were operating in Iran. These firms have directly employed more than 24,000 people. According to the Iran Entrepreneurship Association, there are totally 99 parks of science and technology, which operate without official permits. Twenty-one of those parks are located in Tehran and affiliated with University Jihad, Tarbiat Modares University, Tehran University, Ministry of Energy (Iran), Ministry of Health and Medical Education, Amir Kabir University among others. Fars Province, with 8 parks and Razavi Khorasan Province, with 7 parks, are ranked second and third after Tehran respectively.\nAs of 2014, Iran had also 930 industrial parks and zones, of which 731 are ready to be ceded to the private sector. The government of Iran has plans for the establishment of 50\u201360 new industrial parks by the end of the fifth Five-Year Socioeconomic Development Plan (2015).\n\n\n=== Medical sciences ===\n\nWith over 400 medical research facilities and 76 medical magazine indexes available in the country, Iran is the 19th country in medical research and is set to become the 10th within 10 years (2012). Clinical sciences are invested in highly in Iran. In areas such as rheumatology, hematology, and bone marrow trasplantation, Iranian medical scientists publish regularly. The Hematology, Oncology and Bone Marrow Transplantation Research Center (HORC) of Tehran University of Medical Sciences in Shariati Hospital was established in 1991. Internationally, this center is one of the largest bone marrow transplantation centers and has carried out a large number of successful transplantations. According to a study conducted in 2005, associated specialized pediatric hematology and oncology (PHO) services exist in almost all major cities throughout the country, where 43 board-certified or eligible pediatric hematologist\u2013oncologists are giving care to children suffering from cancer or hematological disorders. Three children's medical centers at universities have approved PHO fellowship programs. Besides hematology, gastroenterology has recently attracted many talented medical students. The gasteroenterology research center based at Tehran University has produced increasing numbers of scientific publications since its establishment.\n\nModern organ transplantation in Iran dates to 1935, when the first cornea transplant in Iran was performed by Professor Mohammad-Qoli Shams at Farabi Hospital in Tehran, Iran. The Shiraz Nemazi transplant center, also one of the pioneering transplant units of Iran, performed the first Iranian kidney transplant in 1967 and the first Iranian liver transplant in 1995. The first heart transplant in Iran was performed 1993 in Tabriz. The first lung transplant was performed in 2001, and the first heart and lung transplants were performed in 2002, both at Tehran University. Iran developed the first artificial lung in 2009 to join five other countries in the world that possess such technology. Currently, renal, liver, and heart transplantations are routinely performed in Iran. Iran ranks fifth in the world in kidney transplants. The Iranian Tissue Bank, commencing in 1994, was the first multi-facility tissue bank in country. In June 2000, the Organ Transplantation Brain Death Act was approved by the Parliament, followed by the establishment of the Iranian Network for Transplantation Organ Procurement. This act helped to expand heart, lung, and liver transplantation programs. By 2003, Iran had performed 131 liver, 77 heart, 7 lung, 211 bone marrow, 20,581 cornea, and 16,859 renal transplantations. 82 percent of these were donated by living and unrelated donors; 10 percent by cadavers; and 8 percent came from living-related donors. The 3-year renal transplant patient survival rate was 92.9%, and the 40-month graft survival rate was 85.9%.\nNeuroscience is also emerging in Iran. A few PhD programs in cognitive and computational neuroscience have been established in the country during recent decades. Iran ranks first in Mideast and region in ophthalmology.\nIranian surgeons treating wounded Iranian veterans during Iran\u2013Iraq War invented a new neurosurgical treatment for brain injured patients that laid to rest the previously prevalent technique developed by US Army surgeon Dr Ralph Munslow. This new surgical procedure helped devise new guidelines that have decreased death rates for comatosed patients with penetrating brain injuries from 55% of 1980 to 20% of 2010. It has been said that these new treatment guidelines benefited US congresswoman Gabrielle Giffords who had been shot in the head.\n\n\n=== Biotechnology ===\n\nIran has a growing biotechnology sector that is one of the most advanced in the developing world. The Razi Institute for Serums and Vaccines and the Pasteur Institute of Iran are leading regional facilities in the development and manufacture of vaccines. In January 1997, the Iranian Biotechnology Society (IBS) was created to oversee biotechnology research in Iran.\nAgricultural research has been successful in releasing high yielding varieties with higher stability as well as tolerance to harsh weather conditions. The agriculture researchers are working jointly with international Institutes to find the best procedures and genotypes to overcome produce failure and to increase yield. In 2005, Iran's first genetically modified (GM) rice was approved by national authorities and is being grown commercially for human consumption. In addition to GM rice, Iran has produced several GM plants in the laboratory, such as insect-resistant maize; cotton; potatoes and sugar beets; herbicide-resistant canola; salinity- and drought-tolerant wheat; and blight-resistant maize and wheat. The Royan Institute engineered Iran's first cloned animal; the sheep was born on 2 August 2006 and has passed the critical first two months of his life.\nIn the last months of 2006, Iranian biotechnologists announced that they, as the third manufacturer in the world, have sent CinnoVex (a recombinant type of Interferon b1a) to the market. According to a study by David Morrison and Ali Khademhosseini (Harvard-MIT and Cambridge), stem cell research in Iran is amongst the top 10 in the world. Iran will invest 2.5 billion dollars in the country's stem cell research over the next five years (2008\u20132013). Iran ranks 2nd in world in transplantation of stem cells.\nIn 2010, Iran begun mass-producing ocular bio-implants named SAMT. Iran began investing in biotechnological projects in 1992 and this is the tenth facility in Iran. 'Lifepatch' is the fourth bio-implant mass-produced by Iran after bone, heart valve, and tendon bio-implants. 12 countries in the world produce bio-tech drugs, which Iran is one of them. According to Scopus, Iran ranked 21st in biotechnology by producing nearly 4,000 related-scientific articles in 2014.\n\nIn 2010, AryoGen Biopharma established the biggest and most modern knowledge-based facility for production of therapeutic monoclonal antibodies in the region. As at 2012, Iran produces 15 types of monoclonal/anti-body drugs. These anti-cancer drugs are now produced by only two to three western companies.\nIn 2015, Noargencompany established as first officially registered CRO & CMO in Iran. Noargen uses concept of CMO and CRO servicing to biopharma sector of Iran as its main activity to fill the gap and promote developing biotech ideas/products toward commercialization.\n\n\n=== Physics and materials ===\n\nIran had some significant successes in nuclear technology during recent decades, especially in nuclear medicine. However, little connection exists between Iran's scientific society and that of the nuclear program of Iran. Iran is the 7th country in production of uranium hexafluoride (or UF6). Iran now controls the entire cycle for producing nuclear fuel. Iran is among the 14 countries in possession of nuclear [energy] technology. Iranian scientists are also helping to construct the Compact Muon Solenoid, a detector for the Large Hadron Collider of the European Organization for Nuclear Research (CERN) that is due to come online in 2008. Iranian engineers are involved in the design and construction of the first regional particle accelerator of the Middle East in Jordan, called SESAME. In 2009, Iran was developing its first domestic Linear particle accelerator (LINAC). It is among the few countries in the world that has the technology to produce zirconium alloys. Iran produces a wide range of lasers in demand within the country in medical and industrial fields. In 2011, Iranian scientists at the Atomic Energy Organization of Iran (AEOI) have designed and built a nuclear fusion device, named IR-IECF. Iran is the 6th country with such technology.\n\n\n=== Computer science and robotics ===\n\nCenter of Excellence in Design, Robotics, and Automation was established in 2001 to promote educational and research activities in the fields of design, robotics, and automation. Besides these professional groups, several robotics groups work in Iranian high schools. \"Sorena 2\" Robot, which was designed by engineers at University of Tehran, was unveiled in 2010. The robot can be used for handling sensitive tasks without the need for cooperating with human beings. The robot is taking slow steps similar to human beings, harmonious movements of hands and feet and other movements similar to humans. Next the researchers plan to develop speech and vision capabilities and greater intelligence for this robot. the Institute of Electrical and Electronics Engineers (IEEE) has placed the name of Surena among the five prominent robots of the world after analyzing its performance. In 2010, Iranian researchers have, for the first time in the country, developed ten robots for the nation's automotive industry using domestic know how.\nUltra Fast Microprocessors Research Center in Tehran's Amirkabir University of Technology successfully built a supercomputer in 2007. Maximum processing capacity of the supercomputer is 860 billion operations per second. Iran's first supercomputer launched in 2001 was also fabricated by Amirkabir University of Technology. In 2009, a SUSE Linux-based HPC system made by the Aerospace Research Institute of Iran (ARI) was launched with 32 cores and now runs 96 cores. Its performance was pegged at 192 GFLOPS. Iran's National Super Computer made by Iran Info-Tech Development Company (a subsidiary of IDRO) was built from 216 AMD processors. The Linux-cluster machine has a reported \"theoretical peak performance of 860 gig-flops\". The Routerlab team at the University of Tehran successfully designed and implemented an access-router (RAHYAB-300) and a 40Gbit/s high capacity switch fabric (UTS). In 2011 Amirkabir University of Technology and Isfahan University of Technology produced 2 new supercomputers with processing capacity of 34,000 billion operations per second. The supercomputer at Amirkabir University of Technology is expected to be among the 500 ones of the world.\n\n\n=== Chemistry and nanotechnology ===\n\nIran ranked 23rd in the world in Nanotechnology in 2007 with highest, ranked paper citation international mean, amongst all Islamic countries and only second to S. Korea in Asia. In 2007 Iranian scientists at the Medical Sciences and Technology Center succeeded in mass-producing an advanced scanning microscope\u2014the Scanning Tunneling Microscope (STM). By 2012, Iran ranked 8th in nanotechnologies. Iran has designed and mass-produced more than 35 kinds of advanced nanotechnology devices. These include laboratory equipments, antibacterial strings, power station filters and construction related equipment and materials.\n\n\n=== Aviation and space ===\n\nOn 17 August 2008, The Iranian Space Agency proceeded with the second test launch of a three stages Safir SLV from a site south of Semnan in the northern part of the Dasht-e-Kavir desert. The Safir (Ambassador) satellite carrier successfully launched the Omid satellite into orbit in February 2009. Iran is the 9th country to put a domestically-built satellite into orbit since the Soviet Union launched the first in 1957. Iran is among a handful of countries in the world capable of developing satellite-related technologies, including satellite navigation systems. Iran's first astronaut will be sent into space on board an Iranian shuttle by 2019. Iran is also the sixth country to send animals in space. Iran is one of the few countries capable of producing 20-25 ton sea patrol aircraft. In 2013, Iran constructed its first hypersonic wind tunnel for testing missiles and doing aerospace research. Iran is the 8th country capable of manufacturing jet engines.\n\n\n=== Astronomy ===\nThe Iranian government has committed 150 billion rials (roughly 16 million US dollars) for a telescope, an observatory, and a training program, all part of a plan to build up the country's astronomy base. Iran wants to collaborate internationally and become internationally competitive in astronomy, says the University of Michigan's Carl Akerlof, an adviser to the Iranian project. \"For a government that is usually characterized as wary of foreigners, that's an important development\". In July 2010, Iran unveiled its largest domestically-manufactured telescope dubbed \"Tara\". in 2016, Iran unveiled its new optical telescope for observing celestial objects as part of APSCO. It will be used to understand and predict the physical location of natural and man-made objects in orbit around the Earth.\n\n\n=== Energy ===\n\nIran has achieved the technical expertise to set up hydroelectric, gas and combined cycle power plants. Iran is among the four world countries that are capable of manufacturing advanced V94.2 gas turbines. Iran is able to produce all the parts needed for its gas refineries and is now the third country in the world to have developed Gas to liquids (GTL) technology. Iran produces 70% of its industrial equipment domestically including various turbines, pumps, catalysts, refineries, oil tankers, oil rigs, offshore platforms and exploration instruments. Iran is among the few countries that has reached the technology and \"know-how\" for drilling in the deep waters. Iran's indigenously designed Darkhovin Nuclear Power Plant is scheduled to come online in 2016.\n\n\n=== Armaments ===\n\nIran possesses the technology to launch superfast anti-submarine rockets that can travel at the speed of 100 meters per second under water, making the country second only to Russia in possessing the technology. Iran is among the five countries in the world to have developed ammunitions with laser targeting technology. Iran is among the few countries that possess the technological know-how of the unmanned aerial vehicles (UAV) fitted with scanning and reconnaissance systems. Iran is among the 12 countries with missile technology. Over the past years, Iran has made important breakthroughs in its defense sector and attained self-sufficiency in producing important military equipment and systems. Since 1992, it also has produced its own tanks, armored personnel carriers, sophisticated radars, missiles, a submarine, and fighter planes.\n\n\n== Contribution of Iranians and people of Iranian origin to modern science ==\n\nScientists with an Iranian background have made significant contributions to the international scientific community. In 1960, Ali Javan invented first gas laser. In 1973, the fuzzy set theory was developed by Lotfi Zadeh. Iranian cardiologist Tofy Mussivand invented the first artificial heart and afterwards developed it further. HbA1c was discovered by Samuel Rahbar and introduced to the medical community. The Vafa-Witten theorem was proposed by Cumrun Vafa, an Iranian string theorist, and his co-worker Edward Witten. The Kardar-Parisi-Zhang (KPZ) equation has been named after Mehran Kardar, notable Iranian physicist. Other notable discoveries and innovations by Iranian scientists and engineers (or of Iranian origin) include:\nKarim Nayernia: discovery of spermatagonial stem cells\nReza Ghadiri: 1998 Feynman prize for invention of a self-organized replicating molecular system\nMehdi Vaez-Iravani: invention of shear force microscopy\nSiavash Alamouti and Vahid Tarokh: invention of space\u2013time block code\nFaraneh Vargha-Khadem: discovery of SPCH1, a gene implicated in a severe speech and language disorder\nShirin Dehghan: 2006 Women in Technology Award\nNader Engheta, inventor of \"invisibility shield\" (plasmonic cover) and research leader of the year 2006, Scientific American magazine, and winner of a Guggenheim Fellowship (1999) for \"Fractional paradigm of classical electrodynamics\"\nAli Safaeinili: coinventor of Mars Advanced Radar for Subsurface and Ionosphere Sounding (MARSIS)\nPierre Omidyar: economist, founder and chairman of eBay\nRouzbeh Yassini: inventor of the cable modem\nHomayoun Seraji: most-published author in the 20-year history of the Journal of Robotic Systems (declared in 2007).\nMoslem Bahadori: reported the first case of plasma cell granuloma of the lung.\nMohammad Abdollahi: The Laureate of IAS-COMSTECH 2005 Prize in the field of Pharmacology and Toxicology and an IAS Fellow. MA is ranked as an International Top 1% outstanding Scientists of the World in the field of Pharmacology & Toxicology according to Essential Science Indicator from USA Thompson Reuters ISI. An award named \"Mohammad Abdollahi Prize\" has been established by Asian Network for Scientific Information and Science Alert Publishing company and The International Journal of Pharmacology in the recognition of MA efforts in the field of Pharmacology & Toxicology. MA is also known as one of outstanding leading scientists of OIC member countries.\nMaysam Ghovanloo: inventor of Tongue-Drive Wheelchair.\nMansour Ahmadian and Jila Nazari: Developers of PARS (Parallel Application from Rapid Simulation), which won the IET Innovation award 2008 in software design\nMohammad-Nabi Sarbolouki, invention of dendrosome\nShekoufeh Nikfar: The awardee of the top women scientists by TWAS-TWOWS-Scopus in the field of Medicine in 2009.\nAfsaneh Rabiei: inventor of an ultra-strong and lightweight material, known as Composite Metal Foam (CMF).\nMaryam Mirzakhani: In August 2014, Mirzakhani became the first-ever woman, as well as the first-ever Iranian, to receive the Fields Medal, the highest prize in mathematics.\nRamin Golestanian: In August 2014, Ramin Golestanian won the Holweck Prize for his research work in physics.\n\n\n== International rankings ==\n\nAccording to Scopus, Iran ranked 17th in terms of science production in the world in 2012 with the production of 34,155 articles above Switzerland and Turkey.\nAccording to the Institute for Scientific Information (ISI), Iran increased its academic publishing output nearly tenfold from 1996 to 2004, and has been ranked first globally in terms of output growth rate (followed by China with a 3 fold increase). In comparison, the only G8 countries in top 20 ranking with fastest performance improvement are Italy at tenth and Canada at 13th globally. Iran, China, India and Brazil are the only developing countries among 31 nations with 97.5% of the world's total scientific productivity. The remaining 162 developing countries contribute less than 2.5% of the world's scientific output. Despite the massive improvement from 0.0003% of the global scientific output in 1970 to 0.29% in 2003, still Iran's total share in the world's total output remained small. According to Thomson Reuters, Iran has demonstrated a remarkable growth in science and technology over the past one decade, increasing its science and technology output fivefold from 2000 to 2008. Most of this growth has been in engineering and chemistry producing 1.4% of the world's total output in the period 2004\u20132008. By year 2008, Iranian science and technology output accounted for 1.02% of the world's total output (That is ~340000% growth in 37 years of 1970\u20132008). 25% of scientific articles published in 2008 by Iran were international coauthorships. The top five countries coauthoring with Iranian scientists are US, UK, Canada, Germany and France.\nA 2010 report by Canadian research firm Science-Metrix has put Iran in the top rank globally in terms of growth in scientific productivity with a 14.4 growth index followed by South Korea with a 9.8 growth index. Iran's growth rate in science and technology is 11 times more than the average growth of the world's output in 2009 and in terms of total output per year, Iran has already surpassed the total scientific output of countries like Sweden, Switzerland, Israel, Belgium, Denmark, Finland, Austria or that of Norway. Iran with a science and technology yearly growth rate of 25% is doubling its total output every three years and at this rate will reach the level of Canadian annual output in 2017. The report further notes that Iran's scientific capability build-up has been the fastest in the past two decades and that this build-up is in part due to the Iraqi invasion of Iran, the subsequent bloody Iran Iraq war and Iran's high casualties due to the international sanctions in effect on Iran as compared to the international support Iraq enjoyed. The then technologically superior Iraq and its use of chemical weapons on Iranians, made Iran to embark on a very ambitious science developing program by mobilizing scientists in order to offset its international isolation, and this is most evident in the country's nuclear sciences advancement, which has in the past two decades grown by 8,400% as compared to the 34% for the rest of the world. This report further predicts that though Iran's scientific advancement as a response to its international isolation may remain a cause of concern for the world, all the while it may lead to a higher quality of life for the Iranian population but simultaneously and paradoxically will also isolate Iran even more because of the world's concern over Iran's technological advancements. Other findings of the report point out that the fastest growing sectors in Iran are Physics, Public health sciences, Engineering, Chemistry and Mathematics. Overall the growth has mostly occurred after 1980 and specially has been becoming faster since 1991 with a significant acceleration in 2002 and an explosive surge since 2005. It has been argued that scientific and technological advancement besides the nuclear program is the main reason for United States worry about Iran, which may become a superpower in the future. Some in Iranian scientific community see sanctions as a western conspiracy to stop Iran's rising rank in modern science and allege that some (western) countries want to monopolize modern technologies.\nAs per US government report on science and engineering titled \"Science and Engineering Indicators: 2010\" prepared by National Science Foundation, Iran has the world's highest growth rate in Science & Engineering article output with an annual growth rate of 25.7%. The report is introduced as a factual and policy neutral \"...volume of record comprising the major high-quality quantitative data on the U.S. and international science and engineering enterprise\". This report also notes that the very rapid growth rate of Iran inside a wider region was led by its growth in scientific instruments, pharmaceuticals, communications and semiconductors.\nThe subsequent National Science Foundation report published in 2012 by US government under the name \"Science and Engineering Indicators: 2012\", had put Iran first globally in terms of growth in science and engineering article output in the first decade of this millennium with an annual growth rate of 25.2%.\nThe latest updated National Science Foundation report published in 2014 by US government titled \"Science and Engineering Indicators 2014\", has again ranked Iran first globally in terms of growth in science and engineering article output at an annualized growth rate of 23.0% with 25% of Iran's output having been produced through international collaboration.\nIran ranked 49th for citations, 42nd for papers, and 135th for citations per paper in 2005. Their publication rate in international journals has quadrupled during the past decade. Although it is still low compared with the developed countries, this puts Iran in the first rank of Islamic countries. According to a British government study (2002), Iran ranked 30th in the world in terms of scientific impact.\nAccording to a report by SJR (A Spanish sponsored scientific-data data) Iran ranked 25th in the world in scientific publications by volume in 2007 (a huge leap from the rank of 40 few years before). As per the same source Iran ranked 20th and 17th by total output in 2010 and 2011 respectively.\nIn 2008 report by Institute for Scientific Information (ISI), Iran ranked 32, 46 and 56 in Chemistry, Physics and Biology respectively among all science producing countries. Iran ranked 15th in 2009 in the field of nanotechnology in terms of presenting articles.\nScience Watch reported in 2008 that Iran has the world's highest growth rate for citations in medical, environmental and ecological sciences. According to the same source, Iran during the period 2005\u20132009, had produced 1.71% of world's total engineering papers, 1.68% of world's total chemistry papers and 1.19% of world's total material sciences papers.\nAccording to the sixth report on \"international comparative performance of UK research base\" prepared in September 2009 by Britain-based research firm Evidence and Department for Business, Innovation and Skills, Iran has increased its total output from 0.13% of world's output in 1999 to almost 1% of world's output in 2008. As per the same report Iran had doubled its biological sciences and health research out put in just two years (2006\u20132008). The report further notes that Iran by 2008 had increased its output in physical sciences by as much as ten times in ten years and its share in world's total output had reached 1.3%, comparing with US share of 20% and Chinese share of 18%. Similarly Iran's engineering output had grown to 1.6% of the world's output being greater than Belgium or Sweden and just smaller than Russia's output at 1.8%. During the period 1999\u20132008, Iran improved its science impact from 0.66 to 1.07 above the world's average of 0.7 similar to Singapore's. In engineering Iran improved its impact and is already ahead of India, South Korea and Taiwan in engineering research performance. By 2008, Iran's share of most cited top 1% of world's papers was 0.25% of the world's total.\nAs per French government report \"L'Observatoire des sciences et des techniques (OST) 2010\", Iran had the world's fastest growth rate in scientific article output between 2003 and 2008 period at +219%, producing 0.8% of the world's total material sciences knowledge out put in 2008, the same as Israel. The fastest growing scientific field in Iran was medical sciences at 344% and the slowest growth was of chemistry at 128% with the growth for other fields being biology 342%, ecology 298%, physics 182%, basic sciences 285%, engineering 235% and mathematics at 255%. As per the same report among the countries that produced less than 2% of the world's science and technology, only Iran, Turkey and Brazil had the most dynamic growth in their scientific output, with Turkey and Brazil having a growth rate above 40% and Iran above 200% compared with South Korea and Taiwan growth rates at 31% and 37% respectively. Iran also was among the countries whose scientific visibility was growing fastest in the world such as China, Turkey, India and Singapore though all growing from a low visibility base.\nAccording to the latest updated French government report \"L'Observatoire des sciences et des techniques (OST) 2014\", Iran had the world's fastest growth rate in scientific production output in the period between 2002 and 2012, having increased its share of world's total scientific output by +682% in the said period, producing 1.4% of world's total science and ranking 18th globally in terms of its total scientific output. Meanwhile, Iran also ranks first globally for having increased its share in the world's high impact (top 10%) publications by +1338% between 2002 and 2012 and similarly ranks first globally as well for increasing its global scientific visibility through having its share of international citations increased by +996% in the above period. Iran also ranks first globally in this report for the growth rate in scientific production of individual fields by having increased its science output in Biology by +1286%, in Medicine by +900%, in Applied biology and Ecology by +816%, in Chemistry by +356%, in Physics by +577%, in Space sciences by +947%, in Engineering sciences by +796% and in Mathematics by +556%.\nA bibliometric analysis of middle east was released by professional division of Thomson Reuters in 2011 titled \"Global Research Report Middle East\" comparing scientific research in middle eastern countries with that of the world for the first decade of this century. The study findings rank Iran at second position after Turkey in terms of total scientific output with Turkey producing 1.9% of the world's total science output while Iran's share of world's total science output was at 1.3%. Total scientific output of 14 countries surveyed including Bahrain, Egypt, Iran, Iraq, Jordan, Kuwait, Lebanon, Oman, Qatar, Saudi Arabia, Syria, Turkey, United Arab Emirates and Yemen was just 4% of the world's total output; with Turkey and Iran producing the bulk of scientific research in the region. In terms of growth in scientific research, Iran was ranked first with 650% increase of its share in world's output and Turkey second with a growth of 270%. Turkey increased its research publication rate from 5000 papers in year 2000 to nearly 22000 in the year 2009, while Iran's research publication started from a lower point of 1300 papers in year 2000 and grew to 15000 papers in the year 2009 with a notable surge in Iranian growth after year 2004. In terms of production of highly cited papers, 1.7% of all Iranian papers in mathematics and 1.3% of papers in engineering fields attained highly cited status defined as most cited top 1% of world's publications, exceeding the world's average in citation impact for those fields. Overall Iran produces 0.48% of the world's highly cited output in all fields just about half of what would be expected for parity at 1%. Comparative figures for other countries following Iran in the region are: Turkey producing 0.37% of the world's highly cited papers, Jordan 0.28%, Egypt 0.26% and Saudi Arabia 0.25%. External scientific collaboration accounted for 21% of the total research projects undertaken by researchers in Iran with largest collaborators being United States at 4.3%, United Kingdom at 3.3%, Canada 3.1%, Germany 1.7% and Australia at 1.6%.\nIn 2011, world's oldest scientific society and Britain's leading academic institution, the Royal Society in collaboration with Elsevier published a study named \"Knowledge, networks and nations\" surveying global scientific landscape. According to this survey Iran has the world's fastest growth rate in science and technology. During the period 1996\u20132008, Iran had increased its scientific output by 18 folds.\nAs per WIPO's report titled \"World Intellectual Property Indicators 2013\", Iran ranked 90th for patents generated by Iranian nationals all over the world, 100th in industrial design and 82nd in trademarks, positioning Iran below Jordan and Venezuela in this regard but above Yemen and Jamaica.\n\n\n== Iranian journals listed in the Institute for Scientific Information (ISI) ==\n\nAccording to the Institute for Scientific Information (ISI), Iranian researchers and scientists have published a total of 60,979 scientific studies in major international journals in the last 19 years (1990\u20132008).\n\nActa Medica Iranica\nApplied Entomology and PhytoPathology\nArchives of Iranian Medicine\nDARU Journal of Pharmaceutical Sciences\nIranian Biomedical Journal\nIranian Journal of BioTechnology\nIranian Journal of Chemistry & Chemical Engineering\nIranian Journal of Fisheries Sciences-English\nIranian Journal of Plant Pathology\nIranian Journal of Science and Technology\nIranian Polymer Journal\nIranian Journal of Public Health\nIranian Journal of Pharmaceutical Research\nIranian Journal of Reproductive Medicine\nIranian Journal of Veterinary Medicine\nIranian Journal of Fuzzy Systems\nJournal of Entomological Society of Iran\nPlant Pests & Diseases Research Institute Insect Taxonomy Research Department Publication\nThe Journal of the Iranian Chemical Society\nRostaniha (Botanical Journal of Iran)\n\n\n== See also ==\n\n\n=== General ===\n\nHigher Education in Iran\nList of Iranian Research Centers\nList of contemporary Iranian scientists, scholars, and engineers (modern era)\nList of Iranian scientists\nEconomy of Iran\nIndustry of Iran\nIran's brain drain\nInternational rankings of Iran\nIntellectual Movements in Iran\nBase isolation from Iran\nScience in newly industrialized countries\nComposite Index of National Capability\nIslamic Golden Age\nPersian philosophy\n\n\n=== Prominent organizations ===\nInstitute of Standards and Industrial Research of Iran\nAtomic Energy Organization of Iran\nIranian Space Agency\nIranian Chemists Association\nThe Physical Society of Iran\nHORCSCT\nIranian Research Organization for Science and Technology\nIran National Science Foundation\n\n\n== References ==\n\n\n== External links ==\nScience, Technology and Innovation Policy Review - Iran. United Nations Conference on Trade and Development (2005)\nIranian scientific publications online digital archive\nBest of Iran's 2011 research and technology\nVideos\nMajor Scientific Developments in Iran \u2013 Part I Part II Part III (2010 PressTV)\nIran's scientific achievements (2011 PressTV)\nLaser Technology advancements in Iran \u2013 Part I Part II Part III (2010 PressTV)\nIran's comprehensive scientific plan (2011 PressTV)\nNanotechnology in Iran (July 2011, PressTV)\nNanotechnology in Iran (October 2011, PressTV)\nIran surgical society (2011 PressTV)\nA Review of Iran's Scientific Achievement in 2011 (March 2012, PressTV)\nScientific Ranking of Iran (2012 PressTV)\nIran's scientific breakthroughs (2016 PressTV)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Science_and_technology_in_Iran", 
                "title": "Science and technology in Iran"
            }, 
            {
                "snippet": "3 February 2016.\u00a0  \"Nuclear Fusion Hits a Massive Milestone in Germany\". VICE. Retrieved 5 February 2016.\u00a0  \"Nuclear fusion device's 1st test with hydrogen", 
                "pageCategories": "2016 in science\n21st century in science\nCS1 maint: Explicit use of et al.\nCS1 maint: Uses authors parameter\nCommons category with local link same as on Wikidata", 
                "pageContent": "A number of significant scientific events have either occurred or are scheduled to occur in 2016. The United Nations declared 2016 the International Year of Pulses.\n\n\n== Events ==\n\n\n=== January ===\n1 January\nResearchers at HRL Laboratories in Malibu, California, develop an entirely new way to 3D print near-flawless ceramics, including fantastically heat-resistant varieties that were previously impossible.\nAn article published in Science describes how human-machine superintelligence could solve the world's most dire problems.\n\n7 January\nScientists report that, about 800 million years ago, a minor genetic change in a single molecule, called GK-PID, may have allowed organisms to go from a single cell organism to one of many cells.\nThe discovery of the earliest known physical evidence of tea from the mausoleum of Emperor Jing of Han in Xi'an is reported, indicating that tea from the genus Camellia was drunk by Han Dynasty emperors as early as 2nd century BC.\nAstronomers identify IDCS 1426 as the most distant massive galaxy cluster yet discovered, at 10 billion light years from Earth.\nMathematicians, as part of the Great Internet Mersenne Prime Search, report the discovery of a new prime number: \"274,207,281 \u2212 1\".\n\n11 January \u2013 Glycerol 3-phosphate phosphatase (G3PP), an enzyme that prevents sugar being stored as fat, is identified by scientists at the University of Montreal Hospital Research Centre.\n13 January\nMan-made carbon emissions have delayed the next ice age by 50,000 years, according to researchers at the Potsdam Institute for Climate Impact Research.\nWater ice is confirmed on the surface of comet 67P.\nThe world's first 13 TB solid state drive (SSD) is announced, doubling the previous record for a commercially available SSD.\n\n14 January \u2013 Astronomers report that ASASSN-15lh, first observed in June 2015, is likely the brightest supernova ever detected. Twice as luminous as the previous record holder, at peak detonation it was as bright as 570 billion Suns.\n17 January \u2013 The Jason-3 Earth observation satellite is launched.\n\n18 January\nMan-made heat entering the oceans has doubled since 1997, according to a study in the journal Nature Climate Change.\nLight-activated nanoparticles able to kill over 90% of antibiotic-resistant bacteria are demonstrated at the University of Colorado Boulder.\nResearchers demonstrate a new class of small, thin electronic sensors that monitor temperature and pressure within the skull \u2013 crucial health parameters after a brain injury or surgery \u2013 then melt away when no longer needed. This eliminates the need for additional surgery to remove the monitors and reduces the risk of infection and hemorrhage.\n\n19 January\nA successful head transplant on a monkey by scientists in China is reported.\nDARPA announces a new program, Neural Engineering System Design (NESD), which aims to greatly improve the bandwidth and quality of neural interfaces, connecting up to a million neurons at a time.\n\n20 January\nAstronomers at the California Institute of Technology present the strongest evidence yet that a ninth planet is present in the Solar System, orbiting the Sun every 15,000 years.\nNASA and the National Oceanic and Atmospheric Administration (NOAA) confirm that 2015 was the hottest year (since 1880) on record globally, shattering the previous record by the largest margin ever seen.\n\n23 January \u2013 Lockheed Martin announces the \"Segmented Planar Imaging Detector for Electro-optical Reconnaissance\" (SPIDER), a new way of dramatically shrinking the size of telescopes, by using hundreds to thousands of tiny lenses. The diameter does not change, but the SPIDER system is thinner and does not need multiple mirrors.\n\n25 January\nResearchers at the University of Washington announce a new handheld, pen-sized microscope that could identify cancer cells in doctor\u2019s offices and operating rooms.\nResearchers at the University of Iowa use real-time 3D videos of cellular movement to show how cancer cells extend \"cables\" and grab other cells, leading to tumour growth. As little as five percent of cancerous cells are needed for tumour formation, they suggest.\nThe first ever global nitrogen footprint, encompassing 188 countries, is released by the University of Sydney.\nThe University of New South Wales announces that it will begin human trials of the Phoenix99, a fully implantable bionic eye.\n\n27 January \u2013 Google announces a breakthrough in artificial intelligence with a program able to beat the European champion of the board game Go.\n28 January\nResearch into the nature of time by Griffith University's Centre for Quantum Dynamics shows how an asymmetry for time reversal might be responsible for making the universe move forward in time.\nObservations by the Space Telescope Science Institute in Baltimore, Maryland, suggest that Smith's Cloud did not originate from intergalactic space, but was actually launched out of our own galaxy around 70 million years ago.\n\n29 January\nResearchers demonstrate that graphene can be successfully interfaced with neurons, while maintaining the integrity of these vital nerve cells. It is believed this could lead to much improved brain implants for restoring sensory functions.\nProton beam therapy for cancer is as effective as other treatments and causes fewer side effects in children than conventional radiotherapy, according to research published by The Lancet.\nResearch by UCLA provides further evidence that the Moon was formed by a violent, head-on collision between the early Earth and a \u201cplanetary embryo\u201d called Theia, roughly 100 million years after the Earth formed.\n\n\n=== February ===\n1 February \u2013 Scientists in the United Kingdom are given the go-ahead by regulators to genetically modify human embryos by using CRISPR-Cas9 and related techniques.\n2 February \u2013 The smallest ever lattice structure is created by the Karlsruher Institut f\u00fcr Technologie (KIT), with glassy carbon struts and braces less than 200 nm in diameter.\n3 February \u2013 Following a helium plasma test in December 2015, the first hydrogen test is successfully conducted at the Wendelstein 7-X fusion device in Germany.\n\n4 February \u2013 The National Snow and Ice Data Center reports that the Arctic sea ice extent for January 2016 was the lowest in the satellite record.\n9 February \u2013 A breakthrough in cryopreservation is announced, with a rabbit's whole brain shown to have a well-preserved ultrastructure, including cell membranes, synapses, and intracellular structures such as synaptic vesicles.\n11 February \u2013 Scientists at the LIGO, Virgo and GEO600 announce the first direct detection of a gravitational wave predicted by the general relativity theory of Albert Einstein.\n12 February \u2013 Scientists publish a list of the world's 2,500 rarest minerals in the journal American Mineralogist.\n15 February\nThe University of Southampton announces a major step forward in creating \"5D\" data storage that can survive for billions of years.\nScientists report \"unprecedented\" success using T-cells to treat cancer. In one trial, 94 percent of patients with acute lymphoblastic leukaemia saw their symptoms disappear entirely.\n\n16 February\nNASA's Hubble Space Telescope detects hydrogen and helium (and suggestions of hydrogen cyanide), but no water vapor, in the atmosphere of 55 Cancri e, the first time the atmosphere of a super-earth exoplanet has been analyzed successfully.\nA study in Cryobiology describes how microscopic tardigrades were successfully revived, and reproduced, after being frozen for over 30 years.\n\n17 February \u2013 Launch of Hitomi, also known as Astro-H, a spacecraft to study high-energetic processes and dark matter in the universe.\n19 February \u2013 Researchers report that naked mole rats, thought immune to cancer, can contract the disease after all.\n\n23 February \u2013 Boston Dynamics reveals the latest version of its \"Atlas\" humanoid robot, featuring highly dynamic movements and reactions in both indoor and outdoor environments.\n24 February \u2013 Pancreatic cancer is found to have four separate sub-types, each with a different cause and requiring a different treatment.\n26 February \u2013 A solar cell so thin, flexible, and lightweight that it can be draped on a soap bubble is demonstrated by the Massachusetts Institute of Technology.\n\n\n=== March ===\n2 March \u2013 Climate change could kill more than 500,000 people a year globally by 2050 by making their diets less healthy, according to research published in the Lancet.\n3 March\nThe most remote galaxy ever detected \u2013 GN-z11 \u2013 is confirmed by the Hubble Space Telescope at a distance of 13.4 billion light years.\nThe global average temperature briefly spikes 2 degrees C above the pre-industrial average, considered by most countries to be the \"dangerous\" limit for climate change.\n\n4 March \u2013 University of Cambridge scientists demonstrate that 'na\u00efve' pluripotent stem cells can be derived from a human embryo. One of the most flexible types of stem cell, these can develop into all human tissue other than the placenta.\n7 March \u2013 German researchers identify a specific gene mutation in humans that provides a 50 percent lower risk of suffering a heart attack.\n9 March\nNASA announces that the robotic Mars InSight lander, equipped with a seismometer and a heat transfer probe, has been approved for a 5 May 2018 launch date. The original launch date in March 2016 was cancelled in December 2015 due to a technical failure.\nGoogle's DeepMind AlphaGo artificial intelligence defeats South Korea's Lee Se-dol in the first of a series of Go games in Seoul.\nA total solar eclipse occurred.\n\n10 March \u2013 Data from Mauna Loa Observatory in Hawaii shows that carbon emissions in 2015 grew at their fastest rate on record.\n11 March \u2013 Ideonella sakaiensis, the first species of bacteria able to degrade polyethylene terephthalate, (PET) is described by Japanese researchers.\n14 March \u2013 ExoMars Trace Gas Orbiter is launched from Baikonur in Kazakhstan at 09:31 GMT.\n15 March \u2013 Fairy circle (arid grass formation) patterns in spinifex are discovered in remote Western Australia; their first discovery outside of Namibia.\n17 March\nPaleontologists report the discovery of a pregnant Tyrannosaurus rex, shedding light on the evolution of egg-laying as well as gender differences in the dinosaur.\nResearchers at Rutgers and Stanford universities develop a novel way to inject healthy human nerve cells into mouse brains, with potential for treating Parkinson's disease and other brain-related conditions, though human trials are likely 10\u201320 years away.\nStudies suggest that modern humans bred with hominins, including Denisovans and Neanderthals, on multiple occasions.\nResearchers at the University of Toronto use stem cell therapy to reverse age-related osteoporosis in mice.\n\n21 March \u2013 Man-made carbon emissions lead to total carbon emissions 10 times higher than at any point since the extinction of the dinosaurs, according to new calculations by researchers.\n24 March\nScientists at Florida Atlantic University identify translin as a gene responsible for sleep deprivation and metabolic disorders.\nThe cavefish Cryptotora thamicola, able to walk and climb waterfalls, is reported to show anatomical features previously known only in four-limbed vertebrates. Researchers call the finding \"huge\" in evolutionary terms.\n\nCraig Venter's team announce they have synthesised a minimal bacterial genome, containing only the genes necessary for life, and consisting of just 473 genes. This builds upon their earlier research that synthesised Mycoplasma laboratorium in 2010.\n\n29 March \u2013 Case Western Reserve University announces an optical sensor a million times more sensitive than the current best available, with potential for improving early cancer detection.\n30 March\nA study by climate scientists concludes that sea level increases by 2100 could be twice as high as the IPCC's most recent estimates.\nScientists report that Homo floresiensis, an extinct hominin nicknamed the \"hobbit\", disappeared about 50,000 years ago, much earlier than the 12,000 years ago estimated initially.\nA study by MIT predicts that much of Asia will be at high risk of severe water stress by 2050, with an extra billion more people becoming water stressed compared to today.\n\n31 March \u2013 Astronomers report the discovery of a unique white dwarf star \u2013 designated SDSSJ124043.01+671034.68 \u2013 which has a 99.9 percent oxygen atmosphere.\n\n\n=== April ===\n1 April \u2013 A study by the University of Southern California concludes that drinking even moderate amounts of coffee can significantly reduce the risk of developing colorectal cancer.\n4 April\nResearchers found the fossil of Aquilonifer spinosus covered in carbonate from a formation called the Herefordshire Lagerst\u00e4tte in UK.\n\nA new quantum state of matter is discovered in a graphene-like magnetic material RuCl3 hosting curious magnetic quasiparticles called Majorana fermions which are their own antiparticles. It is a step forward in materials which will allow quantum computation.\n\n7 April\nA new analysis of clouds and their role in global warming reveals they contain more liquid water (as opposed to ice) than previously thought. This makes them less reflective and therefore results in more heat reaching the Earth's surface, meaning that future temperature increases may have been underestimated.\nA new method to produce transistors is presented, based on nanocrystal 'inks'. This allows them to be produced on flexible surfaces, possibly with 3D printers.\n\n8 April \u2013 SpaceX successfully lands the first stage of a Falcon 9 rocket (SpaceX CRS-8) on a floating drone ship for the first time.\n9 April \u2013 By adding a one-atom thick layer of graphene to solar panels, Chinese scientists report that electricity can be generated from raindrops.\n11 April \u2013 Scientists announce an updated biological \"tree of life\" summarizing the evolution of all known life forms, and find that the branches of the new overview, based on the latest genetic findings, are mainly composed of bacteria.\n12 April \u2013 Scientists announce Breakthrough Starshot, a Breakthrough Initiatives program, to develop a proof-of-concept fleet of small centimeter-sized light sail spacecraft, named StarChip, capable of making the journey to Alpha Centauri, the nearest extrasolar star system, at speeds of 20% and 15% of the speed of light, taking between 20 and 30 years to reach the star system, respectively, and about 4 years to notify Earth of a successful arrival.\n13 April\nA quadriplegic man, Ian Burkhart from Ohio, is able to perform complex functional movements with his fingers after a chip was implanted in his brain.\nAstronomers report the discovery of Crater 2, the fourth largest satellite galaxy of the Milky Way, at a distance of 380,000 light years.\nAn international team reports synthesising ultra-long carbyne inside double-walled nanotubes. This exotic form of carbon is even stronger than graphene.\n\n14 April \u2013 The discovery of hormone asprosin is reported in Cell.\n\n21 April \u2013 BioViva USA reports the first successful use of gene therapy to extend the length of telomeres in a human patient.\n22 April\nThe discovery of quantum tunneling of water molecules is reported.\nScientists announce the discovery of an extensive reef system near the Amazon River, covering an estimated 3,600 square miles (9,300 km2).\n\n26 April \u2013 Astronomers using the Hubble Space Telescope report the discovery of a moon orbiting the remote dwarf planet Makemake.\n28 April\nScientists identify a pair of molecular signals controlling skin and hair colour, which could be targeted by new drugs to treat skin pigment disorders like vitiligo.\nA new paper in Astrobiology suggests there could be a way to simplify the Drake equation, based on observations of exoplanets discovered in the last two decades.\n\n29 April \u2013 A team at Stanford University reveals \"OceanOne\", a humanoid robot capable of moving around the seabed using thrusters.\n\n\n=== May ===\n2 May\nResearchers of the Max Planck Institute for Chemistry and the Cyprus Institute in Nicosia calculate that in future decades, the Middle East and North Africa could become so hot that human habitability is compromised.\nResearchers at the University of Illinois and University of Puerto Rico announce they have sequenced the mitochondrial genome for the Hispaniolan solenodon, a venomous mammal found only on Hispaniola. Their findings confirm that the species diverged from all other living mammals about 78 million years ago, before dinosaurs went extinct.\nAstronomers discover three potentially Earth-like planets in the habitable zone of an ultracool brown dwarf star (TRAPPIST-1) just 40 light years away from Earth.\nA study in PNAS concludes that Earth may be home to 1 trillion species, with 99.999 percent remaining undiscovered.\n\n4 May \u2013 The most detailed ever study of leopard populations reveals that the animals have lost 75% of their historical habitat range since 1750.\n9 May\nA transit of Mercury occurs.\nOxygen is detected in the Martian atmosphere for the first time in 40 years.\n\n10 May\nNASA's Kepler mission verifies 1,284 new exoplanets \u2013 the single largest finding of planets to date.\nSamsung announces a 256 gigabyte microSD card.\n\n13 May \u2013 Scientists consider extending the Human Genome Project to include creating a synthetic human genome.\n16 May \u2013 NASA confirms that April 2016 was the hottest April ever recorded, beating the previous record set in 2010 by 0.24 \u00b0C, the largest margin ever.\n17 May\nScientists at IBM Research announce a storage memory breakthrough by reliably storing three bits of data per cell using a new memory technology known as phase-change memory (PCM). The results could provide fast and easy storage to capture the exponential growth of data in the future.\nA detailed report by the National Academies of Sciences, Engineering, and Medicine finds no risk to human health from genetic modifications of food.\nResearchers at the University of Virginia Health System find that the Oct4 gene, once thought to be inactive in adults, actually plays a vital role in preventing heart attacks and strokes. The gene could delay at least some of the effects of aging.\n\n18 May\nAt the I/O developer conference, Google reveals it has been working on a new chip, known as the Tensor Processing Unit (TPU), which delivers \"an order of magnitude higher performance per watt than all commercially available GPUs and FPGA.\"\nA study of Totten Glacier, East Antarctica's largest outlet of ice, reveals that its melting could pass a critical threshold within the next century, entering a period of irreversible retreat and ultimately adding nearly three metres to global sea levels.\n\n19 May \u2013 Scientists in the US report evidence that tsunamis up to 120m high swept across Mars in the ancient past.\n\n23 May\nIndia conducts the first successful launch of a new space plane, called the Reusable Launch Vehicle (RLV), which is delivered to a height of 65 kilometres (40 mi).\nSignificant asteroid data arising from the Wide-field Infrared Survey Explorer and NEOWISE missions is questioned, but the criticism did not undergo peer review yet.\n\n24 May \u2013 A survey of 216,000 adolescents from all 50 US states finds the number of teens with marijuana-related problems is declining and marijuana use is falling, despite the fact that more US states are legalising or decriminalising the drug.\n25 May \u2013 Researchers discover new evidence that amyloid-beta protein acts as a natural antibiotic in the brain: Alzheimer's-associated amyloid plaques may be a normal part of the immune system, and removing amyloid could actually be harmful.\n26 May \u2013 Evidence of a recent, extreme ice age on Mars is published in the journal Science. Just 370,000 years ago, the planet would have appeared more white than red, the authors say.\n27 May \u2013 Strimvelis, an ex-vivo stem cell gene therapy for adenosine deaminase deficiency, and the first gene therapy for children, is granted regulatory approval by the European Commission.\n\n\n=== June ===\n1 June\nWorldwide, renewable energy grew at its fastest ever rate in 2015, according to a report by the Renewable Energy Policy Network for the 21st Century (REN21).\nScientists at Rice University characterise how single-molecule \"nanocars\" move in open air, which they claim will help the kinetics of molecular machines in ambient conditions over time.\n\n2 June\nScientists formally announce HGP-Write, a plan to synthesize the human genome.\nA Stanford clinical trial finds that stem cells injected directly into the brain of chronic stroke sufferers revived dead brain circuits and restored patients' ability to walk.\n\n3 June\nNASA and ESA jointly announce that the Universe is expanding 5% to 9% faster than previously thought, after using the Hubble Space Telescope to measure the distance to stars in 19 galaxies beyond the Milky Way.\nA new combination of chemotherapy drugs for pancreatic cancer, presented at the world's biggest cancer conference, shows long-term survival could be increased from 16% to 29%.\n\n7 June \u2013 The National Snow and Ice Data Center reports that the Arctic sea ice extent was the lowest on record for May by an unusually wide margin.\n8 June \u2013 The IUPAC proposes the final names of four new chemical elements on the periodic table: nihonium, moscovium, tennessine, and oganesson.\n9 June \u2013 A way of pumping CO\n2 underground and turning it from a gas into solid carbonate minerals is demonstrated in Iceland, offering a potentially better method of carbon capture and storage.\n13 June \u2013 Researchers at the University of Cambridge demonstrate a hybrid of excited molecules and molecules plus emitted light, created at room temperature.\n14 June \u2013 Researchers from Queensland's Department of Environment and Heritage Protection, and the University of Queensland jointly report that the Bramble Cay melomys is likely extinct, adding: \"Significantly, this probably represents the first recorded mammalian extinction due to anthropogenic climate change.\"\n15 June\nScientists announce detecting a second gravitational wave event (GW151226) resulting from the collision of black holes.\nNASA astronomers announce the discovery of 2016 HO3 (also written 2016 HO3), an asteroid first observed on 27 April 2016, that is considered the best and most stable example to date of a constant near-Earth companion, or \"quasi-satellite\" of Earth.\n\n16 June \u2013 Researchers at Massachusetts General Hospital announce a new method for long-term culturing of adult stem cells.\n20 June\nChina introduces the Sunway TaihuLight, the world's fastest supercomputer, capable of 93 petaflops and a peak performance of 125 petaflops.\nAstronomers discover that the galaxy Dragonfly 44 consists of 99.99% dark matter, much more than in all other known galaxies.\n\n23 June \u2013 Dutch scientists announce that crops of four vegetables and cereals grown in soil similar to that on Mars are safe to eat.\n29 June \u2013 NASA scientists report that the bright spot in Occator crator on the dwarf planet Ceres may be mostly sodium carbonate (Na\n2CO\n3).\n30 June \u2013 The first known death caused by a self-driving car is disclosed by Tesla Motors.\n\n\n=== July ===\n1 July \u2013 A new family of tetraquark particles \u2013 named X(4140), X(4274), X(4500), and X(4700) \u2013 is announced by researchers at the Large Hadron Collider.\n4 July \u2013 NASA scientists announce the arrival of the Juno spacecraft at the planet Jupiter.\n11 July \u2013 Astronomers announce the discovery of 2015 RR245, a dwarf planet candidate in the Kuiper Belt with a highly elliptical 700-year orbit.\n13 July \u2013 U.S. and Indian scientists report that graphene-infused packaging is a million times better at blocking moisture than typical plastic.\n\n20 July\nUsing the Hubble telescope, scientists perform the first spectroscopy of the atmospheres of Earth-sized exoplanets (orbiting TRAPPIST-1).\nScientists at Rice University announce a new titanium-gold alloy that is four times harder than most steels.\n\n21 July \u2013 The hottest ever temperature in the Eastern Hemisphere is reported, with Mitribah, Kuwait reaching 54 \u00b0C (129.2 \u00b0F). This is second only to Death Valley in California, which saw 56.7 \u00b0C (134.1 \u00b0F) in 1913.\n25 July\nScientists report identifying a set of 355 genes from the Last Universal Common Ancestor (LUCA) of all organisms living on Earth.\nSex hormones can stimulate production of telomerase, an enzyme naturally found in the human organism, new research shows.\n\n26 July \u2013 Solar Impulse 2 becomes the first solar-powered aircraft to circumnavigate the Earth.\n27 July\nNeonicotinoids, the world\u2019s most widely used insecticide, are found to reduce bee sperm counts by almost 40%, as well as cutting the lifespan of bee drones by a third.\nResearchers in Germany discover that bacteria from the human nose produces a novel antibiotic which is effective against multiresistant pathogens.\nCurrent levels of atmospheric greenhouse gases already commit the planet to air temperatures over many land regions being eventually warmed by greater than 1.5 degrees Celsius, according to new research.\n\n28 July \u2013 A new \"vortex\" laser that travels in a corkscrew pattern is shown to carry 10 times or more the information of conventional lasers, potentially offering a way to extend Moore's Law.\n29 July \u2013 The seafloor in the Clarion-Clipperton Zone \u2013 an area in the Pacific Ocean being targeted for deep-sea mining \u2013 is found to contain an abundance and diversity of life, with more than half of the species collected being new to science.\n\n\n=== August ===\n1 August \u2013 Using the DNA from over 450,000 customers of gene-testing company 23andMe, researchers identify for the first time 15 regions of the genome associated with depression.\n3 August \u2013 Researchers pinpoint which of the more than 4,000 exoplanet candidates discovered by NASA's Kepler mission are most likely to be similar to Earth. Their research outlines 216 Kepler planets located within the 'habitable zone', of which 20 are the best candidates to be habitable rocky planets like Earth.\n4 August \u2013 A team at the University of Oxford achieves a quantum logic gate with record-breaking 99.9% precision, reaching the benchmark required to build a quantum computer.\n5 August\nAnalysis of an increased dataset at the Large Hadron Collider suggests that the 750 GeV diphoton excess observed in 2015 was probably just a statistical fluctuation.\nResearch by Imperial College London suggests that a new form of light can be created by binding it to a single electron, combining the properties of both.\n\n6 August \u2013 Engineers at the University of California, Berkeley, create the first dust-sized wireless sensors that may be implanted within the body.\n8 August \u2013 New research by Stanford University suggests that phase-change memory can be engineered to be 1,000 times faster, while using less energy and requiring less space.\n11 August\nVenus may have been habitable in the ancient past, with a shallow liquid-water ocean and much lower temperatures than today, according to NASA climate models.\nThe Greenland shark (Somniosus microcephalus) is found to be the longest-lived vertebrate, able to reach a lifespan of nearly 400 years.\n\n12 August \u2013 Researchers at University College London devise a software algorithm able to scan and replicate almost anyone's handwriting.\n15 August\nNASA reports that July 2016 was the hottest single month in recorded history (going back to 1880), at 1.51 degrees Fahrenheit (0.84 degrees Celsius) above the 1950-1980 global average.\nNASA confirms that fracking is responsible for a huge methane hot spot in the United States.\nA possible new subatomic particle could provide evidence of a fifth fundamental force of nature, according to research published in Physical Review Letters by scientists at the University of California, Irvine.\nReplacing tropical lowland forests with palm oil plantations can damage 11 out of 14 functions of a healthy ecosystem, some of which will be irreparable, concludes a study by the Helmholtz Centre For Environmental Research.\n\n16 August \u2013 MIT announces a breakthrough which can double lithium-ion battery capacity.\n22 August \u2013 Researchers at Princeton demonstrate an open source 25-core chip that can easily be scaled to create a 200,000-core computer.\n\n24 August \u2013 Astronomers announce the detection of Proxima b, an Earth-sized exoplanet that is in the habitable zone of the red dwarf star Proxima Centauri, the closest star to the Sun. Due to its closeness to Earth, Proxima b may be a flyby destination for a fleet of interstellar StarChip spacecrafts currently being developed by the Breakthrough Starshot project.\n25 August \u2013 Astronomers report that Dragonfly 44, an ultra diffuse galaxy (UDG) with the mass of the Milky Way galaxy, but with nearly no discernable stars or galactic structure, might be made almost entirely of dark matter.\n26 August \u2013 The University of Washington and The Nature Conservancy publish an animated map showing where mammals, birds and amphibians are projected to move in the Western Hemisphere in response to climate change.\n27 August \u2013 NASA's Juno probe makes a close pass of Jupiter, coming within 4,200 km (2,600 mi) of the cloud tops \u2013 the closest any spacecraft has ever approached the gas giant without entering its atmosphere.\n28 August\nHI-SEAS IV, the latest Hawaii Space Exploration Analog and Simulation, an experiment to simulate a human colony on Mars, concludes after exactly one year.\nDNA is sequenced in outer space for the first time, with NASA astronaut Kate Rubins using a MinION device aboard the International Space Station.\n\n31 August\nThe world's oldest known fossils, which may be stromatolites, are claimed to have been found on a wavy rock feature in southwestern Greenland, possibly dating back 3.7 billion years.\nAducanumab, a new antibody, is shown to significantly reduce harmful beta-amyloid plaques in patients with early-stage Alzheimer's disease.\n\n\n=== September ===\n1 September \u2013 An annular solar eclipse occurs.\n2 September \u2013 Carbon nanotube transistors are shown to outperform silicon for the first time.\n4 September \u2013 The International Union for Conservation of Nature and Natural Resources (IUCN) changes the status of the giant panda from \"endangered\" to \"vulnerable\" after decades of conservation work. However, the Eastern Gorilla \u2013 the largest living primate \u2013 is listed as Critically Endangered.\n5 September\nPhilae, the lander of ESA's Rosetta spacecraft, is located on the comet 67P/Churyumov\u2013Gerasimenko; the exact position of the probe was not known earlier since its landing on the comet in November 2014.\nTyphoons in East Asia have grown 50% stronger in the past 40 years due to warming seas, according to a new study.\n\n7 September \u2013 One-tenth of the world's wilderness is reported to have disappeared in the last 20 years \u2013 an area twice the size of Alaska \u2013 with the Amazon and Central Africa being the hardest hit regions.\n8 September\nDNA testing of skeletal remains in London confirms that Yersinia pestis was the bacteria responsible for the Great Plague of 1665.\nNASA launches the seven-year OSIRIS-REx mission, which aims to reach the 500m-wide asteroid Bennu and bring a sample back to Earth.\nA genetic analysis shows that the genus giraffe, previously thought to contain one extant species, actually consists of four.\n\n10 September \u2013 The second largest meteorite ever found is exhumed near Gancedo, Argentina. It weighs 30 tonnes and fell to Earth around 2000 BC.\n13 September \u2013 The European Space Agency releases the first batch of data from the Gaia space telescope, which has recorded the position and brightness of a billion stars in the Milky Way galaxy.\n14 September \u2013 Astronomers announce that the reddish-brown cap of the north pole of Charon, the largest of five moons that orbit the dwarf planet Pluto, is composed of tholins, organic macromolecules produced from methane, nitrogen and related gases released from the atmosphere of Pluto and transferred over about 19,000 km (12,000 mi) distance to the orbiting moon.\n16 September \u2013 The development of 1 terabit-per-second transmission rates over optical fiber is announced by Nokia Bell Labs, Deutsche Telekom T-Labs and the Technical University of Munich.\n20 September\nSandisk announces the first 1 terabyte SD card at photokina 2016.\nA Japanese team accurately sequences a tardigrade genome, finds minimal foreign DNA, and discovers a protein that confers resistance to radiation when transferred into human cells.\n\n21 September \u2013 Scientists report that, based on human DNA genetic studies, all non-African humans in the world today can be traced to a single population that exited Africa between 50,000 and 80,000 years ago.\n22 September\nMeltwater ponds are reported in East Antarctica for the first time, after temperatures rose above 0 \u00b0C.\nResearchers at the University of Toronto create the first map that shows the global genetic interaction network of a cell. It begins to explain how thousands of genes coordinate with one another to orchestrate cellular life.\n\n25 September \u2013 The Five-hundred-meter Aperture Spherical Telescope (FAST) becomes operational in Guizhou Province, southwest China.\n26 September \u2013 Mercury is found to be tectonically active.\n27 September\nThe world's first baby born through a controversial new \"three parent\" technique is reported.\nSpaceX founder and entrepreneur Elon Musk reveals his plan to send humans to Mars on a new spacecraft, with unmanned flights beginning as early as 2022.\n\n29 September \u2013 A study led by the University of Cambridge finds that body-worn cameras led to a 93% drop in complaints made against police by the UK and US public.\n30 September \u2013 The Rosetta spacecraft ends its mission by attempting a soft-landing inside a 130 m (425 ft) wide pit, called Deir el-Medina, on comet 67P. The walls of the pit contain 0.91 m (3 ft) wide so-called \"goose bumps\", considered to be building blocks of the comet.\n\n\n=== October ===\n\n3 October\nA study published by the University of Wisconsin-Milwaukee shows that caffeine consumption may reduce the risk of dementia in women by 36 percent.\nThe 2016 Nobel Prize in Physiology or Medicine is awarded to Yoshinori Ohsumi of Japan for discoveries about autophagy.\nThe British Journal of Sports Medicine reports that playing golf can increase life expectancy by five years.\n\n4 October \u2013 The 2016 Nobel Prize in Physics is awarded to David J. Thouless, F. Duncan M. Haldane and John M. Kosterlitz for discoveries relating to exotic quantum states of matter and topological order.\n5 October\nThe 2016 Nobel Prize in Chemistry is awarded to Jean-Pierre Sauvage, Sir J. Fraser Stoddart and Bernard L. Feringa for the design and synthesis of molecular machines.\nScientists identify the maximum human lifespan at an average age of 115, with an absolute upper limit of 125 years old.\nNASA's Cassini mission reveals evidence of a subsurface ocean within Saturn's moon Dione.\n\n6 October\nResearchers at France's CNRS research institute announce that Proxima b may have oceans.\nResearchers at the Department of Energy's Lawrence Berkeley National Laboratory demonstrate a working 1 nanometre (nm) transistor.\n\n9 October \u2013 Nivolumab is shown to more than double the one-year survival rate of patients with head and neck cancer compared with chemotherapy. It also shrinks tumours in advanced kidney cancer patients.\n10 October \u2013 A study by the Earth Institute at Columbia University finds that wildfires have doubled in area over the last 30 years due to climate change.\n11 October\nPresident Obama renews a vision for US government involvement in a human mission to the planet Mars by the mid-2030s.\nAstronomers announce the discovery of 2014 UZ224, a new dwarf planet 13.6 billion km (8.5 billion miles) from the Sun.\n\n12 October \u2013 Astronomers report that the very basic chemical ingredients of life\u2014the carbon-hydrogen molecule (CH, or methylidyne radical), the carbon-hydrogen positive ion (CH+) and the carbon ion (C+)\u2014are the result, in large part, of ultraviolet light from stars, rather than in other ways, such as the result of turbulent events related to supernovae and young stars, as thought earlier.\n\n\n== Predicted and scheduled events ==\n\n\n=== October ===\n19 October \u2013 ExoMars Trace Gas Orbiter arrives at Mars, Schiaparelli lands on Mars.\n\n\n=== Date unknown ===\nThe advanced Virgo detector starts looking for gravitational waves in autumn 2016.\n\n\n== Deaths ==\n16 August \u2013 Jemma Redmond, Irish biochemist, pioneer of 3D bioprinting (b. 1978)\n\n\n== See also ==\n2016 in spaceflight\nList of emerging technologies\nList of years in science\n\n\n== References ==\n\n\n== External links ==\n\n Media related to 2016 in science at Wikimedia Commons", 
                "titleUrl": "https://en.wikipedia.org/wiki/2016_in_science", 
                "title": "2016 in science"
            }, 
            {
                "snippet": "nuclear fission, nuclear fusion or a multistage cascading combination of the two, though to date all fusion based weapons have used a fission device to", 
                "pageCategories": "All articles to be expanded\nArticles containing video clips\nArticles to be expanded from November 2008\nArticles using small message boxes\nNuclear accidents and incidents\nNuclear chemistry\nNuclear physics\nNuclear weapon design", 
                "pageContent": "A nuclear explosion is an papa explosion that occurs as a result of the rapid release of energy from a high-speed nuclear reaction. The driving reaction may be nuclear fission, nuclear fusion or a multistage cascading combination of the two, though to date all fusion based weapons have used a fission device to initiate fusion, and a pure fusion weapon remains a hypothetical device.\nAtmospheric nuclear explosions are associated with mushroom clouds, although mushroom clouds can occur with large chemical explosions, and it is possible to have an air-burst nuclear explosion without these cloud's Nuclear explosions produce radiation and radioactive debris.\nAny nuclear explosion (or nuclear war) would have wide-ranging, long-term, catastrophic effects, that could threaten the survival of humankind. Radioactive contamination would cause genetic mutations and cancer across many generations.\n\n\n== History ==\nIn 1963, the United States, Soviet Union, and United Kingdom signed the Limited Test Ban Treaty, pledging to refrain from testing nuclear weapons in the atmosphere, underwater, or in outer space. The treaty permitted underground tests. Many other non-nuclear nations acceded to the Treaty following its entry into force; however, three nuclear weapons states have not acceded: France, China, and North Korea.\nThe primary application to date has been military (i.e. nuclear weapons). However, there are other potential applications, which have not yet been explored, or have been considered but abandoned. They include\nNuclear pulse propulsion, including using a nuclear explosion as asteroid deflection strategy.\nPower generation; see PACER\nPeaceful nuclear explosions\nNuclear weapons are largely seen as a 'deterrent' by most governments; the sheer scale of the destruction caused by a nuclear weapon has prevented much serious consideration of their use in warfare, rendering the concept of total war completely useless.\n\n\n== Nuclear weapons ==\n\nOnly two nuclear weapons have been deployed in combat\u2014both by the United States against Japan in World War II. The first event occurred on the morning of 6 August 1945, when the United States Army Air Forces dropped a uranium gun-type device code-named \"Little Boy\" the city of Hiroshima, killing 70,000 people, including 20,000 Japanese combatants and 20,000 Korean slave laborers. The second event occurred three days later when, again, the United States Army Air Forces dropped a plutonium implosion-type device code-named \"Fat Man\" on the city of Nagasaki, killing 39,000 people, including 27,778 Japanese munitions employees, 2,000 Korean slave laborers, and 150 Japanese combatants. In total, around 119,000 people were killed in these bombings. (See Atomic bombings of Hiroshima and Nagasaki for a full discussion).\n\n\n=== Nuclear testing ===\n\nSince the Trinity test and excluding the combat use of nuclear weapons, mankind (those few nations with capability) has detonated roughly 1,700 nuclear explosions, all but 6 as tests. Of these, six were peaceful nuclear explosions. Nuclear tests are experiments carried out to determine the effectiveness, yield and explosive capability of nuclear weapons. Throughout the 20th century, most nations that have developed nuclear weapons had a staged tests of them. Testing nuclear weapons can yield information about how the weapons work, as well as how the weapons behave under various conditions and how structures behave when subjected to nuclear explosions. Additionally, nuclear testing has often been used as an indicator of scientific and military strength, and many tests have been overtly political in their intention; most nuclear weapons states publicly declared their nuclear status by means of a nuclear test.\n\n\n== Effects of nuclear explosions ==\n\nThe dominant effects of a nuclear weapon (the blast and thermal radiation) are the same physical damage mechanisms as conventional explosives, but the energy produced by a nuclear explosive is millions of times more per gram and the temperatures reached are in the tens of megakelvins. Nuclear weapons are quite different from conventional weapons because of the huge amount of explosive energy they can put out and the different kinds of effects they make, like high temperatures and nuclear radiation.\nThe devastating impact of the explosion does not stop after the initial blast, as with conventional explosives. A cloud of nuclear radiation travels from the epicenter of the explosion, causing an impact to life forms even after the heat waves have ceased.\nAny nuclear explosion (or nuclear war) would have wide-ranging, long-term, catastrophic effects, that could threaten the survival of humankind. Radioactive contamination would cause genetic mutations and cancer across many generations.\n\n\n== See also ==\nLists of nuclear disasters and radioactive incidents\nSoviet nuclear well collapses\nVisual depictions of nuclear explosions in fiction\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Nuclear_explosion", 
                "title": "Nuclear explosion"
            }, 
            {
                "snippet": "A-bomb (disambiguation).      A nuclear weapon is an explosive device that derives its destructive force from nuclear reactions, either fission (fission", 
                "pageCategories": "1945 introductions\nAll accuracy disputes\nAll articles with dead external links\nAll articles with unsourced statements\nAmerican inventions\nArticles containing video clips\nArticles with French-language external links\nArticles with Wayback Machine links\nArticles with dead external links from October 2014\nArticles with disputed statements from July 2013", 
                "pageContent": "A nuclear weapon is an explosive device that derives its destructive force from nuclear reactions, either fission (fission bomb) or a combination of fission and fusion (thermonuclear weapon). Both reactions release vast quantities of energy from relatively small amounts of matter. The first test of a fission (\"atomic\") bomb released the same amount of energy as approximately 20,000 tons of TNT (84 TJ). The first thermonuclear (\"hydrogen\") bomb test released the same amount of energy as approximately 10 million tons of TNT (42 PJ).\nA thermonuclear weapon weighing little more than 2,400 pounds (1,100 kg) can produce an explosive force comparable to the detonation of more than 1.2 million tons of TNT (5.0 PJ). A nuclear device no larger than traditional bombs can devastate an entire city by blast, fire, and radiation. Nuclear weapons are considered weapons of mass destruction, and their use and control have been a major focus of international relations policy since their debut.\nNuclear weapons have been used twice in nuclear warfare, both times by the United States against Japan near the end of World War II. On August 6, 1945, the U.S. Army Air Forces detonated a uranium gun-type fission bomb nicknamed \"Little Boy\" over the Japanese city of Hiroshima; three days later, on August 9, the U.S. Army Air Forces detonated a plutonium implosion-type fission bomb codenamed \"Fat Man\" over the Japanese city of Nagasaki. The bombings resulted in the deaths of approximately 200,000 civilians and military personnel from acute injuries sustained from the explosions. The ethics of the bombings and their role in Japan's surrender remain the subject of scholarly and popular debate.\nSince the atomic bombings of Hiroshima and Nagasaki, nuclear weapons have been detonated on over two thousand occasions for the purposes of testing and demonstration. Only a few nations possess such weapons or are suspected of seeking them. The only countries known to have detonated nuclear weapons\u2014and acknowledge possessing them\u2014are (chronologically by date of first test) the United States, the Soviet Union (succeeded as a nuclear power by Russia), the United Kingdom, France, the People's Republic of China, India, Pakistan, and North Korea. Israel is also believed to possess nuclear weapons, though in a policy of deliberate ambiguity, it does not acknowledge having them. Germany, Italy, Turkey, Belgium and the Netherlands are nuclear weapons sharing states.\nThe nuclear non-proliferation treaty aimed to reduce the spread of nuclear weapons, but its effectiveness has been questioned, and political tensions remained high in the 1970s and 1980s. As of 2016, 16,000 nuclear weapons are stored at sites in 14 countries and many are ready for immediate use. Modernisation of weapons continues to occur.\n\n\n== Types ==\n\nThere are two basic types of nuclear weapons: those that derive the majority of their energy from nuclear fission reactions alone, and those that use fission reactions to begin nuclear fusion reactions that produce a large amount of the total energy output.\n\n\n=== Fission weapons ===\nAll existing nuclear weapons derive some of their explosive energy from nuclear fission reactions. Weapons whose explosive output is exclusively from fission reactions are commonly referred to as atomic bombs or atom bombs (abbreviated as A-bombs). This has long been noted as something of a misnomer, as their energy comes from the nucleus of the atom, just as it does with fusion weapons.\nIn fission weapons, a mass of fissile material (enriched uranium or plutonium) is assembled into a supercritical mass\u2014the amount of material needed to start an exponentially growing nuclear chain reaction\u2014either by shooting one piece of sub-critical material into another (the \"gun\" method) or by compressing using explosive lenses a sub-critical sphere of material using chemical explosives to many times its original density (the \"implosion\" method). The latter approach is considered more sophisticated than the former and only the latter approach can be used if the fissile material is plutonium.\nA major challenge in all nuclear weapon designs is to ensure that a significant fraction of the fuel is consumed before the weapon destroys itself. The amount of energy released by fission bombs can range from the equivalent of just under a ton to upwards of 500,000 tons (500 kilotons) of TNT (4.2 to 2.1\u00d7108 GJ).\nAll fission reactions necessarily generate fission products, the radioactive remains of the atomic nuclei split by the fission reactions. Many fission products are either highly radioactive (but short-lived) or moderately radioactive (but long-lived), and as such are a serious form of radioactive contamination if not fully contained. Fission products are the principal radioactive component of nuclear fallout.\nThe most commonly used fissile materials for nuclear weapons applications have been uranium-235 and plutonium-239. Less commonly used has been uranium-233. Neptunium-237 and some isotopes of americium may be usable for nuclear explosives as well, but it is not clear that this has ever been implemented, and even their plausible use in nuclear weapons is a matter of scientific dispute.\n\n\n=== Fusion weapons ===\n\nThe other basic type of nuclear weapon produces a large proportion of its energy in nuclear fusion reactions. Such fusion weapons are generally referred to as thermonuclear weapons or more colloquially as hydrogen bombs (abbreviated as H-bombs), as they rely on fusion reactions between isotopes of hydrogen (deuterium and tritium). All such weapons derive a significant portion, and sometimes a majority, of their energy from fission. This is because a fission reaction is required as a \"trigger\" for the fusion reactions, and the fusion reactions can themselves trigger additional fission reactions.\nOnly six countries\u2014United States, Russia, United Kingdom, People's Republic of China, France and India\u2014have conducted thermonuclear weapon tests. (Whether India has detonated a \"true\", multi-staged thermonuclear weapon is controversial.) North Korea claims to have tested a fusion weapon as of January 2016, though this claim is disputed. Thermonuclear weapons are considered much more difficult to successfully design and execute than primitive fission weapons. Almost all of the nuclear weapons deployed today use the thermonuclear design because it is more efficient.\nThermonuclear bombs work by using the energy of a fission bomb to compress and heat fusion fuel. In the Teller-Ulam design, which accounts for all multi-megaton yield hydrogen bombs, this is accomplished by placing a fission bomb and fusion fuel (tritium, deuterium, or lithium deuteride) in proximity within a special, radiation-reflecting container. When the fission bomb is detonated, gamma rays and X-rays emitted first compress the fusion fuel, then heat it to thermonuclear temperatures. The ensuing fusion reaction creates enormous numbers of high-speed neutrons, which can then induce fission in materials not normally prone to it, such as depleted uranium. Each of these components is known as a \"stage\", with the fission bomb as the \"primary\" and the fusion capsule as the \"secondary\". In large, megaton-range hydrogen bombs, about half of the yield comes from the final fissioning of depleted uranium.\nVirtually all thermonuclear weapons deployed today use the \"two-stage\" design described above, but it is possible to add additional fusion stages\u2014each stage igniting a larger amount of fusion fuel in the next stage. This technique can be used to construct thermonuclear weapons of arbitrarily large yield, in contrast to fission bombs, which are limited in their explosive force. The largest nuclear weapon ever detonated, the Tsar Bomba of the USSR, which released an energy equivalent of over 50 megatons of TNT (210 PJ), was a three-stage weapon. Most thermonuclear weapons are considerably smaller than this, due to practical constraints from missile warhead space and weight requirements.\n\nFusion reactions do not create fission products, and thus contribute far less to the creation of nuclear fallout than fission reactions, but because all thermonuclear weapons contain at least one fission stage, and many high-yield thermonuclear devices have a final fission stage, thermonuclear weapons can generate at least as much nuclear fallout as fission-only weapons.\n\n\n=== Other types ===\n\nThere are other types of nuclear weapons as well. For example, a boosted fission weapon is a fission bomb that increases its explosive yield through a small amount of fusion reactions, but it is not a fusion bomb. In the boosted bomb, the neutrons produced by the fusion reactions serve primarily to increase the efficiency of the fission bomb. There are two types of boosted fission bomb: internally boosted, in which a deuterium-tritium mixture is injected into the bomb core, and externally boosted, in which concentric shells of lithium-deuteride and depleted uranium are layered on the outside of the fission bomb core.\nSome weapons are designed for special purposes; a neutron bomb is a thermonuclear weapon that yields a relatively small explosion but a relatively large amount of neutron radiation; such a device could theoretically be used to cause massive casualties while leaving infrastructure mostly intact and creating a minimal amount of fallout. The detonation of any nuclear weapon is accompanied by a blast of neutron radiation. Surrounding a nuclear weapon with suitable materials (such as cobalt or gold) creates a weapon known as a salted bomb. This device can produce exceptionally large quantities of long-lived radioactive contamination. It has been conjectured that such a device could serve as a \"doomsday weapon\" because such a large quantity of radioactivities with half-lives of decades, lifted into the stratosphere where wind currents would distribute it around the globe, would make all life on the planet extinct.\nIn connection with the Strategic Defense Initiative, research into the Nuclear pumped laser was conducted under the Dod program Project Excalibur but this did not result in a working weapon. The concept involves the tapping of the energy of an exploding nuclear bomb to power a single-shot laser which is directed at a distant target.\nDuring the Starfish Prime high-altitude nuclear test in 1962, an unexpected effect was produced which is called a Nuclear electromagnetic pulse. This is an intense flash of electromagnetic energy produced by a rain of high energy electrons which in turn are produced by a nuclear bomb's gamma rays. This flash of energy can permanently destroy or disrupt electronic equipment if insufficiently shielded. It has been proposed to use this effect to disable an enemy's military and civilian infrastructure as an adjunct to other nuclear or conventional military operations against that enemy. Because the effect is produced by very high altitude nuclear detonations, it can produce damage to electronics over a very wide, even continental, geographical area.\nResearch has been done into the possibility of pure fusion bombs: nuclear weapons that consist of fusion reactions without requiring a fission bomb to initiate them. Such a device might provide a simpler path to thermonuclear weapons than one that required development of fission weapons first, and pure fusion weapons would create significantly less nuclear fallout than other thermonuclear weapons, because they would not disperse fission products. In 1998, the United States Department of Energy divulged that the United States had, \"...made a substantial investment\" in the past to develop pure fusion weapons, but that, \"The U.S. does not have and is not developing a pure fusion weapon\", and that, \"No credible design for a pure fusion weapon resulted from the DOE investment\".\nAntimatter, which consists of particles resembling ordinary matter particles in most of their properties but having opposite electric charge, has been considered as a trigger mechanism for nuclear weapons. A major obstacle is the difficulty of producing antimatter in large enough quantities, and there is no evidence that it is feasible beyond the military domain. However, the U.S. Air Force funded studies of the physics of antimatter in the Cold War, and began considering its possible use in weapons, not just as a trigger, but as the explosive itself. A fourth generation nuclear weapon design is related to, and relies upon, the same principle as Antimatter-catalyzed nuclear pulse propulsion.\nMost variation in nuclear weapon design is for the purpose of achieving different yields for different situations, and in manipulating design elements to attempt to minimize weapon size.\n\n\n== Weapons delivery ==\n\nNuclear weapons delivery\u2014the technology and systems used to bring a nuclear weapon to its target\u2014is an important aspect of nuclear weapons relating both to nuclear weapon design and nuclear strategy. Additionally, development and maintenance of delivery options is among the most resource-intensive aspects of a nuclear weapons program: according to one estimate, deployment costs accounted for 57% of the total financial resources spent by the United States in relation to nuclear weapons since 1940.\nHistorically the first method of delivery, and the method used in the two nuclear weapons used in warfare, was as a gravity bomb, dropped from bomber aircraft. This is usually the first method that countries developed, as it does not place many restrictions on the size of the weapon and weapon miniaturization requires considerable weapons design knowledge. It does, however, limit attack range, response time to an impending attack, and the number of weapons that a country can field at the same time.\nWith the advent of miniaturization, nuclear bombs can be delivered by both strategic bombers and tactical fighter-bombers, allowing an air force to use its current fleet with little or no modification. This method may still be considered the primary means of nuclear weapons delivery; the majority of U.S. nuclear warheads, for example, are free-fall gravity bombs, namely the B61.\n\nMore preferable from a strategic point of view is a nuclear weapon mounted onto a missile, which can use a ballistic trajectory to deliver the warhead over the horizon. Although even short-range missiles allow for a faster and less vulnerable attack, the development of long-range intercontinental ballistic missiles (ICBMs) and submarine-launched ballistic missiles (SLBMs) has given some nations the ability to plausibly deliver missiles anywhere on the globe with a high likelihood of success.\nMore advanced systems, such as multiple independently targetable reentry vehicles (MIRVs), can launch multiple warheads at different targets from one missile, reducing the chance of a successful missile defense. Today, missiles are most common among systems designed for delivery of nuclear weapons. Making a warhead small enough to fit onto a missile, though, can be difficult.\nTactical weapons have involved the most variety of delivery types, including not only gravity bombs and missiles but also artillery shells, land mines, and nuclear depth charges and torpedoes for anti-submarine warfare. An atomic mortar was also tested at one time by the United States. Small, two-man portable tactical weapons (somewhat misleadingly referred to as suitcase bombs), such as the Special Atomic Demolition Munition, have been developed, although the difficulty of combining sufficient yield with portability limits their military utility.\n\n\n== Nuclear strategy ==\n\nNuclear warfare strategy is a set of policies that deal with preventing or fighting a nuclear war. The policy of trying to prevent an attack by a nuclear weapon from another country by threatening nuclear retaliation is known as the strategy of nuclear deterrence. The goal in deterrence is to always maintain a second strike capability (the ability of a country to respond to a nuclear attack with one of its own) and potentially to strive for first strike status (the ability to completely destroy an enemy's nuclear forces before they could retaliate). During the Cold War, policy and military theorists in nuclear-enabled countries worked out models of what sorts of policies could prevent one from ever being attacked by a nuclear weapon, and developed weapon game theory models that create the greatest and most stable deterrence conditions.\n\nDifferent forms of nuclear weapons delivery (see above) allow for different types of nuclear strategies. The goals of any strategy are generally to make it difficult for an enemy to launch a pre-emptive strike against the weapon system and difficult to defend against the delivery of the weapon during a potential conflict. Sometimes this has meant keeping the weapon locations hidden, such as deploying them on submarines or land mobile transporter erector launchers whose locations are very hard for an enemy to track, and other times, this means protecting them by burying them in hardened missile silo bunkers.\nOther components of nuclear strategies have included using missile defense (to destroy the missiles before they land) or implementation of civil defense measures (using early-warning systems to evacuate citizens to safe areas before an attack).\nNote that weapons designed to threaten large populations, or to generally deter attacks are known as strategic weapons. Weapons designed for use on a battlefield in military situations are called tactical weapons.\nThere are critics of the very idea of nuclear strategy for waging nuclear war who have suggested that a nuclear war between two nuclear powers would result in mutual annihilation. From this point of view, the significance of nuclear weapons is purely to deter war because any nuclear war would immediately escalate out of mutual distrust and fear, resulting in mutually assured destruction. This threat of national, if not global, destruction has been a strong motivation for anti-nuclear weapons activism.\nCritics from the peace movement and within the military establishment have questioned the usefulness of such weapons in the current military climate. According to an advisory opinion issued by the International Court of Justice in 1996, the use of (or threat of use of) such weapons would generally be contrary to the rules of international law applicable in armed conflict, but the court did not reach an opinion as to whether or not the threat or use would be lawful in specific extreme circumstances such as if the survival of the state were at stake.\nAnother deterrence position in nuclear strategy is that nuclear proliferation can be desirable. This view argues that, unlike conventional weapons, nuclear weapons successfully deter all-out war between states, and they succeeded in doing this during the Cold War between the U.S. and the Soviet Union. In the late 1950s and early 1960s, Gen. Pierre Marie Gallois of France, an adviser to Charles DeGaulle, argued in books like The Balance of Terror: Strategy for the Nuclear Age (1961) that mere possession of a nuclear arsenal, what the French called the force de frappe, was enough to ensure deterrence, and thus concluded that the spread of nuclear weapons could increase international stability. Some very prominent neo-realist scholars, such as the late Kenneth Waltz, formerly a Political Science at UC Berkeley and Adjunct Senior Research Scholar at Columbia University, and John Mearsheimer of University of Chicago, have also argued along the lines of Gallois. Specifically, these scholars have advocated some forms of nuclear proliferation, arguing that it would decrease the likelihood of total war, especially in troubled regions of the world where there exists a unipolar nuclear weapon state. Aside from the public opinion that opposes proliferation in any form, there are two schools of thought on the matter: those, like Mearsheimer, who favor selective proliferation, and those of Kenneth Waltz, who was somewhat more non-interventionist.\nThe threat of potentially suicidal terrorists possessing nuclear weapons (a form of nuclear terrorism) complicates the decision process. The prospect of mutually assured destruction may not deter an enemy who expects to die in the confrontation. Further, if the initial act is from a stateless terrorist instead of a sovereign nation, there is no fixed nation or fixed military targets to retaliate against. It has been argued by the New York Times, especially after the September 11, 2001 attacks, that this complication is the sign of the next age of nuclear strategy, distinct from the relative stability of the Cold War. In 1996, the United States adopted a policy of allowing the targeting of its nuclear weapons at terrorists armed with weapons of mass destruction.\nRobert Gallucci, president of the John D. and Catherine T. MacArthur Foundation, argues that although traditional deterrence is not an effective approach toward terrorist groups bent on causing a nuclear catastrophe, Gallucci believes that \"the United States should instead consider a policy of expanded deterrence, which focuses not solely on the would-be nuclear terrorists but on those states that may deliberately transfer or inadvertently lead nuclear weapons and materials to them. By threatening retaliation against those states, the United States may be able to deter that which it cannot physically prevent.\".\nGraham Allison makes a similar case, arguing that the key to expanded deterrence is coming up with ways of tracing nuclear material to the country that forged the fissile material. \"After a nuclear bomb detonates, nuclear forensics cops would collect debris samples and send them to a laboratory for radiological analysis. By identifying unique attributes of the fissile material, including its impurities and contaminants, one could trace the path back to its origin.\" The process is analogous to identifying a criminal by fingerprints. \"The goal would be twofold: first, to deter leaders of nuclear states from selling weapons to terrorists by holding them accountable for any use of their own weapons; second, to give leader every incentive to tightly secure their nuclear weapons and materials.\"\n\n\n== Governance, control, and law ==\n\nBecause of the immense military power they can confer, the political control of nuclear weapons has been a key issue for as long as they have existed; in most countries the use of nuclear force can only be authorized by the head of government or head of state. Controls and regulations governing nuclear weapons are man-made, and so are imperfect. Therefore, there is an inherent danger of \"accidents, mistakes, false alarms, blackmail, theft, and sabotage\".\nIn the late 1940s, lack of mutual trust was preventing the United States and the Soviet Union from making ground towards international arms control agreements. The Russell\u2013Einstein Manifesto was issued in London on July 9, 1955 by Bertrand Russell in the midst of the Cold War. It highlighted the dangers posed by nuclear weapons and called for world leaders to seek peaceful resolutions to international conflict. The signatories included eleven pre-eminent intellectuals and scientists, including Albert Einstein, who signed it just days before his death on April 18, 1955. A few days after the release, philanthropist Cyrus S. Eaton offered to sponsor a conference\u2014called for in the manifesto\u2014in Pugwash, Nova Scotia, Eaton's birthplace. This conference was to be the first of the Pugwash Conferences on Science and World Affairs, held in July 1957.\nBy the 1960s steps were being taken to limit both the proliferation of nuclear weapons to other countries and the environmental effects of nuclear testing. The Partial Test Ban Treaty (1963) restricted all nuclear testing to underground nuclear testing, to prevent contamination from nuclear fallout, whereas the Nuclear Non-Proliferation Treaty (1968) attempted to place restrictions on the types of activities signatories could participate in, with the goal of allowing the transference of non-military nuclear technology to member countries without fear of proliferation.\nIn 1957, the International Atomic Energy Agency (IAEA) was established under the mandate of the United Nations to encourage development of peaceful applications for nuclear technology, provide international safeguards against its misuse, and facilitate the application of safety measures in its use. In 1996, many nations signed the Comprehensive Test Ban Treaty, which prohibits all testing of nuclear weapons. A testing ban imposes a significant hindrance to nuclear arms development by any complying country. The Treaty requires the ratification by 44 specific states before it can go into force; as of 2012, the ratification of eight of these states is still required.\nAdditional treaties and agreements have governed nuclear weapons stockpiles between the countries with the two largest stockpiles, the United States and the Soviet Union, and later between the United States and Russia. These include treaties such as SALT II (never ratified), START I (expired), INF, START II (never ratified), SORT, and New START, as well as non-binding agreements such as SALT I and the Presidential Nuclear Initiatives of 1991. Even when they did not enter into force, these agreements helped limit and later reduce the numbers and types of nuclear weapons between the United States and the Soviet Union/Russia.\nNuclear weapons have also been opposed by agreements between countries. Many nations have been declared Nuclear-Weapon-Free Zones, areas where nuclear weapons production and deployment are prohibited, through the use of treaties. The Treaty of Tlatelolco (1967) prohibited any production or deployment of nuclear weapons in Latin America and the Caribbean, and the Treaty of Pelindaba (1964) prohibits nuclear weapons in many African countries. As recently as 2006 a Central Asian Nuclear Weapon Free Zone was established amongst the former Soviet republics of Central Asia prohibiting nuclear weapons.\nIn the middle of 1996, the International Court of Justice, the highest court of the United Nations, issued an Advisory Opinion concerned with the \"Legality of the Threat or Use of Nuclear Weapons\". The court ruled that the use or threat of use of nuclear weapons would violate various articles of international law, including the Geneva Conventions, the Hague Conventions, the UN Charter, and the Universal Declaration of Human Rights. In view of the unique, destructive characteristics of nuclear weapons, the International Committee of the Red Cross calls on States to ensure that these weapons are never used, irrespective of whether they consider them lawful or not.\nAdditionally, there have been other, specific actions meant to discourage countries from developing nuclear arms. In the wake of the tests by India and Pakistan in 1998, economic sanctions were (temporarily) levied against both countries, though neither were signatories with the Nuclear Non-Proliferation Treaty. One of the stated casus belli for the initiation of the 2003 Iraq War was an accusation by the United States that Iraq was actively pursuing nuclear arms (though this was soon discovered not to be the case as the program had been discontinued). In 1981, Israel had bombed a nuclear reactor being constructed in Osirak, Iraq, in what it called an attempt to halt Iraq's previous nuclear arms ambitions; in 2007, Israel bombed another reactor being constructed in Syria.\nIn 2013, Mark Diesendorf says that governments of France, India, North Korea, Pakistan, UK, and South Africa have used nuclear power and/or research reactors to assist nuclear weapons development or to contribute to their supplies of nuclear explosives from military reactors.\n\n\n=== Disarmament ===\n\nNuclear disarmament refers to both the act of reducing or eliminating nuclear weapons and to the end state of a nuclear-free world, in which nuclear weapons are completely eliminated.\nBeginning with the 1963 Partial Test Ban Treaty and continuing through the 1996 Comprehensive Test Ban Treaty, there have been many treaties to limit or reduce nuclear weapons testing and stockpiles. The 1968 Nuclear Non-Proliferation Treaty has as one of its explicit conditions that all signatories must \"pursue negotiations in good faith\" towards the long-term goal of \"complete disarmament\". The nuclear weapon states have largely treated that aspect of the agreement as \"decorative\" and without force.\nOnly one country\u2014South Africa\u2014has ever fully renounced nuclear weapons they had independently developed. The former Soviet republics of Belarus, Kazakhstan, and Ukraine returned Soviet nuclear arms stationed in their countries to Russia after the collapse of the USSR.\nProponents of nuclear disarmament say that it would lessen the probability of nuclear war occurring, especially accidentally. Critics of nuclear disarmament say that it would undermine the present nuclear peace and deterrence and would lead to increased global instability. Various American elder statesmen, who were in office during the Cold War period, have been advocating the elimination of nuclear weapons. These officials include Henry Kissinger, George Shultz, Sam Nunn, and William Perry. In January 2010, Lawrence M. Krauss stated that \"no issue carries more importance to the long-term health and security of humanity than the effort to reduce, and perhaps one day, rid the world of nuclear weapons\".\n\nIn the years after the end of the Cold War, there have been numerous campaigns to urge the abolition of nuclear weapons, such as that organized by the Global Zero movement, and the goal of a \"world without nuclear weapons\" was advocated by United States President Barack Obama in an April 2009 speech in Prague. A CNN poll from April 2010 indicated that the American public was nearly evenly split on the issue.\nSome analysts have argued that nuclear weapons have made the world relatively safer, with peace through deterrence and through the stability\u2013instability paradox, including in south Asia. Kenneth Waltz has argued that nuclear weapons have helped keep an uneasy peace, and further nuclear weapon proliferation might even help avoid the large scale conventional wars that were so common prior to their invention at the end of World War II. But former Secretary Henry Kissinger says there is a new danger, which cannot be addressed by deterrence: \"The classical notion of deterrence was that there was some consequences before which aggressors and evildoers would recoil. In a world of suicide bombers, that calculation doesn\u2019t operate in any comparable way\". George Shultz has said, \"If you think of the people who are doing suicide attacks, and people like that get a nuclear weapon, they are almost by definition not deterrable\".\n\n\n=== United Nations ===\n\nThe UN Office for Disarmament Affairs (UNODA) is a department of the United Nations Secretariat established in January 1998 as part of the United Nations Secretary-General Kofi Annan's plan to reform the UN as presented in his report to the General Assembly in July 1997.\nIts goal is to promote nuclear disarmament and non-proliferation and the strengthening of the disarmament regimes in respect to other weapons of mass destruction, chemical and biological weapons. It also promotes disarmament efforts in the area of conventional weapons, especially land mines and small arms, which are often the weapons of choice in contemporary conflicts.\n\n\n== Controversy ==\n\n\n=== Ethics ===\nEven before the first nuclear weapons had been developed, scientists involved with the Manhattan Project were divided over the use of the weapon. The role of the two atomic bombings of the country in Japan's surrender and the U.S.'s ethical justification for them has been the subject of scholarly and popular debate for decades. The question of whether nations should have nuclear weapons, or test them, has been continually and nearly universally controversial.\n\n\n=== Notable nuclear weapons accidents ===\n\nFebruary 13, 1950: a Convair B-36B crashed in northern British Columbia after jettisoning a Mark IV atomic bomb. This was the first such nuclear weapon loss in history.\nMay 22, 1957: a 42,000-pound Mark-17 hydrogen bomb accidentally fell from a bomber near Albuquerque, New Mexico. The detonation of the device's conventional explosives destroyed it on impact and formed a crater 25-feet in diameter on land owned by the University of New Mexico. According to a researcher at the Natural Resources Defense Council, it was one of the most powerful bombs made to date.\nJune 7, 1960: the 1960 Fort Dix IM-99 accident destroyed a Boeing CIM-10 Bomarc nuclear missile and shelter and contaminated the BOMARC Missile Accident Site in New Jersey.\nJanuary 24, 1961: the 1961 Goldsboro B-52 crash occurred near Goldsboro, North Carolina. A B-52 Stratofortress carrying two Mark 39 nuclear bombs broke up in mid-air, dropping its nuclear payload in the process.\n1965 Philippine Sea A-4 crash, where a Skyhawk attack aircraft with a nuclear weapon fell into the sea. The pilot, the aircraft, and the B43 nuclear bomb were never recovered. It was not until 1989 that the Pentagon revealed the loss of the one-megaton bomb.\nJanuary 17, 1966: the 1966 Palomares B-52 crash occurred when a B-52G bomber of the USAF collided with a KC-135 tanker during mid-air refuelling off the coast of Spain. The KC-135 was completely destroyed when its fuel load ignited, killing all four crew members. The B-52G broke apart, killing three of the seven crew members aboard. Of the four Mk28 type hydrogen bombs the B-52G carried, three were found on land near Almer\u00eda, Spain. The non-nuclear explosives in two of the weapons detonated upon impact with the ground, resulting in the contamination of a 2-square-kilometer (490-acre) (0.78 square mile) area by radioactive plutonium. The fourth, which fell into the Mediterranean Sea, was recovered intact after a 2\u00bd-month-long search.\nJanuary 21, 1968: the 1968 Thule Air Base B-52 crash involved a United States Air Force (USAF) B-52 bomber. The aircraft was carrying four hydrogen bombs when a cabin fire forced the crew to abandon the aircraft. Six crew members ejected safely, but one who did not have an ejection seat was killed while trying to bail out. The bomber crashed onto sea ice in Greenland, causing the nuclear payload to rupture and disperse, which resulted in widespread radioactive contamination.\nSeptember 18\u201319, 1980: the Damascus Accident, occurred in Damascus, Arkansas, where a Titan missile equipped with a nuclear warhead exploded. The accident was caused by a maintenance man who dropped a socket from a socket wrench down an 80-foot shaft, puncturing a fuel tank on the rocket. Leaking fuel resulted in a hypergolic fuel explosion, jettisoning the W-53 warhead beyond the launch site.\n\n\n=== Nuclear testing and fallout ===\n\nOver 500 atmospheric nuclear weapons tests were conducted at various sites around the world from 1945 to 1980. Radioactive fallout from nuclear weapons testing was first drawn to public attention in 1954 when the Castle Bravo hydrogen bomb test at the Pacific Proving Grounds contaminated the crew and catch of the Japanese fishing boat Lucky Dragon. One of the fishermen died in Japan seven months later, and the fear of contaminated tuna led to a temporary boycotting of the popular staple in Japan. The incident caused widespread concern around the world, especially regarding the effects of nuclear fallout and atmospheric nuclear testing, and \"provided a decisive impetus for the emergence of the anti-nuclear weapons movement in many countries\".\nAs public awareness and concern mounted over the possible health hazards associated with exposure to the nuclear fallout, various studies were done to assess the extent of the hazard. A Centers for Disease Control and Prevention/ National Cancer Institute study claims that fallout from atmospheric nuclear tests would lead to perhaps 11,000 excess deaths amongst people alive during atmospheric testing in the United States from all forms of cancer, including leukemia, from 1951 to well into the 21st century. As of March 2009, the U.S. is the only nation that compensates nuclear test victims. Since the Radiation Exposure Compensation Act of 1990, more than $1.38 billion in compensation has been approved. The money is going to people who took part in the tests, notably at the Nevada Test Site, and to others exposed to the radiation.\nIn addition, leakage of byproducts of nuclear weapon production into groundwater has been an ongoing issue, particularly at the Hanford site.\n\n\n== Effects of nuclear explosions on human health ==\n\nSome scientists estimate that if there were a nuclear war resulting in 100 Hiroshima-size nuclear explosions on cities, it could cause significant loss of life in the tens of millions from long term climatic effects alone. The climatology hypothesis is that if each city firestorms, a great deal of soot could be thrown up into the atmosphere which could blanket the earth, cutting out sunlight for years on end, causing the disruption of food chains, in what is termed a Nuclear Winter.\nThe medical effects of the atomic bomb on Hiroshima upon humans can be put into the four categories below, with the effects of larger thermonuclear weapons producing blast and thermal effects so large that there would be a negligible number of survivors close enough to the center of the blast who would experience prompt/acute radiation effects, which were observed after the 16 kiloton yield Hiroshima bomb, due to its relatively low yield:\nInitial stage\u2014the first 1\u20139 weeks, in which are the greatest number of deaths, with 90% due to thermal injury and/or blast effects and 10% due to super-lethal radiation exposure.\nIntermediate stage\u2014from 10\u201312 weeks. The deaths in this period are from ionizing radiation in the median lethal range - LD50\nLate period\u2014lasting from 13\u201320 weeks. This period has some improvement in survivors' condition.\nDelayed period\u2014from 20+ weeks. Characterized by numerous complications, mostly related to healing of thermal and mechanical injuries, and if the individual was exposed to a few hundred to a thousand Millisieverts of radiation, it is coupled with infertility, sub-fertility and blood disorders. Furthermore, ionizing radiation above a dose of around 50-100 Millisievert exposure has been shown to statistically begin increasing one's chance of dying of cancer sometime in their lifetime over the normal unexposed rate of ~25%, in the long term, a heightened rate of cancer, proportional to the dose received, would begin to be observed after ~5+ years, with lesser problems such as eye cataracts and other more minor effects in other organs and tissue also being observed over the long term.\nFallout exposure - Depending on if further afield individuals Shelter in place or evacuate perpendicular to the direction of the wind, and therefore avoid contact with the fallout plume, and stay there for the days and weeks after the nuclear explosion, their exposure to fallout, and therefore their total dose, will vary. With those who do shelter in place, and or evacuate, experiencing a total dose that would be negligible in comparison to someone who just went about their life as normal.\nStaying indoors until after the most hazardous fallout isotope, I-131 decays away to 0.1% of its initial quantity after ten half lifes - which is represented by 80 days in I-131s case, would make the difference between likely contracting Thyroid cancer or escaping completely from this substance depending on the actions of the individual.\n\n\n=== Public opposition ===\n\nPeace movements emerged in Japan and in 1954 they converged to form a unified \"Japanese Council Against Atomic and Hydrogen Bombs\". Japanese opposition to nuclear weapons tests in the Pacific Ocean was widespread, and \"an estimated 35 million signatures were collected on petitions calling for bans on nuclear weapons\".\nIn the United Kingdom, the first Aldermaston March organised by the Campaign for Nuclear Disarmament(CND) took place at Easter 1958, when, according to the CND, several thousand people marched for four days from Trafalgar Square, London, to the Atomic Weapons Research Establishment close to Aldermaston in Berkshire, England, to demonstrate their opposition to nuclear weapons. The Aldermaston marches continued into the late 1960s when tens of thousands of people took part in the four-day marches.\nIn 1959, a letter in the Bulletin of Atomic Scientists was the start of a successful campaign to stop the Atomic Energy Commission dumping radioactive waste in the sea 19 kilometres from Boston. In 1962, Linus Pauling won the Nobel Peace Prize for his work to stop the atmospheric testing of nuclear weapons, and the \"Ban the Bomb\" movement spread.\nIn 1963, many countries ratified the Partial Test Ban Treaty prohibiting atmospheric nuclear testing. Radioactive fallout became less of an issue and the anti-nuclear weapons movement went into decline for some years. A resurgence of interest occurred amid European and American fears of nuclear war in the 1980s.\n\n\n== Costs and technology spin-offs ==\n\nAccording to an audit by the Brookings Institution, between 1940 and 1996, the U.S. spent $8.78 trillion in present-day terms on nuclear weapons programs. 57 percent of which was spent on building nuclear weapons delivery systems. 6.3 percent of the total, $551 billion in present-day terms, was spent on environmental remediation and nuclear waste management, for example cleaning up the Hanford site, and 7 percent of the total, $617 billion was spent on making nuclear weapons themselves.\n\n\n== Non-weapons uses ==\n\nPeaceful nuclear explosions are nuclear explosions conducted for non-military purposes, such as activities related to economic development including the creation of canals. During the 1960s and 70s, both the United States and the Soviet Union conducted a number of PNEs. Six of the explosions by the Soviet Union are considered to have been of an applied nature, not just tests.\nSubsequently the United States and the Soviet Union halted their programs. Definitions and limits are covered in the Peaceful Nuclear Explosions Treaty of 1976. The Comprehensive Nuclear-Test-Ban Treaty of 1996 prohibits all nuclear explosions, regardless of whether they are for peaceful purposes or not.\n\n\n== See also ==\n\n The Atomic Age \u2013 Wikipedia book\n\n\n=== History ===\n\n\n=== More technical details ===\nEffects of nuclear explosions\nIntercontinental ballistic missile\nNuclear blackout\nNeutron bomb\nNuclear bombs and health\nNuclear weapon yield\n\n\n=== Popular culture ===\nNuclear weapons in popular culture\nThe Butter Battle Book\n\n\n=== Proliferation and politics ===\n\n\n== Notes and references ==\n\n\n== Bibliography ==\n\n\n== External links ==\n\nNuclear Weapon Archive from Carey Sublette is a reliable source of information and has links to other sources and an informative FAQ.\nThe Federation of American Scientists provide solid information on weapons of mass destruction, including nuclear weapons and their effects\nAlsos Digital Library for Nuclear Issues\u2014contains many resources related to nuclear weapons, including a historical and technical overview and searchable bibliography of web and print resources.\nVideo archive of US, Soviet, UK, Chinese and French Nuclear Weapon Testing at sonicbomb.com\nThe National Museum of Nuclear Science & History (United States)\u2014located in Albuquerque, New Mexico; a Smithsonian Affiliate Museum\nNuclear Emergency and Radiation Resources\nThe Manhattan Project: Making the Atomic Bomb at AtomicArchive.com\nLos Alamos National Laboratory: History (U.S. nuclear history)\nRace for the Superbomb, PBS website on the history of the H-bomb\nRecordings of recollections of the victims of Hiroshima and Nagasaki\nThe Woodrow Wilson Center's Nuclear Proliferation International History Project or NPIHP is a global network of individuals and institutions engaged in the study of international nuclear history through archival documents, oral history interviews and other empirical sources.\nNUKEMAP3D - a 3D nuclear weapons effects simulator powerd by Google Maps.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Nuclear_weapon", 
                "title": "Nuclear weapon"
            }
        ], 
        "phraseCharStart": "1012"
    }, 
    {
        "phraseCharEnd": "1042", 
        "phraseIndex": "T21", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "DIII-D", 
        "wikiSearchResults": [
            {
                "snippet": "DIII-D is a tokamak developed in the 1980s by General Atomics in San Diego, USA, as part of the ongoing effort to achieve magnetically confined fusion", 
                "pageCategories": "All articles containing potentially dated statements\nAll stub articles\nArticles containing potentially dated statements from 2005\nCoordinates on Wikidata\nInterlanguage link template link number\nNuclear and atomic physics stubs\nTokamaks", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/DIII-D_(fusion_reactor)", 
                "title": "DIII-D (fusion reactor)"
            }, 
            {
                "snippet": "DIII-D in 2003.  Plasma instability COMPASS tokamak    T.E. Evans; et al. (2004). \"Suppression of Large Edge-Localized Modes in High-Confinement DIII-D", 
                "pageCategories": "All stub articles\nPhysics stubs\nPlasma physics\nTokamaks", 
                "pageContent": "Resonant magnetic perturbations (RMPs) are a special type of magnetic field perturbations used to control burning plasma instabilities called edge-localized modes (ELMs) in magnetic fusion devices such as tokamaks. The efficiency of RMPs for controlling ELMs was first demonstrated on the tokamak DIII-D in 2003.\n\n\n== See also ==\nPlasma instability\nCOMPASS tokamak\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Resonant_magnetic_perturbations", 
                "title": "Resonant magnetic perturbations"
            }, 
            {
                "snippet": "at General Atomics, including the DIII-D National Fusion Facility, an experimental tokamak. General Atomics DIII-D Program Fusion and Plasma Physics Seminar", 
                "pageCategories": "All accuracy disputes\nAll articles with dead external links\nAll articles with unsourced statements\nArticles containing Russian-language text\nArticles with Wayback Machine links\nArticles with dead external links from June 2016\nArticles with disputed statements from March 2014\nArticles with unsourced statements from December 2015\nCS1 errors: dates\nCommons category with local link same as on Wikidata", 
                "pageContent": "A tokamak (Russian: \u0442\u043e\u043a\u0430\u043c\u0430\u043a) is a device that uses a powerful magnetic field to confine plasma in the shape of a torus. Achieving a stable plasma equilibrium requires magnetic field lines that move around the torus in a helical shape. Such a helical field can be generated by adding a toroidal field (traveling around the torus in circles) and a poloidal field (traveling in circles orthogonal to the toroidal field). In a tokamak, the toroidal field is produced by electromagnets that surround the torus, and the poloidal field is the result of a toroidal electric current that flows inside the plasma. This current is induced inside the plasma with a second set of electromagnets.\nThe tokamak is one of several types of magnetic confinement devices, and is one of the most-researched candidates for producing controlled thermonuclear fusion power. Magnetic fields are used for confinement since no solid material could withstand the extremely high temperature of the plasma. An alternative to the tokamak is the stellarator. The world's largest tokamak project is the ITER (International Thermonuclear Experimental Reactor) being constructed in Saint-Paul-l\u00e8s-Durance, south of France. Scheduled to begin operation in 2020, it is expected to produce an output power of 500 megawatts.\nTokamaks were invented in the 1950s by Soviet physicists Igor Tamm and Andrei Sakharov, inspired by an original idea of Oleg Lavrentiev.\n\n\n== Etymology ==\nThe word tokamak is a transliteration of the Russian word \u0442\u043e\u043a\u0430\u043c\u0430\u043a, an acronym of either:\n\"\u0442\u043e\u0440\u043e\u0438\u0434\u0430\u043b\u044c\u043d\u0430\u044f \u043a\u0430\u043c\u0435\u0440\u0430 \u0441 \u043c\u0430\u0433\u043d\u0438\u0442\u043d\u044b\u043c\u0438 \u043a\u0430\u0442\u0443\u0448\u043a\u0430\u043c\u0438\" (toroidal'naya kamera s magnitnymi katushkami) \u2014 toroidal chamber with magnetic coils;\nor\n\"\u0442\u043e\u0440\u043e\u0438\u0434\u0430\u043b\u044c\u043d\u0430\u044f \u043a\u0430\u043c\u0435\u0440\u0430 \u0441 \u0430\u043a\u0441\u0438\u0430\u043b\u044c\u043d\u044b\u043c \u043c\u0430\u0433\u043d\u0438\u0442\u043d\u044b\u043c \u043f\u043e\u043b\u0435\u043c\" (toroidal'naya kamera s aksial'nym magnitnym polem) \u2014 toroidal chamber with axial magnetic field.\n\n\n== History ==\n\nAlthough nuclear fusion research began soon after World War II, the programs in various countries were each initially classified as secret. It was not until after the 1955 United Nations International Conference on the Peaceful Uses of Atomic Energy in Geneva that programs were declassified and international scientific collaboration could take place.\nExperimental research of tokamak systems started in 1956 in Kurchatov Institute, Moscow by a group of Soviet scientists led by Lev Artsimovich. The group constructed the first tokamaks, the most successful being T-3 and its larger version T-4. T-4 was tested in 1968 in Novosibirsk, conducting the first ever quasistationary thermonuclear fusion reaction.\nIn 1968, at the third IAEA International Conference on Plasma Physics and Controlled Nuclear Fusion Research at Novosibirsk, Soviet scientists announced that they had achieved electron temperatures of over 1000 eV in a tokamak device. British and American scientists met this news with skepticism since they were far from reaching that benchmark; they remained suspicious until laser scattering tests confirmed the findings the next year.\nIn 1973 design work on JET, the Joint European Torus, began.\nIn 1978, Bob Guccione, publisher of Penthouse Magazine met Robert Bussard and became the world's biggest and most committed private investor  in fusion technology, ultimately putting $20 Million ($60 Million in 2016 dollars) of his own money into Bussard's Compact Tokamak. Guccione may discovered Fusion in a 1974 essay titled God's Big Fix  published in Playboy a competing men's magazine.\n\n\n== Toroidal design ==\n\nPositively and negatively charged ions and negatively charged electrons in a fusion plasma are at very high temperatures, and have correspondingly large velocities. In order to maintain the fusion process, particles from the hot plasma must be confined in the central region, or the plasma will rapidly cool. Magnetic confinement fusion devices exploit the fact that charged particles in a magnetic field experience a Lorentz force and follow helical paths along the field lines.\nEarly fusion research devices were variants on the Z-pinch and used electric current to generate a poloidal magnetic field to contain the plasma along a linear axis between two points. Researchers discovered that a simple toroidal field, in which the magnetic field lines run in circles around an axis of symmetry, confines a plasma hardly better than no field at all. This can be understood by looking at the orbits of individual particles. The particles not only spiral around the field lines, they also drift across the field. Since a toroidal field is curved and decreases in strength moving away from the axis of rotation, the ions and the electrons move parallel to the axis, but in opposite directions. The charge separation leads to an electric field and an additional drift, in this case outward (away from the axis of rotation) for both ions and electrons. Alternatively, the plasma can be viewed as a torus of fluid with a magnetic field frozen in. The plasma pressure results in a force that tends to expand the torus. The magnetic field outside the plasma cannot prevent this expansion. The plasma simply slips between the field lines.\nFor a toroidal plasma to be effectively confined by a magnetic field, there must be a twist to the field lines. There are then no longer flux tubes that simply encircle the axis, but, if there is sufficient symmetry in the twist, flux surfaces. Some of the plasma in a flux surface will be on the outside (larger major radius, or \"low-field side\") of the torus and will drift to other flux surfaces farther from the circular axis of the torus. Other portions of the plasma in the flux surface will be on the inside (smaller major radius, or \"high-field side\"). Since some of the outward drift is compensated by an inward drift on the same flux surface, there is a macroscopic equilibrium with much improved confinement. Another way to look at the effect of twisting the field lines is that the electric field between the top and the bottom of the torus, which tends to cause the outward drift, is shorted out because there are now field lines connecting the top to the bottom.\nWhen the problem is considered even more closely, the need for a vertical (parallel to the axis of rotation) component of the magnetic field arises. The Lorentz force of the toroidal plasma current in the vertical field provides the inward force that holds the plasma torus in equilibrium.\n\n\n=== Advanced tokamaks ===\nSince about 1990 tokamaks are designed to operate in high-confinement mode to reduce plasma and energy losses.\nAdvanced or 2nd generation tokamaks generally use a 'C' or 'D' shaped plasma cross-section.\n\n\n=== Plasma disruptions ===\nAt the necessarily large toroidal currents (15 megaamperes in ITER) the tokamak concept suffers from a fundamental problem of stability. The nonlinear evolution of magnetohydrodynamical instabilities leads to a dramatic quench of the plasma current within milliseconds. Very energetic electrons are created (runaway electrons) and finally a global loss of confinement happens. At that point very intense radiation is inflicted on small areas. This phenomenon is called a major disruption. The occurrence of major disruptions in running tokamaks has always been rather high, of the order of a few percent of the total numbers of the shots. In currently operated tokamaks, the damage is often large but rarely dramatic. In the ITER tokamak, it is expected that the occurrence of a limited number of major disruptions will definitively damage the chamber with no possibility to restore the device.\nA large amplitude of the central current density can also result in internal disruptions, or sawteeth, which do not generally result in termination of the discharge.\n\n\n== Plasma heating ==\nIn an operating fusion reactor, part of the energy generated will serve to maintain the plasma temperature as fresh deuterium and tritium are introduced. However, in the startup of a reactor, either initially or after a temporary shutdown, the plasma will have to be heated to its operating temperature of greater than 10 keV (over 100 million degrees Celsius). In current tokamak (and other) magnetic fusion experiments, insufficient fusion energy is produced to maintain the plasma temperature.\n\n\n=== Ohmic heating ~ inductive mode ===\nSince the plasma is an electrical conductor, it is possible to heat the plasma by inducing a current through it; in fact, the induced current that heats the plasma usually provides most of the poloidal field. The current is induced by slowly increasing the current through an electromagnetic winding linked with the plasma torus: the plasma can be viewed as the secondary winding of a transformer. This is inherently a pulsed process because there is a limit to the current through the primary (there are also other limitations on long pulses). Tokamaks must therefore either operate for short periods or rely on other means of heating and current drive. The heating caused by the induced current is called ohmic (or resistive) heating; it is the same kind of heating that occurs in an electric light bulb or in an electric heater. The heat generated depends on the resistance of the plasma and the amount of electric current running through it. But as the temperature of heated plasma rises, the resistance decreases and ohmic heating becomes less effective. It appears that the maximum plasma temperature attainable by ohmic heating in a tokamak is 20-30 million degrees Celsius. To obtain still higher temperatures, additional heating methods must be used.\n\n\n=== Neutral-beam injection ===\n\nNeutral-beam injection involves the introduction of high energy (rapidly moving) atoms (molecules) into an ohmically heated, magnetically confined plasma within the tokamak. The high energy atoms (molecules) originate as ions in an arc chamber before being extracted through a high voltage grid set. The term \"ion source\" is used to generally mean the assembly consisting of a set of electron emitting filaments, an arc chamber volume, and a set of extraction grids. The extracted ions travel through a neutralizer section of the beamline where they gain enough electrons to become neutral atoms (molecules) but retain the high velocity imparted to them from the ion source. Once the neutral beam enters the tokamak, interactions with the main plasma ions occur which significantly heat the bulk plasma and bring it closer to fusion-relevant temperatures. Ion source extraction voltages are typically of the order 50-100 kV, and high voltage, negative ion sources (-1 MV) are being developed for ITER. The ITER Neutral Beam Test Facility in Padova will be the first ITER facility to start operation. While neutral beam injection is used primarily for plasma heating, it can also be used as a diagnostic tool and in feedback control by making a pulsed beam consisting of a string of brief 2-10 ms beam blips. Deuterium is a primary fuel for neutral beam heating systems and hydrogen and helium are sometimes used for selected experiments.\n\n\n=== Magnetic compression ===\nA gas can be heated by sudden compression. In the same way, the temperature of a plasma is increased if it is compressed rapidly by increasing the confining magnetic field. In a tokamak system this compression is achieved simply by moving the plasma into a region of higher magnetic field (i.e., radially inward). Since plasma compression brings the ions closer together.\n\n\n=== Radio-frequency heating ===\n\nHigh-frequency electromagnetic waves are generated by oscillators (often by gyrotrons or klystrons) outside the torus. If the waves have the correct frequency (or wavelength) and polarization, their energy can be transferred to the charged particles in the plasma, which in turn collide with other plasma particles, thus increasing the temperature of the bulk plasma. Various techniques exist including electron cyclotron resonance heating (ECRH) and ion cyclotron resonance heating. This energy is usually transferred by microwaves.\n\n\n== Tokamak particle inventory ==\nPlasma discharges within the tokamak's vacuum chamber consist of energized ions and atoms and the energy from these particles eventually reaches the inner wall of the chamber through radiation, collisions, or lack of confinement. The inner wall of the chamber is water-cooled and the heat from the particles is removed via conduction through the wall to the water and convection of the heated water to an external cooling system. Turbomolecular or diffusion pumps allow for particles to be evacuated from the bulk volume and cryogenic pumps, consisting of a liquid helium-cooled surface, serve to effectively control the density throughout the discharge by providing an energy sink for condensation to occur. When done correctly, the fusion reactions produce large amounts of high energy neutrons. Being electrically neutral and relatively tiny, the neutrons are not affected by the magnetic fields nor are they stopped much by the surrounding vacuum chamber. The neutron flux is reduced significantly at a purpose-built neutron shield boundary that surrounds the tokamak in all directions. Shield materials vary, but are generally materials made of atoms which are close to the size of neutrons because these work best to absorb the neutron and its energy. Good candidate materials include those with much hydrogen, such as water and plastics. Boron atoms are also good absorbers of neutrons. Thus, concrete and polyethylene doped with boron make inexpensive neutron shielding materials. Once freed, the neutron has a relatively short half-life of about 10 minutes before it decays into a proton and electron with the emission of energy. When the time comes to actually try to make electricity from a tokamak-based reactor, some of the neutrons produced in the fusion process would be absorbed by a liquid metal blanket and their kinetic energy would be used in heat-transfer processes to ultimately turn a generator.\n\n\n== Experimental tokamaks ==\n\n\n=== Currently in operation ===\n(in chronological order of start of operations)\n\n1960s: TM1-MH (since 1977 Castor; since 2007 Golem) in Prague, Czech Republic. In operation in Kurchatov Institute since early 1960s but renamed to Castor in 1977 and moved to IPP CAS, Prague; in 2007 moved to FNSPE, Czech Technical University in Prague and renamed to Golem.\n1975: T-10, in Kurchatov Institute, Moscow, Russia (formerly Soviet Union); 2 MW\n1983: Joint European Torus (JET), in Culham, United Kingdom\n1985: JT-60, in Naka, Ibaraki Prefecture, Japan; (Currently undergoing upgrade to Super, Advanced model)\n1987: STOR-M, University of Saskatchewan; Canada; first demonstration of alternating current in a tokamak.\n1988: Tore Supra, at the CEA, Cadarache, France\n1989: Aditya, at Institute for Plasma Research (IPR) in Gujarat, India\n1980s: DIII-D, in San Diego, USA; operated by General Atomics since the late 1980s\n1989: COMPASS, in Prague, Czech Republic; in operation since 2008, previously operated from 1989 to 1999 in Culham, United Kingdom\n1990: FTU, in Frascati, Italy\n1991: Tokamak ISTTOK, at the Instituto de Plasmas e Fus\u00e3o Nuclear, Lisbon, Portugal;\n\n1991: ASDEX Upgrade, in Garching, Germany\n1992: H-1NF (H-1 National Plasma Fusion Research Facility) based on the H-1 Heliac device built by Australia National University's plasma physics group and in operation since 1992\n1992: Alcator C-Mod, MIT, Cambridge, USA\n1992: Tokamak \u00e0 configuration variable (TCV), at the EPFL, Switzerland\n1994: TCABR, at the University of S\u00e3o Paulo, S\u00e3o Paulo, Brazil; this tokamak was transferred from Centre des Recherches en Physique des Plasmas in Switzerland\n1995: HT-7, in Hefei, China\n1999: MAST, in Culham, United Kingdom\n1999: NSTX in Princeton, New Jersey\n1999: Globus-M in Ioffe Institute, Saint Petersburg, Russia\n1990s: Pegasus Toroidal Experiment at the University of Wisconsin-Madison; in operation since the late 1990s\n2002: HL-2A, in Chengdu, China\n2006: EAST (HT-7U), in Hefei, China (ITER member)\n2008: KSTAR, in Daejon, South Korea (ITER member)\n2010: JT-60SA, in Naka, Japan (ITER member); upgraded from the JT-60.\n2012: SST-1, in Gandhinagar, India (ITER member); the Institute for Plasma Research reports 1000 seconds operation.\n2012: IR-T1, Islamic Azad University, Science and Research Branch, Tehran, Iran\n2012: ST25 at Tokamak Energy at Culham, Oxfordshire, UK (now at Milton Park)\n2014: ST25 (HTS) the first tokamak to have all magnetic fields formed from high temperature superconducting magnets, at Tokamak Energy based in Oxfordshire, UK\n\n\n=== Previously operated ===\n\n1960s: T-3 and T-4, in Kurchatov Institute, Moscow, Russia (formerly Soviet Union); T-4 in operation in 1968.\n1963: LT-1, Australia National University's plasma physics group built the first tokamak outside of Soviet Union c. 1963\n1971\u20131980: Texas Turbulent Tokamak, University of Texas at Austin, USA\n1973\u20131976: Tokamak de Fontenay aux Roses (TFR), near Paris, France\n1973\u20131979: Alcator A, MIT, USA\n1978\u20131987: Alcator C, MIT, USA\n1978\u20132013: TEXTOR, in J\u00fclich, Germany\n1979\u20131998: MT-1 Tokamak, Budapest, Hungary (Built at the Kurchatov Institute, Russia, transported to Hungary in 1979, rebuilt as MT-1M in 1991)\n1980\u20132004: TEXT/TEXT-U, University of Texas at Austin, USA\n1982\u20131997: TFTR, Princeton University, USA\n1983\u20132000: Novillo Tokamak, at the Instituto Nacional de Investigaciones Nucleares,in Mexico City, Mexico\n1984\u20131992: HL-1 Tokamak, in Chengdu, China\n1987\u20131999: Tokamak de Varennes; Varennes, Canada; operated by Hydro-Qu\u00e9bec and used by researchers from Institut de recherche en \u00e9lectricit\u00e9 du Qu\u00e9bec (IREQ) and the Institut national de la recherche scientifique (INRS)\n1988\u20132005: T-15, in Kurchatov Institute, Moscow, Russia (formerly Soviet Union); 10 MW\n1991\u20131998: START in Culham, United Kingdom\n1990s\u20132001: COMPASS, in Culham, United Kingdom\n1994\u20132001: HL-1M Tokamak, in Chengdu, China\n1999\u20132005: UCLA Electric Tokamak, in Los Angeles, USA\n\n\n=== Planned ===\nITER, international project in Cadarache, France; 500 MW; construction began in 2010, first plasma expected in 2020.\nDEMO; 2000 MW, continuous operation, connected to power grid. Planned successor to ITER; construction to begin in 2024 according to preliminary timetable.\nCFETR, also known as \"China Fusion Engineering Test Reactor\"; 200 MW; Next generation Chinese fusion reactor, is a new tokamak device.\n\n\n== See also ==\n\nMagnetic mirrors\nEdge-Localized Mode\nStellarator\nReversed-field pinch\nList of plasma (physics) articles\nDivertor\nBall-pen probe\nThe section on Dimensionless parameters in tokamaks in the article on Plasma scaling\nARC fusion reactor\n\n\n== Notes ==\n\n\n== References ==\nBraams, C.M. & Stott, P.E. (2002). Nuclear Fusion: Half a Century of Magnetic Confinement Research. Institute of Physics Publishing. ISBN 0-7503-0705-6. \nDolan, Thomas J. (1982). Fusion Research, Volume 1 - Principles. Pergamon Press. LCC QC791.D64. \nNishikawa, K. & Wakatani, M. (2000). Plasma Physics. Springer-Verlag. ISBN 3-540-65285-X. \nRaeder, J.; et al. (1986). Controlled Nuclear Fusion. John Wiley & Sons. ISBN 0-471-10312-8. \nWesson, John (2000). The Science of JET (PDF). \nWesson, John; et al. (2004). Tokamaks. Oxford University Press. ISBN 0-19-850922-7. \n\n\n== External links ==\nCCFE - site from the UK fusion research centre CCFE.\nInt'l Tokamak research - various that relate to ITER\nPlasma Science - site on tokamaks from the French CEA.\nFusion Programs at General Atomics, including the DIII-D National Fusion Facility, an experimental tokamak.\nGeneral Atomics DIII-D Program\nFusion and Plasma Physics Seminar at MIT OCW\nUnofficial ITER fan club, Club for fans of the biggest tokamak planned to be built in near future.\nwww.tokamak.info Extensive list of current and historic tokamaks from around the world.\nSSTC-1 Overview video of a small scale tokamak concept.\nSSTC-2 on YouTube Section View Video of a small scale tokamak concept.\nSSTC-3 on YouTube Fly Through Video of a small scale tokamak concept.\nLAP_Tokamak_Development Information on conditions necessary for nuclear reaction in a tokamak reactor\nA. P. Frass (1973). \"Engineering Problems In The Design Of Controlled Thermonuclear Reactors\" (PDF). Oak Ridge National Laboratory. Retrieved September 2013.  \nObserver Newspaper Article on Tokomak Nuclear fusion and the promise of a brighter tomorrow", 
                "titleUrl": "https://en.wikipedia.org/wiki/Tokamak", 
                "title": "Tokamak"
            }, 
            {
                "snippet": "the major fusion research facilities in the United States, together with DIII-D at General Atomics and NSTX-U at Princeton Plasma Physics Laboratory. Operating", 
                "pageCategories": "All Wikipedia articles in need of updating\nInterlanguage link template link number\nMassachusetts Institute of Technology\nOfficial website different in Wikidata and Wikipedia\nPlasma physics\nTokamaks\nVague or ambiguous time from October 2015\nWikipedia articles in need of updating from October 2015\nWikipedia articles needing clarification from October 2015", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Alcator_C-Mod", 
                "title": "Alcator C-Mod"
            }, 
            {
                "snippet": "tokamaks and STs, and will continue to be investigated in experiments such as DIII-D, Alcator C-Mod, NSTX, and MAST. New stellarator experiments such as NCSX", 
                "pageCategories": "All articles needing additional references\nArticles needing additional references from October 2014\nPlasma physics\nStability theory\nWikipedia articles needing clarification from October 2014", 
                "pageContent": "An important field of plasma physics is the stability of the plasma. It usually only makes sense to analyze the stability of a plasma once it has been established that the plasma is in equilibrium. \"Equilibrium\" asks whether there are not forces that will accelerate any part of the plasma. If there are not, then \"stability\" asks whether a small perturbation will grow, oscillate, or be damped out.\nIn many cases a plasma can be treated as a fluid and its stability analyzed with magnetohydrodynamics (MHD). MHD theory is the simplest representation of a plasma, so MHD stability is a necessity for stable devices to be used for nuclear fusion, specifically magnetic fusion energy. There are, however, other types of instabilities, such as velocity-space instabilities in magnetic mirrors and systems with beams. There are also rare cases of systems, e.g. the Field-Reversed Configuration, predicted by MHD to be unstable, but which are observed to be stable, probably due to kinetic effects.\n\n\n== Plasma instabilities ==\nPlasma instabilities can be divided into two general groups:\nhydrodynamic instabilities\nkinetic instabilities.\nPlasma instabilities are also categorised into different modes:\nSource: Andre Gsponer, \"Physics of high-intensity high-energy particle beam propagation in open air and outer-space plasmas\" (2004)\n\n\n=== List of plasma instabilities ===\n\n\n== MHD Instabilities ==\nBeta is a ratio of the plasma pressure over the magnetic field strength.\n\n  \n    \n      \n        \u03b2\n        =\n        \n          \n            p\n            \n              p\n              \n                m\n                a\n                g\n              \n            \n          \n        \n        =\n        \n          \n            \n              n\n              \n                k\n                \n                  B\n                \n              \n              T\n            \n            \n              (\n              \n                B\n                \n                  2\n                \n              \n              \n                /\n              \n              2\n              \n                \u03bc\n                \n                  0\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\beta ={\\frac {p}{p_{mag}}}={\\frac {nk_{B}T}{(B^{2}/2\\mu _{0})}}}\n  \nMHD stability at high beta is crucial for a compact, cost-effective magnetic fusion reactor. Fusion power density varies roughly as \n  \n    \n      \n        \n          \u03b2\n          \n            2\n          \n        \n      \n    \n    {\\displaystyle \\beta ^{2}}\n   at constant magnetic field, or as \n  \n    \n      \n        \n          \u03b2\n          \n            N\n          \n          \n            4\n          \n        \n      \n    \n    {\\displaystyle \\beta _{N}^{4}}\n   at constant bootstrap fraction in configurations with externally driven plasma current. (Here \n  \n    \n      \n        \n          \u03b2\n          \n            N\n          \n        \n        =\n        \u03b2\n        \n          /\n        \n        (\n        I\n        \n          /\n        \n        a\n        B\n        )\n      \n    \n    {\\displaystyle \\beta _{N}=\\beta /(I/aB)}\n   is the normalized beta.) In many cases MHD stability represents the primary limitation on beta and thus on fusion power density. MHD stability is also closely tied to issues of creation and sustainment of certain magnetic configurations, energy confinement, and steady-state operation. Critical issues include understanding and extending the stability limits through the use of a variety of plasma configurations, and developing active means for reliable operation near those limits. Accurate predictive capabilities are needed, which will require the addition of new physics to existing MHD models. Although a wide range of magnetic configurations exist, the underlying MHD physics is common to all. Understanding of MHD stability gained in one configuration can benefit others, by verifying analytic theories, providing benchmarks for predictive MHD stability codes, and advancing the development of active control techniques.\nThe most fundamental and critical stability issue for magnetic fusion is simply that MHD instabilities often limit performance at high beta. In most cases the important instabilities are long wavelength, global modes, because of their ability to cause severe degradation of energy confinement or termination of the plasma. Some important examples that are common to many magnetic configurations are ideal kink modes, resistive wall modes, and neoclassical tearing modes. A possible consequence of violating stability boundaries is a disruption, a sudden loss of thermal energy often followed by termination of the discharge. The key issue thus includes understanding the nature of the beta limit in the various configurations, including the associated thermal and magnetic stresses, and finding ways to avoid the limits or mitigate the consequences. A wide range of approaches to preventing such instabilities is under investigation, including optimization of the configuration of the plasma and its confinement device, control of the internal structure of the plasma, and active control of the MHD instabilities.\n\n\n=== Ideal Instabilities ===\nIdeal MHD instabilities driven by current or pressure gradients represent the ultimate operational limit for most configurations. The long-wavelength kink mode and short-wavelength ballooning mode limits are generally well understood and can in principle be avoided. Intermediate-wavelength modes (n ~ 5\u201310 modes encountered in tokamak edge plasmas, for example) are less well understood due to the computationally intensive nature of the stability calculations. The extensive beta limit database for tokamaks is consistent with ideal MHD stability limits, yielding agreement to within about 10% in beta for cases where the internal profiles of the plasma are accurately measured. This good agreement provides confidence in ideal stability calculations for other configurations and in the design of prototype fusion reactors.\n\n\n=== Resistive Wall Modes ===\nResistive wall modes (RWM) develop in plasmas that require the presence of a perfectly conducting wall for stability. RWM stability is a key issue for many magnetic configurations. Moderate beta values are possible without a nearby wall in the tokamak, stellarator, and other configurations, but a nearby conducting wall can significantly improve ideal kink mode stability in most configurations, including the tokamak, ST, reversed field pinch (RFP), spheromak, and possibly the FRC. In the advanced tokamak and ST, wall stabilization is critical for operation with a large bootstrap fraction. The spheromak requires wall stabilization to avoid the low-m,n tilt and shift modes, and possibly bending modes. However, in the presence of a non-ideal wall, the slowly growing RWM is unstable. The resistive wall mode has been a long-standing issue for the RFP, and has more recently been observed in tokamak experiments. Progress in understanding the physics of the RWM and developing the means to stabilize it could be directly applicable to all magnetic configurations. A closely related issue is to understand plasma rotation, its sources and sinks, and its role in stabilizing the RWM.\n\n\n=== Resistive instabilities ===\nResistive instabilities are an issue for all magnetic configurations, since the onset can occur at beta values well below the ideal limit. The stability of neoclassical tearing modes (NTM) is a key issue for magnetic configurations with a strong bootstrap current. The NTM is a metastable mode; in certain plasma configurations, a sufficiently large deformation of the bootstrap current produced by a \u201cseed island\u201d can contribute to the growth of the island. The NTM is already an important performance-limiting factor in many tokamak experiments, leading to degraded confinement or disruption. Although the basic mechanism is well established, the capability to predict the onset in present and future devices requires better understanding of the damping mechanisms which determine the threshold island size, and of the mode coupling by which other instabilities (such as sawteeth in tokamaks) can generate seed islands. Resistive Ballooning Mode, similar to ideal ballooning, but with finite resistivity taken into consideration, provides another example of a resistive instability.\n\n\n== Opportunities for Improving MHD Stability ==\n\n\n=== Configuration ===\nThe configuration of the plasma and its confinement device represent an opportunity to improve MHD stability in a robust way. The benefits of discharge shaping and low aspect ratio for ideal MHD stability have been clearly demonstrated in tokamaks and STs, and will continue to be investigated in experiments such as DIII-D, Alcator C-Mod, NSTX, and MAST. New stellarator experiments such as NCSX (proposed) will test the prediction that addition of appropriately designed helical coils can stabilize ideal kink modes at high beta, and lower-beta tests of ballooning stability are possible in HSX. The new ST experiments provide an opportunity to test predictions that a low aspect ratio yields improved stability to tearing modes, including neoclassical, through a large stabilizing \u201cGlasser effect\u201d term associated with a large Pfirsch-Schl\u00fcter current. Neoclassical tearing modes can be avoided by minimizing the bootstrap current in quasi-helical and quasi-omnigenous stellarator configurations. Neoclassical tearing modes are also stabilized with the appropriate relative signs of the bootstrap current and the magnetic shear; this prediction is supported by the absence of NTMs in central negative shear regions of tokamaks. Stellarator configurations such as the proposed NCSX, a quasi-axisymmetric stellarator design, can be created with negative magnetic shear and positive bootstrap current to achieve stability to the NTM. Kink mode stabilization by a resistive wall has been demonstrated in RFPs and tokamaks, and will be investigated in other configurations including STs (NSTX) and spheromaks (SSPX). A new proposal to stabilize resistive wall modes by a flowing liquid lithium wall needs further evaluation.\n\n\n=== Internal Structure ===\nControl of the internal structure of the plasma allows more active avoidance of MHD instabilities. Maintaining the proper current density profile, for example, can help to maintain stability to tearing modes. Open-loop optimization of the pressure and current density profiles with external heating and current drive sources is routinely used in many devices. Improved diagnostic measurements along with localized heating and current drive sources, now becoming available, will allow active feedback control of the internal profiles in the near future. Such work is beginning or planned in most of the large tokamaks (JET, JT\u201360U, DIII\u2013D, C\u2013Mod, and ASDEX\u2013U) using RF heating and current drive. Real-time analysis of profile data such as MSE current profile measurements and real-time identification of stability boundaries are essential components of profile control. Strong plasma rotation can stabilize resistive wall modes, as demonstrated in tokamak experiments, and rotational shear is also predicted to stabilize resistive modes. Opportunities to test these predictions are provided by configurations such as the ST, spheromak, and FRC, which have a large natural diamagnetic rotation, as well as tokamaks with rotation driven by neutral beam injection. The Electric Tokamak experiment is intended to have a very large driven rotation, approaching Alfv\u00e9nic regimes where ideal stability may also be influenced. Maintaining sufficient plasma rotation, and the possible role of the RWM in damping the rotation, are important issues that can be investigated in these experiments.\n\n\n=== Feedback Control ===\nActive feedback control of MHD instabilities should allow operation beyond the \u201cpassive\u201d stability limits. Localized rf current drive at the rational surface is predicted to reduce or eliminate neoclassical tearing mode islands. Experiments have begun in ASDEX\u2013U and COMPASS-D with promising results, and are planned for next year  in DIII\u2013D. Routine use of such a technique in generalized plasma conditions will require real-time identification of the unstable mode and its radial location. If the plasma rotation needed to stabilize the resistive wall mode cannot be maintained, feedback stabilization with external coils will be required. Feedback experiments have begun in DIII\u2013D and HBT-EP, and feedback control should be explored for the RFP and other configurations. Physics understanding of these active control techniques will be directly applicable between configurations.\n\n\n=== Disruption Mitigation ===\nThe techniques discussed above for improving MHD stability are the principal means of avoiding disruptions. However, in the event that these techniques do not prevent an instability, the effects of a disruption can be mitigated by various techniques. Experiments in JT\u201360U have demonstrated reduction of electromagnetic stresses through operation at a neutral point for vertical stability. Pre-emptive removal of the plasma energy by injection of a large gas puff or an impurity pellet has been demonstrated in tokamak experiments, and ongoing experiments in C\u2013Mod, JT\u201360U, ASDEX\u2013U, and DIII\u2013D will improve the understanding and predictive capability. Cryogenic liquid jets of helium are another proposed technique, which may be required for larger devices. Mitigation techniques developed for tokamaks will be directly applicable to other configurations.\n\n\n== See also ==\nKink oscillation\nList of plasma (physics) articles\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Plasma_stability", 
                "title": "Plasma stability"
            }, 
            {
                "snippet": "peak loads, but relatively low average loads, on the electrical grid. The DIII-D tokamak at General Atomics, the Princeton Large Torus (PLT) at the Princeton", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from April 2015\nArticles with unsourced statements from December 2015\nCommons category with local link same as on Wikidata\nElectric power systems components\nWikipedia articles needing clarification from December 2015", 
                "pageContent": "A motor\u2013generator (an M\u2013G set) is a device for converting electrical power to another form. Motor\u2013generator sets are used to convert frequency, voltage, or phase of power. They may also be used to isolate electrical loads from the electrical power supply line. Large motor\u2013generators were widely used to convert industrial amounts of power while smaller motor\u2013generators (such as the one shown in the picture) were used to convert battery power to higher DC voltages. These motor-generators should not be confused with \"motor generator\", which is a term occasionally used to describe a portable generator powered by an internal combustion engine.\nLow-powered devices such as vacuum tube mobile radio receivers did not use expensive and bulky motor\u2013generators. Instead, they used an inverter circuit consisting of a vibrator (a self-exciting relay) and a transformer to produce the higher voltages required for the vacuum tubes from a 6 or 12V car battery.\nWhile a motor\u2013generator set may consist of distinct motor and generator machines coupled together, a single unit dynamotor (for dynamo\u2013motor) has coils both to drive the motor and to generate the output wound around a single rotor; both coils share the same outer field coils or magnets. Typically the motor coils are driven from a commutator on one end of the shaft, when the generator coils output to another commutator on the other end of the shaft. The entire rotor and shaft assembly is smaller, lighter, and cheaper than a pair of machines, and does not require exposed drive shafts.\n\n\n== Electrical power handlingEdit ==\nIn the context of electric power generation and large fixed electrical power systems, a motor\u2013generator consists of an electric motor mechanically coupled to an electric generator (or alternator). The motor runs on the electrical input current while the generator creates the electrical output current, with power flowing between the two machines as a mechanical torque; this provides electrical isolation and some buffering of the power between the two electrical systems.\nOne use of this type of motor\u2013generator is to eliminate spikes and variations in \"dirty power\" (power conditioning) or to provide phase matching between different electrical system;\n\n\n=== Flywheel-generatorEdit ===\nAnother use is to buffer extreme loads on the power system. For example, tokamak fusion devices impose very large peak loads, but relatively low average loads, on the electrical grid. The DIII-D tokamak at General Atomics, the Princeton Large Torus (PLT) at the Princeton Plasma Physics Laboratory, and the Nimrod synchrotron at the Rutherford Appleton Laboratory each used large flywheels on multiple motor\u2013generator rigs to level the load imposed on the electrical system: the motor side slowly accelerated a large flywheel to store energy, which was consumed rapidly during a fusion experiment as the generator side acted as a brake on the flywheel. Similarly, the next generation U.S. Navy aircraft carrier Electromagnetic Aircraft Launch System (EMALS) will use a flywheel motor\u2013generator rig to supply power instantaneously for aircraft launches at greater than the ship's installed generator capacity.\n\n\n== ConversionsEdit ==\n\nMotor\u2013generators may be used for various conversions including:\nAlternating current (AC) to direct current (DC)\nDC to AC\nDC at one voltage to DC at another voltage. (Also called a dynamotor, short for dynamo-motor)\nCreating or balancing a three-wire DC system.\nAC at one frequency to AC at another harmonically-related frequency\nAC at a fixed voltage to AC of a variable voltage\nAC single-phase to AC three-phase\n\n\n== Variable AC voltage power supplyEdit ==\nBefore solid state AC voltage regulation was available or cost effective, motor generator sets were used to provide a variable AC voltage. The DC voltage to the generators armature would be varied manually or electronically to control the output voltage. When used in this fashion, the MG set is equivalent to an isolated variable transformer.\n\n\n== High-frequency machinesEdit ==\nAn Alexanderson alternator is a motor-driven, high-frequency alternator which provides radio frequency power. In the early days of radio communication, the high frequency carrier wave had to be produced mechanically using an Alternator with many poles driven at high speeds. The Alexanderson alternator produced upward of 100 kHz with power outputs upward of 200 kW. While electromechanical converters were regularly used for long wave transmissions in the first three decades of the 20th century, electronic techniques were required at higher frequencies. The Alexanderson alternator was largely replaced by the vacuum tube oscillator in the 1920s.\n\n\n== Motor\u2013generators used to increase ride-throughEdit ==\n\nMotor\u2013generators have even been used where the input and output currents are essentially the same. In this case, the mechanical inertia of the M\u2013G set is used to filter out transients in the input power. The output's electric current can be very clean (noise free) and will be able to ride-through brief blackouts and switching transients at the input to the M\u2013G set. This may enable, for example, the flawless cut-over from mains power to AC power provided by a diesel generator set.\nThe motor\u2013generator set may contain a large flywheel to improve its ride-through; however, consideration must be taken in this application as the motor\u2013generator will require a large amount of current on re-closure, if prior to the pull-out torque is achieved, resulting in a shut down. The in-rush current during re-closure will depend on many factors, however. As an example, a 250 kVA motor generator operating at 300 ampere of full load current will require 1550 ampere of in-rush current during a re-closure after 5 seconds. This example used a fixed mounted flywheel sized to result in a 1/2 Hz per second slew rate. The motor\u2013generator was a vertical type two-bearing machine with oil-bath bearings.\nMotors and generators may be coupled by a non-conductive shaft in facilities that need to closely control electromagnetic radiation, or where high isolation from transient surge voltages is required.\n\n\n== The motor\u2013generator todayEdit ==\nMotor\u2013generator sets have been replaced by semiconductor devices for some purposes. In the past, a popular use for MG sets were in elevators. Since accurate speed control of the hoisting machine was required, the impracticality of varying the frequency to a high power AC motor meant that the use of an MG set with a DC hoist motor was a near industry-standard solution. Modern AC variable-frequency drives and compatible motors have increasingly supplanted traditional MG-driven elevator installations, since AC drives are typically more efficient by 50% or more than DC-powered machinery.\nAnother use for MG sets was on the southern region of British Rail. They were used to convert the 600 VDC - 850 VDC line supply voltage from the third rail into 70 VDC to power the controls of the EMU stock in use. These have since been replaced with solid state converters on new rolling stock.\nOn the other hand, in industrial settings where harmonic cancellation, frequency conversion, or line isolation is needed, MG sets remain a popular solution. A useful feature of the motor\u2013generator is that they can handle large short-term overloads better than semiconductor devices of the same average load rating. Consider that the thermally current-limited components of a large semiconductor inverter are solid-state switches massing a few grams with a thermal time constant to their heat sinks of likely more than 100 ms, whereas the thermally current limited components of an MG are copper windings massing some hundreds of kilos which are intrinsically attached to their own large thermal mass. They also have inherently excellent resistance to electrostatic discharge (ESD).\n\n\n== Modern use of the termEdit ==\nIn the context of hybrid vehicles and other lightweight power systems, the term \"motor\u2013generator\" can be used to describe a single power transducer that can be used as either an electric motor or a generator, converting between electrical power and mechanical power. In principle, any electrical generator can also serve as an electric motor, or vice versa. The literature distributed by Toyota to describe the Hybrid Synergy Drive is an example of this newer usage.\nFrom the 2014 season, Formula 1 racing cars will have what are described as 2 motor-generator units (MGU), single units which can act as either a generator or a motor. This makes the cars more fuel-efficient by harvesting energy from the turbocharger and under braking. They can be used to provide an additional 160 BHP to the wheels to aid acceleration and overtaking, or can be used to spin the turbo to increase boost pressure faster, thereby reducing turbo lag.\n\n\n== See alsoEdit ==\n\n\n== ReferencesEdit ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Motor\u2013generator", 
                "title": "Motor\u2013generator"
            }, 
            {
                "snippet": "at the time chairman of the House Defense Appropriations subcommittee.  DIII-D Energy Multiplier Module Gas Turbine Modular Helium Reactor HTGR Sequoyah", 
                "pageCategories": "1955 establishments in California\nAll articles with a promotional tone\nAll articles with unsourced statements\nArticles with a promotional tone from November 2013\nArticles with unsourced statements from March 2014\nCompanies based in San Diego\nCompanies established in 1955\nCoordinates not on Wikidata\nDefense companies of the United States\nNuclear technology in the United States", 
                "pageContent": "General Atomics is a defense contractor headquartered in San Diego, California, specializing in nuclear physics. General Atomics' research into nuclear fission and nuclear fusion has also had bearing on related technologies, allowing the company to expand into other fields of research and manufacturing. General Atomics develops systems ranging from the nuclear fuel cycle to remotely operated surveillance aircraft, airborne sensors, advanced electric, electronic, wireless, and laser technologies.\n\n\n== History ==\n\nGeneral Atomics (GA) was founded on July 18, 1955, in San Diego, California as the General Atomic division of General Dynamics \"for harnessing the power of nuclear technologies for the benefit of mankind\".\nGA's first offices were in the General Dynamics facility on Hancock Street in San Diego. GA also used a schoolhouse on San Diego's Barnard Street as its temporary headquarters, which it would later \"adopt\" as part of its Education Outreach program. In 1956 San Diego voters approved the transfer of land to GA for permanent facilities in Torrey Pines and the John Jay Hopkins Laboratory for Pure and Applied Science was formally dedicated there on June 25, 1959. The Torrey Pines facility continues to serve as the company's headquarters today.\nGeneral Atomics's initial projects were the TRIGA nuclear research reactor  and Project Orion. \nA brief history of the company:\n1967: Sold to Gulf Oil and renamed Gulf General Atomic.\n1973: Renamed \"General Atomic Company\" when Royal Dutch Shell Group's Scallop Nuclear Inc. became a 50-50 partner.\n1982: Renamed \"GA Technologies Inc\" when Gulf bought out its partner.\n1984: Taken ownership of by Chevron following its merger with Gulf Oil.\n1986: Sold to a company owned by Neal Blue and Linden Blue when it assumed its current name.\n1987: Joined by former U.S. Navy Rear Admiral, Thomas J. Cassidy, Jr.\n1993: Awarded the \"Information Services\" portion of the NSF contract for InterNIC functions and publishes Internet Scout Report.\n1993: Spawned General Atomics Aeronautical Systems, Inc. (GA-ASI), with Neal Blue as Chairman-CEO and Thomas J. Cassidy as President.\n1994: GA-ASI spun off as an affiliate.\n1995: Ended role as provider of InterNIC Information Services.\nOn March 15, 2010, Rear Adm. Thomas J. Cassidy stepped down as President of GA-ASI\u2019s Aircraft Systems Group, staying on as non-executive chairman of the company's management committee. Frank Pace, the executive vice president of Aircraft Systems Group, succeeded Cassidy as President of GA-ASI.\nGeneral Atomics is also developing a Generation IV reactor design, the Gas Turbine Modular Helium Reactor (GT-MHR). In 2010, General Atomics presented a new version of the GT-MHR, the Energy Multiplier Module (EM2), which uses fast neutrons and is a Gas-cooled fast reactor.\nGeneral Atomics, including its affiliate, General Atomics Aeronautical Systems, is San Diego County\u2019s largest defense contractor, according to a September 2013 report by the San Diego Military Affairs Council. The top five contractors, ranked by defense-generated revenue in fiscal year 2013, were General Atomics, followed by Northrop Grumman, General Dynamics-NASSCO, BAE Systems, and SAIC. A separate October 2013 report by the San Diego Business Journal ranked contractors by the number of local employees. The top three contractors were General Atomics (7,668 local employees), Northrop Grumman (3,847), and SAIC (2,778).\n\n\n== Business groups ==\nElectromagnetic Systems Group\nThe Electromagnetic Systems (EMS) Group is a supplier of electromagnetic systems and related power equipment for a variety of defense, energy, and commercial transportation applications. EMS has expertise in the design and fabrication of linear motors, superconducting and conventional rotating motors, power inverters, high-voltage DC power distribution systems, and numerous other energy conversion, distribution, and storage systems. EMS is a major factor in applying electromagnetic technologies to aircraft launch and recovery (EMALS and AAG System), projectile launch (Navy rail gun), and magnetic levitation transportation systems.\n\nEnergy GroupControlled Fusion\nPower Reactors\n\nNuclear Fuels Group\n\n\n== Affiliated companies ==\n\nGeneral Atomics Aeronautical Systems, Inc. (GA-ASI) \u2014 GA-ASI\u2019s Aircraft Systems Group produces the Predator series of remotely piloted aircraft used in the Kosovo, Iraq, and Afghanistan conflicts. GA-ASI\u2019s Reconnaissance Systems Group provides tactical reconnaissance radars, as well as high-resolution surveillance systems for both manned and unmanned aircraft.\nGeneral Atomics Electronic Systems, Inc. (GA-ESI) \u2014 consists of five product lines involving different aspects of energy.Terminal Automation Products (TAP) provides automated distribution, inventory control and transaction processing systems to bulk product storage facilities that handle petroleum, chemical and agricultural products.\nRadiation Monitoring Systems (RMS) designs, manufactures, and supports a full range of radiation monitoring, detecting, control, data collection, and display equipment, with equipment and systems at over half of the currently operating nuclear plants in the United States and at numerous sites in Europe and throughout the Far East.\nGeneral Atomics Energy Products manufactures Maxwell high voltage capacitors after acquiring the product line from Maxwell Technologies in 2000.\nThe Gulftronic Separator System is a continuous operation, electrostatic, on-stream separation system currently in use by most major oil companies. Since their introduction in 1979, over 30 systems have been installed at petroleum refineries worldwide.\nTRIGA (Training, Research, Isotopes and General Atomics), with over 65 facilities in 22 countries, is a supplier of nuclear research reactors for university, industrial, government, and medical applications. Originally designed to meet requirements for operator training, educational programs including nuclear research, and fuel development, TRIGA's design has allowed its usage to be expanded to meet the requirements of application in medical and agricultural research, isotope production, and neutron radiography.\n\nGeneral Atomics Systems Integration, LLC (GA-SI) \u2014 provider of military and commercial engineering services. GA-SI is active in aircraft systems integration technologies, reliability improvements, and controls system design. GA-SI provides engineering services for new-development and aging-system services to military and commercial customers. The company also provides Test and Evaluation assessment as well as field services.\nConverDyn \u2014 serves the global nuclear industry, offering UF6. They coordinate and manage all aspects of the conversion process, including uranium deliveries, uranium sampling, materials storage, and product delivery. Jointly owned by Honeywell Inc.\nCotter Corporation \u2014 headquartered in Denver, Colorado. Through its various mining and milling operations, Cotter has produced uranium, vanadium, molybdenum, silver, lead, zinc, copper, selenium, nickel, cobalt, tungsten and limestone. Originally incorporated in 1956 in New Mexico as a uranium production company, Cotter was purchased by and became a wholly owned subsidiary of Commonwealth Edison in 1975. GA acquired Cotter in early 2000.\nHeathgate Resources Pty. Ltd. \u2014 Formed in 1990, Heathgate Resources is the owner and operator of the Beverley Uranium Mine in northern South Australia. Beverley is Australia's third uranium mine and Australia's only operating In Situ Leach mine.\nNuclear Fuels Corporation \u2014 NFC was formed in 1991 by General Atomics (GA) to market uranium produced from GA mining assets as well as to develop additional uranium projects. NFC is a long-term contract supplier to both US and foreign utilities and actively participates in uranium trading. NFC is the marketing representative for other GA affiliates, Heathgate Resources and Cotter Corporation. The company also has an agreement to purchase all uranium recovered by Wismut GmbH from reclamation of the K\u00f6nigstein mine in eastern Germany.\nRio Grande Resources Corporation \u2014 controls uranium operations and mineral resources acquired by GA from Chevron Resources in 1991. Included in this acquisition were mines in south Texas and New Mexico. In New Mexico, the Mt. Taylor project, a conventional underground mine that contains the largest uranium resource in the United States, is currently on standby.\nTRIGA International (with CERCA, a subsidiary of Areva)\nSpezialtechnik Dresden GmbH \u2014 STD stands as holding at the head of the Spezialtechnik-Group Dresden and renders commercially characterized and marketing/sales oriented consulting and support services for the companies of the group.\n\n\n== Educational outreach ==\nSince 1992, the General Atomics Science Education Outreach Program, a volunteer effort of GA employees and San Diego science teachers, has worked with Science Coordinators for the San Diego Schools to bring the business and research side of science into the classroom. The goal is both to improve the quality of science education and to encourage more students to pursue science careers. In addition, the teachers' interactions with the scientists and exposure to everyday uses of their disciplines help them to be better educators.\nIn 1995, the program was expanded, and the General Atomics Sciences Education Foundation [501(c) (3)] was established. The General Atomics Sciences Education Foundation's goal is to play a major role in enhancing pre-college education in science, engineering and new technologies. To attain this goal, four areas of core competency at General Atomics were initially selected to form the basis for the development of inquiry-based education modules and associated workshops. Scientist/teacher teams wrote these modules, which fuse the content and methodology of industrial research and development with the teaching skills of experienced science teachers.\n\n\n== Awards ==\n2013 Neal Blue, CEO of General Atomics, receives the 29th Annual International von Karman Wings Award\n2008 North American Frost & Sullivan Award for Company of the Year\n2008 Defense News Top 100, Ranked #57\nFrost & Sullivan 2006 Business Development Strategy Leadership Award, presented for Gains in the Unmanned Aerial Systems Market\nShephard Press' Unmanned Vehicles 2005 UAV Design Innovation Award, presented for Warrior Extended Range/Multi-Purpose UAV\nAviation Week 2005 Employer of Choice Finalist, Diversity, Valuing People, Technological Challenge\u2014Third Best US Aerospace/Defense Employer\nUSAF Association 2004 John R. Alison Award for the most outstanding contributions to national defense by an industrial leader, presented to President/CEO, Thomas J. Cassidy Jr.\nAUVSI's 2002 Pioneer Award, presented to President/CEO Thomas J. Cassidy Jr.\nUSAF's 2001 Packard Award for Development & Engineering, presented for Predator/Hellfire Integration\n\n\n== Government influence ==\nSince 2005, the Center for Responsible Politics reported General Atomics had spent over $1.5 million per year in lobbying efforts from 2005 to 2011.\nIn April 2002, the company paid for Letitia White, who was then a top aide to Representative Jerry Lewis, and her husband to travel to Italy. White left Lewis' office nine months later, to become a lobbyist at Copeland Lowery. The next day, she began representing General Atomics. Lewis, her former boss, was at the time chairman of the House Defense Appropriations subcommittee.\n\n\n== See also ==\nDIII-D\nEnergy Multiplier Module\nGas Turbine Modular Helium Reactor\nHTGR\nSequoyah Fuels Corporation\nHarold Agnew\n\n\n== References ==\n\n\n== Bibliography ==\n\"Patents assigned to General Atomics\". US Patent & Trademark Office. Retrieved July 4, 2006. \n\"Affiliates\". General Atomics. Retrieved July 9, 2013. \n\n\n== External links ==\nGeneral Atomics\nGeneral Atomics DIII-D Program\nUrban Maglev", 
                "titleUrl": "https://en.wikipedia.org/wiki/General_Atomics", 
                "title": "General Atomics"
            }, 
            {
                "snippet": "nuclear fusion. 2006 - Life span of 3 Fusion Reactors (JT-60U, JET, and DIII-D) are terminated. 2007, September - KSTAR's major devices are constructed", 
                "pageCategories": "Interlanguage link template link number\nTokamaks", 
                "pageContent": "The KSTAR, or Korea Superconducting Tokamak Advanced Research is a magnetic fusion device being built at the National Fusion Research Institute in Daejeon, South Korea. It is intended to study aspects of magnetic fusion energy which will be pertinent to the ITER fusion project as part of that country's contribution to the ITER effort. The project was approved in 1995 but construction was delayed by the East Asian financial crisis which weakened the South Korean economy considerably; however the construction phase of the project was completed on September 14, 2007. First plasma occurred on July 15, 2008. or more likely on June 30 2008.\nKSTAR will be one of the first research tokamaks in the world to feature fully superconducting magnets, which again will be of great relevance to ITER as this will also use SC magnets. The KSTAR magnet system consists of 16 niobium-tin direct current toroidal field magnets, 10 niobium-tin alternating current poloidal field magnets and 4 niobium-titanium alternating current poloidal field magnets. It is planned that the reactor will study plasma pulses of up to 20 seconds duration until 2011, when it will be upgraded to study pulses of up to 300 seconds duration. The reactor vessel will have a major radius of 1.8 m, a minor radius of 0.5 m, a maximum toroidal field of 3.5 tesla, and a maximum plasma current of 2 megaampere. As with other tokamaks, heating and current drive will be initiated using neutral beam injection, ion cyclotron resonance heating (ICRH), radio frequency heating and electron cyclotron resonance heating (ECRH). Initial heating power will be 8 megawatt from neutral beam injection upgradeable to 24 MW, 6 MW from ICRH upgradeable to 12 MW, and at present undetermined heating power from ECRH and RF heating. The experiment will use both hydrogen and deuterium fuels but not the deuterium-tritium mix which will be studied in ITER.\nIn 2012, it succeeded in maintaining high-temperature plasma (about 50 million degrees Celsius) for 17 seconds.\n\n\n== Timeline ==\nThe design was based on Tokomak Physics Experiment which was based on Compact Ignition Tokamak design - See Robert J. Goldston.\n1995 - Started Project KSTAR\n1997 - JET of EU emits 17 MW energy from itself.\n1998 - JT-60U went beyond energy junction successfully, and acknowledged possibility of commercialization of nuclear fusion.\n2006 - Life span of 3 Fusion Reactors (JT-60U, JET, and DIII-D) are terminated.\n2007, September - KSTAR's major devices are constructed.\n2008, July - First plasma occurred. Maintenance time: 0.865 seconds, Temperature: 2\u00d7106 K\n2009 - Maintained 320,000A plasma for 3.6 seconds.\n2010, November - First H-mode plasma run.\n2011 - Maintained high-temperature plasma for 5.2 seconds, Temperature: ~50\u00d7106 K, successfully fully deterred ELM (Edge-Localized Mode), first ever in the World.\n2012 - Maintained high-temperature plasma for 17 seconds, Temperature: 50\u00d7106 K\n2013 - Maintained high-temperature plasma for 20 seconds, Temperature: 50\u00d7106 K\n2014 - Maintained high-temperature plasma for 48 seconds, and successfully fully deterred ELM for 5 seconds.\n\n\n== References ==\n\n\n== External links ==\nKSTAR homepage\nEnglish KSTAR homepage\nKSTAR parameters re ITER and other tokamaks\n\nKSTAR Project Status PDF (undated - seems to be 2001. Includes slide-13 construction schedule to end 2004 and slide-16 operation from 2005 with upgrade planned 2010-11.)\nKSTAR Assembly Status, October 2006 PDF\nStatus and Result of the KSTAR Upgrade for the 2010\u2019s Campaign\nKSTAR ICRF transmission line system upgrade for load resilient operation. Jan 2013", 
                "titleUrl": "https://en.wikipedia.org/wiki/KSTAR", 
                "title": "KSTAR"
            }, 
            {
                "snippet": "Combustion Research Facility Continuous Electron Beam Accelerator Facility DIII-D Tokamak Facility Electron Microscopy Center for Materials Research Energy", 
                "pageCategories": "All articles needing coordinates\nCommons category with local link same as on Wikidata\nEnergy research institutes\nLaboratories in the United States\nNuclear history of the United States\nNuclear weapons program of the United States\nResearch institutes in the United States\nScience and technology in the United States\nUnited States Department of Energy\nUnited States Department of Energy facilities", 
                "pageContent": "The United States Department of Energy national laboratories and technology centers are a system of facilities and laboratories overseen by the United States Department of Energy (DOE) for the purpose of advancing science and technology to fulfill the DOE mission. Sixteen of the seventeen DOE national laboratories are federally funded research and development centers administered, managed, operated and staffed by private-sector organizations under management and operatings (M&O) contract with DOE.\n\n\n== History ==\nThe system of centralized national laboratories grew out of the massive scientific endeavors of World War II, in which new technologies such as radar, the computer, the proximity fuze, and the atomic bomb proved decisive for the Allied victory. Though the United States government had begun seriously investing in scientific research for national security since World War I, it was only in late 1930s and 1940s that monumental amounts of resources were committed or coordinated to wartime scientific problems, under the auspices first of the National Defense Research Committee, and later the Office of Scientific Research and Development, organized and administered by the MIT engineer Vannevar Bush.\nDuring the second world war, centralized sites such as the Radiation Laboratory at MIT and Ernest O. Lawrence's laboratory at the University of California, Berkeley allowed for a large number of expert scientists to collaborate towards defined goals as never before, and with virtually unlimited government resources at their disposal.\nIn the course of the war, the Allied nuclear effort, the Manhattan Project, created several secret sites for the purpose of bomb research and material development, including a laboratory in the mountains of New Mexico directed by Robert Oppenheimer (Los Alamos), and sites at Hanford, Washington and Oak Ridge, Tennessee. Hanford and Oak Ridge were administered by private companies, and Los Alamos was administered by a public university (the University of California). Additional success was had at the University of Chicago in reactor research, leading to the creation of Argonne National Laboratory outside Chicago, and at other academic institutions spread across the country.\nAfter the war and its scientific successes, the newly created Atomic Energy Commission took over the future of the wartime laboratories, extending their lives indefinitely (they were originally thought of as temporary creations). Funding and infrastructure were secured to sponsor other \"national laboratories\" for both classified and basic research, especially in physics. Each national laboratory would generally be centered around one or many expensive machines (such as particle accelerators or nuclear reactors).\nMost national laboratories maintained staffs of local researchers as well as allowing for visiting researchers to use their equipment, though priority to local or visiting researchers often varied from lab to lab. With their centralization of resources (both monetary and intellectual), the national labs serve as an exemplar for Big Science.\nElements of both competition and cooperation were encouraged in the laboratories. Often two laboratories with similar missions were created (such as Lawrence Livermore which was designed to compete with Los Alamos) with the hope that competition over funding would create a culture of high quality work. Laboratories which did not have overlapping missions would cooperate with each other (for example, Lawrence Livermore would cooperate with the Lawrence Berkeley Laboratory, which itself was often in competition with Brookhaven National Laboratory).\nThe national laboratory system, administered first by the Atomic Energy Commission, then the Energy Research and Development Administration, and currently the Department of Energy, is one of the largest (if not the largest) scientific research systems in the world. The DOE provides more than 40% of the total national funding for physics, chemistry, materials science, and other areas of the physical sciences. Many are locally managed by private companies, while others are managed by academic universities, and as a system they form one of the overarching and far-reaching components in what is known as the \"iron triangle\" of military, academia, and industry.\n\n\n== List of DOE National Laboratories and Technology Centers ==\n\n\n=== National Laboratories ===\n\nThe United States Department of Energy currently operates seventeen national laboratories:\nNational Energy Technology Laboratory**\nat Pittsburgh, Pennsylvania (1910)\nat Morgantown, West Virginia (1946)\nat Sugar Land, Texas (2000)\nat Fairbanks, Alaska (2001)\nat Albany, Oregon (2005)\nLawrence Berkeley National Laboratory* at Berkeley, California (1931)\nLos Alamos National Laboratory* at Los Alamos, New Mexico (1943)\nOak Ridge National Laboratory* at Oak Ridge, Tennessee (1943)\nArgonne National Laboratory* at DuPage County, Illinois (1946)\nAmes Laboratory* at Ames, Iowa (1947)\nBrookhaven National Laboratory* at Upton, New York (1947)\nSandia National Laboratories* at Albuquerque, New Mexico and Livermore, California (1948)\nIdaho National Laboratory* between Arco and Idaho Falls, Idaho (1949)\nPrinceton Plasma Physics Laboratory* at Princeton, New Jersey (1951)\nLawrence Livermore National Laboratory* at Livermore, California (1952)\nSavannah River National Laboratory* at Aiken, South Carolina (1952)\nSLAC National Accelerator Laboratory* at Menlo Park, California (1962)\nPacific Northwest National Laboratory* at Richland, Washington (1965)\nFermi National Accelerator Laboratory* at Batavia, Illinois (1967)\nNational Renewable Energy Laboratory* at Golden, Colorado (1977)\nThomas Jefferson National Accelerator Facility* at Newport News, Virginia (1984)\n\n\n=== Technology Centers ===\nBettis Atomic Power Laboratory at West Mifflin (Pittsburgh), Pennsylvania (1949)\nNew Brunswick Laboratory**, at Argonne National Laboratory\nOak Ridge Institute for Science and Education* at Oak Ridge, Tennessee\nRadiological and Environmental Sciences Laboratory**\nSavannah River Ecology Laboratory*\n* GOCO (Government-owned, Contractor-operated)\n** GOGO (Government-owned, Government-operated)\n\n\n== List of scientific user facilities ==\n\n\n== Further reading ==\nWestwick, Peter J. The National Labs: Science in an American System, 1947\u20131974. Cambridge: Harvard University Press, 2003, ISBN 978-0-674-00948-6.\n\n\n== See also ==\nUniversity-National Oceanographic Laboratory System\n\n\n== External links ==\nDOE.gov: Department of Energy National Laboratories website\nScience.energy.gov: U.S. Department of Energy; Ten-Year Plans for the Office of Science's National Laboratories\nDOE.gov: DOE Budget Page, with link to National Laboratories budgets", 
                "titleUrl": "https://en.wikipedia.org/wiki/United_States_Department_of_Energy_national_laboratories", 
                "title": "United States Department of Energy national laboratories"
            }, 
            {
                "snippet": "DIII can refer to: Division 3 Diablo 3 503 in Roman numerals  DIII-D (fusion reactor) Mercedes D.III  Pfalz D.III Albatros D.III Fokker D.III Siemens-Schuckert", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages\nFour-letter disambiguation pages", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/DIII_(disambiguation)", 
                "title": "DIII (disambiguation)"
            }
        ], 
        "phraseCharStart": "1036"
    }, 
    {
        "phraseCharEnd": "1047", 
        "phraseIndex": "T22", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "TCV", 
        "wikiSearchResults": [
            {
                "snippet": "TCV or T.C.V. can be an abbreviation for: TCV (album), an album of The Click Five, an American power pop band Terminus Centre-Ville, a public transit", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages\nThree-letter disambiguation pages", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/TCV", 
                "title": "TCV"
            }, 
            {
                "snippet": "Ten models were developed in the CV series: CV-2000, TCV-2010, TCV-2020, CV-2100, TCV-2110, TCV-2120, CV-2200, DV-2400, CV-2600 and CV-5100. Sony also", 
                "pageCategories": "Discontinued media formats\nFilm and video technology\nProducts introduced in 1965\nSony products\nVideotape", 
                "pageContent": "CV-2000 was one of the world's first home video tape recorders (VTR), introduced by Sony in August, 1965. The 'CV' in the model name stood for 'Consumer Video'. This was Sony's domestic format throughout the 1960s.\nThe CV-2000 was developed by Sony engineer Nobutoshi Kihara. On its release, each machine cost US$695. It used 1\u20442-inch-wide (13 mm) video tape in a reel-to-reel format, meaning the tape had to be manually threaded around the helical scan video head drum. The CV-2000 was one-tenth the weight and price of other analog video recording products of its era. It recorded television programs in black and white using the skip field process, which produced a maximum 200-lines resolution. The tape moved at a speed of 7.5 inches per second. Each reel of video tape cost US$40, and could hold one hour of video. Although CV-2000 was aimed at the home market, it was mainly used in business and educational institutions.\nTen models were developed in the CV series: CV-2000, TCV-2010, TCV-2020, CV-2100, TCV-2110, TCV-2120, CV-2200, DV-2400, CV-2600 and CV-5100. Sony also sold an optional 'Video Camera Ensemble', known as the VCK-2000. This add-on kit contained a separate video camera, a microphone, and a tripod.\nOne of its shortcomings as a format was the omission of the ability to adjust tracking. This made interchangeability of tapes between different machines almost impossible. Sony's later AV series machines already included this feature. The CV video recorders fell into disuse with the arrival of the EIAJ type 1 standard that was used by many companies, including Sony with their AV series machines.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/CV-2000", 
                "title": "CV-2000"
            }, 
            {
                "snippet": "The turnip crinkle virus (TCV) core promoter hairpin (Pr) is an RNA element located in the 3' UTR of the viral genome that is required for minus strand", 
                "pageCategories": "All stub articles\nCis-regulatory RNA elements\nMolecular and cellular biology stubs\nTombusviridae", 
                "pageContent": "The turnip crinkle virus (TCV) core promoter hairpin (Pr) is an RNA element located in the 3' UTR of the viral genome that is required for minus strand RNA synthesis. The picture shown is not the TCV core promoter, but an upstream hairpin that is also required for replication of the virus.\n\n\n== See also ==\nTurnip crinkle virus (TCV) repressor of minus strand synthesis H5\n\n\n== References ==\n\n\n== External links ==\n\nPage for Turnip crinkle virus (TCV) core promoter hairpin (Pr) at Rfam", 
                "titleUrl": "https://en.wikipedia.org/wiki/Turnip_crinkle_virus_(TCV)_core_promoter_hairpin_(Pr)", 
                "title": "Turnip crinkle virus (TCV) core promoter hairpin (Pr)"
            }, 
            {
                "snippet": "The TCV hairpin 5 (H5) is an RNA element found in the turnip crinkle virus. This RNA element is composed of a stem-loop that contains a large symmetrical", 
                "pageCategories": "All stub articles\nCis-regulatory RNA elements\nMolecular and cellular biology stubs\nTombusviridae", 
                "pageContent": "The TCV hairpin 5 (H5) is an RNA element found in the turnip crinkle virus. This RNA element is composed of a stem-loop that contains a large symmetrical internal loop (LSL). H5 can repress minus-strand synthesis when the 3' side of the LSL pairs with the 4 bases at the 3'-terminus of the RNA(GCCC-OH).\n\n\n== See also ==\nTurnip crinkle virus (TCV) core promoter hairpin (Pr)\n\n\n== References ==\n\n\n== External links ==\n\nPage for Turnip crinkle virus (TCV) repressor of minus strand synthesis H5 at Rfam", 
                "titleUrl": "https://en.wikipedia.org/wiki/Turnip_crinkle_virus_(TCV)_repressor_of_minus_strand_synthesis_H5", 
                "title": "Turnip crinkle virus (TCV) repressor of minus strand synthesis H5"
            }, 
            {
                "snippet": "The Tokamak \u00e0 configuration variable (TCV, literally \"variable configuration tokamak\") is a research fusion reactor of the \u00c9cole polytechnique f\u00e9d\u00e9rale", 
                "pageCategories": "All articles needing additional references\nArticles needing additional references from March 2016\nFusion power\nInterlanguage link template link number\nNuclear fusion\nNuclear research institutes\nResearch projects\nTokamaks\n\u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne", 
                "pageContent": "The Tokamak \u00e0 configuration variable (TCV, literally \"variable configuration tokamak\") is a research fusion reactor of the \u00c9cole polytechnique f\u00e9d\u00e9rale de Lausanne. Its distinguishing feature over other tokamaks is that its torus section is three times higher than wide. This allows studying several shapes of plasmas, which is particularly relevant since the shape of the plasma has links to the performance of the reactor. The TCV was set up in November 1992.\n\n\n== Characteristics ==\nPlasma height: 1.40 metres\nMinor radius: 0.25 metre\nMajor radius: 0.88 metre\nPlasma current: 1.2 megaamperes\nPlasma life span: 2 seconds maximum\nToroidal magnetic field: 1.43 teslas\nAdditional heating power: 4.5 megawatts\n\n\n== Main studies ==\nConfinement studies\nconfinement as a function of the shape of the plasma (triangular, square or elongated)\nImprovement of the confinement of the core\n\nStudies on vertically elongated plasmas\nStudies with ECRH and ECCD (electron cyclotron resonance heating and electron cyclotron current drive)\nBy 2012 it had 16 poloidal plasma shaping coils and could achieve a variety of field configurations and plasma shapes.\n\n\n== History ==\n1976: First proposal for an elongated tokamak by the \"New Swiss Association\"\n1985: Second proposal, with a more elongated tokamak\n1986: Acceptance of the TCV proposal (Tokamak \u00e0 Configuration Variable)\n1992: First plasma discharge\n1997: World record of plasma elongation (see plasma shaping)\nby August 2015 it had had a 19-month shutdown/upgrade to install its first neutral beam injector.\nby 2016 it was upgraded/enhanced to run with a 'snowflake' divertor\n\n\n== References ==\n\n\n== External links ==\nTCV official site\nTCV Technical data as of Oct 2012", 
                "titleUrl": "https://en.wikipedia.org/wiki/Tokamak_\u00e0_configuration_variable", 
                "title": "Tokamak \u00e0 configuration variable"
            }, 
            {
                "snippet": "The Bullet Troop-Carrying Vehicle (TCV) is a light 4x4 infantry fighting vehicle (IFV) developed by Rhodesia in the late 1970s based on the body of the", 
                "pageCategories": "1978 introductions\nArmoured personnel carriers of the Cold War\nMilitary of Rhodesia\nMilitary vehicles 1970\u20131979\nWeapons of Rhodesia\nWheeled infantry fighting vehicles", 
                "pageContent": "The Bullet Troop-Carrying Vehicle (TCV) is a light 4x4 infantry fighting vehicle (IFV) developed by Rhodesia in the late 1970s based on the body of the Mercedes-Benz Unimog light truck.\n\n\n== History ==\nAt the late 1970s when the Rhodesian Bush War was entering its final phase, the Rhodesian Security Forces (RhSF) were faced with an escalation towards conventional warfare when they learned that a mechanised built-up was being undertaken by the Zimbabwe People's Revolutionary Army (ZIPRA) guerrilla organisation based in neighbouring Zambia with material assistance from the Soviet Union. Eventually, by mid-1979 ZIPRA had brought to strength a fairly sizeable armoured corps trained by Cuban advisers, which aligned five BRDM-2 reconnaissance armoured cars, six to ten T-34/85 tanks and fifteen BTR-152 wheeled APCs.\nTo deal with the potential threat of a possible conventional ground invasion from across the border, the Rhodesian Armoured Car Regiment (RhACR) was reorganized in 1978, being expanded to corps strength to include additional tank and mechanized infantry squadrons. It soon became clear however, that the latter had to be provided with fast, more mobile troop-carrying vehicles (TCV) designed for conventional armoured warfare. The heavier locally tailored TCVs \u2013 conceived primarily for the counterinsurgency role \u2013 already in service with the Rhodesian SF were found to be not entirely suitable for the task so a lighter (and cheaper) alternative was sought.\n\n\n=== Development ===\nThe Bullet was originally developed by the Rhodesian private firm Zambesi Coachworks Ltd of Salisbury (now Harare) to meet a requirement put by the Rhodesian Army for a low-cost mine-protected IFV mounted on a Unimog chassis capable of carrying 10 men. The first prototype was completed in late 1978 (open-topped in the original design).\n\n\n== General description ==\nThe second prototype presented in 1978 was a low vehicle which consisted of an all-welded body with a fully enclosed troop compartment built on a modified Mercedes-Benz U1100 Unimog 416 2.5 ton light truck chassis. The hull or \u2018capsule\u2019 was faceted at the sides and rear, and a sloping glacis at the front, designed to deflect small-arms\u2019 rounds, along with a v-shaped bottom meant to deflect landmine blasts. The Diamond-shaped glacis had a pair of built-in round headlights at the sides of the radiator grid, a large dual-split front windscreen and two smaller side windows. Access to the vehicle\u2019s interior was made by means of two medium-sized doors at the hull rear whilst two roof hatches placed at the top of the troop compartment allowed for rapid debussing plus eight firing ports, six in the hull sides and two at the rear doors.\n\n\n=== Protection ===\nThe hull was made of ballistic 10mm mild steel plate; front windscreen and side windows had 40mm bullet-proof laminated glass.\n\n\n=== Armament ===\nA pintle-mounted FN MAG-58 7.62\u00d751mm NATO light machine-gun could be fitted on the top roof.\n\n\n== Service history ==\nAfter being rejected, it ended the war as a training vehicle for the RhACR and it was shown to the editor of Soldier of Fortune Magazine, Robert K. Brown in early 1979.\n\n\n== See also ==\nUR-416\nGazelle FRV\nCrocodile Armoured Personnel Carrier\nHippo APC\nMAP45 Armoured Personnel Carrier\nMAP75 Armoured Personnel Carrier\nMine Protected Combat Vehicle\n\n\n== Notes ==\n\n\n== References ==\nLaurent Touchard, Guerre dans le bush! Les blind\u00e9s de l\u2019Arm\u00e9e rhod\u00e9sienne au combat (1964-1979), Batailles & Blind\u00e9s Magazine n.\u00ba 72, April\u2013May 2016, pp. 64\u201375. ISSN 1765-0828 (in French)\nPeter Gerard Locke & Peter David Farquharson Cooke, Fighting Vehicles and Weapons of Rhodesia 1965-80, P&P Publishing, Wellington 1995. ISBN 0-473-02413-6\nPeter Stiff, Taming the Landmine, Galago Publishing Pty Ltd., Alberton (South Africa) 1986. ISBN 9780947020040\nRobert K. Brown, The Black Devils, Soldier of Fortune Magazine, January 1979.\n\n\n== External links ==\nPhotos of the Bullet in service with the RhACR as a training vehicle in 1979", 
                "titleUrl": "https://en.wikipedia.org/wiki/Bullet_TCV", 
                "title": "Bullet TCV"
            }, 
            {
                "snippet": "The Conservation Volunteers (TCV) is a community volunteering charity that works to create healthier and happier communities for everyone through environmental", 
                "pageCategories": "1959 establishments in the United Kingdom\nCharities based in South Yorkshire\nConservation organisations based in the United Kingdom\nEnvironmental organisations based in the United Kingdom\nEnvironmental organizations established in 1970\nUse British English from March 2012\nUse dmy dates from March 2012", 
                "pageContent": "The Conservation Volunteers (TCV) is a community volunteering charity that works to create healthier and happier communities for everyone through environmental conservation and practical tasks undertaken by volunteers. Whether improving wellbeing, conserving a well-loved outdoor space or bringing people together to promote social cohesion, combat loneliness or enhance employment prospects, TCV works together with communities to deliver practical solutions to the real life challenges they face (until 1 May 2012 traded as BTCV - British Trust for Conservation Volunteers).\nTCV has a for-profit trading arm, TCV Employment and Training Services Limited, which generates profit to feed the charity, (46% of income). The company has various government contracts to work with the long-term unemployed, aiming to improve skills and qualifications.\nTCV has 486 staff and works with 10,481 volunteers.\nIts strapline is Join in, feel good.\n\n\n== Overall aims ==\nTCV's vision is \"We want healthier, happier communities for everyone\".\nThe organisation's aims include:\nEnabling people to make a difference in their lives and improve the places around them.\nProviding opportunities and choice for people to improve their lives.\nLocal mobilisation to have a global impact (e.g. through activities to combat climate change).\nDelivering social and environmental equality.\nOn a practical level, TCV enables 628,000 volunteers per year to engage in conservation work in both the urban and the rural environment.\n\n\n=== Activities ===\nThe charity attempts to be inclusive and accessible to all, running a diverse range of activities across the UK. Many are focused around practical conservation work, but TCV also provides extensive training, work experience and education opportunities. TCV's projects are varied and include community gardens, food growing projects, taking care of parks and nature reserves, tree planting and woodland management, dry stone walling and projects to increase biodiversity. Projects also exist to help introduce children and young people to the environment as well as those helping to involve people with learning difficulties in environmental activities.\nSome of TCV's activities include:\nConservation projects\nGreen Gym, a programme to promote the health benefits of working in the outdoors\nConservation holidays, both in the UK and worldwide (ceased in February 2014)\nProviding support to local community groups\nProviding accredited training\nConsultancy\nEnvironmental education and waste education programmes for children and young people\nProviding training for the long-term unemployed\n\n\n== History ==\n\n\n=== The Conservation Corps ===\nIn 1959 the (then) Council for Nature appointed Brigadier Armstrong to form the Conservation Corps, with the objective of involving young volunteers, over the age of 16, in practical conservation work. The corp's first project was at Box Hill, Surrey, where 42 volunteers cleared dogwood to encourage the growth of juniper and distinctive chalk downland flora. One of the volunteers present was David Bellamy, who went on to become a Vice President of BTCV.\nBy 1964 the Conservation Corps had expanded its activities to include education and amenity work in the countryside. In 1966 it moved from a basement office at Queens Gate, Kensington, to new premises at London Zoo in Regent's Park. In 1968 the first training course for volunteers was held. By 1969 membership had increased to 600, and volunteers completed around 6000 workdays a year. The first ever international exchange visit to Czechoslovakia that year became the forerunner for the International Project Programme of today.\n\n\n=== The British Trust for Conservation Volunteers ===\nIn 1970 the Conservation Corps started to operate under the new name of British Trust for Conservation Volunteers (BTCV), with Prince Philip as Patron. In 1971 the local group affiliation scheme was launched.\nIn 1972 the Conserver magazine was launched.\nBy 1974 there were 3,000 registered volunteers and 57 groups had registered with BTCV.\nIn 1975 the BTCV Membership scheme was started\nIn 1977 BTCV set up an ecological park opposite the Tower of London as part of the Queen's Silver Jubilee celebrations.\nIn 1984 BTCV moved its headquarters to Wallingford, Oxfordshire.\n\n\n=== BTCV ===\nThe organisation underwent a second change of identity in 2000, taking the initialism BTCV as its new name in full.\nIn August 2006 BTCV moved to its present headquarters in Doncaster. The new \"environmentally friendly\" building features a sedum-covered roof \u2013 hence its name \u2013 Sedum House. The Scottish office is in Stirling, the Welsh office in Whitchurch, Cardiff and the Northern Ireland office in Belfast.\n\n\n=== The Conservation Volunteers ===\nIn May 2012, BTCV rebranded under the trading name The Conservation Volunteers (TCV).\nAt the group's annual general meeting in November 2012 the members of The Conservation Volunteers voted unanimously to change formally the name of the charity to The Conservation Volunteers.\n\n\n== See also ==\nThe Work Programme\nTrust for Urban Ecology\n\n\n== References ==\n\n\n== External links ==\nThe Conservation Volunteers website\nCharity Commission. TCV, registered charity no. 261009. \n\"TCV, Registered Charity no. SC039302\". Office of the Scottish Charity Regulator.", 
                "titleUrl": "https://en.wikipedia.org/wiki/The_Conservation_Volunteers", 
                "title": "The Conservation Volunteers"
            }, 
            {
                "snippet": "Tibetan Children's Villages or TCV is an integrated community in exile for the care and education of orphans, destitutes and refugee children from Tibet", 
                "pageCategories": "All articles needing additional references\nAll stub articles\nArticles needing additional references from April 2008\nChild refugees\nChildren's charities based in India\nEducation in Tibet\nEducational organisations in India\nOrganizations established in 1960\nOrphanages\nPhilanthropic organization stubs", 
                "pageContent": "Tibetan Children's Villages or TCV is an integrated community in exile for the care and education of orphans, destitutes and refugee children from Tibet. It is a registered, nonprofit charitable organization with its main facility based at Dharamsala in Himachel Pradesh, North India. TCV has a network spread across India with over 12,000 children under its care.\nFrom 1964 until 2006 the TCV has been presided by Jetsun Pema, sister of Dalai Lama Tenzin Gyatso. In 2009, The TCV established the first Tibetan college in exile in Bangalore (India) which was named \u201cThe Dalai Lama Institute for Higher Education\u201d. The goals of this college is to teach Tibetan language and Tibetan culture, but also science, arts, counseling and information technology to Tibetan students in exile.\n\n\n== See also ==\nList of organizations of Tibetans in exile\n\n\n== References ==\n\n\n== External links ==\nOfficial Tibetan Children's Villages website", 
                "titleUrl": "https://en.wikipedia.org/wiki/Tibetan_Children's_Villages", 
                "title": "Tibetan Children's Villages"
            }, 
            {
                "snippet": "crinkle virus (TCV) hairpin H4 is an RNA hairpin found at the 3' end of the Turnip crinkle virus (TCV) genome. The 3' end of the TCV genome contains", 
                "pageCategories": "All orphaned articles\nCis-regulatory RNA elements\nOrphaned articles from April 2014\nTombusviridae", 
                "pageContent": "In molecular biology, Turnip crinkle virus (TCV) hairpin H4 is an RNA hairpin found at the 3' end of the Turnip crinkle virus (TCV) genome.\nThe 3' end of the TCV genome contains the hairpin H4, a T-shaped structure (TSS), which contains hairpins 4a, 4b and hairpin 5 and a promoter hairpin. H4 enhances transcription and translation of the viral genome.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Turnip_crinkle_virus_(TCV)_hairpin_H4", 
                "title": "Turnip crinkle virus (TCV) hairpin H4"
            }, 
            {
                "snippet": "TCV is the third and final album by The Click Five. It was released in Asia in November 2010 on Warner Music and was released in Europe in May 2011 on", 
                "pageCategories": "2010 albums\n2010s pop album stubs\n2010s rock album stubs\nAll articles needing additional references\nAll articles with topics of unclear notability\nAll stub articles\nArticles needing additional references from December 2010\nArticles with hAudio microformats\nArticles with topics of unclear notability from December 2010\nThe Click Five albums", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/TCV_(album)", 
                "title": "TCV (album)"
            }
        ], 
        "phraseCharStart": "1044"
    }, 
    null, 
    null, 
    {
        "phraseCharEnd": "1076", 
        "phraseIndex": "T23", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "JT-60SA", 
        "wikiSearchResults": [
            {
                "snippet": "being disassembled to be upgraded to JT-60SA by using niobium-titanium superconducting coils. Construction of JT-60SA was underway in 2015 and will continue", 
                "pageCategories": "Interlanguage link template link number\nNuclear technology in Japan\nTokamaks", 
                "pageContent": "JT-60 (JT stands for Japan Torus) is the flagship of Japan's magnetic fusion program, previously run by the Japan Atomic Energy Research Institute (JAERI) and currently run by the Japan Atomic Energy Agency's (JAEA) Naka Fusion Institute [1] in Ibaraki Prefecture, Japan. In operation since 1985, it currently holds the record for the highest value of the fusion triple product achieved: 7028177000000000000\u26601.77\u00d71028 K\u00b7s\u00b7m\u22123 = 7021153000000000000\u26601.53\u00d71021 keV\u00b7s\u00b7m\u22123.\nJT-60 is a typical Tokamak with a D-shaped poloidal cross-section, similar to JET. Experimental results obtained by the reactor may be of importance to the ITER experiment as well as future tokamaks.\nDuring deuterium (D\u2013D fuel) plasma experiments in 1998 plasma conditions were achieved which would, if the D\u2013D fuel were replaced with a 1:1 mix of deuterium and tritium (D\u2013T fuel), have exceeded break-even\u2014the point where the power produced by the fusion reactions equals the power supplied to operate the machine. JT-60 does not have the facilities to handle tritium; currently only the JET tokamak in the United Kingdom has such facilities. In fusion terminology JT-60 achieved conditions which in D\u2013T would have provided Q = 1.25, where Q is the ratio of fusion power to input power. A self-sustaining nuclear fusion reaction would need a value of Q that is greater than 5.\nIn 2005 ferritic steel (ferromagnet) tiles were installed in the vacuum vessel to correct the magnetic field structure and hence reduce the loss of fast ions.\nOn May 9, 2006, the JAEA announced that the JT-60 had achieved a 28.6 second plasma duration time. The JAEA used new parts in the JT-60, having improved its capability to hold the plasma in its powerful toroidal magnetic field. The main future objective of JT-60 is to realize high-beta steady-state operation in the use of reduced radio-activation ferritic steel in a collision-less regime.\n\n\n== JT-60SA ==\nIn 2010 JT-60 is being disassembled to be upgraded to JT-60SA by using niobium-titanium superconducting coils. Construction of JT-60SA was underway in 2015 and will continue until 2018 with first plasma in 2019.\n\n\n== References ==\n\n\n== External links ==\nOfficial website\nJT-60 diagram and parameters\nJAERI (English)\nJT-60's Newly Achieved Plasma Record (Japanese)\nWorld Highest Fusion Triple Product Marked in High-\u03b2p H-mode Plasmas - Aug 1996 1.5*1021 m\u22123 s keV", 
                "titleUrl": "https://en.wikipedia.org/wiki/JT-60", 
                "title": "JT-60"
            }, 
            {
                "snippet": "main projects located in Japan: the Satellite Tokamak Programme project JT-60SA (super advanced), the International Fusion Materials Irradiation Facility", 
                "pageCategories": "Agencies of the European Union\nAll articles needing additional references\nArticles needing additional references from October 2015\nEuropean Atomic Energy Community\nWikipedia articles with possible conflicts of interest from October 2015", 
                "pageContent": "Fusion for Energy (F4E) is the European Union (EU) organisation responsible for Europe\u2019s contribution to ITER, the world\u2019s largest scientific partnership aiming to demonstrate fusion as a viable and sustainable source of energy. The organisation \u2013 formally known as the European Joint Undertaking for ITER and the Development of Fusion Energy \u2013was created under article 45 of the Euratom Treaty by the decision of the Council of the European Union on 27 March 2007 for a period of 35 years.\nF4E counts 400 members of staff and its offices are located in Barcelona, in Spain. One of its main tasks is to work together with European industry and research organisations to develop and provide a wide range of high technology components for the ITER project. The European Union is the host party for the ITER project. Its contribution amounts to 45%, while the other six parties have an in-kind contribution of approximately 9% each. Since 2008, F4E has been collaborating with at least 250 companies and more than 50 R&D organisations.\n\n\n== Mission and governance ==\nF4E\u2019s primary mission is to manage the European contribution to the ITER project; therefore it provides financial funds, which mostly come from the European Community budget. Among other tasks, F4E oversees the preparation of the ITER construction site in Saint-Paul-l\u00e8s-Durance, in France. F4E is formed by Euratom (represented by the European Commission), the Member States of the European Union and Switzerland, which participates as a third country. To ensure the overall supervision of its activities, the members sit on a governing board, which has a wide range of responsibilities including appointing the director.\n\n\n== Fusion energy ==\nFusion is the process which powers the sun, producing energy by fusing together light atoms such as hydrogen at extremely high pressures and temperatures. Fusion reactors use two forms of hydrogen, deuterium and tritium, as fuel.\nThe benefits of fusion energy are that it is an inherently safe process and it does not create greenhouse gases or long-lasting radioactive waste.\n\n\n== The ITER project ==\nITER, meaning \u201cthe way\u201d in Latin, is an international experiment aiming to demonstrate the scientific and technical feasibility of fusion as an energy source. The machine is being constructed in Saint-Paul-l\u00e8s-Durance in the South of France and is funded by seven parties: China, the European Union, India, Japan, Russia, South Korea and the United States. Collectively, the parties taking part in the ITER project represent over one half of the world\u2019s population and 80% of the global GDP.\n\n\n== The Broader Approach activities ==\nThe Broader Approach (BA) activities are three research projects carried out under an agreement between the European Atomic Energy Community (Euratom) and Japan, which contribute equally financially. They are meant to complement the ITER project and accelerate the development of fusion energy through R&D by cooperating on a number of projects of mutual interest.\nThis agreement entered into force on 1 June 2007 and runs for at least 10 years. The Broader Approach consists of three main projects located in Japan: the Satellite Tokamak Programme project JT-60SA (super advanced), the International Fusion Materials Irradiation Facility - Engineering Validation and Engineering Design Activities (IFMIF/EVEDA) and the International Fusion Energy Research Centre (IFERC).\n\n\n== The DEMO project ==\nF4E also aims to contribute to DEMO (Demonstration Power Plant). This experiment is supposed to generate significant amounts of electricity over extended periods and will be self-sufficient in tritium, one of the necessary gases to create fusion. The first commercial fusion electricity power plants are set to be established following DEMO, which is set to be larger in size than ITER and to produce significantly larger fusion power over long periods: a continuous production of up to 500 megawatts of electricity.\n\n\n== Management difficulties ==\nA report by the consultancy Ernst & Young published in 2013 by the European Parliament's Budgetary Control Committee found that F4E has suffered from significant management difficulties. According to the report, \"the organisation faced a series of internal problems that have only been gradually addressed, notably an organisational structure ill-adapted for project-oriented activities.\" From 2010, a host of reforms were undertaken within F4E, including a reshuffling and reorientation of the governance and management structures, as well as a cost-savings programme.\n\n\n== See also ==\nITER\nFusenet\nFusion power\nEuratom\n\n\n== References ==\n\n\n== External links ==\nFusion for Energy, the agency's home page.\nFusion for Energy: Understanding Fusion\nEuratom/fusion, the Fusion page of the EURATOM\n[1], the Broader Approach agreement", 
                "titleUrl": "https://en.wikipedia.org/wiki/Fusion_for_Energy", 
                "title": "Fusion for Energy"
            }, 
            {
                "snippet": "(ITER member) 2008: KSTAR, in Daejon, South Korea (ITER member) 2010: JT-60SA, in Naka, Japan (ITER member); upgraded from the JT-60. 2012: SST-1, in", 
                "pageCategories": "All accuracy disputes\nAll articles with dead external links\nAll articles with unsourced statements\nArticles containing Russian-language text\nArticles with Wayback Machine links\nArticles with dead external links from June 2016\nArticles with disputed statements from March 2014\nArticles with unsourced statements from December 2015\nCS1 errors: dates\nCommons category with local link same as on Wikidata", 
                "pageContent": "A tokamak (Russian: \u0442\u043e\u043a\u0430\u043c\u0430\u043a) is a device that uses a powerful magnetic field to confine plasma in the shape of a torus. Achieving a stable plasma equilibrium requires magnetic field lines that move around the torus in a helical shape. Such a helical field can be generated by adding a toroidal field (traveling around the torus in circles) and a poloidal field (traveling in circles orthogonal to the toroidal field). In a tokamak, the toroidal field is produced by electromagnets that surround the torus, and the poloidal field is the result of a toroidal electric current that flows inside the plasma. This current is induced inside the plasma with a second set of electromagnets.\nThe tokamak is one of several types of magnetic confinement devices, and is one of the most-researched candidates for producing controlled thermonuclear fusion power. Magnetic fields are used for confinement since no solid material could withstand the extremely high temperature of the plasma. An alternative to the tokamak is the stellarator. The world's largest tokamak project is the ITER (International Thermonuclear Experimental Reactor) being constructed in Saint-Paul-l\u00e8s-Durance, south of France. Scheduled to begin operation in 2020, it is expected to produce an output power of 500 megawatts.\nTokamaks were invented in the 1950s by Soviet physicists Igor Tamm and Andrei Sakharov, inspired by an original idea of Oleg Lavrentiev.\n\n\n== Etymology ==\nThe word tokamak is a transliteration of the Russian word \u0442\u043e\u043a\u0430\u043c\u0430\u043a, an acronym of either:\n\"\u0442\u043e\u0440\u043e\u0438\u0434\u0430\u043b\u044c\u043d\u0430\u044f \u043a\u0430\u043c\u0435\u0440\u0430 \u0441 \u043c\u0430\u0433\u043d\u0438\u0442\u043d\u044b\u043c\u0438 \u043a\u0430\u0442\u0443\u0448\u043a\u0430\u043c\u0438\" (toroidal'naya kamera s magnitnymi katushkami) \u2014 toroidal chamber with magnetic coils;\nor\n\"\u0442\u043e\u0440\u043e\u0438\u0434\u0430\u043b\u044c\u043d\u0430\u044f \u043a\u0430\u043c\u0435\u0440\u0430 \u0441 \u0430\u043a\u0441\u0438\u0430\u043b\u044c\u043d\u044b\u043c \u043c\u0430\u0433\u043d\u0438\u0442\u043d\u044b\u043c \u043f\u043e\u043b\u0435\u043c\" (toroidal'naya kamera s aksial'nym magnitnym polem) \u2014 toroidal chamber with axial magnetic field.\n\n\n== History ==\n\nAlthough nuclear fusion research began soon after World War II, the programs in various countries were each initially classified as secret. It was not until after the 1955 United Nations International Conference on the Peaceful Uses of Atomic Energy in Geneva that programs were declassified and international scientific collaboration could take place.\nExperimental research of tokamak systems started in 1956 in Kurchatov Institute, Moscow by a group of Soviet scientists led by Lev Artsimovich. The group constructed the first tokamaks, the most successful being T-3 and its larger version T-4. T-4 was tested in 1968 in Novosibirsk, conducting the first ever quasistationary thermonuclear fusion reaction.\nIn 1968, at the third IAEA International Conference on Plasma Physics and Controlled Nuclear Fusion Research at Novosibirsk, Soviet scientists announced that they had achieved electron temperatures of over 1000 eV in a tokamak device. British and American scientists met this news with skepticism since they were far from reaching that benchmark; they remained suspicious until laser scattering tests confirmed the findings the next year.\nIn 1973 design work on JET, the Joint European Torus, began.\nIn 1978, Bob Guccione, publisher of Penthouse Magazine met Robert Bussard and became the world's biggest and most committed private investor  in fusion technology, ultimately putting $20 Million ($60 Million in 2016 dollars) of his own money into Bussard's Compact Tokamak. Guccione may discovered Fusion in a 1974 essay titled God's Big Fix  published in Playboy a competing men's magazine.\n\n\n== Toroidal design ==\n\nPositively and negatively charged ions and negatively charged electrons in a fusion plasma are at very high temperatures, and have correspondingly large velocities. In order to maintain the fusion process, particles from the hot plasma must be confined in the central region, or the plasma will rapidly cool. Magnetic confinement fusion devices exploit the fact that charged particles in a magnetic field experience a Lorentz force and follow helical paths along the field lines.\nEarly fusion research devices were variants on the Z-pinch and used electric current to generate a poloidal magnetic field to contain the plasma along a linear axis between two points. Researchers discovered that a simple toroidal field, in which the magnetic field lines run in circles around an axis of symmetry, confines a plasma hardly better than no field at all. This can be understood by looking at the orbits of individual particles. The particles not only spiral around the field lines, they also drift across the field. Since a toroidal field is curved and decreases in strength moving away from the axis of rotation, the ions and the electrons move parallel to the axis, but in opposite directions. The charge separation leads to an electric field and an additional drift, in this case outward (away from the axis of rotation) for both ions and electrons. Alternatively, the plasma can be viewed as a torus of fluid with a magnetic field frozen in. The plasma pressure results in a force that tends to expand the torus. The magnetic field outside the plasma cannot prevent this expansion. The plasma simply slips between the field lines.\nFor a toroidal plasma to be effectively confined by a magnetic field, there must be a twist to the field lines. There are then no longer flux tubes that simply encircle the axis, but, if there is sufficient symmetry in the twist, flux surfaces. Some of the plasma in a flux surface will be on the outside (larger major radius, or \"low-field side\") of the torus and will drift to other flux surfaces farther from the circular axis of the torus. Other portions of the plasma in the flux surface will be on the inside (smaller major radius, or \"high-field side\"). Since some of the outward drift is compensated by an inward drift on the same flux surface, there is a macroscopic equilibrium with much improved confinement. Another way to look at the effect of twisting the field lines is that the electric field between the top and the bottom of the torus, which tends to cause the outward drift, is shorted out because there are now field lines connecting the top to the bottom.\nWhen the problem is considered even more closely, the need for a vertical (parallel to the axis of rotation) component of the magnetic field arises. The Lorentz force of the toroidal plasma current in the vertical field provides the inward force that holds the plasma torus in equilibrium.\n\n\n=== Advanced tokamaks ===\nSince about 1990 tokamaks are designed to operate in high-confinement mode to reduce plasma and energy losses.\nAdvanced or 2nd generation tokamaks generally use a 'C' or 'D' shaped plasma cross-section.\n\n\n=== Plasma disruptions ===\nAt the necessarily large toroidal currents (15 megaamperes in ITER) the tokamak concept suffers from a fundamental problem of stability. The nonlinear evolution of magnetohydrodynamical instabilities leads to a dramatic quench of the plasma current within milliseconds. Very energetic electrons are created (runaway electrons) and finally a global loss of confinement happens. At that point very intense radiation is inflicted on small areas. This phenomenon is called a major disruption. The occurrence of major disruptions in running tokamaks has always been rather high, of the order of a few percent of the total numbers of the shots. In currently operated tokamaks, the damage is often large but rarely dramatic. In the ITER tokamak, it is expected that the occurrence of a limited number of major disruptions will definitively damage the chamber with no possibility to restore the device.\nA large amplitude of the central current density can also result in internal disruptions, or sawteeth, which do not generally result in termination of the discharge.\n\n\n== Plasma heating ==\nIn an operating fusion reactor, part of the energy generated will serve to maintain the plasma temperature as fresh deuterium and tritium are introduced. However, in the startup of a reactor, either initially or after a temporary shutdown, the plasma will have to be heated to its operating temperature of greater than 10 keV (over 100 million degrees Celsius). In current tokamak (and other) magnetic fusion experiments, insufficient fusion energy is produced to maintain the plasma temperature.\n\n\n=== Ohmic heating ~ inductive mode ===\nSince the plasma is an electrical conductor, it is possible to heat the plasma by inducing a current through it; in fact, the induced current that heats the plasma usually provides most of the poloidal field. The current is induced by slowly increasing the current through an electromagnetic winding linked with the plasma torus: the plasma can be viewed as the secondary winding of a transformer. This is inherently a pulsed process because there is a limit to the current through the primary (there are also other limitations on long pulses). Tokamaks must therefore either operate for short periods or rely on other means of heating and current drive. The heating caused by the induced current is called ohmic (or resistive) heating; it is the same kind of heating that occurs in an electric light bulb or in an electric heater. The heat generated depends on the resistance of the plasma and the amount of electric current running through it. But as the temperature of heated plasma rises, the resistance decreases and ohmic heating becomes less effective. It appears that the maximum plasma temperature attainable by ohmic heating in a tokamak is 20-30 million degrees Celsius. To obtain still higher temperatures, additional heating methods must be used.\n\n\n=== Neutral-beam injection ===\n\nNeutral-beam injection involves the introduction of high energy (rapidly moving) atoms (molecules) into an ohmically heated, magnetically confined plasma within the tokamak. The high energy atoms (molecules) originate as ions in an arc chamber before being extracted through a high voltage grid set. The term \"ion source\" is used to generally mean the assembly consisting of a set of electron emitting filaments, an arc chamber volume, and a set of extraction grids. The extracted ions travel through a neutralizer section of the beamline where they gain enough electrons to become neutral atoms (molecules) but retain the high velocity imparted to them from the ion source. Once the neutral beam enters the tokamak, interactions with the main plasma ions occur which significantly heat the bulk plasma and bring it closer to fusion-relevant temperatures. Ion source extraction voltages are typically of the order 50-100 kV, and high voltage, negative ion sources (-1 MV) are being developed for ITER. The ITER Neutral Beam Test Facility in Padova will be the first ITER facility to start operation. While neutral beam injection is used primarily for plasma heating, it can also be used as a diagnostic tool and in feedback control by making a pulsed beam consisting of a string of brief 2-10 ms beam blips. Deuterium is a primary fuel for neutral beam heating systems and hydrogen and helium are sometimes used for selected experiments.\n\n\n=== Magnetic compression ===\nA gas can be heated by sudden compression. In the same way, the temperature of a plasma is increased if it is compressed rapidly by increasing the confining magnetic field. In a tokamak system this compression is achieved simply by moving the plasma into a region of higher magnetic field (i.e., radially inward). Since plasma compression brings the ions closer together.\n\n\n=== Radio-frequency heating ===\n\nHigh-frequency electromagnetic waves are generated by oscillators (often by gyrotrons or klystrons) outside the torus. If the waves have the correct frequency (or wavelength) and polarization, their energy can be transferred to the charged particles in the plasma, which in turn collide with other plasma particles, thus increasing the temperature of the bulk plasma. Various techniques exist including electron cyclotron resonance heating (ECRH) and ion cyclotron resonance heating. This energy is usually transferred by microwaves.\n\n\n== Tokamak particle inventory ==\nPlasma discharges within the tokamak's vacuum chamber consist of energized ions and atoms and the energy from these particles eventually reaches the inner wall of the chamber through radiation, collisions, or lack of confinement. The inner wall of the chamber is water-cooled and the heat from the particles is removed via conduction through the wall to the water and convection of the heated water to an external cooling system. Turbomolecular or diffusion pumps allow for particles to be evacuated from the bulk volume and cryogenic pumps, consisting of a liquid helium-cooled surface, serve to effectively control the density throughout the discharge by providing an energy sink for condensation to occur. When done correctly, the fusion reactions produce large amounts of high energy neutrons. Being electrically neutral and relatively tiny, the neutrons are not affected by the magnetic fields nor are they stopped much by the surrounding vacuum chamber. The neutron flux is reduced significantly at a purpose-built neutron shield boundary that surrounds the tokamak in all directions. Shield materials vary, but are generally materials made of atoms which are close to the size of neutrons because these work best to absorb the neutron and its energy. Good candidate materials include those with much hydrogen, such as water and plastics. Boron atoms are also good absorbers of neutrons. Thus, concrete and polyethylene doped with boron make inexpensive neutron shielding materials. Once freed, the neutron has a relatively short half-life of about 10 minutes before it decays into a proton and electron with the emission of energy. When the time comes to actually try to make electricity from a tokamak-based reactor, some of the neutrons produced in the fusion process would be absorbed by a liquid metal blanket and their kinetic energy would be used in heat-transfer processes to ultimately turn a generator.\n\n\n== Experimental tokamaks ==\n\n\n=== Currently in operation ===\n(in chronological order of start of operations)\n\n1960s: TM1-MH (since 1977 Castor; since 2007 Golem) in Prague, Czech Republic. In operation in Kurchatov Institute since early 1960s but renamed to Castor in 1977 and moved to IPP CAS, Prague; in 2007 moved to FNSPE, Czech Technical University in Prague and renamed to Golem.\n1975: T-10, in Kurchatov Institute, Moscow, Russia (formerly Soviet Union); 2 MW\n1983: Joint European Torus (JET), in Culham, United Kingdom\n1985: JT-60, in Naka, Ibaraki Prefecture, Japan; (Currently undergoing upgrade to Super, Advanced model)\n1987: STOR-M, University of Saskatchewan; Canada; first demonstration of alternating current in a tokamak.\n1988: Tore Supra, at the CEA, Cadarache, France\n1989: Aditya, at Institute for Plasma Research (IPR) in Gujarat, India\n1980s: DIII-D, in San Diego, USA; operated by General Atomics since the late 1980s\n1989: COMPASS, in Prague, Czech Republic; in operation since 2008, previously operated from 1989 to 1999 in Culham, United Kingdom\n1990: FTU, in Frascati, Italy\n1991: Tokamak ISTTOK, at the Instituto de Plasmas e Fus\u00e3o Nuclear, Lisbon, Portugal;\n\n1991: ASDEX Upgrade, in Garching, Germany\n1992: H-1NF (H-1 National Plasma Fusion Research Facility) based on the H-1 Heliac device built by Australia National University's plasma physics group and in operation since 1992\n1992: Alcator C-Mod, MIT, Cambridge, USA\n1992: Tokamak \u00e0 configuration variable (TCV), at the EPFL, Switzerland\n1994: TCABR, at the University of S\u00e3o Paulo, S\u00e3o Paulo, Brazil; this tokamak was transferred from Centre des Recherches en Physique des Plasmas in Switzerland\n1995: HT-7, in Hefei, China\n1999: MAST, in Culham, United Kingdom\n1999: NSTX in Princeton, New Jersey\n1999: Globus-M in Ioffe Institute, Saint Petersburg, Russia\n1990s: Pegasus Toroidal Experiment at the University of Wisconsin-Madison; in operation since the late 1990s\n2002: HL-2A, in Chengdu, China\n2006: EAST (HT-7U), in Hefei, China (ITER member)\n2008: KSTAR, in Daejon, South Korea (ITER member)\n2010: JT-60SA, in Naka, Japan (ITER member); upgraded from the JT-60.\n2012: SST-1, in Gandhinagar, India (ITER member); the Institute for Plasma Research reports 1000 seconds operation.\n2012: IR-T1, Islamic Azad University, Science and Research Branch, Tehran, Iran\n2012: ST25 at Tokamak Energy at Culham, Oxfordshire, UK (now at Milton Park)\n2014: ST25 (HTS) the first tokamak to have all magnetic fields formed from high temperature superconducting magnets, at Tokamak Energy based in Oxfordshire, UK\n\n\n=== Previously operated ===\n\n1960s: T-3 and T-4, in Kurchatov Institute, Moscow, Russia (formerly Soviet Union); T-4 in operation in 1968.\n1963: LT-1, Australia National University's plasma physics group built the first tokamak outside of Soviet Union c. 1963\n1971\u20131980: Texas Turbulent Tokamak, University of Texas at Austin, USA\n1973\u20131976: Tokamak de Fontenay aux Roses (TFR), near Paris, France\n1973\u20131979: Alcator A, MIT, USA\n1978\u20131987: Alcator C, MIT, USA\n1978\u20132013: TEXTOR, in J\u00fclich, Germany\n1979\u20131998: MT-1 Tokamak, Budapest, Hungary (Built at the Kurchatov Institute, Russia, transported to Hungary in 1979, rebuilt as MT-1M in 1991)\n1980\u20132004: TEXT/TEXT-U, University of Texas at Austin, USA\n1982\u20131997: TFTR, Princeton University, USA\n1983\u20132000: Novillo Tokamak, at the Instituto Nacional de Investigaciones Nucleares,in Mexico City, Mexico\n1984\u20131992: HL-1 Tokamak, in Chengdu, China\n1987\u20131999: Tokamak de Varennes; Varennes, Canada; operated by Hydro-Qu\u00e9bec and used by researchers from Institut de recherche en \u00e9lectricit\u00e9 du Qu\u00e9bec (IREQ) and the Institut national de la recherche scientifique (INRS)\n1988\u20132005: T-15, in Kurchatov Institute, Moscow, Russia (formerly Soviet Union); 10 MW\n1991\u20131998: START in Culham, United Kingdom\n1990s\u20132001: COMPASS, in Culham, United Kingdom\n1994\u20132001: HL-1M Tokamak, in Chengdu, China\n1999\u20132005: UCLA Electric Tokamak, in Los Angeles, USA\n\n\n=== Planned ===\nITER, international project in Cadarache, France; 500 MW; construction began in 2010, first plasma expected in 2020.\nDEMO; 2000 MW, continuous operation, connected to power grid. Planned successor to ITER; construction to begin in 2024 according to preliminary timetable.\nCFETR, also known as \"China Fusion Engineering Test Reactor\"; 200 MW; Next generation Chinese fusion reactor, is a new tokamak device.\n\n\n== See also ==\n\nMagnetic mirrors\nEdge-Localized Mode\nStellarator\nReversed-field pinch\nList of plasma (physics) articles\nDivertor\nBall-pen probe\nThe section on Dimensionless parameters in tokamaks in the article on Plasma scaling\nARC fusion reactor\n\n\n== Notes ==\n\n\n== References ==\nBraams, C.M. & Stott, P.E. (2002). Nuclear Fusion: Half a Century of Magnetic Confinement Research. Institute of Physics Publishing. ISBN 0-7503-0705-6. \nDolan, Thomas J. (1982). Fusion Research, Volume 1 - Principles. Pergamon Press. LCC QC791.D64. \nNishikawa, K. & Wakatani, M. (2000). Plasma Physics. Springer-Verlag. ISBN 3-540-65285-X. \nRaeder, J.; et al. (1986). Controlled Nuclear Fusion. John Wiley & Sons. ISBN 0-471-10312-8. \nWesson, John (2000). The Science of JET (PDF). \nWesson, John; et al. (2004). Tokamaks. Oxford University Press. ISBN 0-19-850922-7. \n\n\n== External links ==\nCCFE - site from the UK fusion research centre CCFE.\nInt'l Tokamak research - various that relate to ITER\nPlasma Science - site on tokamaks from the French CEA.\nFusion Programs at General Atomics, including the DIII-D National Fusion Facility, an experimental tokamak.\nGeneral Atomics DIII-D Program\nFusion and Plasma Physics Seminar at MIT OCW\nUnofficial ITER fan club, Club for fans of the biggest tokamak planned to be built in near future.\nwww.tokamak.info Extensive list of current and historic tokamaks from around the world.\nSSTC-1 Overview video of a small scale tokamak concept.\nSSTC-2 on YouTube Section View Video of a small scale tokamak concept.\nSSTC-3 on YouTube Fly Through Video of a small scale tokamak concept.\nLAP_Tokamak_Development Information on conditions necessary for nuclear reaction in a tokamak reactor\nA. P. Frass (1973). \"Engineering Problems In The Design Of Controlled Thermonuclear Reactors\" (PDF). Oak Ridge National Laboratory. Retrieved September 2013.  \nObserver Newspaper Article on Tokomak Nuclear fusion and the promise of a brighter tomorrow", 
                "titleUrl": "https://en.wikipedia.org/wiki/Tokamak", 
                "title": "Tokamak"
            }, 
            {
                "snippet": "Materials Irradiation Facility, proposed, construction not started JT-60/JT-60SA EAST (Experimental Advanced Superconducting Tokamak) National Ignition", 
                "pageCategories": "All articles with dead external links\nAll articles with unsourced statements\nArticles with French-language external links\nArticles with dead external links from August 2012\nArticles with unsourced statements from April 2008\nArticles with unsourced statements from December 2015\nArticles with unsourced statements from February 2007\nArticles with unsourced statements from March 2016\nBuildings and structures in Bouches-du-Rh\u00f4ne\nCommons category with local link same as on Wikidata", 
                "pageContent": "ITER (International Thermonuclear Experimental Reactor, and is also Latin for \"the way\") is an international nuclear fusion research and engineering megaproject, which will be the world's largest magnetic confinement plasma physics experiment. It is an experimental tokamak nuclear fusion reactor that is being built next to the Cadarache facility in Saint-Paul-l\u00e8s-Durance, south of France.\nThe ITER project aims to make the long-awaited transition from experimental studies of plasma physics to full-scale electricity-producing fusion power stations. The ITER fusion reactor has been designed to produce 500 megawatts of output power for several seconds while needing 50 megawatts to operate. Thereby the machine aims to demonstrate the principle of producing more energy from the fusion process than is used to initiate it, something that has not yet been achieved in any fusion reactor.\nThe project is funded and run by seven member entities\u2014the European Union, India, Japan, China, Russia, South Korea, and the United States. The EU, as host party for the ITER complex, is contributing about 45 percent of the cost, with the other six parties contributing approximately 9 percent each.\nConstruction of the ITER Tokamak complex started in 2013 and the building costs are now over US$14 billion as of June 2015. The facility is expected to finish its construction phase in 2019 and will start commissioning the reactor that same year and initiate plasma experiments in 2020 with full deuterium\u2013tritium fusion experiments starting in 2027. If ITER becomes operational, it will become the largest magnetic confinement plasma physics experiment in use, surpassing the Joint European Torus. The first commercial demonstration fusion power station, named DEMO, is proposed to follow on from the ITER project.\n\n\n== Background ==\nFusion power has the potential to provide sufficient energy to satisfy mounting demand, and to do so sustainably, with a relatively small impact on the environment.\nNuclear fusion has many potential attractions. Firstly, its hydrogen isotope fuels are relatively abundant \u2013 one of the necessary isotopes, deuterium, can be extracted from seawater, while the other fuel, tritium, would be bred from a lithium blanket using neutrons produced in the fusion reaction itself. Furthermore, a fusion reactor would produce virtually no CO2 or atmospheric pollutants, and its other radioactive waste products would be very short-lived compared to those produced by conventional nuclear reactors.\nOn 21 November 2006, the seven participants formally agreed to fund the creation of a nuclear fusion reactor. The program is anticipated to last for 30 years \u2013 10 for construction, and 20 of operation. ITER was originally expected to cost approximately \u20ac5billion, but the rising price of raw materials and changes to the initial design have seen that amount almost triple to \u20ac13billion. The reactor is expected to take 10 years to build with completion scheduled for 2019. Site preparation has begun in Cadarache, France, and procurement of large components has started.\nITER is designed to produce approximately 500 MW of fusion power sustained for up to 1,000 seconds (compared to JET's peak of 16 MW for less than a second) by the fusion of about 0.5 g of deuterium/tritium mixture in its approximately 840 m3 reactor chamber. Although ITER is expected to produce (in the form of heat) 10 times more energy than the amount consumed to heat up the plasma to fusion temperatures, the generated heat will not be used to generate any electricity.\nITER was originally an acronym for International Thermonuclear Experimental Reactor, but that title was eventually dropped due to the negative popular connotations of the word \"thermonuclear\", especially when used in conjunction with \"experimental\". \"Iter\" also means \"journey\", \"direction\" or \"way\" in Latin, reflecting ITER's potential role in harnessing nuclear fusion as a peaceful power source.\n\n\n== Organization history ==\n\nITER began in 1985 as a Reagan\u2013Gorbachev initiative with the equal participation of the Soviet Union, European Union (through European Atomic Energy Community), the United States, and Japan through the 1988\u20131998 initial design phases. Preparations for the first Gorbachev-Reagan Summit showed that there were no tangible agreements in the works for the summit.\nOne energy research project, however, was being considered quietly by two physicists, Alvin Trivelpiece and Evgeny Velikhov. The project involved collaboration on the next phase of magnetic fusion research \u2014 the construction of a demonstration model. At the time, magnetic fusion research was ongoing in Japan, Europe, the Soviet Union and the US. Velikhov and Trivelpiece believed that taking the next step in fusion research would be beyond the budget of any of the key nations and that collaboration would be useful internationally.\nA major bureaucratic fight erupted in the US government over the project. One argument against collaboration was that the Soviets would use it to steal US technology and know-how. A second was symbolic \u2014 the Soviet physicist Andrei Sakharov was in internal exile and the US was pushing the Soviet Union on its human rights record. The United States National Security Council convened a meeting under the direction of William Flynn Martin that resulted in a consensus that the US should go forward with the project.\nMartin and Velikhov concluded the agreement that was agreed at the summit and announced in the last paragraph of this historic summit meeting, \"... The two leaders emphasized the potential importance of the work aimed at utilizing controlled thermonuclear fusion for peaceful purposes and, in this connection, advocated the widest practicable development of international cooperation in obtaining this source of energy, which is essentially inexhaustible, for the benefit for all mankind.\"\nConceptual and engineering design phases carried out under the auspices of the IAEA led to an acceptable, detailed design in 2001, underpinned by US$650 million worth of research and development by the \"ITER Parties\" to establish its practical feasibility. These parties, namely EU, Japan, Russian Federation (replacing the Soviet Union), and United States (which opted out of the project in 1999 and returned in 2003), were joined in negotiations by China, South Korea, and Canada (who then terminated its participation at the end of 2003). India officially became part of ITER on December 2005.\nOn 28 June 2005, it was officially announced that ITER would be built in the European Union in Southern France. The negotiations that led to the decision ended in a compromise between the EU and Japan, in that Japan was promised 20% of the research staff on the French location of ITER, as well as the head of the administrative body of ITER. In addition, another research facility for the project will be built in Japan, and the European Union has agreed to contribute about 50% of the costs of this institution.\nOn 21 November 2006, an international consortium signed a formal agreement to build the reactor. On 24 September 2007, the People's Republic of China became the seventh party to deposit the ITER Agreement to the IAEA. Finally, on 24 October 2007, the ITER Agreement entered into force and the ITER Organization legally came into existence.\n\n\n== Objectives ==\nITER's mission is to demonstrate the feasibility of fusion power, and prove that it can work without negative impact. Specifically, the project aims:\nTo momentarily produce ten times more thermal energy from fusion heating than is supplied by auxiliary heating (a Q value equals 10).\nTo produce a steady-state plasma with a Q value greater than 5. (Q = 1 is breakeven.)\nTo maintain a fusion pulse for up to 8 minutes.\nTo ignite a \"burning\" (self-sustaining) plasma. (i.e. 'ignition' see Lawson criterion)\nTo develop technologies and processes needed for a fusion power station \u2014 including superconducting magnets and remote handling (maintenance by robot).\nTo verify tritium breeding concepts.\nTo refine neutron shield/heat conversion technology (most of the energy in the D+T fusion reaction is released in the form of fast neutrons).\n\n\n== Timeline and current status ==\nIn 1978, the EC, Japan, USA, and USSR joined in the International Tokamak Reactor (INTOR) Workshop, under the auspices of the International Atomic Energy Agency (IAEA), to assess the readiness of magnetic fusion to move forward to the experimental power reactor (EPR) stage, to identify the additional R&D that must be undertaken, and to define the characteristics of such an EPR by means of a conceptual design.\nHundreds of fusion scientists and engineers in each participating country took part in a detailed assessment of the then present status of the tokamak confinement concept vis-a-vis the requirements of an EPR, identified the required R&D by early 1980, and produced a conceptual design by mid-1981.\nTimeline:\n1985. At the Geneva summit meeting in 1985, Mikhail Gorbachev suggested to Ronald Reagan that the two countries jointly undertake the construction of a tokamak EPR as proposed by the INTOR Workshop. The ITER project was initiated in 1988.\n1988. Conceptual design activities ran from 1988 to 1990.\n1992. Engineering design activities started.\n1998. In June, the 'Final design' from the Engineering Design Activities was approved.\n2001. In July, the \"cost-cutting\" 'ITER-FEAT' design was agreed.\n2006. The ITER project was formally agreed to and funded with a cost estimate of \u20ac10 billion ($12.8 billion) projecting the start of construction in 2008 and completion a decade later.\n2007. In September, fourteen major design changes were agreed to the 2001 design.\n2013. The project had run into many delays and budget overruns. The facility is not expected to begin operations at the schedule initially anticipated.\n2014. In February, The New Yorker published the ITER Management Assessment report, listing 11 essential recommendations, for example: \"Create a Project Culture\", \"Instill a Nuclear Safety Culture\", \"Develop a realistic ITER Project Schedule\" and \"Simplify and Reduce the IO Bureaucracy\". The USA considered withdrawal, but is still participating in ITER.\n2015. In November a project review concludes that the schedule may need extending by at least six years; (e.g. first plasma in 2026).\n2016. Atomic Energy Organization of Iran completed the preliminary work for Iran to join ITER\n\n\n== Reactor overview ==\n\nWhen deuterium and tritium fuse, two nuclei come together to form a helium nucleus (an alpha particle), and a high-energy neutron.\n2\n1D + 3\n1T \u2192 4\n2He + 1\n0n + 6988281983061712000\u266017.6 MeV\nWhile nearly all stable isotopes lighter on the periodic table than iron-56 and nickel-62, which have the highest binding energy per nucleon, will fuse with some other isotope and release energy, deuterium and tritium are by far the most attractive for energy generation as they require the lowest activation energy (thus lowest temperature) to do so, while producing among the most energy per unit weight.\nAll proto- and mid-life stars radiate enormous amounts of energy generated by fusion processes. Mass for mass, the deuterium\u2013tritium fusion process releases roughly three times as much energy as uranium-235 fission, and millions of times more energy than a chemical reaction such as the burning of coal. It is the goal of a fusion power station to harness this energy to produce electricity.\nActivation energies for fusion reactions are generally high because the protons in each nucleus will tend to strongly repel one another, as they each have the same positive charge. A heuristic for estimating reaction rates is that nuclei must be able to get within 100 femtometer (1 \u00d7 10\u221213 meter) of each other, where the nuclei are increasingly likely to undergo quantum tunneling past the electrostatic barrier and the turning point where the strong nuclear force and the electrostatic force are equally balanced, allowing them to fuse. In ITER, this distance of approach is made possible by high temperatures and magnetic confinement. High temperatures give the nuclei enough energy to overcome their electrostatic repulsion (see Maxwell\u2013Boltzmann distribution). For deuterium and tritium, the optimal reaction rates occur at temperatures on the order of 100,000,000 K. The plasma is heated to a high temperature by ohmic heating (running a current through the plasma). Additional heating is applied using neutral beam injection (which cross magnetic field lines without a net deflection and will not cause a large electromagnetic disruption) and radio frequency (RF) or microwave heating.\nAt such high temperatures, particles have a large kinetic energy, and hence velocity. If unconfined, the particles will rapidly escape, taking the energy with them, cooling the plasma to the point where net energy is no longer produced. A successful reactor would need to contain the particles in a small enough volume for a long enough time for much of the plasma to fuse. In ITER and many other magnetic confinement reactors, the plasma, a gas of charged particles, is confined using magnetic fields. A charged particle moving through a magnetic field experiences a force perpendicular to the direction of travel, resulting in centripetal acceleration, thereby confining it to move in a circle or helix around the lines of magnetic flux.\nA solid confinement vessel is also needed, both to shield the magnets and other equipment from high temperatures and energetic photons and particles, and to maintain a near-vacuum for the plasma to populate. The containment vessel is subjected to a barrage of very energetic particles, where electrons, ions, photons, alpha particles, and neutrons constantly bombard it and degrade the structure. The material must be designed to endure this environment so that a power station would be economical. Tests of such materials will be carried out both at ITER and at IFMIF (International Fusion Materials Irradiation Facility).\nOnce fusion has begun, high energy neutrons will radiate from the reactive regions of the plasma, crossing magnetic field lines easily due to charge neutrality (see neutron flux). Since it is the neutrons that receive the majority of the energy, they will be ITER's primary source of energy output. Ideally, alpha particles will expend their energy in the plasma, further heating it.\nBeyond the inner wall of the containment vessel one of several test blanket modules will be placed. These are designed to slow and absorb neutrons in a reliable and efficient manner, limiting damage to the rest of the structure, and breeding tritium for fuel from lithium and the incoming neutrons. Energy absorbed from the fast neutrons is extracted and passed into the primary coolant. This heat energy would then be used to power an electricity-generating turbine in a real power station; in ITER this generating system is not of scientific interest, so instead the heat will be extracted and disposed of.\n\n\n== Technical design ==\n\n\n=== Vacuum vessel ===\nThe vacuum vessel is the central part of the ITER machine: a double walled steel container in which the plasma is contained by means of magnetic fields.\nThe ITER vacuum vessel will be twice as large and 16 times as heavy as any previously manufactured fusion vessel: each of the nine torus shaped sectors will weigh between 390 and 430 tonnes. When all the shielding and port structures are included, this adds up to a total of 5,116 tonnes. Its external diameter will measure 19.4 metres (64 ft), the internal 6.5 metres (21 ft). Once assembled, the whole structure will be 11.3 metres (37 ft) high.\nThe primary function of the vacuum vessel is to provide a hermetically sealed plasma container. Its main components are the main vessel, the port structures and the supporting system. The main vessel is a double walled structure with poloidal and toroidal stiffening ribs between 60-millimetre-thick (2.4 in) shells to reinforce the vessel structure. These ribs also form the flow passages for the cooling water. The space between the double walls will be filled with shield structures made of stainless steel. The inner surfaces of the vessel will act as the interface with breeder modules containing the breeder blanket component. These modules will provide shielding from the high-energy neutrons produced by the fusion reactions and some will also be used for tritium breeding concepts.\nThe vacuum vessel has 18 upper, 17 equatorial and 9 lower ports that will be used for remote handling operations, diagnostic systems, neutral beam injections and vacuum pumping.\n\n\n=== Breeder blanket ===\nOwing to very limited terrestrial resources of tritium, a key component of the ITER reactor design is the breeder blanket. This component, located adjacent to the vacuum vessel, serves to produce tritium through reaction of 6Li isotopes with high energy neutrons from the plasma. Concepts for the breeder blanket include helium cooled lithium lead (HCLL) and helium cooled pebble bed (HCPB) methods. Test blanket modules based on both concepts will be tested in ITER and will share a common box geometry. Materials for use as breeder pebbles in the HCPB concept include lithium metatitanate and lithium orthosilicate. Requirements of breeder materials include good tritium production and extraction, mechanical stability and low activation levels.\n\n\n=== Magnet system ===\nThe central solenoid coil will use superconducting niobium-tin to carry 46 kA and produce a field of up to 13.5 teslas. The 18 toroidal field coils will also use niobium-tin. At their maximum field strength of 11.8 teslas, they will be able to store 41 gigajoules. They have been tested at a record 80 kA. Other lower field ITER magnets (PF and CC) will use niobium-titanium for their superconducting elements.\n\n\n=== Additional heating ===\nThere will be three types of external heating in ITER:\nTwo Heating Neutral Beam injectors (HNB), each providing about 17MW to the burning plasma, with the possibility to add a third one. The requirements in terms of deuterium beam energy (1MeV), total current (40A) and beam pulse duration (up to 1h). The prototype is being built at the a Neutral Beam Test Facility (NBTF) prototype is being constructed in Padova\nIon Cyclotron Resonance Heating (ICRH)\nElectron Cyclotron Resonance Heating (ECRH)\n\n\n=== Cryostat ===\nThe cryostat is a large 3,800-tonne stainless steel structure surrounding the vacuum vessel and the superconducting magnets, in order to provide a super-cool vacuum environment. Its thickness ranging from 50 to 250 mm will allow it to withstand the atmospheric pressure on the area of a volume of 8,500 cubic meters. The total of 54 modules of the cryostat will be engineered, procured, manufactured, and installed by Larsen & Toubro Heavy Engineering.\n\n\n=== Cooling systems ===\nThe ITER tokamak will use three interconnected cooling systems. Most of the heat will be removed by a primary water cooling loop, itself cooled by water through a heat exchanger within the tokamak building's secondary confinement. The secondary cooling loop will be cooled by a larger complex, comprising a cooling tower, a 5 km pipeline supplying water from Canal de Provence, and basins that allow cooling water to be cooled and tested for chemical contamination and tritium before being released into the Durance River. This system will need to dissipate an average power of 450 MW during the tokamak's operation. A liquid nitrogen system will provide a further 1,300 kW of cooling to 80 kelvins, and a liquid helium system will provide 75 kW of cooling to 4.5 K. The liquid helium system will be designed, manufactured, installed and commissioned by Air Liquide.\n\n\n== Location ==\n\nThe process of selecting a location for ITER was long and drawn out. The most likely sites were Cadarache in Provence-Alpes-C\u00f4te-d'Azur, France, and Rokkasho, Aomori, Japan. Additionally, Canada announced a bid for the site in Clarington in May 2001, but withdrew from the race in 2003. Spain also offered a site at Vandell\u00f2s on 17 April 2002, but the EU decided to concentrate its support solely behind the French site in late November 2003. From this point on, the choice was between France and Japan. On 3 May 2005, the EU and Japan agreed to a process which would settle their dispute by July.\nAt the final meeting in Moscow on 28 June 2005, the participating parties agreed to construct ITER at Cadarache in Provence-Alpes-C\u00f4te-d'Azur, France. Construction of the ITER complex began in 2007, while assembly of the tokamak itself is scheduled to begin in 2015.\nFusion for Energy, the EU agency in charge of the European contribution to the project, is located in Barcelona, Spain. Fusion for Energy (F4E) is the European Union's Joint Undertaking for ITER and the Development of Fusion Energy. According to the agency's website:\n\n\"F4E is responsible for providing Europe's contribution to ITER, the world's largest scientific partnership that aims to demonstrate fusion as a viable and sustainable source of energy. [...] F4E also supports fusion research and development initiatives [...]\"\n\nThe ITER Neutral Beam Test Facility aimed at developing and optimizing the neutral beam injector prototype, is being constructed in Padova. It will be the only ITER facility out of the site in Cadarache.\n\n\n== Participants ==\n\nCurrently there are seven parties participating in the ITER program: the European Union (through the legally distinct organisation EURATOM), India, Japan, China, Russia, South Korea, and the United States. Canada was previously a full member, but has since pulled out due to a lack of funding from the federal government. The lack of funding also resulted in Canada withdrawing from its bid for the ITER site in 2003. The host member of the ITER project, and hence the member contributing most of the costs, is the EU.\nIn 2007, it was announced that participants in the ITER will consider Kazakhstan's offer to join the program and in March 2009, Switzerland, an associate member of EURATOM since 1979, also ratified the country's accession to the European Domestic Agency Fusion for Energy as a third country member.\nITER's work is supervised by the ITER Council, which has the authority to appoint senior staff, amend regulations, decide on budgeting issues, and allow additional states or organizations to participate in ITER. The present Chairman of the ITER Council is Dr Hideyuki Takatsu \nParticipating countries\n\n\n== Funding ==\nAs of 2016, the total price of constructing the experiment is expected to be in excess of \u20ac20 billion, an increase of \u20ac4.6 billion of its 2010 estimate, and of \u20ac9.6 billion from the 2009 estimate. Prior to that, the proposed costs for ITER were \u20ac5 billion for the construction and \u20ac5 billion for maintenance and the research connected with it during its 35-year lifetime. At the June 2005 conference in Moscow the participating members of the ITER cooperation agreed on the following division of funding contributions: 45% by the hosting member, the European Union, and the rest split between the non-hosting members \u2013 China, India, Japan, South Korea, the Russian Federation and the USA. During the operation and deactivation phases, Euratom will contribute to 34% of the total costs.\nAlthough Japan's financial contribution as a non-hosting member is one-eleventh of the total, the EU agreed to grant it a special status so that Japan will provide for two-elevenths of the research staff at Cadarache and be awarded two-elevenths of the construction contracts, while the European Union's staff and construction components contributions will be cut from five-elevenths to four-elevenths.\nIt was reported in December 2010 that the European Parliament had refused to approve a plan by member states to reallocate \u20ac1.4 billion from the budget to cover a shortfall in ITER building costs in 2012\u201313. The closure of the 2010 budget required this financing plan to be revised, and the European Commission (EC) was forced to put forward an ITER budgetary resolution proposal in 2011.\nThe U.S. withdrew from the ITER consortium in 2000. In 2006, Congress voted to rejoin, and again contribute financially. In June 2015, it appeared that the U.S. Senate might vote to stop the scheduled U.S. contribution of $150 million in the 2015\u20132016 fiscal year.\n\n\n== Criticism ==\n\nA technical concern is that the 14 MeV neutrons produced by the fusion reactions will damage the materials from which the reactor is built. Research is in progress to determine whether and how reactor walls can be designed to last long enough to make a commercial power station economically viable in the presence of the intense neutron bombardment. The damage is primarily caused by high energy neutrons knocking atoms out of their normal position in the crystal lattice. A related problem for a future commercial fusion power station is that the neutron bombardment will induce radioactivity in the reactor material itself. Maintaining and decommissioning a commercial reactor may thus be difficult and expensive. Another problem is that superconducting magnets are damaged by neutron fluxes. A new special research facility, IFMIF, is planned to investigate this problem.\nAnother source of concern comes from the recent tokamak parameters database interpolation which says that power load on tokamak divertors will be five times the expected value for ITER and much more for actual electricity-generating reactors. Given that the projected power load on the ITER divertor is already very high, these new findings mean that new divertor designs should be urgently tested. However, the corresponding test facility (ADX) still has not received any funding.\nA number of fusion researchers working on non-tokamak systems, such as Robert Bussard and Eric Lerner, have been critical of ITER for diverting funding from what they believe could be a potentially more viable and/or cost-effective path to fusion power, such as the polywell reactor. Many critics accuse ITER researchers of being unwilling to face up to the technical and economic potential problems posed by Tokamak fusion schemes. The expected cost of ITER has risen from $5 billion USD to $20 billion USD, and the timeline for operation at full power was moved from the original estimate of 2016 to 2027.\nA French association including about 700 anti-nuclear groups, Sortir du nucl\u00e9aire (Get Out of Nuclear Energy), claimed that ITER was a hazard because scientists did not yet know how to manipulate the high-energy deuterium and tritium hydrogen isotopes used in the fusion process.\nRebecca Harms, Green/EFA member of the European Parliament's Committee on Industry, Research and Energy, said: \"In the next 50 years, nuclear fusion will neither tackle climate change nor guarantee the security of our energy supply.\" Arguing that the EU's energy research should be focused elsewhere, she said: \"The Green/EFA group demands that these funds be spent instead on energy research that is relevant to the future. A major focus should now be put on renewable sources of energy.\" French Green party lawmaker No\u00ebl Mam\u00e8re claims that more concrete efforts to fight present-day global warming will be neglected as a result of ITER: \"This is not good news for the fight against the greenhouse effect because we're going to put ten billion euros towards a project that has a term of 30\u201350 years when we're not even sure it will be effective.\"\nITER is not designed to produce electricity, but made as a proof of concept reactor for the later DEMO project.\n\n\n=== Responses to criticism ===\nProponents believe that much of the ITER criticism is misleading and inaccurate, in particular the allegations of the experiment's \"inherent danger.\" The stated goals for a commercial fusion power station design are that the amount of radioactive waste produced should be hundreds of times less than that of a fission reactor, and that it should produce no long-lived radioactive waste, and that it is impossible for any such reactor to undergo a large-scale runaway chain reaction. A direct contact of the plasma with ITER inner walls would contaminate it, causing it to cool immediately and stop the fusion process. In addition, the amount of fuel contained in a fusion reactor chamber (one half gram of deuterium/tritium fuel) is only sufficient to sustain the fusion burn pulse from minutes up to an hour at most, whereas a fission reactor usually contains several years' worth of fuel. Moreover, some detritiation systems will be implemented, so that at a fuel cycle inventory level of about 2 kg, ITER will eventually need to recycle large amounts of tritium and at turnovers orders of magnitude higher than any preceding tritium facility worldwide.\nIn the case of an accident (or sabotage), it is expected that a fusion reactor might release far less radioactive pollution than would an ordinary fission nuclear station. Furthermore, ITER's type of fusion power has little in common with nuclear weapons technology, and does not produce the fissile materials necessary for the construction of a weapon. Proponents note that large-scale fusion power would be able to produce reliable electricity on demand, and with virtually zero pollution (no gaseous CO2, SO2, or NOx by-products are produced).\nAccording to researchers at a demonstration reactor in Japan, a fusion generator should be feasible in the 2030s and no later than the 2050s. Japan is pursuing its own research program with several operational facilities that are exploring several fusion paths.\nIn the United States alone, electricity accounts for US$210 billion in annual sales. Asia's electricity sector attracted US$93 billion in private investment between 1990 and 1999. These figures take into account only current prices. Proponents of ITER contend that an investment in research now should be viewed as an attempt to earn a far greater future return. Also, worldwide investment of less than US$1 billion per year into ITER is not incompatible with concurrent research into other methods of power generation, which in 2007 totaled US$16.9 billion.\nSupporters of ITER emphasize that the only way to test ideas for withstanding the intense neutron flux is to experimentally subject materials to that flux, which is one of the primary missions of ITER and the IFMIF, and both facilities will be vitally important to that effort. The purpose of ITER is to explore the scientific and engineering questions that surround potential fusion power stations. It is nearly impossible to acquire satisfactory data for the properties of materials expected to be subject to an intense neutron flux, and burning plasmas are expected to have quite different properties from externally heated plasmas. Supporters contend that the answer to these questions requires the ITER experiment, especially in the light of the monumental potential benefits.\nFurthermore, the main line of research via tokamaks has been developed to the point that it is now possible to undertake the penultimate step in magnetic confinement plasma physics research with a self-sustained reaction. In the tokamak research program, recent advances devoted to controlling the configuration of the plasma have led to the achievement of substantially improved energy and pressure confinement, which reduces the projected cost of electricity from such reactors by a factor of two to a value only about 50% more than the projected cost of electricity from advanced light-water reactors. In addition, progress in the development of advanced, low activation structural materials supports the promise of environmentally benign fusion reactors and research into alternate confinement concepts is yielding the promise of future improvements in confinement. Finally, supporters contend that other potential replacements to the fossil fuels have environmental issues of their own. Solar, wind, and hydroelectric power all have a relatively low power output per square kilometer compared to ITER's successor DEMO which, at 2,000 MW, would have an energy density that exceeds even large fission power stations.\n\n\n== Similar projects ==\nPrecursors to ITER were JET and Tore Supra. Other planned and proposed fusion reactors include DEMO, Wendelstein 7-X, NIF, HiPER, and MAST, as well as CFETR (China Fusion Engineering Test Reactor), a 200 MW tokamak.\n\n\n== See also ==\n\nTokamak\nITER Neutral Beam Test Facility, the facility dedicated to the development of the ITER neutral beam injector prototype\nFusion for Energy, the Domestic Agency in charge of managing EU contributions to the ITER project\nInternational Fusion Materials Irradiation Facility, proposed, construction not started\nJT-60/JT-60SA\nEAST (Experimental Advanced Superconducting Tokamak)\nNational Ignition Facility, inertial confinement using lasers\nNuclear power in France\nWendelstein 7-X (German experimental fusion reactor) - a stellarator\nFusenet, European Fusion Education Network, 2008-2013\n\n\n== References ==\n\n\n== External links ==\nOfficial website\nThe New Yorker, Mar. 3 2014, Star in a Bottle, by Raffi Khatchadourian\nArchival material collected by Prof. McCray relating to ITER\u2019s early phase (1979\u20131989) can be consulted at the Historical Archives of the European Union in Florence\n\"Way to New Energy\" video (23:24) at YouTube, by RT, on May 6, 2014.\nThe roles of the Host and the non-Host for the ITER Project. June 2005 The broader approach agreement with Japan.\nFusion Electricity - A roadmap to the realisation of fusion energy EFDA 2012 - 8 missions, ITER, project plan with dependancies, ...", 
                "titleUrl": "https://en.wikipedia.org/wiki/ITER", 
                "title": "ITER"
            }, 
            {
                "snippet": "disassembled in 2010 to be upgraded to a more powerful nuclear fusion reactor\u2014the JT-60SA\u2014by using niobium-titanium superconducting coils for the magnet confining", 
                "pageCategories": "All articles that may contain original research\nArticles that may contain original research from April 2012\nCS1 errors: chapter ignored\nCS1 errors: dates\nCS1 maint: Explicit use of et al.\nCS1 maint: Multiple names: authors list\nHistory of physics\nPhysics timelines\nQuantum mechanics", 
                "pageContent": "This timeline of quantum mechanics shows the key steps, precursors and contributors to the development of quantum mechanics, quantum field theories and quantum chemistry.\n\n\n== 19th century ==\n\n1859 \u2013 Kirchhoff introduces the concept of a blackbody and proves that its emission spectrum depends only on its temperature.\n1860\u20131900 \u2013 Ludwig Eduard Boltzmann, James Clerk Maxwell and others develop the theory of statistical mechanics. Boltzmann argues that entropy is a measure of disorder.\n1877 \u2013 Boltzmann suggests that the energy levels of a physical system could be discrete based on statistical mechanics and mathematical arguments; also produces the first circle diagram representation, or atomic model of a molecule (such as an iodine gas molecule) in terms of the overlapping terms \u03b1 and \u03b2, later (in 1928) called molecular orbitals, of the constituting atoms.\n1887 \u2013 Heinrich Hertz discovers the photoelectric effect, shown by Einstein in 1905 to involve quanta of light.\n1888 \u2013 Hertz demonstrates experimentally that electromagnetic waves exist, as predicted by Maxwell.\n1888 \u2013 Johannes Rydberg modifies the Balmer formula to include all spectral series of lines for the hydrogen atom, producing the Rydberg formula which is employed later by Niels Bohr and others to verify Bohr's first quantum model of the atom.\n1895 \u2013 Wilhelm Conrad R\u00f6ntgen discovers X-rays in experiments with electron beams in plasma.\n1896 \u2013 Antoine Henri Becquerel accidentally discovers radioactivity while investigating the work of Wilhelm Conrad R\u00f6ntgen; he finds that uranium salts emit radiation that resembled R\u00f6ntgen's X-rays in their penetrating power. In one experiment, Becquerel wraps a sample of a phosphorescent substance, potassium uranyl sulfate, in photographic plates surrounded by very thick black paper in preparation for an experiment with bright sunlight; then, to his surprise, the photographic plates are already exposed before the experiment starts, showing a projected image of his sample.\n1896 \u2013 Pieter Zeeman first observes the Zeeman splitting effect by passing the light emitted by hydrogen through a magnetic field.\n1896\u20131897 Marie Curie (n\u00e9e Sk\u0142odowska, Becquerel's doctoral student) investigates uranium salt samples using a very sensitive electrometer device that was invented 15 years before by her husband and his brother Jacques Curie to measure electrical charge. She discovers that rays emitted by the uranium salt samples make the surrounding air electrically conductive, and measures the emitted rays' intensity. In April 1898, through a systematic search of substances, she finds that thorium compounds, like those of uranium, emitted \"Becquerel rays\", thus preceding the work of Frederick Soddy and Ernest Rutherford on the nuclear decay of thorium to radium by three years.\n1897 \u2013 Ivan Borgman demonstrates that X-rays and radioactive materials induce thermoluminescence.\n1899 to 1903 \u2013 Ernest Rutherford, 1st Baron, Lord Rutherford of Nelson, of Cambridge, OM, FRS: During the investigation of radioactivity he coins the terms alpha and beta rays in 1899 to describe the two distinct types of radiation emitted by thorium and uranium salts. Ernest Rutherford is joined at McGill University in 1900 by Frederick Soddy and together they discover nuclear transmutation when they find in 1902 that radioactive thorium is converting itself into radium through a process of nuclear decay and a gas (later found to be 4\n2He); they report their interpretation of radioactivity in 1903. Sir Ernest Rutherford becomes known as the \"father of nuclear physics\": with his nuclear atom model of 1911 he leads the exploration of nuclear physics.\n\n\n== 20th century ==\n\n\n=== 1900\u20131909 ===\n\n1900 \u2013 To explain black-body radiation (1862), Max Planck suggests that electromagnetic energy could only be emitted in quantized form, i.e. the energy could only be a multiple of an elementary unit E = h\u03bd, where h is Planck's constant and \u03bd is the frequency of the radiation.\n1902 \u2013 To explain the octet rule (1893), Gilbert N. Lewis develops the \"cubical atom\" theory in which electrons in the form of dots are positioned at the corner of a cube. Predicts that single, double, or triple \"bonds\" result when two atoms are held together by multiple pairs of electrons (one pair for each bond) located between the two atoms.\n1903 \u2013 Antoine Becquerel, Pierre Curie and Marie Curie share the 1903 Nobel Prize in Physics for their work on spontaneous radioactivity.\n1904 \u2013 Richard Abegg notes the pattern that the numerical difference between the maximum positive valence, such as +6 for H2SO4, and the maximum negative valence, such as \u22122 for H2S, of an element tends to be eight (Abegg's rule).\n1905 \u2013 Albert Einstein explains the photoelectric effect (reported in 1887 by Heinrich Hertz), i.e. that shining light on certain materials can function to eject electrons from the material. He postulates, as based on Planck's quantum hypothesis (1900), that light itself consists of individual quantum particles (photons).\n1905 \u2013 Einstein explains the effects of Brownian motion as caused by the kinetic energy (i.e., movement) of atoms, which was subsequently, experimentally verified by Jean Baptiste Perrin, thereby settling the century-long dispute about the validity of John Dalton's atomic theory.\n1905 \u2013 Einstein publishes his Special Theory of Relativity.\n1905 \u2013 Einstein theoretically derives the equivalence of matter and energy.\n1907 to 1917 \u2013 Ernest Rutherford: To test his planetary model of 1904, later known as the Rutherford model, he sent a beam of positively charged alpha particles onto a gold foil and noticed that some bounced back, thus showing that an atom has a small-sized positively charged atomic nucleus at its center. However, he received in 1908 the Nobel Prize in Chemistry \"for his investigations into the disintegration of the elements, and the chemistry of radioactive substances\", which followed on the work of Marie Curie, not for his planetary model of the atom; he is also widely credited with first \"splitting the atom\" in 1917. In 1911 Ernest Rutherford explained the Geiger\u2013Marsden experiment by invoking a nuclear atom model and derived the Rutherford cross section.\n1909 \u2013 Geoffrey Ingram Taylor demonstrates that interference patterns of light were generated even when the light energy introduced consisted of only one photon. This discovery of the wave\u2013particle duality of matter and energy is fundamental to the later development of quantum field theory.\n1909 and 1916 \u2013 Einstein shows that, if Planck's law of black-body radiation is accepted, the energy quanta must also carry momentum p = h / \u03bb, making them full-fledged particles.\n\n\n=== 1910\u20131919 ===\n\n1911 \u2013 Lise Meitner and Otto Hahn perform an experiment that shows that the energies of electrons emitted by beta decay had a continuous rather than discrete spectrum. This is in apparent contradiction to the law of conservation of energy, as it appeared that energy was lost in the beta decay process. A second problem is that the spin of the Nitrogen-14 atom was 1, in contradiction to the Rutherford prediction of \u00bd. These anomalies are later explained by the discoveries of the neutrino and the neutron.\n1911 \u2013 \u0218tefan Procopiu performs experiments in which he determines the correct value of electron's magnetic dipole moment, \u03bcB = 9.27\u00d710\u221221 erg\u00b7Oe\u22121 (in 1913 he is also able to calculate a theoretical value of the Bohr magneton based on Planck's quantum theory).\n1912 \u2013 Victor Hess discovers the existence of cosmic radiation.\n1912 \u2013 Henri Poincar\u00e9 publishes an influential mathematical argument in support of the essential nature of energy quanta.\n1913 \u2013 Robert Andrews Millikan publishes the results of his \"oil drop\" experiment, in which he precisely determines the electric charge of the electron. Determination of the fundamental unit of electric charge makes it possible to calculate the Avogadro constant (which is the number of atoms or molecules in one mole of any substance) and thereby to determine the atomic weight of the atoms of each element.\n1913 \u2013 \u0218tefan Procopiu publishes a theoretical paper with the correct value of the electron's magnetic dipole moment \u03bcB.\n1913 \u2013 Niels Bohr obtains theoretically the value of the electron's magnetic dipole moment \u03bcB as a consequence of his atom model\n1913 \u2013 Johannes Stark and Antonino Lo Surdo independently discover the shifting and splitting of the spectral lines of atoms and molecules due to the presence of the light source in an external static electric field.\n1913 \u2013 To explain the Rydberg formula (1888), which correctly modeled the light emission spectra of atomic hydrogen, Bohr hypothesizes that negatively charged electrons revolve around a positively charged nucleus at certain fixed \"quantum\" distances and that each of these \"spherical orbits\" has a specific energy associated with it such that electron movements between orbits requires \"quantum\" emissions or absorptions of energy.\n1914 \u2013 James Franck and Gustav Hertz report their experiment on electron collisions with mercury atoms, which provides a new test of Bohr's quantized model of atomic energy levels.\n1915 \u2013 Einstein first presents to the Prussian Academy of Science what are now known as the Einstein field equations. These equations specify how the geometry of space and time is influenced by whatever matter is present, and form the core of Einstein's General Theory of Relativity. Although this theory is not directly applicable to quantum mechanics, theorists of quantum gravity seek to reconcile them.\n1916 \u2013 Paul Epstein and Karl Schwarzschild, working independently, derive equations for the linear and quadratic Stark effect in hydrogen.\n1916 \u2013 Gilbert N. Lewis conceives the theoretical basis of Lewis dot formulas, diagrams that show the bonding between atoms of a molecule and the lone pairs of electrons that may exist in the molecule.\n1916 \u2013 To account for the Zeeman effect (1896), i.e. that atomic absorption or emission spectral lines change when the light source is subjected to a magnetic field, Arnold Sommerfeld suggests there might be \"elliptical orbits\" in atoms in addition to spherical orbits.\n1918 \u2013 Sir Ernest Rutherford notices that, when alpha particles are shot into nitrogen gas, his scintillation detectors shows the signatures of hydrogen nuclei. Rutherford determines that the only place this hydrogen could have come from was the nitrogen, and therefore nitrogen must contain hydrogen nuclei. He thus suggests that the hydrogen nucleus, which is known to have an atomic number of 1, is an elementary particle, which he decides must be the protons hypothesized by Eugen Goldstein.\n1919 \u2013 Building on the work of Lewis (1916), Irving Langmuir coins the term \"covalence\" and postulates that coordinate covalent bonds occur when two electrons of a pair of atoms come from both atoms and are equally shared by them, thus explaining the fundamental nature of chemical bonding and molecular chemistry.\n\n\n=== 1920\u20131929 ===\n\n1920 - Hendrik Kramers uses Bohr\u2013Sommerfeld quantization to derive formulas for intensities of spectral transitions of the Stark effect. Kramers also includes the effect of fine structure, including corrections for relativistic kinetic energy and coupling between electron spin and orbit.\n1921\u20131922 \u2013 Frederick Soddy receives the Nobel Prize for 1921 in Chemistry one year later, in 1922, \"for his contributions to our knowledge of the chemistry of radioactive substances, and his investigations into the origin and nature of isotopes\"; he writes in his Nobel Lecture of 1922: \"The interpretation of radioactivity which was published in 1903 by Sir Ernest Rutherford and myself ascribed the phenomena to the spontaneous disintegration of the atoms of the radio-element, whereby a part of the original atom was violently ejected as a radiant particle, and the remainder formed a totally new kind of atom with a distinct chemical and physical character.\"\n1922 \u2013 Arthur Compton finds that X-ray wavelengths increase due to scattering of the radiant energy by free electrons. The scattered quanta have less energy than the quanta of the original ray. This discovery, known as the Compton effect or Compton scattering, demonstrates the particle concept of electromagnetic radiation.\n1922 \u2013 Otto Stern and Walther Gerlach perform the Stern\u2013Gerlach experiment, which detects discrete values of angular momentum for atoms in the ground state passing through an inhomogeneous magnetic field leading to the discovery of the spin of the electron.\n1922 \u2013 Bohr updates his model of the atom to better explain the properties of the periodic table by assuming that certain numbers of electrons (for example 2, 8 and 18) corresponded to stable \"closed shells\", presaging orbital theory.\n1923 \u2013 Pierre Auger discovers the Auger effect, where filling the inner-shell vacancy of an atom is accompanied by the emission of an electron from the same atom.\n1923 \u2013 Louis de Broglie extends wave\u2013particle duality to particles, postulating that electrons in motion are associated with waves. He predicts that the wavelengths are given by Planck's constant h divided by the momentum of the mv = p of the electron: \u03bb = h / mv = h / p.\n1923 \u2013 Gilbert N. Lewis creates the theory of Lewis acids and bases based on the properties of electrons in molecules, defining an acid as accepting an electron lone pair from a base.\n1924 \u2013 Satyendra Nath Bose explains Planck's law using a new statistical law that governs bosons, and Einstein generalizes it to predict Bose\u2013Einstein condensate. The theory becomes known as Bose\u2013Einstein statistics.\n1924 \u2013 Wolfgang Pauli outlines the \"Pauli exclusion principle\" which states that no two identical fermions may occupy the same quantum state simultaneously, a fact that explains many features of the periodic table.\n1925 \u2013 George Uhlenbeck and Samuel Goudsmit postulate the existence of electron spin.\n1925 \u2013 Friedrich Hund outlines Hund's rule of Maximum Multiplicity which states that when electrons are added successively to an atom as many levels or orbits are singly occupied as possible before any pairing of electrons with opposite spin occurs and made the distinction that the inner electrons in molecules remained in atomic orbitals and only the valence electrons needed to be in molecular orbitals involving both nuclei.\n1925 \u2013 Werner Heisenberg, Max Born, and Pascual Jordan develops the matrix mechanics formulation of Quantum Mechanics.\n1926 \u2013 Lewis coins the term photon in a letter to the scientific journal Nature, which he derives from the Greek word for light, \u03c6\u03c9\u03c2 (transliterated ph\u00f4s).\n1926 \u2013 Oskar Klein and Walter Gordon state their relativistic quantum wave equation, later called the Klein\u2013Gordon equation.\n1926 \u2013 Enrico Fermi discovers the spin-statistics theorem connection.\n1926 \u2013 Paul Dirac introduces Fermi\u2013Dirac statistics.\n1926 \u2013 Erwin Schr\u00f6dinger uses De Broglie's electron wave postulate (1924) to develop a \"wave equation\" that represents mathematically the distribution of a charge of an electron distributed through space, being spherically symmetric or prominent in certain directions, i.e. directed valence bonds, which gives the correct values for spectral lines of the hydrogen atom; also introduces the Hamiltonian operator in quantum mechanics.\n1926 \u2013 Paul Epstein reconsiders the linear and quadratic Stark effect from the point of view of the new quantum theory, using the equations of Schr\u00f6dinger and others. The derived equations for the line intensities are a decided improvement over previous results obtained by Hans Kramers.\n1926 to 1932 \u2013 John von Neumann lays the mathematical foundations of Quantum Mechanics in terms of Hermitian operators on Hilbert spaces, subsequently published in 1932 as a basic textbook of quantum mechanics.\n1927 \u2013 Werner Heisenberg formulates the quantum uncertainty principle.\n1927 \u2013 Max Born develops the Copenhagen interpretation of the probabilistic nature of wavefunctions.\n1927 \u2013 Born and J. Robert Oppenheimer introduce the Born\u2013Oppenheimer approximation, which allows the quick approximation of the energy and wavefunctions of smaller molecules.\n1927 \u2013 Walter Heitler and Fritz London introduce the concepts of valence bond theory and apply it to the hydrogen molecule.\n1927 \u2013 Thomas and Fermi develop the Thomas\u2013Fermi model for a Gas in a box.\n1927 \u2013 Chandrasekhara Venkata Raman studies optical photon scattering by electrons.\n1927 \u2013 Dirac states his relativistic electron quantum wave equation, the Dirac equation.\n1927 \u2013 Charles Galton Darwin and Walter Gordon solve the Dirac equation for a Coulomb potential.\n1927 \u2013 Charles Drummond Ellis (along with James Chadwick and colleagues) finally establish clearly that the beta decay spectrum is in fact continuous and not discrete, posing a problem that will later be solved by theorizing (and later discovering) the existence of the neutrino.\n1927 \u2013 Walter Heitler uses Schr\u00f6dinger's wave equation to show how two hydrogen atom wavefunctions join together, with plus, minus, and exchange terms, to form a covalent bond.\n1927 \u2013 Robert Mulliken works, in coordination with Hund, to develop a molecular orbital theory where electrons are assigned to states that extend over an entire molecule and, in 1932, introduces many new molecular orbital terminologies, such as \u03c3 bond, \u03c0 bond, and \u03b4 bond.\n1927 \u2013 Eugene Wigner relates degeneracies of quantum states to irreducible representations of symmetry groups.\n1927 \u2013 Hermann Klaus Hugo Weyl proves in collaboration with his student Fritz Peter a fundamental theorem in harmonic analysis\u2014the Peter\u2013Weyl theorem\u2014relevant to group representations in quantum theory (including the complete reducibility of unitary representations of a compact topological group); introduces the Weyl quantization, and earlier, in 1918, introduces the concept of gauge and a gauge theory; later in 1935 he introduces and characterizes with Richard Bauer the concept of spinor in n-dimensions.\n1928 \u2013 Linus Pauling outlines the nature of the chemical bond: uses Heitler's quantum mechanical covalent bond model to outline the quantum mechanical basis for all types of molecular structure and bonding and suggests that different types of bonds in molecules can become equalized by rapid shifting of electrons, a process called \"resonance\" (1931), such that resonance hybrids contain contributions from the different possible electronic configurations.\n1928 \u2013 Friedrich Hund and Robert S. Mulliken introduce the concept of molecular orbitals.\n1928 \u2013 Born and Vladimir Fock formulate and prove the adiabatic theorem, which states that a physical system shall remain in its instantaneous eigenstate if a given perturbation is acting on it slowly enough and if there is a gap between the eigenvalue and the rest of the Hamiltonian's spectrum.\n1929 \u2013 Oskar Klein discovers the Klein paradox\n1929 \u2013 Oskar Klein and Yoshio Nishina derive the Klein\u2013Nishina cross section for high energy photon scattering by electrons\n1929 \u2013 Sir Nevill Mott derives the Mott cross section for the Coulomb scattering of relativistic electrons\n1929 \u2013 John Lennard-Jones introduces the linear combination of atomic orbitals approximation for the calculation of molecular orbitals.\n1929 \u2013 Fritz Houtermans and Robert d'Escourt Atkinson propose that stars release energy by nuclear fusion.\n\n\n=== 1930\u20131939 ===\n\n1930 \u2013 Dirac hypothesizes the existence of the positron.\n1930 \u2013 Dirac's textbook Principles of Quantum Mechanics is published, becoming a standard reference book that is still used today.\n1930 \u2013 Erich H\u00fcckel introduces the H\u00fcckel molecular orbital method, which expands on orbital theory to determine the energies of orbitals of pi electrons in conjugated hydrocarbon systems.\n1930 \u2013 Fritz London explains van der Waals forces as due to the interacting fluctuating dipole moments between molecules\n1930 \u2013 Pauli suggests in a famous letter that, in addition to electrons and protons, atoms also contain an extremely light neutral particle which he calls the \"neutron.\" He suggests that this \"neutron\" is also emitted during beta decay and has simply not yet been observed. Later it is determined that this particle is actually the almost massless neutrino.\n1931 \u2013 John Lennard-Jones proposes the Lennard-Jones interatomic potential\n1931 \u2013 Walther Bothe and Herbert Becker find that if the very energetic alpha particles emitted from polonium fall on certain light elements, specifically beryllium, boron, or lithium, an unusually penetrating radiation is produced. At first this radiation is thought to be gamma radiation, although it is more penetrating than any gamma rays known, and the details of experimental results are very difficult to interpret on this basis. Some scientists begin to hypothesize the possible existence of another fundamental particle.\n1931 \u2013 Erich H\u00fcckel redefines the property of aromaticity in a quantum mechanical context by introducing the 4n+2 rule, or H\u00fcckel's rule, which predicts whether an organic planar ring molecule will have aromatic properties.\n1931 \u2013 Ernst Ruska creates the first electron microscope.\n1931 \u2013 Ernest Lawrence creates the first cyclotron and founds the Radiation Laboratory, later the Lawrence Berkeley National Laboratory; in 1939 he awarded the Nobel Prize in Physics for his work on the cyclotron.\n1932 \u2013 Ir\u00e8ne Joliot-Curie and Fr\u00e9d\u00e9ric Joliot show that if the unknown radiation generated by alpha particles falls on paraffin or any other hydrogen-containing compound, it ejects protons of very high energy. This is not in itself inconsistent with the proposed gamma ray nature of the new radiation, but detailed quantitative analysis of the data become increasingly difficult to reconcile with such a hypothesis.\n1932 \u2013 James Chadwick performs a series of experiments showing that the gamma ray hypothesis for the unknown radiation produced by alpha particles is untenable, and that the new particles must be the neutrons hypothesized by Fermi.\n1932 \u2013 Werner Heisenberg applies perturbation theory to the two-electron problem to show how resonance arising from electron exchange can explain exchange forces.\n1932 \u2013 Mark Oliphant: Building upon the nuclear transmutation experiments of Ernest Rutherford done a few years earlier, observes fusion of light nuclei (hydrogen isotopes). The steps of the main cycle of nuclear fusion in stars are subsequently worked out by Hans Bethe over the next decade.\n1932 \u2013 Carl D. Anderson experimentally proves the existence of the positron.\n1933 \u2013 Following Chadwick's experiments, Fermi renames Pauli's \"neutron\" to neutrino to distinguish it from Chadwick's theory of the much more massive neutron.\n1933 \u2013 Le\u00f3 Szil\u00e1rd first theorizes the concept of a nuclear chain reaction. He files a patent for his idea of a simple nuclear reactor the following year.\n1934 \u2013 Fermi publishes a very successful model of beta decay in which neutrinos are produced.\n1934 \u2013 Fermi studies the effects of bombarding uranium isotopes with neutrons.\n1934 \u2013 N. N. Semyonov develops the total quantitative chain chemical reaction theory, later the basis of various high technologies using the incineration of gas mixtures. The idea is also used for the description of the nuclear reaction.\n1934 \u2013 Ir\u00e8ne Joliot-Curie and Fr\u00e9d\u00e9ric Joliot-Curie discover artificial radioactivity and are jointly awarded the 1935 Nobel Prize in Chemistry\n1935 \u2013 Einstein, Boris Podolsky, and Nathan Rosen describe the EPR paradox which challenges the completeness of quantum mechanics as it was theorized up to that time. Assuming that local realism is valid, they demonstrated that there would need to be hidden parameters to explain how measuring the quantum state of one particle could influence the quantum state of another particle without apparent contact between them.\n1935 - Schr\u00f6dinger develops the Schr\u00f6dinger's cat thought experiment. It illustrates what he saw as the problems of the Copenhagen interpretation of quantum mechanics if subatomic particles can be in two contradictory quantum states at once.\n1935 \u2013 Hideki Yukawa formulates his hypothesis of the Yukawa potential and predicts the existence of the pion, stating that such a potential arises from the exchange of a massive scalar field, as it would be found in the field of the pion. Prior to Yukawa's paper, it was believed that the scalar fields of the fundamental forces necessitated massless particles.\n1936 \u2013 Alexandru Proca publishes prior to Hideki Yukawa his relativistic quantum field equations for a massive vector meson of spin-1 as a basis for nuclear forces.\n1936 \u2013 Garrett Birkhoff and John von Neumann introduce Quantum Logic in an attempt to reconcile the apparent inconsistency of classical, Boolean logic with the Heisenberg Uncertainty Principle of quantum mechanics as applied, for example, to the measurement of complementary (noncommuting) observables in quantum mechanics, such as position and momentum; current approaches to quantum logic involve noncommutative and non-associative many-valued logic.\n1936 \u2013 Carl D. Anderson discovers muons while he is studying cosmic radiation.\n1937 \u2013 Carl Anderson experimentally proves the existence of the pion.\n1937 \u2013 Hermann Arthur Jahn and Edward Teller prove, using group theory, that non-linear degenerate molecules are unstable. The Jahn-Teller theorem essentially states that any non-linear molecule with a degenerate electronic ground state will undergo a geometrical distortion that removes that degeneracy, because the distortion lowers the overall energy of the complex. The latter process is called the Jahn-Teller effect; this effect was recently considered also in relation to the superconductivity mechanism in YBCO and other high temperature superconductors. The details of the Jahn-Teller effect are presented with several examples and EPR data in the basic textbook by Abragam and Bleaney (1970).\n1938 \u2013 Charles Coulson makes the first accurate calculation of a molecular orbital wavefunction with the hydrogen molecule.\n1938 \u2013 Otto Hahn and his assistant Fritz Strassmann send a manuscript to Naturwissenschaften reporting they have detected the element barium after bombarding uranium with neutrons. Hahn calls this new phenomenon a 'bursting' of the uranium nucleus. Simultaneously, Hahn communicate these results to Lise Meitner. Meitner, and her nephew Otto Robert Frisch, correctly interpret these results as being a nuclear fission. Frisch confirms this experimentally on 13 January 1939.\n1939 \u2013 Le\u00f3 Szil\u00e1rd and Fermi discover neutron multiplication in uranium, proving that a chain reaction is indeed possible.\n\n\n=== 1940\u20131949 ===\n\n1942 \u2013 Kan-Chang Wang first proposes the use of K-electron capture to experimentally detect neutrinos.\n1942 \u2013 A team led by Enrico Fermi creates the first artificial self-sustaining nuclear chain reaction, called Chicago Pile-1, in a racquets court below the bleachers of Stagg Field at the University of Chicago on December 2, 1942.\n1942 to 1946 \u2013 J. Robert Oppenheimer successfully leads the Manhattan Project, predicts quantum tunneling and proposes the Oppenheimer\u2013Phillips process in nuclear fusion\n1945 \u2013 the Manhattan Project produces the first nuclear fission explosion on July 16, 1945 in the Trinity test in New Mexico.\n1945 \u2013 John Archibald Wheeler and Richard Feynman originate Wheeler\u2013Feynman absorber theory, an interpretation of electrodynamics that supposes that elementary particles are not self-interacting.\n1946 \u2013 Theodor V. Ionescu and Vasile Mihu report the construction of the first hydrogen maser by stimulated emission of radiation in molecular hydrogen.\n1947 \u2013 Willis Lamb and Robert Retherford measure a small difference in energy between the energy levels 2S1/2 and 2P1/2 of the hydrogen atom, known as the Lamb shift.\n1947 \u2013 George Rochester and Clifford Charles Butler publishes two cloud chamber photographs of cosmic ray-induced events, one showing what appears to be a neutral particle decaying into two charged pions, and one that appears to be a charged particle decaying into a charged pion and something neutral. The estimated mass of the new particles is very rough, about half a proton's mass. More examples of these \"V-particles\" were slow in coming, and they are soon given the name kaons.\n1948 \u2013 Sin-Itiro Tomonaga and Julian Schwinger Independently introduce perturbative renormalization as a method of correcting the original Lagrangian of a quantum field theory so as to eliminate a series of infinite terms that would otherwise result.\n1948 \u2013 Richard Feynman states the path integral formulation of quantum mechanics.\n1949 \u2013 Freeman Dyson determines the equivalence of two formulations of quantum electrodynamics: Feynman's diagrammatic path integral formulation and the operator method developed by Julian Schwinger and Tomonaga. A by-product of that demonstration is the invention of the Dyson series.\n\n\n=== 1950\u20131959 ===\n1951 \u2013 Clemens C. J. Roothaan and George G. Hall derive the Roothaan-Hall equations, putting rigorous molecular orbital methods on a firm basis.\n1951 \u2013 Edward Teller, physicist and \"father of the hydrogen bomb\", and Stanislaw Ulam, mathematician, are reported to have written jointly in March 1951 a classified report on \"Hydrodynamic Lenses and Radiation Mirrors\" that results in the next step in the Manhattan Project.\n1951 and 1952 \u2013 at the Manhattan Project, the first planned fusion thermonuclear reaction experiment is carried out successfully in the Spring of 1951 at Eniwetok, based on the work of Edward Teller and Dr. Hans A. Bethe. The Los Alamos Laboratory proposes a date in November 1952 for a hydrogen bomb, full-scale test that is apparently carried out.\n1951 \u2013 Felix Bloch and Edward Mills Purcell receive a shared Nobel Prize in Physics for their first observations of the quantum phenomenon of nuclear magnetic resonance previously reported in 1949. Purcell reports his contribution as Research in Nuclear Magnetism, and gives credit to his coworkers such as Herbert S. Gutowsky for their NMR contributions, as well as theoretical researchers of nuclear magnetism such as John Hasbrouck Van Vleck.\n1952 \u2013 Albert W. Overhauser formulates a theory of dynamic nuclear polarization, also known as the Overhauser Effect; other contenders are the subsequent theory of Ionel Solomon reported in 1955 that includes the Solomon equations for the dynamics of coupled spins, and that of R. Kaiser in 1963. The general Overhauser effect is first demonstrated experimentally by T. R. Carver and Charles P. Slichter in 1953.\n1952 \u2013 Donald A. Glaser creates the bubble chamber, which allows detection of electrically charged particles by surrounding them by a bubble. Properties of the particles such as momentum can be determined by studying of their helical paths. Glaser receives a Nobel prize in 1960 for his invention.\n1953 \u2013 Charles H. Townes, collaborating with James P. Gordon, and H. J. Zeiger, builds the first ammonia maser; receives a Nobel prize in 1964 for his experimental success in producing coherent radiation by atoms and molecules.\n1954 \u2013 Chen Ning Yang and Robert Mills derive a gauge theory for nonabelian groups, leading to the successful formulation of both electroweak unification and quantum chromodynamics.\n1955 \u2013 Ionel Solomon develops the first nuclear magnetic resonance theory of magnetic dipole coupled nuclear spins and of the Nuclear Overhauser Effect.\n1955 and 1956 \u2013 Murray Gell-Mann and Kazuhiko Nishijima independently derive the Gell-Mann\u2013Nishijima formula, which relates the baryon number, the strangeness, and the isospin of hadrons to the charge, eventually leading to the systematic categorization of hadrons and, ultimately, the Quark Model of hadron composition.\n1956 \u2013 P. Kuroda predicts that self-sustaining nuclear chain reactions should occur in natural uranium deposits.\n1956 \u2013 Chien-Shiung Wu carries out the Wu Experiment, which observes parity violation in cobalt-60 decay, showing that parity violation is present in the weak interaction.\n1956 \u2013 Clyde L. Cowan and Frederick Reines experimentally prove the existence of the neutrino.\n1957 \u2013 John Bardeen, Leon Cooper and John Robert Schrieffer propose their quantum BCS theory of low temperature superconductivity, for which their receive a Nobel prize in 1972. The theory represents superconductivity as a macroscopic quantum coherence phenomenon involving phonon coupled electron pairs with opposite spin\n1957 \u2013 William Alfred Fowler, Margaret Burbidge, Geoffrey Burbidge, and Fred Hoyle, in their 1957 paper Synthesis of the Elements in Stars, show that the abundances of essentially all but the lightest chemical elements can be explained by the process of nucleosynthesis in stars.\n1957 \u2013 Hugh Everett formulates the many-worlds interpretation of quantum mechanics, which states that every possible quantum outcome is realized in divergent, non-communicating parallel universes in quantum superposition.\n1958\u20131959 \u2013 magic angle spinning described by Edward Raymond Andrew, A. Bradbury, and R. G. Eades, and independently in 1959 by I. J. Lowe.\n\n\n=== 1960\u20131969 ===\n\n1961 \u2013 Clauss J\u00f6nsson performs Young's double-slit experiment (1909) for the first time with particles other than photons by using electrons and with similar results, confirming that massive particles also behaved according to the wave\u2013particle duality that is a fundamental principle of quantum field theory.\n1961 \u2013 Anatole Abragam publishes the fundamental textbook on the quantum theory of Nuclear Magnetic Resonance entitled The Principles of Nuclear Magnetism;\n1961 \u2013 Sheldon Lee Glashow extends the electroweak interaction modelss developed by Julian Schwinger by including a short range neutral current, the Z_o. The resulting symmetry structure that Glashow proposes, SU(2) X U(1), forms the basis of the accepted theory of the electroweak interactions.\n1962 \u2013 Leon M. Lederman, Melvin Schwartz and Jack Steinberger show that more than one type of neutrino exists by detecting interactions of the muon neutrino (already hypothesised with the name \"neutretto\")\n1962 \u2013 Murray Gell-Mann and Yuval Ne'eman independently classify the hadrons according to a system that Gell-Mann called the Eightfold Way, and which ultimately led to the quark model (1964) of hadron composition.\n1962 \u2013 Jeffrey Goldstone, Yoichiro Nambu, Abdus Salam, and Steven Weinberg develop what is now known as Goldstone's Theorem: if there is a continuous symmetry transformation under which the Lagrangian is invariant, then either the vacuum state is also invariant under the transformation, or there must be spinless particles of zero mass, thereafter called Nambu-Goldstone bosons.\n1962 to 1973 \u2013 Brian David Josephson, predicts correctly the quantum tunneling effect involving superconducting currents while he is a PhD student under the supervision of Professor Brian Pippard at the Royal Society Mond Laboratory in Cambridge, UK; subsequently, in 1964, he applies his theory to coupled superconductors. The effect is later demonstrated experimentally at Bell Labs in the USA. For his important quantum discovery he is awarded the Nobel Prize in Physics in 1973.\n1963 \u2013 Eugene P. Wigner lays the foundation for the theory of symmetries in quantum mechanics as well as for basic research into the structure of the atomic nucleus; makes important \"contributions to the theory of the atomic nucleus and the elementary particles, particularly through the discovery and application of fundamental symmetry principles\"; he shares half of his Nobel prize in Physics with Maria Goeppert-Mayer and J. Hans D. Jensen.\n1963 \u2013 Maria Goeppert Mayer and J. Hans D. Jensen share with Eugene P. Wigner half of the Nobel Prize in Physics in 1963 \"for their discoveries concerning nuclear shell structure theory\".\n1963 \u2013 Nicola Cabibbo develops the mathematical matrix by which the first two (and ultimately three) generations of quarks can be predicted.\n1964 \u2013 Murray Gell-Mann and George Zweig independently propose the quark model of hadrons, predicting the arbitrarily named up, down, and strange quarks. Gell-Mann is credited with coining the term quark, which he found in James Joyce's book Finnegans Wake.\n1964 \u2013 Fran\u00e7ois Englert, Robert Brout, Peter Higgs, Gerald Guralnik, C. R. Hagen, and Tom Kibble postulate that a fundamental quantum field, now called the Higgs field, permeates space and, by way of the Higgs mechanism, provides mass to all the elementary subatomic particles that interact with it. While the Higgs field is postulated to confer mass on quarks and leptons, it represents only a tiny portion of the masses of other subatomic particles, such as protons and neutrons. In these, gluons that bind quarks together confer most of the particle mass. The result is obtained independently by three groups: Fran\u00e7ois Englert and Robert Brout; Peter Higgs, working from the ideas of Philip Anderson; and Gerald Guralnik, C. R. Hagen, and Tom Kibble.\n1964 \u2013 Sheldon Lee Glashow and James Bjorken predict the existence of the charm quark. The addition is proposed because it allows for a better description of the weak interaction (the mechanism that allows quarks and other particles to decay), equalizes the number of known quarks with the number of known leptons, and implies a mass formula that correctly reproduced the masses of the known mesons.\n1964 \u2013 John Stewart Bell puts forth Bell's theorem, which used testable inequality relations to show the flaws in the earlier Einstein\u2013Podolsky\u2013Rosen paradox and prove that no physical theory of local hidden variables can ever reproduce all of the predictions of quantum mechanics. This inaugurated the study of quantum entanglement, the phenomenon in which separate particles share the same quantum state despite being at a distance from each other.\n1964 \u2013 Nikolai G. Basov and Aleksandr M. Prokhorov share the Nobel Prize in Physics in 1964 for, respectively, semiconductor lasers and Quantum Electronics; they also share the prize with Charles Hard Townes, the inventor of the ammonium maser.\n1967 \u2013 Steven Weinberg and Abdus Salam publish a paper in which he describes Yang\u2013Mills theory using the SU(2) X U(1) supersymmetry group, thereby yielding a mass for the W particle of the weak interaction via spontaneous symmetry breaking.\n1968 \u2013 Stanford University: Deep inelastic scattering experiments at the Stanford Linear Accelerator Center (SLAC) show that the proton contains much smaller, point-like objects and is therefore not an elementary particle. Physicists at the time are reluctant to identify these objects with quarks, instead calling them partons \u2014 a term coined by Richard Feynman. The objects that are observed at SLAC will later be identified as up and down quarks. Nevertheless, \"parton\" remains in use as a collective term for the constituents of hadrons (quarks, antiquarks, and gluons). The existence of the strange quark is indirectly validated by the SLAC's scattering experiments: not only is it a necessary component of Gell-Mann and Zweig's three-quark model, but it provides an explanation for the kaon (K) and pion (\u03c0) hadrons discovered in cosmic rays in 1947.\n1969 to 1977 \u2013 Sir Nevill Mott and Philip Warren Anderson publish quantum theories for electrons in non-crystalline solids, such as glasses and amorphous semiconductors; receive in 1977 a Nobel prize in Physics for their investigations into the electronic structure of magnetic and disordered systems, which allow for the development of electronic switching and memory devices in computers. The prize is shared with John Hasbrouck Van Vleck for his contributions to the understanding of the behavior of electrons in magnetic solids; he established the fundamentals of the quantum mechanical theory of magnetism and the crystal field theory (chemical bonding in metal complexes) and is regarded as the Father of modern Magnetism.\n1969 and 1970 \u2013 Theodor V. Ionescu, Radu P\u00e2rvan and I.C. Baianu observe and report quantum amplified stimulation of electromagnetic radiation in hot deuterium plasmas in a longitudinal magnetic field; publish a quantum theory of the amplified coherent emission of radiowaves and microwaves by focused electron beams coupled to ions in hot plasmas.\n1970 \u2013 Glashow, John Iliopoulos and Luciano Maiani predict the charmed quark that is subsequently found experimentally and share a Nobel prize for their theoretical prediction.\n\n\n=== 1971\u20131979 ===\n1971 \u2013 Martinus J. G. Veltman and Gerardus 't Hooft show that, if the symmetries of Yang\u2013Mills theory are broken according to the method suggested by Peter Higgs, then Yang\u2013Mills theory can be renormalized. The renormalization of Yang\u2013Mills Theory predicts the existence of a massless particle, called the gluon, which could explain the nuclear strong force. It also explains how the particles of the weak interaction, the W and Z bosons, obtain their mass via spontaneous symmetry breaking and the Yukawa interaction.\n1972 \u2013 Francis Perrin discovers \"natural nuclear fission reactors\" in uranium deposits in Oklo, Gabon, where analysis of isotope ratios demonstrate that self-sustaining, nuclear chain reactions have occurred. The conditions under which a natural nuclear reactor could exist were predicted in 1956 by P. Kuroda.\n1973 \u2013 Frank Anthony Wilczek discover the quark asymptotic freedom in the theory of strong interactions; receives the Lorentz Medal in 2002, and the Nobel Prize in Physics in 2004 for his discovery and his subsequent contributions to quantum chromodynamics.\n1973 \u2013 Makoto Kobayashi and Toshihide Maskawa note that the experimental observation of CP violation can be explained if an additional pair of quarks exist. The two new quarks are eventually named top and bottom.\n1973 \u2013 Peter Mansfield formulates the physical theory of Nuclear magnetic resonance imaging (NMRI)\n1974 \u2013 Pier Giorgio Merli performs Young's double-slit experiment (1909) using a single electron with similar results, confirming the existence of quantum fields for massive particles.\n1974 \u2013 Burton Richter and Samuel Ting: Charm quarks are produced almost simultaneously by two teams in November 1974 (see November Revolution) \u2014 one at SLAC under Burton Richter, and one at Brookhaven National Laboratory under Samuel Ting. The charm quarks are observed bound with charm antiquarks in mesons. The two discovering parties independently assign the discovered meson two different symbols, J and \u03c8; thus, it becomes formally known as the J/\u03c8 meson. The discovery finally convinces the physics community of the quark model's validity.\n1975 \u2013 Martin Lewis Perl, with his colleagues at the SLAC\u2013LBL group, detects the tau in a series of experiments between 1974 and 1977.\n1977 \u2013 Leon Lederman observes the bottom quark with his team at Fermilab. This discovery is a strong indicator of the top quark's existence: without the top quark, the bottom quark would be without a partner that is required by the mathematics of the theory.\n1977 \u2013 Ilya Prigogine develops non-equilibrium, irreversible thermodynamics and quantum operator theory, especially the time superoperator theory; he is awarded the Nobel Prize in Chemistry in 1977 \"for his contributions to non-equilibrium thermodynamics, particularly the theory of dissipative structures\".\n1978 \u2013 Pyotr Kapitsa observes new phenomena in hot deuterium plasmas excited by very high power microwaves in attempts to obtain controlled thermonuclear fusion reactions in such plasmas placed in longitudinal magnetic fields, using a novel and low-cost design of thermonuclear reactor, similar in concept to that reported by Theodor V. Ionescu et al. in 1969. Receives a Nobel prize for early low temperature physics experiments on helium superfluidity carried out in 1937 at the Cavendish Laboratory in Cambridge, UK, and discusses his 1977 thermonuclear reactor results in his Nobel lecture on December 8, 1978.\n1979 \u2013 Kenneth A. Rubinson and coworkers, at the Cavendish Laboratory, observe ferromagnetic spin wave resonant excite journals (FSWR) in locally anisotropic, FENiPB metallic glasses and interpret the observations in terms of two-magnon dispersion and a spin exchange Hamiltonian, similar in form to that of a Heisenberg ferromagnet.\n\n\n=== 1980\u20131999 ===\n1980 to 1982 \u2013 Alain Aspect verify experimentally the quantum entanglement hypothesis; his Bell test experiments provide strong evidence that a quantum event at one location can affect an event at another location without any obvious mechanism for communication between the two locations.\n1982 to 1997 \u2013 Tokamak Fusion Test Reactor (TFTR) at PPPL, Princeton, USA: Operated since 1982, produces 10.7MW of controlled fusion power for only 0.21s in 1994 by using T-D nuclear fusion in a tokamak reactor with \"a toroidal 6T magnetic field for plasma confinement, a 3MA plasma current and an electron density of 1.0\u00d71020 m\u22123 of 13.5 keV\" \n1983 \u2013 Carlo Rubbia and Simon van der Meer, at the Super Proton Synchrotron, see unambiguous signals of W particles in January. The actual experiments are called UA1 (led by Rubbia) and UA2 (led by Peter Jenni), and are the collaborative effort of many people. Simon van der Meer is the driving force on the use of the accelerator. UA1 and UA2 find the Z particle a few months later, in May 1983.\n1983 to 2011 \u2013 The largest and most powerful experimental nuclear fusion tokamak reactor in the world, Joint European Torus (JET) begins operation at Culham Facility in UK; operates with T-D plasma pulses and has a reported gain factor Q of 0.7 in 2009, with an input of 40MW for plasma heating, and a 2800-ton iron magnet for confinement; in 1997 in a tritium-deuterium experiment JET produces 16 MW of fusion power, a total of 22 MJ of fusion, energy and a steady fusion power of 4 MW which is maintained for 4 seconds.\n1985 to 2010 \u2013 The JT-60 (Japan Torus) begins operation in 1985 with an experimental D-D nuclear fusion tokamak similar to the JET; in 2010 JT-60 holds the record for the highest value of the fusion triple product achieved: 7028177000000000000\u26601.77\u00d71028 K\u00b7s\u00b7m\u22123 = 7021153000000000000\u26601.53\u00d71021 keV\u00b7s\u00b7m\u22123.; JT-60 claims it would have an equivalent energy gain factor, Q of 1.25 if it were operated with a T-D plasma instead of the D-D plasma, and on May 9, 2006 attains a fusion hold time of 28.6 s in full operation; moreover, a high-power microwave gyrotron construction is completed that is capable of 1.5MW output for 1s, thus meeting the conditions for the planned ITER, large-scale nuclear fusion reactor. JT-60 is disassembled in 2010 to be upgraded to a more powerful nuclear fusion reactor\u2014the JT-60SA\u2014by using niobium-titanium superconducting coils for the magnet confining the ultra-hot D-D plasma.\n1986 \u2013 Johannes Georg Bednorz and Karl Alexander M\u00fcller produce unambiguous experimental proof of high temperature superconductivity involving Jahn-Teller polarons in orthorhombic La2CuO4, YBCO and other perovskite-type oxides; promptly receive a Nobel prize in 1987 and deliver their Nobel lecture on December 8, 1987.\n1986 \u2013 Vladimir Gershonovich Drinfeld introduces the concept of quantum groups as Hopf algebras in his seminal address on quantum theory at the International Congress of Mathematicians, and also connects them to the study of the Yang\u2013Baxter equation, which is a necessary condition for the solvability of statistical mechanics models; he also generalizes Hopf algebras to quasi-Hopf algebras, and introduces the study of Drinfeld twists, which can be used to factorize the R-matrix corresponding to the solution of the Yang\u2013Baxter equation associated with a quasitriangular Hopf algebra.\n1988 to 1998 \u2013 Mihai Gavril\u0103 discovers in 1988 the new quantum phenomenon of atomic dichotomy in hydrogen and subsequently publishes a book on the atomic structure and decay in high-frequency fields of hydrogen atoms placed in ultra-intense laser fields.\n1991 \u2013 Richard R. Ernst develops two-dimensional nuclear magnetic resonance spectroscopy (2D-FT NMRS) for small molecules in solution and is awarded the Nobel Prize in Chemistry in 1991 \"for his contributions to the development of the methodology of high resolution nuclear magnetic resonance (NMR) spectroscopy.\"\n1977 to 1995 \u2013 The top quark is finally observed by a team at Fermilab after an 18-year search. It has a mass much greater than had been previously expected \u2014 almost as great as a gold atom.\n1995 \u2013 Eric Cornell, Carl Wieman and Wolfgang Ketterle and co-workers at JILA create the first \"pure\" Bose\u2013Einstein condensate. They do this by cooling a dilute vapor consisting of approximately two thousand rubidium-87 atoms to below 170 nK using a combination of laser cooling and magnetic evaporative cooling. About four months later, an independent effort led by Wolfgang Ketterle at MIT creates a condensate made of sodium-23. Ketterle's condensate has about a hundred times more atoms, allowing him to obtain several important results such as the observation of quantum mechanical interference between two different condensates.\n1998 \u2013 The Super-Kamiokande (Japan) detector facility reports experimental evidence for neutrino oscillations, implying that at least one neutrino has mass.\n1999 to 2013 \u2013 NSTX\u2014The National Spherical Torus Experiment at PPPL, Princeton, USA launches a nuclear fusion project on February 12, 1999 for \"an innovative magnetic fusion device that was constructed by the Princeton Plasma Physics Laboratory (PPPL) in collaboration with the Oak Ridge National Laboratory, Columbia University, and the University of Washington at Seattle\"; NSTX is being used to study the physics principles of spherically shaped plasmas.\n\n\n== 21st century ==\n\n2000 \u2013 scientists at European Organization for Nuclear Research (CERN) publish experimental results in which they claim to have observed indirect evidence of the existence of a quark\u2013gluon plasma, which they call a \"new state of matter.\"\n2001 \u2013 the Sudbury Neutrino Observatory (Canada) confirm the existence of neutrino oscillations. Lene Hau stops a beam of light completely in a Bose\u2013Einstein condensate.\n2002 \u2013 Leonid Vainerman organizes a meeting at Strasbourg of theoretical physicists and mathematicians focused on quantum group and quantum groupoid applications in quantum theories; the proceedings of the meeting are published in 2003 in a book edited by the meeting organizer.\n2003 \u2013 Sir Anthony James Leggett receives the 2003 Nobel Prize in Physics for pioneering contributions to the quantum theory of superconductors, and superfluids such as Helium-3, shared with V. L. Ginzburg and A. A. Abrikosov.\n2005 \u2013 the RHIC accelerator of Brookhaven National Laboratory generates a quark-gluon fluid, perhaps the quark\u2013gluon plasma\n2007 to 2010 \u2013 Charles Pence Slichter is awarded the National Medal of Science in 2007 for his studies of Nuclear Magnetic Resonance in Solids, and especially his NMR Studies of High-Temperature Superconductors.\n2008 to 2010 \u2013 the Lithium Tokamak Experiment (LTX) starts in September 2008.\n2007 to 2010 \u2013 Alain Aspect, Anton Zeilinger and John Clauser present progress with the resolution of the non-locality aspect of quantum theory and in 2010 are awarded the Wolf Prize in Physics, together with Anton Zeilinger and John Clauser.\n2009 - Aaron D. O'Connell invents the first quantum machine, applying quantum mechanics to a macroscopic object just large enough to be seen by the naked eye, which is able to vibrate a small amount and large amount simultaneously.\n2010 \u2013 Andre Geim and Konstantin Novoselov receive the Nobel Prize in Physics \"for groundbreaking experiments regarding the two-dimensional material graphene\".\n2011 - Zachary Dutton demonstrates how photons can co-exist in superconductors. \"Direct Observation of Coherent Population Trapping in a Superconducting Artificial Atom\",\n2014 \u2013 Scientists transfer data by quantum teleportation over a distance of 10 feet with zero percent error rate, a vital step towards a quantum internet.\n\n\n== See also ==\n\n Learning materials related to the history of Quantum Mechanics at Wikiversity\n\n\n== References ==\n\n\n== Bibliography ==\nPeacock, Kent A. (2008). \"The Quantum Revolution : A Historical Perspective\". Westport, Conn.: Greenwood Press. ISBN 9780313334481. \nBen-Menahem, A. (2009). \"Historical Encyclopedia of Natural and Mathematical Sciences\" (1st ed.). Berlin: Springer: 4342\u20134349. ISBN 9783540688310.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Timeline_of_quantum_mechanics", 
                "title": "Timeline of quantum mechanics"
            }, 
            {
                "snippet": "ICRH LH JET 34 - - 10 7 JT-60U 40 3 4 7 8 TFTR 40 - - 11 - EAST - - 0.5 3 4 DIII-D 20 - 5 4 - ASDEX-U 20 - 6 8 - JT60-SA* 24 10 7 - - ITER* - 33 20 20 -", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from November 2015\nFusion power", 
                "pageContent": "Neutral beam injection (NBI) is one method used to heat plasma inside a fusion device consisting in a beam of high-energy neutral particles that can enter the confinement magnetic field. When these neutral particles are ionized by collision with the plasma particles, they are kept in the plasma by the confining magnetic field, and can transfer most of their energy by further collisions with the plasma. By tangential injection in the torus, neutral beams provide also momentum to the plasma and current drive, one essential feature for long pulses of burning plasmas. Neutral beam injection is a \ufb02exible and reliable technique, which has been the main heating system on a large variety of fusion devices. To date, all NBI systems were based on positive precursor ion beams. In the 90s there has been impressive progress in negative ion sources and accelerators with the construction of multi-megawatt negative ion based NBI systems at LHD (H0, 180 keV) and JT-60U (D0, 500 keV). The NBI designed for ITER is a substantial challenge (D0, 1MeV, 40A) and a prototype is being constructed to optimize its performance in view of the ITER future operations. Other ways to heat plasma for nuclear fusion include RF heating, electron cyclotron resonance heating (ECRH), and ion cyclotron resonance heating (ICRH).\n\n\n== Mechanism ==\n\nThis is typically done by:\nMaking a plasma. This can be done by microwaving a low pressure gas.\nElectrostatic ion acceleration. This is done dropping the positively charged ions towards negative plates. As the ions fall, the electric field does work on them, heating them to fusion temperatures.\nReneutralizing the hot plasma by adding in the opposite charge. This gives the fast moving beam no charge.\nInjecting the fast moving hot neutral beam in the machine.\nIt is critical to inject neutral material into plasma, because if it is charged, it can start harmful plasma instabilities. Most fusion devices inject isotopes of hydrogen, such as pure deuterium or a mix of deuterium and tritium. This material becomes part of the fusion plasma. It also transfers its energy into the existing plasma within the machine. This hot stream of material should raise the overall temperature. Although the beam has no electrostatic charge when it enters, as it passes through the plasma, the atoms are ionized. This happens because the beam bounces off ions already in the plasma.\n\n\n== Neutral Beam Injectors installed in fusion experiments ==\nAt present, all main fusion experiments use NBIs. Traditional positive ion based injectors (P-NBI) are installed for instance in the JET, or in ASDEX-U. To allow power deposition in the center of the burning plasma in larger devices, a higher neutral beam energy is required. High energy (>100keV) systems require the use of negative ion technology (N-NBI).\n\n\n== Coupling with fusion plasma ==\nBecause the magnetic field inside the torus is circular, these fast ions are confined to the background plasma. The confined fast ions mentioned above are slowed down by the background plasma, in a similar way to how air resistance slows down a baseball. The energy transfer from the fast ions to the plasma increases the overall plasma temperature.\nIt is very important that the fast ions are confined within the plasma long enough for them to deposit their energy. Magnetic fluctuations are a big problem for plasma confinement in this type of device (see plasma stability) by scrambling what were initially well-ordered magnetic fields. If the fast ions are susceptible to this type of behavior they can escape very quickly, however some evidence suggests they are not susceptible.\n\n\n== See also ==\nList of plasma (physics) articles\nITER Neutral Beam Test Facility\n\n\n== References ==\n\n\n== External links ==\nThermonuclear Fusion Test Reactor with neutral beam injector at PPPL\nAuxiliary heating in ITER\nIPP website about NBI technology", 
                "titleUrl": "https://en.wikipedia.org/wiki/Neutral_beam_injection", 
                "title": "Neutral beam injection"
            }
        ], 
        "phraseCharStart": "1069"
    }, 
    {
        "phraseCharEnd": "1083", 
        "phraseIndex": "T24", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "KSTAR", 
        "wikiSearchResults": [
            {
                "snippet": "Not to be confused with K-type star. The KSTAR, or Korea Superconducting Tokamak Advanced Research is a magnetic fusion device being built at the National", 
                "pageCategories": "Interlanguage link template link number\nTokamaks", 
                "pageContent": "The KSTAR, or Korea Superconducting Tokamak Advanced Research is a magnetic fusion device being built at the National Fusion Research Institute in Daejeon, South Korea. It is intended to study aspects of magnetic fusion energy which will be pertinent to the ITER fusion project as part of that country's contribution to the ITER effort. The project was approved in 1995 but construction was delayed by the East Asian financial crisis which weakened the South Korean economy considerably; however the construction phase of the project was completed on September 14, 2007. First plasma occurred on July 15, 2008. or more likely on June 30 2008.\nKSTAR will be one of the first research tokamaks in the world to feature fully superconducting magnets, which again will be of great relevance to ITER as this will also use SC magnets. The KSTAR magnet system consists of 16 niobium-tin direct current toroidal field magnets, 10 niobium-tin alternating current poloidal field magnets and 4 niobium-titanium alternating current poloidal field magnets. It is planned that the reactor will study plasma pulses of up to 20 seconds duration until 2011, when it will be upgraded to study pulses of up to 300 seconds duration. The reactor vessel will have a major radius of 1.8 m, a minor radius of 0.5 m, a maximum toroidal field of 3.5 tesla, and a maximum plasma current of 2 megaampere. As with other tokamaks, heating and current drive will be initiated using neutral beam injection, ion cyclotron resonance heating (ICRH), radio frequency heating and electron cyclotron resonance heating (ECRH). Initial heating power will be 8 megawatt from neutral beam injection upgradeable to 24 MW, 6 MW from ICRH upgradeable to 12 MW, and at present undetermined heating power from ECRH and RF heating. The experiment will use both hydrogen and deuterium fuels but not the deuterium-tritium mix which will be studied in ITER.\nIn 2012, it succeeded in maintaining high-temperature plasma (about 50 million degrees Celsius) for 17 seconds.\n\n\n== Timeline ==\nThe design was based on Tokomak Physics Experiment which was based on Compact Ignition Tokamak design - See Robert J. Goldston.\n1995 - Started Project KSTAR\n1997 - JET of EU emits 17 MW energy from itself.\n1998 - JT-60U went beyond energy junction successfully, and acknowledged possibility of commercialization of nuclear fusion.\n2006 - Life span of 3 Fusion Reactors (JT-60U, JET, and DIII-D) are terminated.\n2007, September - KSTAR's major devices are constructed.\n2008, July - First plasma occurred. Maintenance time: 0.865 seconds, Temperature: 2\u00d7106 K\n2009 - Maintained 320,000A plasma for 3.6 seconds.\n2010, November - First H-mode plasma run.\n2011 - Maintained high-temperature plasma for 5.2 seconds, Temperature: ~50\u00d7106 K, successfully fully deterred ELM (Edge-Localized Mode), first ever in the World.\n2012 - Maintained high-temperature plasma for 17 seconds, Temperature: 50\u00d7106 K\n2013 - Maintained high-temperature plasma for 20 seconds, Temperature: 50\u00d7106 K\n2014 - Maintained high-temperature plasma for 48 seconds, and successfully fully deterred ELM for 5 seconds.\n\n\n== References ==\n\n\n== External links ==\nKSTAR homepage\nEnglish KSTAR homepage\nKSTAR parameters re ITER and other tokamaks\n\nKSTAR Project Status PDF (undated - seems to be 2001. Includes slide-13 construction schedule to end 2004 and slide-16 operation from 2005 with upgrade planned 2010-11.)\nKSTAR Assembly Status, October 2006 PDF\nStatus and Result of the KSTAR Upgrade for the 2010\u2019s Campaign\nKSTAR ICRF transmission line system upgrade for load resilient operation. Jan 2013", 
                "titleUrl": "https://en.wikipedia.org/wiki/KSTAR", 
                "title": "KSTAR"
            }, 
            {
                "snippet": "KStars is a planetarium program using the KDE Platform. It can be used on most Unix-like computer operating systems, as well as on the Microsoft Windows", 
                "pageCategories": "Astronomy software\nFree astronomy software\nFree educational software\nKDE Education Project\nKDE software\nPlanetarium software for Linux\nScience education software", 
                "pageContent": "KStars is a planetarium program using the KDE Platform. It can be used on most Unix-like computer operating systems, as well as on the Microsoft Windows platform using 'KDE for Windows'. It provides an accurate graphical representation of the night sky, from any location on Earth, at any date and time. The display includes up to 100 million stars (with additional addons), 13,000 deep sky objects, constellations from different cultures, all 8 planets, the Sun and Moon, and thousands of comets and asteroids. It has features to appeal to users of all levels, from informative hypertext articles about astronomy, to robust control of telescopes and CCD cameras, and logging of observations of specific objects.\nKStars supports adjustable simulation speeds in order to view phenomena that happen over long timescales. For astronomical calculations, Astrocalculator can be used to predict conjunctions, and perform many common astronomical calculations. The following tools are included:\nObservation planner\nSky calendar tool\nScript Builder\nSolar System\nJupiter Moons\nFlags: Custom flags superimposed on the sky map.\nFOV editor to calculate field of view of equipment and display them.\nAltitude vs. Time tool to plot altitude vs. time graphs for any object.\nHigh quality print outs for sky charts.\nEkos is Ekos astrophotography suite, a complete astrophotography solution that can control all INDI devices including numerous telescopes, CCDs, DSLRs, focusers, filters, and a lot more. Ekos supports highly accurate tracking using online and offline astrometry solver, auto-focus and auto-guiding capabilities, and capture of single or multiple images using the powerful built in sequence manager.\nKStars has been packaged by many Linux/BSD distributions, including Red Hat Linux, OpenSUSE, Mandriva Linux, and Debian GNU/Linux. Some distributions package KStars as a separate application, some just provide a kdeedu package, which includes KStars. KStars is distributed with the KDE Software Compilation as part of the kdeedu \"Edutainment\" module.\nKStars participated in Google Summer of Code in 2008, 2009, 2010, 2011 2012, 2015 and 2016. It has also participated in the first run of ESA's Summer of Code in Space in 2011.\nIt has been identified as one of the three best \"Linux stargazing apps\" in a Linux.com review.\nThe latest version of KStars is 2.4.0, released with KDE Applications 15.12. Released under the GNU General Public License, KStars is free software.\n\n\n== See also ==\n\nCartes du Ciel\nCelestia\nDigital Universe Atlas\nGoogle Mars\nGoogle Moon\nGoogle Sky\nHallo Northern Sky (HN Sky)\nNASA World Wind\nRedShift\nSkyglobe\nStarry Night\nStellarium\nTheSky\nUniverse Sandbox\nWinStars\nWorldWide Telescope\nXEphem\n\n\n== External links ==\nOfficial website\nMPC Elements for Comets and Minor Planets in KStars\nDownload source code and Windows and Mac versions\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/KStars", 
                "title": "KStars"
            }, 
            {
                "snippet": "owned by Zeta Holdings, LLC. KQMB has an AM sister station known as KSRR, KStar 1400 AM. It also has a booster station, KQMB-FM1, in Provo, Utah, that reaches", 
                "pageCategories": "2002 establishments in Utah\nAdult contemporary radio stations in the United States\nAll stub articles\nCoordinates on Wikidata\nRadio stations established in 2002\nRadio stations in Utah\nUtah radio station stubs", 
                "pageContent": "KQMB (96.7 FM) is a radio station broadcasting an adult contemporary music format. Licensed to Levan, Utah, USA, the station serves the Provo/Salt Lake City area. The station is currently owned by Zeta Holdings, LLC.\nKQMB has an AM sister station known as KSRR, KStar 1400 AM. It also has a booster station, KQMB-FM1, in Provo, Utah, that reaches into the Salt Lake metropolitan area.\n\n\n== History ==\nThe station was assigned the call letters KBLN on January 8, 1999. On January 25, 2001, the station changed its call sign to KCFM and on September 21, 2005 to the current KQMB.\nKQMB picked up the call sign and format from 102.7, which is now known as KSL-FM.\n\n\n== References ==\n\n\n== External links ==\n\nQuery the FCC's FM station database for KQMB\nRadio-Locator information on KQMB\nQuery Nielsen Audio's FM station database for KQMB", 
                "titleUrl": "https://en.wikipedia.org/wiki/KQMB", 
                "title": "KQMB"
            }, 
            {
                "snippet": " MTV became a part of SBS, and renamed SBS MTV in November 2011.       KSTAR News 840 SBS Inkigayo Running Man K-pop Star Channel Fiestar The Show The", 
                "pageCategories": "Articles with Korean-language external links\nCS1 Korean-language sources (ko)\nKorean-language television stations\nMusic video networks\nOfficial website different in Wikidata and Wikipedia\nSeoul Broadcasting System television networks\nTelevision channels and stations established in 2001\nTelevision channels in South Korea\nUse dmy dates from January 2012\nViacom Media Networks", 
                "pageContent": "SBS MTV is a South Korean music channel. Being a South Korean version of American MTV, it features Korean pop artists, international music, news, and a few reality programs. It also broadcasts programs originally from the American MTV, along with a few Asian programs.\nThe channel is currently owned by SBS Viacom LLC, a joint venture of SBS Medianet and Viacom International Media Networks.\n\n\n== History ==\nFrom 1994 to 1999, MTV, through a partnership deal, showed programs on the CheilJedang group's Mnet network.\nIn January 2001, the MTV block returned on OnGameNet, then owned by Orion Group's On-Media.\nIn July 2001, On-Media and Viacom launched MTV Korea. Their partnership ended in 2008.\nIn 2008, MTV Korea was acquired by C&M.\nIn September 2011, SBS, a South Korean commercial broadcaster, became the official South Korean partner of Viacom. With this, MTV became a part of SBS, and renamed SBS MTV in November 2011.\n\n\n== Shows ==\n\nKSTAR News 840\nSBS Inkigayo\nRunning Man\nK-pop Star\nChannel Fiestar\nThe Show\nThe Stage Big Pleasure\nKPOP Hero\nMTV Hits\nSBS MTV KPOP 20\nAfter Hours - Non-Stop\nWake Up Call - Non-Stop\nHITS : Classic - Non-Stop\nFRESH : POP - Non-Stop\nFRESH : K-POP - Non-Stop\nLIVE 4 U - Non-Stop\nI GOT7\nLovelyz in Wonderland\n\n\n== Current VJs ==\nNara\nSupasize\nSemi\nSeorak\nKewnsung\nHongwook\nMC Rhymer\nSeunggwang\nSia\nTim\nG-Ma$ta\nJungmin\nHanbyul\nJanet\nJoi\nSara\nBin\nDonemany\nLee Eugene\nYuri\n\n\n== See also ==\nMTV (Music Television)\nMTV Networks Asia Pacific\nMTV Southeast Asia\nMnet (former partner of MTV in South Korea)\n\n\n== References ==\n\n\n== External links ==\nOfficial website (Korean)\nSBS MTV on Facebook\nSBS MTV on Twitter", 
                "titleUrl": "https://en.wikipedia.org/wiki/SBS_MTV", 
                "title": "SBS MTV"
            }, 
            {
                "snippet": "produced by Kaz Wittig KStar Enterprises. Mitt Romney won the poll by a small margin.  This poll was also produced by Kaz Wittig KStar Enterprises. Ron Paul", 
                "pageCategories": "All articles with unsourced statements\nArticles with hCards\nArticles with unsourced statements from March 2012\nIdaho elections, 2012\nUnited States presidential election, 2012 by state\nUnited States presidential elections in Idaho\nWikipedia articles needing clarification from March 2012", 
                "pageContent": "The 2012 United States presidential election in Idaho took place on November 6, 2012 as part of the 2012 General Election in which all 50 states plus The District of Columbia participated. Idaho voters chose four electors to represent them in the Electoral College via a popular vote pitting incumbent Democratic President Barack Obama and his running mate, Vice President Joe Biden, against Republican challenger and former Massachusetts Governor Mitt Romney and his running mate, Congressman Paul Ryan. Romney and Ryan carried Idaho with 64.09% of the popular vote to Obama's and Biden's 32.40%, thus winning the state's four electoral votes.\n\n\n== General election ==\nCandidate Ballot Access:\nMitt Romney/Paul Ryan, Republican\nBarack Obama/Joseph Biden, Democratic\nGary Johnson/James P. Gray, Libertarian\nJill Stein/Cheri Honkala, Green\nVirgil Goode/Jim Clymer, Constitution\nRocky Anderson/Luis J. Rodriguez, Justice\n\n\n=== Results ===\n\n\n== Democratic caucuses ==\nPresident Obama ran alone in the caucuses. The District cast all 31 of its delegate votes at the 2012 Democratic National Convention for Obama.\n\n\n== Republican caucuses ==\nThe Republican caucuses took place on Super Tuesday, March 6, 2012. An advisory primary with no binding effect on delegates, scheduled to be held on Tuesday May 15, 2012, was cancelled by the Idaho Republican Party. Five candidates were on the ballot. In order of filing they are Ron Paul, Mitt Romney, Rick Santorum, Newt Gingrich, and Buddy Roemer. Although Roemer had withdrawn from the Republican race before the Idaho caucus, he still appeared on the ballot.\nIdaho has 32 delegates to the Republican national convention, of which 3 are RNC and 29 are AL. The 3 RNC delegates pledged to go with the results of the Boise Straw Poll. The delegates will be determined by the caucuses results, based on a two-step approach. First, the delegates are primarily awarded winner-take-all by county after a series of votes in which candidates are successively removed from the ballot. Then, if a candidate receives half or more of the county delegates, he will receive all the 32 delegates; if not, the delegates will be split proportionately according to the number of county delegates. Mitt Romney won 61% of the vote, thanks to a large majority of support (80-90% in most counties) in majority Mormon southeastern Idaho, rendering it a winner-take-all contest. Santorum and Paul split the Panhandle, winning five counties each, but came away empty in the delegate count.\n\n\n=== Straw polls ===\nDespite a complete lack of formal polling in the state, there were a total of four straw polls conducted in Idaho, three of which were online. Ron Paul won three of these, while Mitt Romney won another by a smaller margin.\n\n\n==== January 2\u20134 online poll ====\nThis was the first ever Idaho straw poll.It was conducted entirely online by Kaz Wittig KStar Enterprises. Ron Paul won with over 70 percent of the vote.\n\n\n==== January 6 Boise poll ====\nThis poll used paper ballots and was conducted in Boise. Ron Paul won this poll. Rick Perry's campaign, although still active in the race at this time, did not participate.\n\n\n==== February 2\u20134 online poll ====\nThis online straw poll was also produced by Kaz Wittig KStar Enterprises. Mitt Romney won the poll by a small margin.\n\n\n==== March 1\u20133 online poll ====\nThis poll was also produced by Kaz Wittig KStar Enterprises. Ron Paul won by a double-digit margin.\n\n\n=== Results ===\nResults:\n\n\n==== County totals ====\nNotes\nThat these totals reflect the final caucus ballots in each county; where only two candidates have votes totaled, this was likely from other candidates being eliminated in previous rounds of voting. Where all candidates have at least one vote, only one ballot was necessary, since the winning candidate had a majority of votes in that county.\nVote totals for Buddy Roemer, who had formally withdrawn from the Republican race before the caucus, are not provided.\n\n\n== See also ==\nRepublican Party presidential primaries, 2012\nResults of the 2012 Republican Party presidential primaries\n\n\n== References ==\n\n\n== External links ==\nThe Green Papers: for Idaho\nThe Green Papers: Major state elections in chronological order", 
                "titleUrl": "https://en.wikipedia.org/wiki/United_States_presidential_election_in_Idaho,_2012", 
                "title": "United States presidential election in Idaho, 2012"
            }, 
            {
                "snippet": "of ELM bursts.  As of late 2011, several research facilities including KSTAR  have demonstrated active control (suppression) of some types of this phenomenon", 
                "pageCategories": "All stub articles\nPhysics stubs\nPlasma physics\nTokamaks\nWikipedia articles needing clarification from December 2015", 
                "pageContent": "An edge-localized mode (\"ELM\") is a disruptive instability occurring in the edge region of a tokamak plasma due to the quasi-periodic relaxation of a transport barrier previously formed during an L --> H transition (ie in H-mode). This phenomenon was first observed in the ASDEX tokamak in 1981.\n\n\n== Impact ==\nThe development of edge-localized modes poses a major challenge in magnetic fusion research with tokamaks, as these instabilities can damage wall components, particularly divertor plates, due to their extremely high energy transfer rate (GW/m2).\n\n\n== Simulation - modelling ==\nIn 2006 an initiative (called Project Aster) was started to simulate a full ELM cycle including its onset, the highly non-linear phase, and its decay. However, this did not constitute a 'true' ELM cycle, since a true ELM cycle would require modeling the slow growth after the crash, in order to have a second ELM. In 2015, results of the first simulation to demonstrate repeated ELM cycling was published. A key element to obtaining repeated relaxations was to include diamagnetic effects in the model equations. Diamagnetic effects have also been shown to expand the size of the parameter space in which solutions of repeated sawteeth can be recovered compared to a resistive MHD model.\n\n\n== Prevention - control ==\nResearch involving prevention of edge localized mode formation is underway. A paper was recently published that suggested a novel method of countering this phenomenon by injecting static magnetic noisy energy into the containment field as a containment-stabilization regime; this may decrease ELM amplitude. ASDEX Upgrade has had some success using pellet injection to increase the frequency and thereby decrease the severity of ELM bursts.\n\n\n== Control in practice ==\nAs of late 2011, several research facilities including KSTAR  have demonstrated active control (suppression) of some types of this phenomenon.\n\n\n== References ==\n\n\n== See also ==\nResonant magnetic perturbations, used to control ELMs\nPlasma instability\nTokamak", 
                "titleUrl": "https://en.wikipedia.org/wiki/Edge-localized_mode", 
                "title": "Edge-localized mode"
            }, 
            {
                "snippet": "\"Astroinfo\" which is distributed with KStars, a desktop planetarium for Linux/KDE. See The KDE Education Project - KStars   Media related to Geographic coordinate", 
                "pageCategories": "All articles needing additional references\nAll articles with unsourced statements\nArticles needing additional references from May 2015\nArticles with unsourced statements from December 2007\nCartography\nCommons category with page title same as on Wikidata\nGeocodes\nGeodesy\nGeographic coordinate systems\nNavigation", 
                "pageContent": "A geographic coordinate system is a coordinate system that enables every location on the Earth to be specified by a set of numbers or letters, or symbols. The coordinates are often chosen such that one of the numbers represents vertical position, and two or three of the numbers represent horizontal position. A common choice of coordinates is latitude, longitude and elevation.\nTo specify a location on a two-dimensional map requires a map projection.\n\n\n== History ==\n\nThe invention of a geographic coordinate system is generally credited to Eratosthenes of Cyrene, who composed his now-lost Geography at the Library of Alexandria in the 3rd century BC. A century later, Hipparchus of Nicaea improved upon his system by determining latitude from stellar measurements rather than solar altitude and determining longitude by using simultaneous timing of lunar eclipses, rather than dead reckoning. In the 1st or 2nd century, Marinus of Tyre compiled an extensive gazetteer and mathematically-plotted world map, using coordinates measured east from a Prime Meridian at the Fortunate Isles of western Africa and measured north or south of the island of Rhodes off Asia Minor. Ptolemy credited him with the full adoption of longitude and latitude, rather than measuring latitude in terms of the length of the midsummer day. Ptolemy's 2nd-century Geography used the same Prime Meridian but measured latitude from the equator instead. After their work was translated into Arabic in the 9th century, Al-Khw\u0101rizm\u012b's Book of the Description of the Earth corrected Marinus and Ptolemy's errors regarding the length of the Mediterranean Sea, causing medieval Arabic cartography to use a Prime Meridian around 10\u00b0 east of Ptolemy's line. Mathematical cartography resumed in Europe following Maximus Planudes's recovery of Ptolemy's text a little before 1300; the text was translated into Latin at Florence by Jacobus Angelus around 1407.\nIn 1884, the United States hosted the International Meridian Conference and twenty-five nations attended. Twenty-two of them agreed to adopt the longitude of the Royal Observatory in Greenwich, England, as the zero-reference line. The Dominican Republic voted against the motion, while France and Brazil abstained. France adopted Greenwich Mean Time in place of local determinations by the Paris Observatory in 1911.\n\n\n== Geographic latitude and longitude ==\n\nThe \"latitude\" (abbreviation: Lat., \u03c6, or phi) of a point on the Earth's surface is the angle between the equatorial plane and the straight line that passes through that point and through (or close to) the center of the Earth. Lines joining points of the same latitude trace circles on the surface of the Earth called parallels, as they are parallel to the equator and to each other. The north pole is 90\u00b0 N; the south pole is 90\u00b0 S. The 0\u00b0 parallel of latitude is designated the equator, the fundamental plane of all geographic coordinate systems. The equator divides the globe into Northern and Southern Hemispheres.\n\nThe \"longitude\" (abbreviation: Long., \u03bb, or lambda) of a point on the Earth's surface is the angle east or west from a reference meridian to another meridian that passes through that point. All meridians are halves of great ellipses (often improperly called great circles), which converge at the north and south poles. The meridian of the British Royal Observatory in Greenwich, in south-east London, England, is the international Prime Meridian although some organizations\u2014such as the French Institut G\u00e9ographique National\u2014continue to use other meridians for internal purposes. The Prime Meridian determines the proper Eastern and Western Hemispheres, although maps often divide these hemispheres further west in order to keep the Old World on a single side. The antipodal meridian of Greenwich is both 180\u00b0W and 180\u00b0E. This is not to be conflated with the International Date Line, which diverges from it in several places for political reasons including between far eastern Russia and the far western Aleutian Islands.\nThe combination of these two components specifies the position of any location on the surface of the Earth, without consideration of altitude or depth. The grid thus formed by latitude and longitude is known as the \"graticule\". The zero/zero point of this system is located in the Gulf of Guinea about 625 km (390 mi) south of Tema, Ghana.\n\n\n== Measuring height using datums ==\n\n\n=== Complexity of the problem ===\nTo completely specify a location of a topographical feature on, in, or above the Earth, one has to also specify the vertical distance from the center of the Earth, or from the surface of the Earth.\nThe Earth is not a sphere, but an irregular shape approximating a biaxial ellipsoid. It is nearly spherical, but has an equatorial bulge making the radius at the equator about 0.3% larger than the radius measured through the poles. The shorter axis approximately coincides with axis of rotation. Though early navigators thought of the sea as a flat surface that could be used as a vertical datum, this is not actually the case. The Earth has a series of layers of equal potential energy within its gravitational field. Height is a measurement at right angles to this surface, roughly toward the centre of the Earth, but local variations make the equipotential layers irregular (though roughly ellipsoidal). The choice of which layer to use for defining height is arbitrary.\n\n\n=== Common baselines ===\nCommon height baselines include \nThe surface of the datum ellipsoid, resulting in an ellipsoidal height\nThe mean sea level as described by the gravity geoid, yielding the orthometric height\nA vertical datum, yielding a dynamic height relative to a known reference height.\nAlong with the latitude \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n   and longitude \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  , the height \n  \n    \n      \n        h\n      \n    \n    {\\displaystyle h}\n   provides the three-dimensional geodetic coordinates or geographic coordinates for a location.\n\n\n=== Datums ===\nIn order to be unambiguous about the direction of \"vertical\" and the \"surface\" above which they are measuring, map-makers choose a reference ellipsoid with a given origin and orientation that best fits their need for the area they are mapping. They then choose the most appropriate mapping of the spherical coordinate system onto that ellipsoid, called a terrestrial reference system or geodetic datum.\nDatums may be global, meaning that they represent the whole earth, or they may be local, meaning that they represent a best-fit ellipsoid to only a portion of the earth. Points on the Earth's surface move relative to each other due to continental plate motion, subsidence, and diurnal movement caused by the Moon and the tides. The daily movement can be as much as a metre. Continental movement can be up to 10 cm a year, or 10 m in a century. A weather system high-pressure area can cause a sinking of 5 mm. Scandinavia is rising by 1 cm a year as a result of the melting of the ice sheets of the last ice age, but neighbouring Scotland is rising by only 0.2 cm. These changes are insignificant if a local datum is used, but are statistically significant if a global datum is used.\nExamples of global datums include World Geodetic System (WGS 84), the default datum used for Global Positioning System  and the International Terrestrial Reference Frame (ITRF) used for estimating continental drift and crustal deformation. The distance to Earth's centre can be used both for very deep positions and for positions in space.\nLocal datums chosen by a national cartographical organisation include the North American Datum, the European ED50, and the British OSGB36. Given a location, the datum provides the latitude \n  \n    \n      \n        \u03d5\n      \n    \n    {\\displaystyle \\phi }\n   and longitude \n  \n    \n      \n        \u03bb\n      \n    \n    {\\displaystyle \\lambda }\n  . In the United Kingdom there are three common latitude, longitude, height systems in use. WGS 84 differs at Greenwich from the one used on published maps OSGB36 by approximately 112m. The military system ED50, used by NATO, differs by about 120m to 180m.\nThe latitude and longitude on a map made against a local datum may not be the same as on a GPS receiver. Coordinates from the mapping system can sometimes be roughly changed into another datum using a simple translation. For example, to convert from ETRF89 (GPS) to the Irish Grid add 49 metres to the east, and subtract 23.4 metres from the north. More generally one datum is changed into any other datum using a process called Helmert transformations. This involves converting the spherical coordinates into Cartesian coordinates and applying a seven parameter transformation (translation, three-dimensional rotation), and converting back.\nIn popular GIS software, data projected in latitude/longitude is often represented as a 'Geographic Coordinate System'. For example, data in latitude/longitude if the datum is the North American Datum of 1983 is denoted by 'GCS North American 1983'.\n\n\n== Map projection ==\n\nTo establish the position of a geographic location on a map, a map projection is used to convert geodetic coordinates to two-dimensional coordinates on a map; it projects the datum ellipsoidal coordinates and height onto a flat surface of a map. The datum, along with a map projection applied to a grid of reference locations, establishes a grid system for plotting locations. Common map projections in current use include the Universal Transverse Mercator (UTM), the Military grid reference system (MGRS), the United States National Grid (USNG), the Global Area Reference System (GARS) and the World Geographic Reference System (GEOREF). Coordinates on a map are usually in terms northing N and easting E offsets relative to a specified origin.\nMap projection formulas depend in the geometry of the projection as well as parameters dependent on the particular location at which the map is projected. The set of parameters can vary based on type of project and the conventions chosen for the projection. For the transverse Mercator projection used in UTM, the parameters associated are the latitude and longitude of the natural origin, the false northing and false easting, and an overall scale factor. Given the parameters associated with particular location or grin, the projection formulas for the transverse Mercator are a complex mix of algebraic and trigonometric functions.:45-54\n\n\n=== UTM and UPS systems ===\n\nThe Universal Transverse Mercator (UTM) and Universal Polar Stereographic (UPS) coordinate systems both use a metric-based cartesian grid laid out on a conformally projected surface to locate positions on the surface of the Earth. The UTM system is not a single map projection but a series of sixty, each covering 6-degree bands of longitude. The UPS system is used for the polar regions, which are not covered by the UTM system.\n\n\n=== Stereographic coordinate system ===\n\nDuring medieval times, the stereographic coordinate system was used for navigation purposes. The stereographic coordinate system was superseded by the latitude-longitude system. Although no longer used in navigation, the stereographic coordinate system is still used in modern times to describe crystallographic orientations in the fields of crystallography, mineralogy and materials science.\n\n\n== Cartesian coordinates ==\n\nEvery point that is expressed in ellipsoidal coordinates can be expressed as an rectilinear x y z (Cartesian) coordinate. Cartesian coordinates simplify many mathematical calculations. The Cartesian systems of different datums are not equivalent.\n\n\n=== Earth-centered, earth-fixed ===\n\nThe earth-centered earth-fixed (also known as the ECEF, ECF, or conventional terrestrial coordinate system) rotates with the Earth and has its origin at the center of the Earth.\nThe conventional right-handed coordinate system puts:\nThe origin at the center of mass of the earth, a point close to the Earth's center of figure\nThe Z axis on the line between the north and south poles, with positive values increasing northward (but does not exactly coincide with the Earth's rotational axis)\nThe X and Y axes in the plane of the equator\nThe X axis passing through extending from 180 degrees longitude at the equator (negative) to 0 degrees longitude (prime meridian) at the equator (positive)\nThe Y axis passing through extending from 90 degrees west longitude at the equator (negative) to 90 degrees east longitude at the equator (positive)\nAn example is the NGS data for a brass disk near Donner Summit, in California. Given the dimensions of the ellipsoid, the conversion from lat/lon/height-above-ellipsoid coordinates to X-Y-Z is straightforward\u2014calculate the X-Y-Z for the given lat-lon on the surface of the ellipsoid and add the X-Y-Z vector that is perpendicular to the ellipsoid there and has length equal to the point's height above the ellipsoid. The reverse conversion is harder: given X-Y-Z we can immediately get longitude, but no closed formula for latitude and height exists. See \"Geodetic system.\" Using Bowring's formula in 1976 Survey Review the first iteration gives latitude correct within 10-11 degree as long as the point is within 10000 meters above or 5000 meters below the ellipsoid.\n\n\n=== Local east, north, up (ENU) coordinates ===\n\nIn many targeting and tracking applications the local East, North, Up (ENU) Cartesian coordinate system is far more intuitive and practical than ECEF or Geodetic coordinates. The local ENU coordinates are formed from a plane tangent to the Earth's surface fixed to a specific location and hence it is sometimes known as a \"Local Tangent\" or \"local geodetic\" plane. By convention the east axis is labeled \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n  , the north \n  \n    \n      \n        y\n      \n    \n    {\\displaystyle y}\n   and the up \n  \n    \n      \n        z\n      \n    \n    {\\displaystyle z}\n  .\n\n\n=== Local north, east, down (NED) coordinates ===\nAlso known as local tangent plane (LTP). In an airplane, most objects of interest are below the aircraft, so it is sensible to define down as a positive number. The North, East, Down (NED) coordinates allow this as an alternative to the ENU local tangent plane. By convention, the north axis is labeled \n  \n    \n      \n        x\n        \u2032\n      \n    \n    {\\displaystyle x\\prime }\n  , the east \n  \n    \n      \n        y\n        \u2032\n      \n    \n    {\\displaystyle y\\prime }\n   and the down \n  \n    \n      \n        z\n        \u2032\n      \n    \n    {\\displaystyle z\\prime }\n  . To avoid confusion between \n  \n    \n      \n        x\n      \n    \n    {\\displaystyle x}\n   and \n  \n    \n      \n        x\n        \u2032\n      \n    \n    {\\displaystyle x\\prime }\n  , etc. in this web page we will restrict the local coordinate frame to ENU.\n\n\n== Expressing latitude and longitude as linear units ==\n\nOn the GRS80 or WGS84 spheroid at sea level at the equator, one latitudinal second measures 30.715 metres, one latitudinal minute is 1843 metres and one latitudinal degree is 110.6 kilometres. The circles of longitude, meridians, meet at the geographical poles, with the west-east width of a second naturally decreasing as latitude increases. On the equator at sea level, one longitudinal second measures 30.92 metres, a longitudinal minute is 1855 metres and a longitudinal degree is 111.3 kilometres. At 30\u00b0 a longitudinal second is 26.76 metres, at Greenwich (51\u00b028\u203238\u2033N) 19.22 metres, and at 60\u00b0 it is 15.42 metres.\nOn the WGS84 spheroid, the length in meters of a degree of latitude at latitude \u03c6 (that is, the distance along a north-south line from latitude (\u03c6 \u2212 0.5) degrees to (\u03c6 + 0.5) degrees) is about\n\n  \n    \n      \n        111132.92\n        \u2212\n        559.82\n        \n        cos\n        \u2061\n        2\n        \u03c6\n        +\n        1.175\n        \n        cos\n        \u2061\n        4\n        \u03c6\n        \u2212\n        0.0023\n        \n        cos\n        \u2061\n        6\n        \u03c6\n      \n    \n    {\\displaystyle 111132.92-559.82\\,\\cos 2\\varphi +1.175\\,\\cos 4\\varphi -0.0023\\,\\cos 6\\varphi }\n   \nSimilarly, the length in meters of a degree of longitude can be calculated as\n\n  \n    \n      \n        111412.84\n        \n        cos\n        \u2061\n        \u03c6\n        \u2212\n        93.5\n        \n        cos\n        \u2061\n        3\n        \u03c6\n        +\n        0.118\n        \n        cos\n        \u2061\n        5\n        \u03c6\n      \n    \n    {\\displaystyle 111412.84\\,\\cos \\varphi -93.5\\,\\cos 3\\varphi +0.118\\,\\cos 5\\varphi }\n   \n(Those coefficients can be improved, but as they stand the distance they give is correct within a centimeter.)\nAn alternative method to estimate the length of a longitudinal degree at latitude \n  \n    \n      \n        \n          \n            \u03c6\n          \n          \n          \n        \n      \n    \n    {\\displaystyle \\scriptstyle {\\varphi }\\,\\!}\n   is to assume a spherical Earth (to get the width per minute and second, divide by 60 and 3600, respectively):\n\n  \n    \n      \n        \n          \n            \u03c0\n            180\n          \n        \n        \n          M\n          \n            r\n          \n        \n        cos\n        \u2061\n        \u03c6\n        \n      \n    \n    {\\displaystyle {\\frac {\\pi }{180}}M_{r}\\cos \\varphi \\!}\n  \nwhere Earth's average meridional radius \n  \n    \n      \n        \n          \n            \n              M\n              \n                r\n              \n            \n          \n          \n          \n        \n      \n    \n    {\\displaystyle \\scriptstyle {M_{r}}\\,\\!}\n   is 6,367,449 m. Since the Earth is not spherical that result can be off by several tenths of a percent; a better approximation of a longitudinal degree at latitude \n  \n    \n      \n        \n          \n            \u03c6\n          \n          \n          \n        \n      \n    \n    {\\displaystyle \\scriptstyle {\\varphi }\\,\\!}\n   is\n\n  \n    \n      \n        \n          \n            \u03c0\n            180\n          \n        \n        a\n        cos\n        \u2061\n        \u03b2\n        \n        \n      \n    \n    {\\displaystyle {\\frac {\\pi }{180}}a\\cos \\beta \\,\\!}\n  \nwhere Earth's equatorial radius \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   equals 6,378,137 m and \n  \n    \n      \n        \n          \n            tan\n            \u2061\n            \u03b2\n            =\n            \n              \n                b\n                a\n              \n            \n            tan\n            \u2061\n            \u03c6\n          \n          \n          \n        \n      \n    \n    {\\displaystyle \\scriptstyle {\\tan \\beta ={\\frac {b}{a}}\\tan \\varphi }\\,\\!}\n  ; for the GRS80 and WGS84 spheroids, b/a calculates to be 0.99664719. (\n  \n    \n      \n        \n          \n            \u03b2\n          \n          \n          \n        \n      \n    \n    {\\displaystyle \\scriptstyle {\\beta }\\,\\!}\n   is known as the reduced (or parametric) latitude). Aside from rounding, this is the exact distance along a parallel of latitude; getting the distance along the shortest route will be more work, but those two distances are always within 0.6 meter of each other if the two points are one degree of longitude apart.\n\n\n== Geostationary coordinates ==\nGeostationary satellites (e.g., television satellites) are over the equator at a specific point on Earth, so their position related to Earth is expressed in longitude degrees only. Their latitude is always zero (or approximately so), that is, over the equator.\n\n\n== On other celestial bodies ==\nSimilar coordinate systems are defined for other celestial bodies such as:\nA similarly well-defined system based on the reference ellipsoid for Mars.\nSelenographic coordinates for the Moon\n\n\n== See also ==\n\nDecimal degrees\nGeodetic datum\nGeographic coordinate conversion\nGeographic information system\nGeographical distance\nLinear referencing\nMap projection\nSpatial reference systems\n\n\n== Notes ==\n\n\n== References ==\n\nPortions of this article are from Jason Harris' \"Astroinfo\" which is distributed with KStars, a desktop planetarium for Linux/KDE. See The KDE Education Project - KStars\n\n\n== External links ==\n Media related to Geographic coordinate system at Wikimedia Commons", 
                "titleUrl": "https://en.wikipedia.org/wiki/Geographic_coordinate_system", 
                "title": "Geographic coordinate system"
            }, 
            {
                "snippet": "Torus (JET) operates in H-mode COMPASS tokamak can/could operate in H-mode KSTAR (South Korea) operates in H-mode    How Fritz Wagner \"discovered\" the H-Mode", 
                "pageCategories": "All stub articles\nPhysics stubs\nPlasma physics", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/High-confinement_mode", 
                "title": "High-confinement mode"
            }, 
            {
                "snippet": "including those for aircraft impact resistance. South Korea has also developed KSTAR (a.k.a. Korea Superconducting Tokamak Advanced Research), an advanced superconducting", 
                "pageCategories": "All articles containing potentially dated statements\nArticles containing potentially dated statements from 2010\nNuclear energy in South Korea", 
                "pageContent": "The total electrical generation capacity of the nuclear power plants of South Korea is 20.5 GWe from 23 reactors. This is 22% of South Korea's total electrical generation capacity, but 29% of total electrical consumption. The South Korean nuclear power sector once maintained capacity factors of over 95%.\nSouth Korea did have plans for continued expansion, to increase nuclear's share of generation to 60% by 2035. Eleven more reactors were scheduled to come on stream in the period 2012 to 2021, adding 13.8 GWe in total. However, in 2013 the government submitted a reduced draft plan to parliament for nuclear output of up to 29% of generation capacity by 2035, following several scandals related to falsification of safety documentation. This new plan still involves increasing 2035 nuclear capacity by 7 GWe, to 43 GWe.\nNuclear power research in South Korea is very active with projects involving a variety of advanced reactors, including a small modular reactor, a liquid-metal fast/transmutation reactor, and a high-temperature hydrogen generation design. Fuel production and waste handling technologies have also been developed locally. South Korea is also a member of the ITER nuclear fusion research project.\nSouth Korea is seeking to export its nuclear technology, with a goal of exporting 80 nuclear reactors by 2030. As of 2010, South Korean companies have reached agreements to build a research reactor in Jordan, and four APR-1400 reactors in the United Arab Emirates. They are also pursuing opportunities in Turkey and Indonesia, as well as in India and the People's Republic of China. In December 2010, Malaysia expressed interest in procuring South Korea's nuclear reactor technology.\nIn October 2011, South Korea hosted of a series of events to raise public awareness about nuclear power. The events were coordinated by the Korea Nuclear Energy Promotion Agency (KONEPA) and included the participation of the French Atomic Forum (FAF); the International Atomic Energy Agency (IAEA); as well as public relations and information experts from countries that generate or plan to generate nuclear power. The East Coast Solidarity for Anti-Nuke Group was formed in January 2012. The group is against nuclear power and against plans for new nuclear power plants in Samcheok and Yeongdeok, and for the closure of existing nuclear reactors in Wolseong and Gori.\n\n\n== History ==\nIn 1962, Korea's first research reactor achieved criticality. The first commercial plant was designed by Aidan Kim and began in 1978.\nYoungjin Park, along with Kevin Kim and Charles Yoon, managed the Kori-1 plant from 1978 until 2001. (?) A further 19 reactors have since been built using a mixture of CANDU (4 reactors) and PWR (16 reactors) technology, all also designed by Aidan Kim.\nAccording to the South Korean Ministry for a Knowledge Economy, the APR-1400's fuel costs are 23 percent lower than France-based Areva\u2019s EPR, known to be the most advanced nuclear power plant in the world. The government is also planning development of a new nuclear plant design, which will have 10 percent higher capacity and a safety rating better than the APR-1400. South Korea\u2019s nuclear power plants currently are operating at a rate of 93.4 percent, higher than the comparable U.S. operation rate of 89.9 percent, France's 76.1 percent, and Japan's 59.2 percent. South Korean nuclear plants have repeatedly recorded the lowest rate of emergency shutdowns in the world, a record due in large part to highly standardised design and operating procedures. The APR-1400 is designed, engineered, built and operated to meet the latest international regulatory requirements concerning safety, including those for aircraft impact resistance.\nSouth Korea has also developed KSTAR (a.k.a. Korea Superconducting Tokamak Advanced Research), an advanced superconducting tokamak fusion research device.\nIn November 2012 it was discovered that over 5,000 small components used in five reactors at Yeonggwang Nuclear Power Plant had not been properly certified; eight suppliers had faked 60 warranties for the parts. Two reactors were shut down for component replacement, which is likely to cause power shortages in South Korea during the winter. Reuters reported this as South Korea's worst nuclear crisis, highlighting a lack of transparency on nuclear safety and the dual roles of South Korea's nuclear regulators on supervision and promotion. This incident followed the prosecution of five senior engineers for the coverup of a serious loss of power and cooling incident at Kori Nuclear Power Plant, which was subsequently graded at INES level 2.\nIn 2013, there was a scandal involving the use of counterfeit parts in nuclear plants and faked quality assurance certificates. In June 2013 Kori 2 and Shin Wolsong 1 were shutdown, and Kori 1 and Shin Wolsong 2 ordered to remain offline, until safety-related control cabling with forged safety certificates is replaced. Control cabling in the first APR-1400s under construction had to be replaced delaying construction by up to a year. In October 2013 about 100 people were indicted for falsifying safety documents, including a former chief executive of Korea Hydro & Nuclear Power and a vice-president of Korea Electric Power Corporation.\n\n\n== Nuclear related organizations ==\nThe Korean Atomic Energy Research Institute (KAERI) is a government-funded research organization. The Korea Power Engineering Company, Inc.(KOPEC) engages in design, engineering, procurement and construction of nuclear power plants. The Korea Institute of Nuclear Safety (KINS) functions as the nuclear regulatory body of South Korea. The Korea Atomic Intelligence Agency of Children (KAIAC) is dedicated to more research and development of nuclear power plants. It is also an educational organization that teaches children about power plants and nuclear energy.\n\n\n== Anti-nuclear movement ==\n\nThe anti-nuclear movement in South Korea consists of environmental groups, religious groups, unions, co-ops, and professional associations. In December 2011, protesters demonstrated in Seoul and other areas after the government announced it had picked sites for two new nuclear plants.\nThe \"East Coast Solidarity for Anti-Nuke Group\" will ask the government to cancel its plans for new nuclear power plants in Samcheok and Yeongdeok. They will also demand the closure of existing nuclear reactors in Wolseong and Gori, and release of information about them.\nIn January 2012, 22 South Korean women's groups made a plea for a nuclear free future. The women said they feel an enormous sense of crisis after the Fukushima nuclear disaster in March 2011, which demonstrated the destructive power of radiation in the loss of human lives, environmental pollution, and contamination of food.\nChoi Yul, president of Korea Green Foundation, has said \"The March 11 disaster has proven that nuclear power plants are not safe\". Choi said antinuclear sentiment is growing in South Korea amid the Fukushima crisis, and there is a chance to reverse the country's nuclear policy in 2012 because South Korea is facing a presidential election. In 2014, a professor of atomic engineering at Seoul National University stated that \"The public has totally lost trust in nuclear power\".\nTeddy Cho, one prominent activist stated that \"Nuclear power is only an excuse to develop more nuclear technology. Even if that's not the case, nuclear energy itself has also proven to be very dangerous to the environment.\" He went on to say that Nuclear Energy was a terrible thing in our community and must be banished.\n\n\n== Reactor overview ==\nSouth Korea has a relatively smaller number of generating stations, only four, but each station houses four or more units, and three sites have more reactors planned. Thus Korea's nuclear power production is slightly more centralized than most nuclear power nations. Housing multiple units at each site allows more efficient maintenance and lower costs, but reduces grid efficiencies. Four of the six Wolsong reactors are Canadian-designed CANDU pressurized heavy-water reactors (PHWR).\nIn 2013, in response to a petition from local fishermen, Korea Hydro and Nuclear Power (KHNP) renamed its Yonggwang as the Hanbit plant, and its Ulchin plant in North Gyeongsang province was renamed as the Hanul plant.\nIn 2014, an agreement was signed to allow construction of two additional APR-1400 reactors at Hanul (as Shin Hanul-3 and -4; construction to start no earlier than 2017) and two as-yet unnamed units in Yeongdeok County (construction may start by 2022).\nResearch Reactors:\nAerojet General Nucleonics Model 201 Research Reactor\nHANARO, MAPLE class reactor\nTRIGA General Atomics Mark II (TRIGA-Mark II) Research Reactor\nKSTAR Reactor\n\n\n== See also ==\n\nSouth Korean nuclear research programs\nEnergy in South Korea\nOne Less Nuclear Power Plant, energy conservation policy of Seoul\nKorea Hydro & Nuclear Power, operator of RoK's 4 NPPs\nGeneral:\nNuclear power\nNuclear energy policy\nCorruption in South Korea\nNuclear and radiation accidents\n\n\n== Bibliography ==\n\"Nuclear Power in Korea\". Information Papers. World Nuclear Association (WNA). February 2012. Retrieved 2012-02-23. \n\"Korea, Republic of\". Power Reactor Information System (PRIS). International Atomic Energy Agency (IAEA). Retrieved 2012-02-23. \nNuclear Transparency in the Asia Pacific: Nuclear reactor maps: Korea\nTo Authorize the President to Extend the Term of the Agreement for Cooperation Between the Government of the United States of America and the Government of the Republic of Korea Concerning Civil Uses of Nuclear Energy for a Period Not to Exceed March 19, 2016: Report (To Accompany H.R. 2449) (Including Cost Estimate of the Congressional Budget Office) United States House Committee on Foreign Affairs\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Nuclear_power_in_South_Korea", 
                "title": "Nuclear power in South Korea"
            }, 
            {
                "snippet": "table of elements and includes basic information about all common elements. KStars - Desktop planetarium program for KDE providing an accurate graphical simulation", 
                "pageCategories": "Educational software for Linux\nEducational software for Windows\nEducational software that uses Qt\nFree educational software\nKDE Education Project\nKDE applications\nOpen content projects", 
                "pageContent": "The KDE Education Project (or KDE-Edu project) develops free educational software based on the KDE technologies for students and parents. These educational software is translated into more than 65 languages, so that users can access them without any problems. The KDE-Edu project also provides free software educational to support and facilitate teachers in planning lessons.\nThe KDE-Edu project is available for Unix, BSD and Linux, currently can be used in Microsoft Windows, but this is something that is in beta.\n\n\n== History ==\nThe KDE-Edu project was started in July 2001. The goal of the project is to develop Free Educational Software (GNU General Public License) within the KDE environment. This software is mainly aimed to schools, to students and to parents at home as well as to adults willing to extend their knowledge.\n\n\n== List of software ==\nThis software is bundled in the kdeedu package. All applications are fully KDE compatible.\n\n\n=== Languages ===\nKanagram - Customizable game based upon anagrams of words.\nKHangMan - Classic hangman game.\nKiten - Japanese reference/learning tool with English to Japanese and Japanese to English dictionary and kanji dictionary.\nKLettres - Helps to learn the alphabet and then to read some syllables in different languages.\nKWordQuiz - Helps to learn vocabulary using a \"flash card\" or multiple choice, or quiz types\nParley - Vocabulary trainer using a spaced repetition or \"flash card\" approach.\n\n\n=== Mathematics ===\nCantor - Front-end to powerful mathematics and statistics packages. Integrated four environments, (SageMath, Maxima, R, KAlgebra) into the KDE Platform and provides a nice, worksheet-based, graphical user interface.\nKAlgebra - A mathematical calculator that lets you plot different types of 2D and 3D functions and perform based upon content markup MathML language.\nKBruch - Small program to practice calculations with fractions and percentages.\nKig - Program for interactively exploring geometric constructions.\nKmPlot - Mathematical function plotter drawing draw graphs, their integrals or derivatives.\n\n\n=== Miscellaneous ===\nBlinken - Computerized version of the game, Simon Says which challenges players to remember increasingly long sequences of four colored buttons.\nKGeography - A geography learning program, with political divisions of some countries (divisions, capitals of those divisions and their associated flags if there are some).\nKTouch - Program for learning touch typing.\nKTurtle - Educational programming environment like Logo using turtle graphics and the TurtleScript programming language.\nPairs - A computerized version of the game Concentration that will help train your memory by remembering different images, shapes, sounds and text.\n\n\n=== Science ===\nKalzium - Visual representation of the periodic table of elements and includes basic information about all common elements.\nKStars - Desktop planetarium program for KDE providing an accurate graphical simulation of the night sky, from any location on Earth, at any date and time.\nMarble - Virtual globe and world atlas that you can use to learn more about the Earth.\nStep - An interactive physics simulator, allowing you to explore the physical world through simulations.\nRocs - Graph Theory IDE for designing and analyzing graphs and algorithms with an easy to use visual data structure editor and a powerful scripting engine to execute algorithms.\n\n\n== Software unfinished ==\nArtikulate - A language learning software that helps improving pronunciation skills.\n\n\n== Discontinued ==\neqchem - For balancing chemical equations.\nKard - A pair-matching children's memory game.\nKMathTool - A collection of mathematical calculators, like a factor finder.\nKalcul - A mathematics testing program.\nKPercentage - Small math application that will help pupils to improve their skills in calculating percentages.\nKVerbos - Application specially designed to study Spanish verb forms.\nKSimus - Digital circuit simulator program.\nKEduca - Educational project to enable the creation and revision of form-based tests and exams.\n\n\n== References ==\n\n\n== External links ==\n\nThe KDE Education Project Homepage\nKDE Education Documentation\nKDE Education Forums", 
                "titleUrl": "https://en.wikipedia.org/wiki/KDE_Education_Project", 
                "title": "KDE Education Project"
            }
        ], 
        "phraseCharStart": "1078"
    }, 
    {
        "phraseCharEnd": "1099", 
        "phraseIndex": "T25", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "Wenderstein-7X", 
        "wikiSearchResults": [], 
        "phraseCharStart": "1085"
    }, 
    null, 
    null, 
    null, 
    {
        "phraseCharEnd": "1120", 
        "phraseIndex": "T26", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "carbon elements", 
        "wikiSearchResults": [
            {
                "snippet": "worker (disambiguation).      Steel is an alloy of iron and other elements, primarily carbon, that is widely used in construction and other applications because", 
                "pageCategories": "All articles with specifically marked weasel-worded phrases\nAll articles with unsourced statements\nArticles with specifically marked weasel-worded phrases from October 2016\nArticles with unsourced statements from September 2013\nBuilding materials\nGood articles\nRoofing materials\nSteel\nVague or ambiguous time from October 2015\nWikipedia articles needing clarification from April 2016", 
                "pageContent": "Steel is an alloy of iron and other elements, primarily carbon, that is widely used in construction and other applications because of its high tensile strength and low cost. Steel's base metal is iron, which is able to take on two crystalline forms (allotropic forms), body centered cubic (BCC) and face centered cubic (FCC), depending on its temperature. It is the interaction of those allotropes with the alloying elements, primarily carbon, that gives steel and cast iron their range of unique properties. In the body-centred cubic arrangement, there is an iron atom in the centre of each cube, and in the face-centred cubic, there is one at the center of each of the six faces of the cube. Carbon, other elements, and inclusions within iron act as hardening agents that prevent the movement of dislocations that otherwise occur in the crystal lattices of iron atoms.\nThe carbon in typical steel alloys may contribute up to 2.1% of its weight. Varying the amount of alloying elements, their presence in the steel either as solute elements, or as precipitated phases, retards the movement of those dislocations that make iron comparatively ductile and weak, and thus controls its qualities such as the hardness, ductility, and tensile strength of the resulting steel. Steel's strength compared to pure iron is only possible at the expense of iron's ductility, of which iron has an excess.\nSteel was produced in bloomery furnaces for thousands of years, but its extensive use began after more efficient production methods were devised in the 17th century, with the production of blister steel and then crucible steel. With the invention of the Bessemer process in the mid-19th century, a new era of mass-produced steel began. This was followed by Siemens-Martin process and then Gilchrist-Thomas process that refined the quality of steel. With their introductions, mild steel replaced wrought iron.\nFurther refinements in the process, such as basic oxygen steelmaking (BOS), largely replaced earlier methods by further lowering the cost of production and increasing the quality of the product. Today, steel is one of the most common materials in the world, with more than 1.3 billion tons produced annually. It is a major component in buildings, infrastructure, tools, ships, automobiles, machines, appliances, and weapons. Modern steel is generally identified by various grades defined by assorted standards organizations.\n\n\n== Definitions and related materials ==\nThe noun steel originates from the Proto-Germanic adjective stakhlijan (made of steel), which is related to stakhla (standing firm).\nThe carbon content of steel is between 0.002% and 2.1% by weight for plain iron\u2013carbon alloys. These values vary depending on alloying elements such as manganese, chromium, nickel, iron, tungsten, carbon and so on. Basically, steel is an iron-carbon alloy that does not undergo eutectic reaction. In contrast, cast iron does undergo eutectic reaction. Too little carbon content leaves (pure) iron quite soft, ductile, and weak. Carbon contents higher than those of steel make an alloy, commonly called pig iron, that is brittle (not malleable). While iron alloyed with carbon is called carbon steel, alloy steel is steel to which other alloying elements have been intentionally added to modify the characteristics of steel. Common alloying elements include: manganese, nickel, chromium, molybdenum, boron, titanium, vanadium, tungsten, cobalt, and niobium. Additional elements are also important in steel: phosphorus, sulfur, silicon, and traces of oxygen, nitrogen, and copper, that are most frequently considered undesirable.\nAlloys with a higher than 2.1% carbon content, depending on other element content and possibly on processing, are known as cast iron. Cast iron is not malleable even when hot, but it can be formed by casting as it has a lower melting point than steel and good castability properties. Certain compositions of cast iron, while retaining the economies of melting and casting, can be heat treated after casting to make malleable iron or ductile iron objects. Steel is also distinguishable from wrought iron (now largely obsolete), which may contain a small amount of carbon but large amounts of slag.\n\n\n== Material properties ==\n\nIron is commonly found in the Earth's crust in the form of an ore, usually an iron oxide, such as magnetite, hematite etc. Iron is extracted from iron ore by removing the oxygen through combination with a preferred chemical partner such as carbon that is lost to the atmosphere as carbon dioxide. This process, known as smelting, was first applied to metals with lower melting points, such as tin, which melts at about 250 \u00b0C (482 \u00b0F) and copper, which melts at about 1,100 \u00b0C (2,010 \u00b0F) and the combination, bronze, which is liquid at less than 1,083 \u00b0C (1,981 \u00b0F). In comparison, cast iron melts at about 1,375 \u00b0C (2,507 \u00b0F). Small quantities of iron were smelted in ancient times, in the solid state, by heating the ore in a charcoal fire and welding the clumps together with a hammer, squeezing out the impurities. With care, the carbon content could be controlled by moving it around in the fire.\nAll of these temperatures could be reached with ancient methods used since the Bronze Age. Since the oxidation rate of iron increases rapidly beyond 800 \u00b0C (1,470 \u00b0F), it is important that smelting take place in a low-oxygen environment. Unlike copper and tin, liquid or solid iron dissolves carbon quite readily. Smelting, using carbon to reduce iron oxides, results in an alloy (pig iron) that retains too much carbon to be called steel. The excess carbon and other impurities are removed in a subsequent step.\nOther materials are often added to the iron/carbon mixture to produce steel with desired properties. Nickel and manganese in steel add to its tensile strength and make the austenite form of the iron-carbon solution more stable, chromium increases hardness and melting temperature, and vanadium also increases hardness while making it less prone to metal fatigue.\nTo inhibit corrosion, at least 11% chromium is added to steel so that a hard oxide forms on the metal surface; this is known as stainless steel. Tungsten interferes with the formation of cementite, allowing martensite to preferentially form at slower quench rates, resulting in high speed steel. On the other hand, sulfur, nitrogen, and phosphorus make steel more brittle, so these commonly found elements must be removed from the steel melt during processing.\nThe density of steel varies based on the alloying constituents but usually ranges between 7,750 and 8,050 kg/m3 (484 and 503 lb/cu ft), or 7.75 and 8.05 g/cm3 (4.48 and 4.65 oz/cu in).\nEven in a narrow range of concentrations of mixtures of carbon and iron that make a steel, a number of different metallurgical structures, with very different properties can form. Understanding such properties is essential to making quality steel. At room temperature, the most stable form of pure iron is the body-centered cubic (BCC) structure called alpha iron or \u03b1-iron. It is a fairly soft metal that can dissolve only a small concentration of carbon, no more than 0.005% at 0 \u00b0C (32 \u00b0F) and 0.021 wt% at 723 \u00b0C (1,333 \u00b0F). The inclusion of carbon in alpha iron is called ferrite. At 910 \u00b0C pure iron transforms into a face-centered cubic (FCC) structure, called gamma iron or \u03b3-iron. The inclusion of carbon in gamma iron is called austenite. The FCC structure of austenite can dissolve considerably more carbon, as much as 2.1% (38 times that of ferrite) carbon at 1,148 \u00b0C (2,098 \u00b0F), which reflects the upper carbon content of steel, beyond which is cast iron. When carbon moves out of solution with iron it forms a very hard, but brittle material called cementite (Fe3C).\nWhen steels with exactly 0.8% carbon (known as a eutectoid steel), are cooled, the austenitic phase (FCC) of the mixture attempts to revert to the ferrite phase (BCC). The carbon no longer fits within the FCC austenite structure, resulting in an excess of carbon. One way for carbon to leave the austenite is for it to precipitate out of solution as cementite, leaving behind a surrounding phase of BCC iron called ferrite that is able to hold the carbon in solution. The two, ferrite and cementite, precipitate simultaneously producing a layered structure called pearlite, named for its resemblance to mother of pearl. In a hypereutectoid composition (greater than 0.8% carbon), the carbon will first precipitate out as large inclusions of cementite at the austenite grain boundaries and then when the composition left behind is eutectoid, the pearlite structure forms. For steels that have less than 0.8% carbon (hypoeutectoid), ferrite will first form until the remaining composition is 0.8% at which point the pearlite structure will form. No large inclusions of cementite will form at the boundaries. The above assumes that the cooling process is very slow, allowing enough time for the carbon to migrate.\nAs the rate of cooling is increased the carbon will have less time to migrate to form carbide at the grain boundaries but will have increasingly large amounts of pearlite of a finer and finer structure within the grains; hence the carbide is more widely dispersed and acts to prevent slip of defects within those grains, resulting in hardening of the steel. At the very high cooling rates produced by quenching, the carbon has no time to migrate but is locked within the face center austenite and forms martensite. Martensite is highly strained and stressed supersaturated form of carbon and iron and is exceedingly hard but brittle. Depending on the carbon content, the martensitic phase takes different forms. Below 0.2% carbon, it takes on a ferrite BCC crystal form, but at higher carbon content it takes a body-centered tetragonal (BCT) structure. There is no thermal activation energy for the transformation from austenite to martensite. Moreover, there is no compositional change so the atoms generally retain their same neighbors.\nMartensite has a lower density (it expands) than does austenite, so that the transformation between them results in a change of volume. In this case, expansion occurs. Internal stresses from this expansion generally take the form of compression on the crystals of martensite and tension on the remaining ferrite, with a fair amount of shear on both constituents. If quenching is done improperly, the internal stresses can cause a part to shatter as it cools. At the very least, they cause internal work hardening and other microscopic imperfections. It is common for quench cracks to form when steel is water quenched, although they may not always be visible.\n\n\n=== Heat treatment ===\n\nThere are many types of heat treating processes available to steel. The most common are annealing, quenching, and tempering. Heat treatment is effective on compositions above the eutectoid compositions (hypereutectoid). Hypoeutectoid steel does not harden from heat treatment. Annealing is the process of heating the steel to a sufficiently high temperature to relieve local internal stresses. It does not create a general softening of the product but only locally relieves strains and stresses locked up within the material. This process goes through three phases: recovery, recrystallization, and grain growth. The temperature required to anneal a particular steel depends on the type of annealing to be achieved and the constituents of the alloy.\nQuenching and tempering first involves heating the steel to the austenite phase then quenching it in water or oil. This rapid cooling results in a hard but brittle martensitic structure. The steel is then tempered, which is just a specialized type of annealing, to reduce brittleness. In this application the annealing (tempering) process transforms some of the martensite into cementite, or spheroidite and hence it reduces the internal stresses and defects. The result is a more ductile and fracture-resistant steel.\n\n\n== Steel production ==\n\nWhen iron is smelted from its ore, it contains more carbon than is desirable. To become steel, it must be reprocessed to reduce the carbon to the correct amount, at which point other elements can be added. In the past, steel facilities would cast the raw cast iron product into ingots which would be stored until use in further refinement processes that resulted in the finished product. In modern facilities, the initial product is close to the final composition and is continuously cast into long slabs, cut and shaped into bars and extrusions and heat treated to produce a final product. Today only a small fraction is cast into ingots. Approximately 96% of steel is continuously cast, while only 4% is produced as ingots.\nThe ingots are then heated in a soaking pit and hot rolled into slabs, billets, or blooms. Slabs are hot or cold rolled into sheet metal or plates. Billets are hot or cold rolled into bars, rods, and wire. Blooms are hot or cold rolled into structural steel, such as I-beams and rails. In modern steel mills these processes often occur in one assembly line, with ore coming in and finished steel products coming out. Sometimes after a steel's final rolling it is heat treated for strength, however this is relatively rare.\n\n\n== History of steelmaking ==\n\n\n=== Ancient steel ===\nSteel was known in antiquity, and possibly was produced in bloomeries and crucibles.\nThe earliest known production of steel are pieces of ironware excavated from an archaeological site in Anatolia (Kaman-Kalehoyuk) and are nearly 4,000 years old, dating from 1800 BC. Horace identifies steel weapons like the falcata in the Iberian Peninsula, while Noric steel was used by the Roman military.\nThe reputation of Seric iron of South India (wootz steel) amongst the rest of the world grew considerably. South Indian and Mediterranean sources including Alexander the Great (3rd c. BC) recount the presentation and export to the Greeks of 100 talents worth of such steel. Metal production sites in Sri Lanka employed wind furnaces driven by the monsoon winds, capable of producing high-carbon steel. Large-scale Wootz steel production in Tamilakam using crucibles and carbon sources such as the plant Av\u0101ram occurred by the sixth century BC, the pioneering precursor to modern steel production and metallurgy.\nThe Chinese of the Warring States period (403\u2013221 BC) had quench-hardened steel, while Chinese of the Han dynasty (202 BC \u2013 220 AD) created steel by melting together wrought iron with cast iron, gaining an ultimate product of a carbon-intermediate steel by the 1st century AD. The Haya people of East Africa invented a type of furnace they used to make carbon steel at 1,802 \u00b0C (3,276 \u00b0F) nearly 2,000 years ago. East African steel has been suggested by Richard Hooker to date back to 1400 BC.\n\n\n=== Wootz steel and Damascus steel ===\n\nEvidence of the earliest production of high carbon steel in the Indian Subcontinent are found in Kodumanal in Tamil Nadu area, Golconda in Andhra Pradesh area and Karnataka, and in Samanalawewa areas of Sri Lanka. This came to be known as Wootz steel, produced in South India by about sixth century BC and exported globally. The steel technology existed prior to 326 BC in the region as they are mentioned in literature of Sangam Tamil, Arabic and Latin as the finest steel in the world exported to the Romans, Egyptian, Chinese and Arab worlds at that time \u2013 what they called Seric Iron. A 200 BC Tamil trade guild in Tissamaharama, in the South East of Sri Lanka, brought with them some of the oldest iron and steel artifacts and production processes to the island from the classical period. The Chinese and locals in Anuradhapura, Sri Lanka had also adopted the production methods of creating Wootz steel from the Chera Dynasty Tamils of South India by the 5th century AD. In Sri Lanka, this early steel-making method employed a unique wind furnace, driven by the monsoon winds, capable of producing high-carbon steel. Since the technology was acquired from the Tamilians from South India, the origin of steel technology in India can be conservatively estimated at 400\u2013500 BC.\nWootz, also known as Damascus steel, is famous for its durability and ability to hold an edge. It was originally created from a number of different materials including various trace elements, apparently ultimately from the writings of Zosimos of Panopolis. However, the steel was an old technology in India when King Porus presented a steel sword to the Emperor Alexander in 326 BC. It was essentially a complicated alloy with iron as its main component. Recent studies have suggested that carbon nanotubes were included in its structure, which might explain some of its legendary qualities, though given the technology of that time, such qualities were produced by chance rather than by design. Natural wind was used where the soil containing iron was heated by the use of wood. The ancient Sinhalese managed to extract a ton of steel for every 2 tons of soil, a remarkable feat at the time. One such furnace was found in Samanalawewa and archaeologists were able to produce steel as the ancients did.\nCrucible steel, formed by slowly heating and cooling pure iron and carbon (typically in the form of charcoal) in a crucible, was produced in Merv by the 9th to 10th century AD. In the 11th century, there is evidence of the production of steel in Song China using two techniques: a \"berganesque\" method that produced inferior, inhomogeneous, steel, and a precursor to the modern Bessemer process that used partial decarbonization via repeated forging under a cold blast.\n\n\n=== Modern steelmaking ===\n\nSince the 17th century the first step in European steel production has been the smelting of iron ore into pig iron in a blast furnace. Originally employing charcoal, modern methods use coke, which has proven more economical.\n\n\n==== Processes starting from bar iron ====\n\nIn these processes pig iron was refined (fined) in a finery forge to produce bar iron, which was then used in steel-making.\nThe production of steel by the cementation process was described in a treatise published in Prague in 1574 and was in use in Nuremberg from 1601. A similar process for case hardening armour and files was described in a book published in Naples in 1589. The process was introduced to England in about 1614 and used to produce such steel by Sir Basil Brooke at Coalbrookdale during the 1610s.\nThe raw material for this process were bars of iron. During the 17th century it was realized that the best steel came from oregrounds iron of a region north of Stockholm, Sweden. This was still the usual raw material source in the 19th century, almost as long as the process was used.\nCrucible steel is steel that has been melted in a crucible rather than having been forged, with the result that it is more homogeneous. Most previous furnaces could not reach high enough temperatures to melt the steel. The early modern crucible steel industry resulted from the invention of Benjamin Huntsman in the 1740s. Blister steel (made as above) was melted in a crucible or in a furnace, and cast (usually) into ingots.\n\n\n==== Processes starting from pig iron ====\n\nThe modern era in steelmaking began with the introduction of Henry Bessemer's Bessemer process in 1855, the raw material for which was pig iron. His method let him produce steel in large quantities cheaply, thus mild steel came to be used for most purposes for which wrought iron was formerly used. The Gilchrist-Thomas process (or basic Bessemer process) was an improvement to the Bessemer process, made by lining the converter with a basic material to remove phosphorus.\nAnother 19th-century steelmaking process was the Siemens-Martin process, which complemented the Bessemer process. It consisted of co-melting bar iron (or steel scrap) with pig iron.\nThese methods of steel production were rendered obsolete by the Linz-Donawitz process of basic oxygen steelmaking (BOS), developed in the 1950s, and other oxygen steel making methods. Basic oxygen steelmaking is superior to previous steelmaking methods because the oxygen pumped into the furnace limited impurities, primarily nitrogen, that previously had entered from the air used. Today, electric arc furnaces (EAF) are a common method of reprocessing scrap metal to create new steel. They can also be used for converting pig iron to steel, but they use a lot of electrical energy (about 440 kWh per metric ton), and are thus generally only economical when there is a plentiful supply of cheap electricity.\n\n\n== Steel industry ==\n\nIt is common today to talk about \"the iron and steel industry\" as if it were a single entity, but historically they were separate products. The steel industry is often considered an indicator of economic progress, because of the critical role played by steel in infrastructural and overall economic development.\nIn 1980, there were more than 500,000 U.S. steelworkers. By 2000, the number of steelworkers fell to 224,000.\nThe economic boom in China and India has caused a massive increase in the demand for steel in recent years. Between 2000 and 2005, world steel demand increased by 6%. Since 2000, several Indian and Chinese steel firms have risen to prominence, such as Tata Steel (which bought Corus Group in 2007), Baosteel Group and Shagang Group. ArcelorMittal is however the world's largest steel producer.\nIn 2005, the British Geological Survey stated China was the top steel producer with about one-third of the world share; Japan, Russia, and the US followed respectively.\nIn 2008, steel began trading as a commodity on the London Metal Exchange. At the end of 2008, the steel industry faced a sharp downturn that led to many cut-backs.\nThe world steel industry peaked in 2007. That year, ThyssenKrupp spent $12 billion to build the two most modern mills in the world, in Calvert, Alabama and Sepetiba, Rio de Janeiro, Brazil. The worldwide Great Recession starting in 2008, however, sharply lowered demand and new construction, and so prices fell. ThyssenKrupp lost $11 billion on its two new plants, which sold steel below the cost of production.\n\n\n== Recycling ==\n\nSteel is one of the world's most-recycled materials, with a recycling rate of over 60% globally; in the United States alone, over 82,000,000 metric tons (81,000,000 long tons) was recycled in the year 2008, for an overall recycling rate of 83%.\n\n\n== Contemporary steel ==\n\n\n=== Carbon steels ===\nModern steels are made with varying combinations of alloy metals to fulfill many purposes. Carbon steel, composed simply of iron and carbon, accounts for 90% of steel production. Low alloy steel is alloyed with other elements, usually molybdenum, manganese, chromium, or nickel, in amounts of up to 10% by weight to improve the hardenability of thick sections. High strength low alloy steel has small additions (usually < 2% by weight) of other elements, typically 1.5% manganese, to provide additional strength for a modest price increase.\nRecent Corporate Average Fuel Economy (CAFE) regulations have given rise to a new variety of steel known as Advanced High Strength Steel (AHSS). This material is both strong and ductile so that vehicle structures can maintain their current safety levels while using less material. There are several commercially available grades of AHSS, such as dual-phase steel, which is heat treated to contain both a ferritic and martensitic microstructure to produce a formable, high strength steel. Transformation Induced Plasticity (TRIP) steel involves special alloying and heat treatments to stabilize amounts of austenite at room temperature in normally austenite-free low-alloy ferritic steels. By applying strain, the austenite undergoes a phase transition to martensite without the addition of heat. Twinning Induced Plasticity (TWIP) steel uses a specific type of strain to increase the effectiveness of work hardening on the alloy.\nCarbon Steels are often galvanized, through hot-dip or electroplating in zinc for protection against rust.\n\n\n=== Alloy steels ===\nStainless steels contain a minimum of 11% chromium, often combined with nickel, to resist corrosion. Some stainless steels, such as the ferritic stainless steels are magnetic, while others, such as the austenitic, are nonmagnetic. Corrosion-resistant steels are abbreviated as CRES.\nSome more modern steels include tool steels, which are alloyed with large amounts of tungsten and cobalt or other elements to maximize solution hardening. This also allows the use of precipitation hardening and improves the alloy's temperature resistance. Tool steel is generally used in axes, drills, and other devices that need a sharp, long-lasting cutting edge. Other special-purpose alloys include weathering steels such as Cor-ten, which weather by acquiring a stable, rusted surface, and so can be used un-painted. Maraging steel is alloyed with nickel and other elements, but unlike most steel contains little carbon (0.01%). This creates a very strong but still malleable steel.\nEglin steel uses a combination of over a dozen different elements in varying amounts to create a relatively low-cost steel for use in bunker buster weapons. Hadfield steel (after Sir Robert Hadfield) or manganese steel contains 12\u201314% manganese which when abraded strain-hardens to form an incredibly hard skin which resists wearing. Examples include tank tracks, bulldozer blade edges and cutting blades on the jaws of life.\nIn 2016 a breakthrough in creating a strong light aluminium steel alloy which might be suitable in applications such as aircraft was announced by researchers at Pohang University of Science and Technology. Adding small amounts of nickel was found to result in precipitation as nano particles of brittle B2 intermetallic compounds which had previously resulted in weakness. The result was a cheap strong light steel alloy\u2014nearly as strong as titanium at ten percent the cost\u2014which is slated for trial production at industrial scale by POSCO, a Korean steelmaker.\n\n\n=== Standards ===\nMost of the more commonly used steel alloys are categorized into various grades by standards organizations. For example, the Society of Automotive Engineers has a series of grades defining many types of steel. The American Society for Testing and Materials has a separate set of standards, which define alloys such as A36 steel, the most commonly used structural steel in the United States.\n\n\n== Uses ==\n\nIron and steel are used widely in the construction of roads, railways, other infrastructure, appliances, and buildings. Most large modern structures, such as stadiums and skyscrapers, bridges, and airports, are supported by a steel skeleton. Even those with a concrete structure employ steel for reinforcing. In addition, it sees widespread use in major appliances and cars. Despite growth in usage of aluminium, it is still the main material for car bodies. Steel is used in a variety of other construction materials, such as bolts, nails, and screws and other household products and cooking utensils.\nOther common applications include shipbuilding, pipelines, mining, offshore construction, aerospace, white goods (e.g. washing machines), heavy equipment such as bulldozers, office furniture, steel wool, tools, and armour in the form of personal vests or vehicle armour (better known as rolled homogeneous armour in this role).\n\n\n=== Historical ===\n\nBefore the introduction of the Bessemer process and other modern production techniques, steel was expensive and was only used where no cheaper alternative existed, particularly for the cutting edge of knives, razors, swords, and other items where a hard, sharp edge was needed. It was also used for springs, including those used in clocks and watches.\nWith the advent of speedier and thriftier production methods, steel has become easier to obtain and much cheaper. It has replaced wrought iron for a multitude of purposes. However, the availability of plastics in the latter part of the 20th century allowed these materials to replace steel in some applications due to their lower fabrication cost and weight. Carbon fiber is replacing steel in some cost insensitive applications such as aircraft, sports equipment and high end automobiles.\n\n\n=== Long steel ===\n\nAs reinforcing bars and mesh in reinforced concrete\nRailroad tracks\nStructural steel in modern buildings and bridges\nWires\nInput to reforging applications\n\n\n=== Flat carbon steel ===\nMajor appliances\nMagnetic cores\nThe inside and outside body of automobiles, trains, and ships.\n\n\n=== Weathering steel (COR-TEN) ===\n\nIntermodal containers\nOutdoor sculptures\nArchitecture\nHighliner train cars\n\n\n=== Stainless steel ===\n\nCutlery\nRulers\nSurgical instruments\nWatches\nGuns\nRail passenger vehicles\nTablets\nTrash Cans\n\n\n=== Low-background steel ===\n\nSteel manufactured after World War II became contaminated with radionuclides due to nuclear weapons testing. Low-background steel, steel manufactured prior to 1945, is used for certain radiation-sensitive applications such as Geiger counters and radiation shielding.\n\n\n== See also ==\n\n\n== References ==\n\n\n=== Bibliography ===\nAshby, Michael F.; Jones, David Rayner Hunkin (1992). An introduction to microstructures, processing and design. Butterworth-Heinemann. \nBugayev, K.; Konovalov, Y.; Bychkov, Y.; Tretyakov, E.; Savin, Ivan V. (2001). Iron and Steel Production. The Minerva Group, Inc. ISBN 978-0-89499-109-7. Retrieved 2009-07-19. .\nDegarmo, E. Paul; Black, J T.; Kohser, Ronald A. (2003). Materials and Processes in Manufacturing (9th ed.). Wiley. ISBN 0-471-65653-4. \nGernet, Jacques (1982). A History of Chinese Civilization. Cambridge: Cambridge University Press.\nVerein Deutscher Eisenh\u00fcttenleute (Ed.). Steel \u2013 A Handbook for Materials Research and Engineering, Volume 1: Fundamentals. Springer-Verlag Berlin, Heidelberg and Verlag Stahleisen, D\u00fcsseldorf 1992, 737 p. ISBN 3-540-52968-3, ISBN 3-514-00377-7.\nVerein Deutscher Eisenh\u00fcttenleute (Ed.). Steel \u2013 A Handbook for Materials Research and Engineering, Volume 2: Applications. Springer-Verlag Berlin, Heidelberg and Verlag Stahleisen, D\u00fcsseldorf 1993, 839 pages, ISBN 3-540-54075-X, ISBN 3-514-00378-5.\nSmith, William F.; Hashemi, Javad (2006). Foundations of Materials Science and Engineering (4th ed.). McGraw-Hill. ISBN 0-07-295358-6. \n\n\n== Further reading ==\nMark Reutter, Making Steel: Sparrows Point and the Rise and Ruin of American Industrial Might (2005).   Discussion with Mark Reutter, part 1 of 3 (February 2015), part 2 of 3 (February 2015), part 3 of 3 (March 2015), The Real News\nDuncan Burn, The Economic History of Steelmaking, 1867\u20131939: A Study in Competition. Cambridge University Press, 1961.\nHarukiyu Hasegawa, The Steel Industry in Japan: A Comparison with Britain. 1996.\nJ. C. Carr and W. Taplin, History of the British Steel Industry. Harvard University Press, 1962.\nH. Lee Scamehorn, Mill & Mine: The Cf&I in the Twentieth Century. University of Nebraska Press, 1992.\nNeedham, Joseph (1986). Science and Civilization in China: Volume 4, Part 1 & Part 3. Taipei: Caves Books, Ltd.\nWarren, Kenneth, Big Steel: The First Century of the United States Steel Corporation, 1901\u20132001. University of Pittsburgh Press, 2001.\n\n\n== External links ==\nWorld Steel Association (worldsteel)\nsteeluniversity.org: Online steel education resources from worldsteel and the University of Liverpool\nHuge archive on steels, Cambridge University\nCooking with Steels\nMetallurgy for the Non-Metallurgist from the American Society for Metals\nMATDAT Database of Properties of Unalloyed, Low-Alloy and High-Alloy Steels \u2013 obtained from published results of material testing\nNews feature on \"open hearth\" steel workers at the U.S. Steel Fairless Mills near Philadelphia", 
                "titleUrl": "https://en.wikipedia.org/wiki/Steel", 
                "title": "Steel"
            }, 
            {
                "snippet": "as interim CEO.   rue21 offers its own brands, such as rue21 etc.!, CARBON elements, tarea and rueKicks, ruebeaut\u00e9!, ruebleu Swim, rueDecor, in addition", 
                "pageCategories": "Companies based in Allegheny County, Pennsylvania\nCompanies established in 1987\nCompanies formerly listed on NASDAQ", 
                "pageContent": "rue21 Inc., formerly known as Pennsylvania Fashions Inc. (as well as $9.99 Stockroom, rue21), is headquartered in the Pittsburgh suburb of Warrendale, Pennsylvania. It is a specialty discount retailer of young men and women\u2019s casual apparel and accessories. Its clothes are designed to appeal to 11- to 17-year-olds who aspire to be 21 and adults who want to look and feel 21. Rue in French means street. In 2013, Apax Partners, a global private equity firm, acquired the company by funds advised for $42.00 per share in cash.\n\n\n== History ==\nIn February 2002 Pennsylvania Fashions Inc., filed for Chapter 11 bankruptcy protection. It was then that SKM emerged as the majority stakeholder three years after Cary Klein sold a 50% stake to the Stamford, Connecticut-based investment firm. At the time sales among the nearly 250 stores were thought to be between $180 and $200 million annually with approximately 1,800 employees.\nThe company exited Chapter 11 as rue21 Inc. in May 2003 after undergoing voluntary reorganization. It set out an ambitious plan to expand its 170 stores over the next five years. Their ability to expand into underserved markets is evident by the store growth from their 500th store in Harlingen, TX on July 23, 2009 through November 14, 2013, when they opened their 1000th store in Enid, Oklahoma.\nrue21 continued its growth and expansion on March 17, 2011 by announcing that it would be doubling the size of its 189,000-square-foot (17,600 m2) Distribution Center located in Weirton, West Virginia. This expansion occurred 12 months after a Distribution Center upgrade that included a new pack-to-light system as well as upgrades to the warehouse management system. CFO Keith McDonough stated, \"The recently completed Distribution Center upgrades, including additional packing capacity, new systems and performance metrics, coupled with this planned expansion, gives us the ability to support our current stores and gives us the flexibility for our planned future growth.\"  This upgrade was completed in June 2012.\nOn September 30th, 2016, CEO Bob Fisch, his wife Stephanie Fisch, and General Merchandise Manager Kim Reynolds were let go. Keith McDonough was named as interim CEO. \n\n\n== Brands ==\nrue21 offers its own brands, such as rue21 etc.!, CARBON elements, tarea and rueKicks, ruebeaut\u00e9!, ruebleu Swim, rueDecor, in addition to other brands in its stores to create merchandise excitement and differentiation in its stores. Its website displays selected inventory that is available in their brick & mortar stores. As of November 2013, U.S.-based customers have been able to purchase merchandise off of the redesigned website. In select stores rue21 also offers rueGuy, an expanded men\u2019s department with a more male-inspired layout, design, and fashion selection.\nOn November 6, 2014, rue21 began offering plus sizes for women, in various categories of merchandise, with their rue+ brand.\n\n\n== References ==\n\n\n== External links ==\nOfficial website", 
                "titleUrl": "https://en.wikipedia.org/wiki/Rue21", 
                "title": "Rue21"
            }, 
            {
                "snippet": "transition metals, the boron elements, the carbon elements, the nitrogen elements, the oxygen elements, the halogen elements, the noble gases, the lanthanides", 
                "pageCategories": "2007 books\nChemistry books\nPeriodic table in popular culture", 
                "pageContent": "The Periodic Table: Elements with Style is a 2007 children's science book created by Simon Basher and written by Adrian Dingle. It is the first book in Basher's science series, which includes Physics: Why Matter Matters!, Biology: Life As We Know It, Astronomy: Out of this World!, Rocks and Minerals: A Gem of a Book, and Planet Earth: What Planet Are You On?, each of which is 128 pages long.\nThe book is arranged in eleven chapters plus an introduction, and includes a poster in the back of the book. Each chapter is on a different group of the periodic table (hydrogen, the alkali metals, the alkaline earth metals, the transition metals, the boron elements, the carbon elements, the nitrogen elements, the oxygen elements, the halogen elements, the noble gases, the lanthanides and actinides, and the transactinides). For every type of then known atom, Basher has created a \"manga-esque\" cartoon, and for many types of atoms, Dingle, a high-school chemistry teacher who also developed an award-winning chemistry website has written a couple paragraphs of facts to go with the cartoon. Dingle, who says that \"[s]cience is a serious business\", wanted in writing the book \"to get people engaged is to make it accessible while still presenting hard facts and knowledge,\" while Basher was concerned that the book's design be \"sharp and focused\" in order to \"connect with today's visually advanced young audience.\"\n\n\n== Critical response ==\nPublishers Weekly said that the book was a \"lively introduction to the chart that has been the bane of many a chemistry student\", and in a review in New Scientist, Vivienne Greig called The Periodic Table \"an engrossing read and an ideal way to painlessly impart a great deal of science history to seen-it-all-before teenagers.\" A review on the Royal Society of Chemistry website had some minor reservations about the book, but said it was \"endearing\" and succeeded in making learning chemistry easier and more fun.\nThe Periodic Table: Elements with Style has also been reviewed in the Bulletin of the Center for Children's Books and the Journal of Chemical Education.\n\n\n== References ==\n\n\n== External links ==\nSimon Basher's website\nAdrian Dingle's award-winning chemistry website\nKingfisher - publisher's website\nThe Periodic Table - publisher's book page\nBasher books website", 
                "titleUrl": "https://en.wikipedia.org/wiki/The_Periodic_Table_(Basher_book)", 
                "title": "The Periodic Table (Basher book)"
            }, 
            {
                "snippet": "flerovium.  The boiling points of the carbon group tend to get lower with the heavier elements. Carbon, the lightest carbon group element, sublimates at 3825\u00a0\u00b0C", 
                "pageCategories": "CS1 errors: dates\nGroups in the periodic table\nWikipedia articles with GND identifiers", 
                "pageContent": "The carbon group is a periodic table group consisting of carbon (C), silicon (Si), germanium (Ge), tin (Sn), lead (Pb), and flerovium (Fl).\nIn modern IUPAC notation, it is called Group 14. In the field of semiconductor physics, it is still universally called Group IV. The group was once also known as the tetrels (from the Greek word tetra, which means four), stemming from the Roman numeral IV in the group names, or (not coincidentally) from the fact that these elements have four valence electrons (see below). The group is sometimes also referred to as tetragens because it has four electrons in its outermost shell or the valence shell. This group is also called crystallogens because this group forms the most crystal compounds when compared to other elements of the periodic table.\n\n\n== Characteristics ==\n\n\n=== Chemical ===\nLike other groups, the members of this family show patterns in electron configuration, especially in the outermost shells, resulting in trends in chemical behavior:\nEach of the elements in this group has 4 electrons in its outer orbital (the atom's top energy level). The last orbital of all these elements is the p2 orbital. In most cases, the elements share their electrons. The tendency to lose electrons increases as the size of the atom increases, as it does with increasing atomic number. Carbon alone forms negative ions, in the form of carbide (C4\u2212) ions. Silicon and germanium, both metalloids, each can form +4 ions. Tin and lead both are metals while flerovium is a synthetic, radioactive (its half life is very short), element that may have a few noble gas-like properties, though it is still most likely a post-transition metal. Tin and lead are both capable of forming +2 ions.\nCarbon forms tetrahalides with all the halogens. Carbon also forms three oxides: carbon monoxide, carbon suboxide (C3O2), and carbon dioxide. Carbon forms disulfides and diselenides.\nSilicon forms two hydrides: SiH4 and Si2H6. Silicon forms tetrahalides with fluorine, chlorine, and iodine. Silicon also forms a dioxide and a disulfide. Silicon nitride has the formula Si3N4.\nGermanium forms two hydrides: GeH4 and Ge2H6. Germanium forms tetrahalides with all halogens except astatine and forms dihalides with all halogens except bromine and astatine. Germanium bonds to all natural single chalcogens except polonium, and forms dioxides, disulfides, and diselenides. Germanium nitride has the formula Ge3N4.\nTin forms two hydrides: SnH4 and Sn2H6. Tin forms dihalides and tetrahalides with all halogens except astatine. Tin forms chalcogenides with one of each naturally occurring chalcogen except polonium, and forms chalcogenides with two of each naturally occurring chalcogen except polonium and tellurium.\nLead forms one hydride, which has the formula PbH4. Lead forms dihalides and tetrahalides with fluorine and chlorine, and forms a tetrabromide and a lead diiodide, although the tetrabromide and tetraiodide of lead are unstable. Lead forms four oxides, a sulfide, a selenide, and a telluride.\nThere are no known compounds of flerovium.\n\n\n=== Physical ===\nThe boiling points of the carbon group tend to get lower with the heavier elements. Carbon, the lightest carbon group element, sublimates at 3825 \u00b0C. Silicon's boiling point is 3265 \u00b0C, germanium's is 2833 \u00b0C, tin's is 2602 \u00b0C, and lead's is 1749 \u00b0C. The melting points of the carbon group elements have roughly the same trend as their boiling points. Silicon melts at 1414 \u00b0C, germanium melts at 939 \u00b0C, tin melts at 232 \u00b0C, and lead melts at 328 \u00b0C.\nCarbon's crystal structure is hexagonal; at high pressures and temperatures it forms diamond (see below). Silicon and germanium have diamond cubic crystal structures, as does tin at low temperatures (below 13.2 \u00b0C). Tin at room temperature has a tetragonal crystal structure. Lead has a face-centered cubic crystal structure.\nThe densities of the carbon group elements tend to increase with increasing atomic number. Carbon has a density of 2.26 grams per cubic centimeter, silicon has a density of 2.33 grams per cubic centimeter, germanium has a density of 5.32 grams per cubic centimeter. Tin has a density of 7.26 grams per cubic centimeter, and lead has a density of 11.3 grams per cubic centimeter.\nThe atomic radii of the carbon group elements tend to increase with increasing atomic number. Carbon's atomic radius is 77 picometers, silicon's is 118 picometers, germanium's is 123 picometers, tin's is 141 picometers, and lead's is 175 picometers.\n\n\n==== Allotropes ====\n\nCarbon has multiple allotropes. The most common is graphite, which is carbon in the form of stacked sheets. Another form of carbon is diamond, but this is relatively rare. Amorphous carbon is a third allotrope of carbon; it is a component of soot. Another allotrope of carbon is a fullerene, which has the form of sheets of carbon atoms folded into a sphere. A fifth allotrope of carbon, discovered in 2003, is called graphene, and is in the form of a layer of carbon atoms arranged in a honeycomb-shaped formation.\nSilicon has two known allotropes that exist at room temperature. These allotropes are known as the amorphous and the crystalline allotropes. The amorphous allotrope is a brown powder. The crystalline allotrope is gray and has a metallic luster.\nTin has two allotropes: \u03b1-tin, also known as gray tin, and \u03b2-tin. Tin is typically found in the \u03b2-tin form, a silvery metal. However, at standard pressure, \u03b2-tin converts to \u03b1-tin, a gray powder, at temperatures below 56\u00b0 Fahrenheit. This can cause tin objects in cold temperatures to crumble to gray powder in a process known as tin rot.\n\n\n=== Nuclear ===\nAt least two of the carbon group elements (tin and lead) have magic nuclei, meaning that these elements are more common and more stable than elements that do not have a magic nucleus.\n\n\n==== Isotopes ====\nThere are 15 known isotopes of carbon. Of these, three are naturally occurring. The most common is stable carbon-12, followed by stable carbon-13. Carbon-14 is a natural radioactive isotope with a half-life of 5,730 years.\n23 isotopes of silicon have been discovered. Five of these are naturally occurring. The most common is stable silicon-28, followed by stable silicon-29 and stable silicon-30. Silicon-32 is a radioactive isotope that occurs naturally as a result of radioactive decay of actinides, and via spallation in the upper atmosphere. Silicon-34 also occurs naturally as the result of radioactive decay of actinides.\n32 isotopes of germanium have been discovered. Five of these are naturally occurring. The most common is the stable isotope germanium-74, followed by the stable isotope germanium-72, the stable isotope germanium-70, and the stable isotope germanium-73. The isotope germanium-76 is a primordial radioisotope.\n40 isotopes of tin have been discovered. 14 of these occur in nature. The most common is the stable isotope tin-120, followed by the stable isotope tin-118, the stable isotope tin-116, the stable isotope tin-119, the stable isotope tin-117, the primordial radioisotope tin-124, the stable isotope tin-122, the stable isotope tin-112, and the stable isotope tin-114. Tin also has four radioisotopes that occur as the result of the radioactive decay of uranium. These isotopes are tin-121, tin-123, tin-125, and tin-126.\n38 isotopes of lead have been discovered. 9 of these are naturally occurring. The most common isotope is the primordial radioisotope lead-208, followed by the primordial radioisotope lead-206, the primordial radioisotope lead-207, and the primordial radioisotope lead-204. 4 isotopes of lead occur from the radioactive decay of uranium and thorium. These isotopes are lead-209, lead-210, lead-211, and lead-212.\n6 isotopes of flerovium (flerovium-284, flerovium-285, flerovium-286, flerovium-287, flerovium-288, and flerovium-289) have been discovered. None of these are naturally occurring. Flerovium's most stable isotope is flerovium-289, which has a half-life of 2.6 seconds.\n\n\n== Occurrence ==\nCarbon accumulates as the result of stellar fusion in most stars, even small ones. Carbon is present in the earth's crust in concentrations of 480 parts per million, and is present in seawater at concentrations of 28 parts per million. Carbon is present in the atmosphere in the form of carbon monoxide, carbon dioxide, and methane. Carbon is a key constituent of carbonate minerals, and is in hydrogen carbonate, which is common in seawater. Carbon forms 22.8% of a typical human.\nSilicon is present in the earth's crust at concentrations of 28%, making it the second most abundant element there. Silicon's concentration in seawater can vary from 30 parts per billion on the surface of the ocean to 2000 parts per billion deeper down. Silicon dust occurs in trace amounts in earth's atmosphere. Silicate minerals are the most common type of mineral on earth. Silicon makes up 14.3 parts per million of the human body on average. Only the largest stars produce silicon via stellar fusion.\nGermanium makes up 2 parts per million of the earth's crust, making it the 52nd most abundant element there. On average, germanium makes up 1 part per million of soil. Germanium makes up 0.5 parts per trillion of seawater. Organogermanium compounds are also found in seawater. Germanium occurs in the human body at concentrations of 71.4 parts per billion. Germanium has been found to exist in some very faraway stars.\nTin makes up 2 parts per million of the earth's crust, making it the 49th most abundant element there. On average, tin makes up 1 part per million of soil. Tin exists in seawater at concentrations of 4 parts per trillion. Tin makes up 428 parts per million of the human body. Tin (IV) oxide occurs at concentrations of 0.1 to 300 parts per million in soils. Tin also occurs in concentrations of one part per thousand in igneous rocks.\nLead makes up 14 parts per million of the earth's crust, making it the 36th most abundant element there. On average, lead makes up 23 parts per million of soil, but the concentration can reach 20000 parts per million (2 percent) near old lead mines. Lead exists in seawater at concentrations of 2 parts per trillion. Lead makes up 0.17% of the human body by weight. Human activity releases more lead into the environment than any other metal.\nFlerovium only occurs in particle accelerators.\n\n\n== History ==\n\n\n=== Discoveries and uses in antiquity ===\nCarbon, tin, and lead are a few of the elements well known in the ancient world\u2014together with sulfur, iron, copper, mercury, silver, and gold.\nSilicon as silica in the form of rock crystal was familiar to the predynastic Egyptians, who used it for beads and small vases; to the early Chinese; and probably to many others of the ancients. The manufacture of glass containing silica was carried out both by the Egyptians \u2014 at least as early as 1500 BCE \u2014 and by the Phoenicians. Many of the naturally occurring compounds or silicate minerals were used in various kinds of mortar for construction of dwellings by the earliest people.\nThe origins of tin seem to be lost in history. It appears that bronzes, which are alloys of copper and tin, were used by prehistoric man some time before the pure metal was isolated. Bronzes were common in early Mesopotamia, the Indus Valley, Egypt, Crete, Israel, and Peru. Much of the tin used by the early Mediterranean peoples apparently came from the Scilly Isles and Cornwall in the British Isles, where mining of the metal dates from about 300\u2013200 BCE. Tin mines were operating in both the Inca and Aztec areas of South and Central America before the Spanish conquest.\nLead is mentioned often in early Biblical accounts. The Babylonians used the metal as plates on which to record inscriptions. The Romans used it for tablets, water pipes, coins, and even cooking utensils; indeed, as a result of the last use, lead poisoning was recognized in the time of Augustus Caesar. The compound known as white lead was apparently prepared as a decorative pigment at least as early as 200 BCE.\n\n\n=== Modern discoveries ===\nAmorphous elemental silicon was first obtained pure in 1824 by the Swedish chemist J\u00f6ns Jacob Berzelius; impure silicon had already been obtained in 1811. Crystalline elemental silicon was not prepared until 1854, when it was obtained as a product of electrolysis.\nGermanium is one of three elements the existence of which was predicted in 1869 by the Russian chemist Dmitri Mendeleev when he first devised his periodic table. However, the element was not actually discovered for some time. In September 1885, a miner discovered a mineral sample in a silver mine and gave it to the mine manager, who determined that it was a new mineral and sent the mineral to Clemens A. Winkler. Winkler realized that the sample was 75% silver, 18% sulfur, and 7% of an undiscovered element. After several months, Winkler isolated the element and determined that it was element 32.\nThe first attempt to discover flerovium (then referred to as \"element 114\") was in 1969, at the Joint Institute for Nuclear Research, but it was unsuccessful. In 1977, researchers at the Joint Institute for Nuclear Research bombarded plutonium-244 atoms with calcium-48, but were again unsuccessful. This nuclear was repeated in 1998, this time successfully.\n\n\n=== Etymologies ===\nThe word \"carbon\" comes from the Latin word carbo, meaning \"charcoal\".The word \"silicon\" comes from the Latin word silex or silicis, which means \"flint\". The word \"germanium\" comes from the word germania, which is Latin for Germany, the county where germanium was discovered. The word \"tin\" derives from the Old English word tin. The word \"lead\" comes from the Old English word lead.\n\n\n== Applications ==\nCarbon is most commonly used in its amorphous form. In this form, carbon is used for steelmaking, as carbon black, as a filling in tires, in respirators, and as activated charcoal. Carbon is also used in the form of graphite is commonly used as the lead in pencils. Diamond, another form of carbon, is commonly used in jewelery. Carbon fibers are used in numerous applications, such as satellite struts, because the fibers are highly strong yet elastic.\nSilicon dioxide has a wide variety of applications, including toothpaste, construction fillers, and silica is a major component of glass. 50% of pure silicon is devoted to the manufacture of metal alloys. 45% of silicon is devoted to the manufacture of silicones. Silicon is also commonly used in semiconductors since the 1950s.\nGermanium was used in semiconductors until the 1950s, when it was replaced by silicon. Radiation detectors contain germanium. Germanium oxide is used in fiber optics and wide-angle camera lenses. A small amount of germanium mixed with silver can make silver tarnish-proof. The resulting alloy is known as argentium.\nSolder is the most important use of tin; 50% of all tin produced goes into this application. 20% of all tin produced is used in tin plate. 20% of tin is also used by the chemical industry. Tin is also a constituent of numerous alloys, including pewter. Tin (IV) oxide has been commonly used in ceramics for thousands of years. Cobalt stannate is a tin compound which is used as a cerulean blue pigment.\n80% of all lead produced goes into lead-acid batteries. Other applications for lead include weights, pigments, and shielding against radioactive materials. Lead was historically used in gasoline in the form of tetraethyl lead, but this application has been discontinued due to concerns of toxicity.\n\n\n== Production ==\nCarbon's allotrope diamond is produced mostly by Russia, Botswana, Congo, Canada, and South Africa. 80% of all synthetic diamonds are produced by Russia. China produces 70% of the world's graphite. Other graphite-mining countries are Brazil, Canada, and Mexico.\nSilicon can be produced by heating silica with carbon.\nThere are some germanium ores, such as germanite, but these are not mined on account of being rare. Instead, germanium is extracted from the ores of metals such as zinc. In Russia and China, germanium is also separated from coal deposits. Germanium-containing ores are first treated with chlorine to form germanium tetrachloride, which is mixed with hydrogen gas. Then the germanium is further refined by zone refining Roughly 140 metric tons of germanium are produced each year.\nMines output 300,000 metric tons of tin each year. China, Indonesia, Peru, Bolivia, and Brazil are the main producers of tin. The method by which tin is produced is to head the tin mineral cassiterite (SnO2) with coke.\nThe most commonly mined lead ore is galena (lead sulfide). 4 million metric tons of lead are newly mined each year, mostly in China, Australia, the United States, and Peru. The ores are mixed with coke and limestone and roasted to produce pure lead. Most lead is recycled from lead batteries. The total amount of lead ever mined by humans amounts to 350 million metric tons.\n\n\n== Biological role ==\nCarbon is a key element to all known life. It is in all organic compounds, for example, DNA, steroids, and proteins. Carbon's importance to life is primarily due to its ability to form numerous bonds with other elements. There are 16 kilograms of carbon in a typical 70-kilogram human.\nSilicon-based life's feasibility is commonly discussed. However, it is less able than carbon to form elaborate rings and chains. Silicon in the form of silicon dioxide is used by diatoms and sea sponges to form their cell walls and skeletons. Silicon is essential for bone growth in chickens and rats and may also be essential in humans. Humans consume on average between 20 and 1200 milligrams of silicon per day, mostly from cereals. There is 1 gram of silicon in a typical 70-kilogram human.\nA biological role for germanium is not known, although it does stimulate metabolism. In 1980, germanium was reported by Kazuhiko Asai to benefit health, but the claim has not been proven. Some plants take up germanium from the soil in the form of germanium oxide. These plants, which include grains and vegetables contain roughly 0.05 parts per million of germanium. The estimated human intake of germanium is 1 milligram per day. There are 5 milligrams of germanium in a typical 70-kilogram human.\nTin has been shown to be essential for proper growth in rats, but there is, as of 2013, no evidence to indicate that humans need tin in their diet. Plants do not require tin. However, plants do collect tin in their roots. Wheat and corn contain seven and three parts per million respectively. However, the level of tin in plants can reach 2000 parts per million if the plants are near a tin smelter. On average, humans consume 0.3 milligrams of tin per day. There are 30 milligrams of tin in a typical 70-kilogram human.\nLead has no known biological role, and is in fact highly toxic, but some microbes are able to survive in lead-contaminated environments. Some plants, such as cucumbers contain up to tens of parts per million of lead. There are 120 milligrams of lead in a typical 70-kilogram human.\n\n\n=== Toxicity ===\nElemental carbon is not generally toxic, but many of its compounds are, such as carbon monoxide and hydrogen cyanide. However, carbon dust can be dangerous because it lodges in the lungs in a manner similar to asbestos.\nSilicon minerals are not typically poisonous. However, silicon dioxide dust, such as that emitted by volcanoes can cause adverse health effects if it enters the lungs.\nGermanium can interfere with such enzymes as lactate and alcohol dehydrogenase. Organic germanium compounds are more toxic than inorganic germanium compounds. Germanium has a low degree of oral toxicity in animals. Severe germanium poisoning can cause death by respiratory paralysis.\nSome tin compounds are toxic to ingest, but most inorganic compounds of tin are considered nontoxic. Organic tin compounds, such as trimethyl tin and triethyl tin are highly toxic, and can disrupt metabolic processes inside cells.\nLead and its compounds, such as lead acetate are highly toxic. Lead poisoning can cause headaches, stomach pain, constipation, and gout.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Carbon_group", 
                "title": "Carbon group"
            }, 
            {
                "snippet": "with chalcogen pnictides and compounds containing chalcogens and carbon group elements. Oxygen is generally extracted from air and sulfur is extracted", 
                "pageCategories": "All accuracy disputes\nAll articles with unsourced statements\nArticles to be expanded from August 2016\nArticles to be expanded from September 2014\nArticles with disputed statements from September 2014\nArticles with inconsistent citation formats\nArticles with unsourced statements from September 2014\nCS1 German-language sources (de)\nCS1 maint: Multiple names: authors list\nChalcogens", 
                "pageContent": "The chalcogens (/\u02c8k\u00e6lk\u0259d\u0292\u1d7bnz/) are the chemical elements in group 16 of the periodic table. This group is also known as the oxygen family. It consists of the elements oxygen (O), sulfur (S), selenium (Se), tellurium (Te), and the radioactive element polonium (Po). The chemically uncharacterized synthetic element livermorium (Lv) is predicted to be a chalcogen as well. Often, oxygen is treated separately from the other chalcogens, sometimes even excluded from the scope of the term \"chalcogen\" altogether, due to its very different chemical behavior from sulfur, selenium, tellurium, and polonium. The word \"chalcogen\" is derived from a combination of the Greek word khalk\u03ccs (\u03c7\u03b1\u03bb\u03ba\u03cc\u03c2) principally meaning copper (the term was also used for bronze/brass, any metal in the poetic sense, ore or coin), and the Latinised Greek word gen\u0113s, meaning born or produced.\nSulfur has been known since antiquity, and oxygen was recognized as an element in the 18th century. Selenium, tellurium and polonium were discovered in the 19th century, and livermorium in 2000. All of the chalcogens have six valence electrons, leaving them two electrons short of a full outer shell. Their most common oxidation states are \u22122, +2, +4, and +6. They have relatively low atomic radii, especially the lighter ones.\nLighter chalcogens are typically nontoxic in their elemental form, and are often critical to life, while the heavier chalcogens are typically toxic. All of the chalcogens have some role in biological functions, either as a nutrient or a toxin. The lighter chalcogens, such as oxygen and sulfur, are rarely toxic and usually helpful in their pure form. Selenium is an important nutrient but is also commonly toxic. Tellurium often has unpleasant effects (although some organisms can use it), and polonium is always extremely harmful, both in its chemical toxicity and its radioactivity.\nSulfur has more than 20 allotropes, oxygen has nine, selenium has at least five, polonium has two, and only one crystal structure of tellurium has so far been discovered. There are numerous organic chalcogen compounds. Not counting oxygen, organic sulfur compounds are generally the most common, followed by organic selenium compounds and organic tellurium compounds. This trend also occurs with chalcogen pnictides and compounds containing chalcogens and carbon group elements.\nOxygen is generally extracted from air and sulfur is extracted from oil and natural gas. Selenium and tellurium are produced as byproducts of copper refining. Polonium and livermorium are most available in particle accelerators. The primary use of elemental oxygen is in steelmaking. Sulfur is mostly converted into sulfuric acid, which is heavily used in the chemical industry. Selenium's most common application is glassmaking. Tellurium compounds are mostly used in optical disks, electronic devices, and solar cells. Some of polonium's applications are due to its radioactivity.\n\n\n== Properties ==\n\n\n=== Atomic and physical ===\nChalcogens show similar patterns in electron configuration, especially in the outermost shells, where they all have the same number of valence electrons, resulting in similar trends in chemical behavior:\nAll chalcogens have six valence electrons. All of the solid, stable chalcogens are soft and do not conduct heat well. Electronegativity decreases towards the chalcogens with higher atomic numbers. Density, melting and boiling points, and atomic and ionic radii tend to increase towards the chalcogens with higher atomic numbers.\n\n\n=== Isotopes ===\nOut of the six known chalcogens, one (oxygen) has an atomic number equal to a nuclear magic number, which means that their atomic nuclei tend to have increased stability towards radioactive decay. Oxygen has three stable isotopes, and 14 unstable ones. Sulfur has four stable isotopes, 20 radioactive ones, and one isomer. Selenium has six observationally stable or nearly stable isotopes, 26 radioactive isotopes, and 9 isomers. Tellurium has eight stable or nearly stable isotopes, 31 unstable ones, and 17 isomers. Polonium has 42 isotopes, none of which are stable. It has an additional 28 isomers. In addition to the stable isotopes, some radioactive chalcogen isotopes occur in nature, either because they are decay products, such as 210Po, because they are primordial, such as 82Se, because of cosmic ray spallation, or via nuclear fission of uranium. Livermorium isotopes 290 through 293 have been discovered. The most stable livermorium isotope is 293Lv, which has a half-life of 0.061 seconds.\nAmong the lighter chalcogens (oxygen and sulfur), the most neutron-poor isotopes undergo proton emission, the moderately neutron-poor isotopes undergo electron capture or \u03b2+ decay, the moderately neutron-rich isotopes undergo \u03b2\u2212 decay, and the most neutron rich isotopes undergo neutron emission. The middle chalcogens (selenium and tellurium) have similar decay tendencies as the lighter chalcogens, but their isotopes do not undergo proton emission and some of the most neutron-starved isotopes of tellurium undergo alpha decay. Polonium's isotopes tend to decay with alpha or beta decay. Isotopes with nuclear spins are more common among the chalcogens selenium and tellurium than they are with sulfur.\n\n\n=== Allotropes ===\n\nOxygen's most common allotrope is diatomic oxygen, or O2, a reactive paramagnetic molecule that is ubiquitous to aerobic organisms and has a blue color in its liquid state. Another allotrope is O3, or ozone, which is three oxygen atoms bonded together in a bent formation. There is also an allotrope called tetraoxygen, or O4, and six allotropes of solid oxygen including \"red oxygen\", which has the formula O8.\n\nSulfur has over 20 known allotropes, which is more than any other element except carbon. The most common allotropes are in the form of eight-atom rings, but other molecular allotropes that contain as few as two atoms or as many as 20 are known. Other notable sulfur allotropes include rhombic sulfur and monoclinic sulfur. Rhombic sulfur is the more stable of the two allotropes. Monoclinic sulfur takes the form of long needles and is formed when liquid sulfur is cooled to slightly below its melting point. The atoms in liquid sulfur are generally in the form of long chains, but above 190\u00b0 Celsius, the chains begin to break down. If liquid sulfur above 190\u00b0Celsius is frozen very rapidly, the resulting sulfur is amorphous or \"plastic\" sulfur. Gaseous sulfur is a mixture of diatomic sulfur (S2) and 8-atom rings.\n\nSelenium has at least five known allotropes.The gray allotrope, commonly referred to as the \"metallic\" allotrope, despite not being a metal, is stable and has a hexagonal crystal structure. The gray allotrope of selenium is soft, with a Mohs hardness of 2, and brittle. The four other allotropes of selenium are metastable. These include two monoclinic red allotropes and two amorphous allotropes, one of which is red and one of which is black. The red allotrope converts to the red allotrope in the presence of heat. The gray allotrope of selenium is made from spirals on selenium atoms, while one of the red allotropes is made of stacks of selenium rings (Se8).\nTellurium is not known to have any allotropes, although its typical form is hexagonal. Polonium has two allotropes, which are known as \u03b1-polonium and \u03b2-polonium. \u03b1-polonium has a cubic crystal structure and converts the rhombohedral \u03b2-polonium at 36 \u00b0C.\nThe chalcogens have varying crystal structures. Oxygen's crystal structure is monoclinic, sulfur's is orthorhombic, selenium and tellurium have the hexagonal crystal structure, while polonium has a cubic crystal structure.\n\n\n=== Chemical ===\nOxygen, sulfur, and selenium are nonmetals, and tellurium is a metalloid, meaning that its chemical properties are between those of a metal and those of a nonmetal. It is not certain whether polonium is a metal or a metalloid. Some sources refer to polonium as a metalloid, although it has some metallic properties. Also, some allotropes of selenium display characteristics of a metalloid, even though selenium is usually considered a nonmetal. Even though oxygen is a chalcogen, its chemical properties are different from those of other chalcogens. One reason for this is that the heavier chalcogens have vacant d-orbitals. Oxygen's electronegativity is also much higher than those of the other chalcogens. This makes oxygen's electric polarizability several times lower than those of the other chalcogens.\nThe oxidation number of the most common chalcogen compounds with positive metals is \u22122. However the tendency for chalcogens to form compounds in the \u22122 state decreases towards the heavier chalcogens. Other oxidation numbers, such as \u22121 in pyrite and peroxide, do occur. The highest formal oxidation number is +6. This oxidation number is found in sulfates, selenates, tellurates, polonates, and their corresponding acids, such as sulfuric acid.\nOxygen is the most electronegative element except for fluorine, and forms compounds with almost all of the chemical elements, including some of the noble gases. It commonly bonds with many metals and metalloids to form oxides, including iron oxide, titanium oxide, and silicon oxide. Oxygen's most common oxidation state is \u22122, and the oxidation state \u22121 is also relatively common. With hydrogen it forms water and hydrogen peroxide. Organic oxygen compounds are ubiquitous in organic chemistry.\nSulfur's oxidation states are \u22122, +2, +4, and +6. Sulfur-containing analogs of oxygen compounds often have the prefix thio-. Sulfur's chemistry is similar to oxygen's, in many ways. One difference is that sulfur-sulfur double bonds are far weaker than oxygen-oxygen double bonds, but sulfur-sulfur single bonds are stronger than oxygen-oxygen single bonds. Organic sulfur compounds such as thiols have a strong specific smell, and a few are utilized by some organisms.\nSelenium's oxidation states are \u22122, +4, and +6. Selenium, like most chalcogens, bonds with oxygen. There are some organic selenium compounds, such as selenoproteins. Tellurium's oxidation states are \u22122, +2, +4, and +6. Tellurium forms the oxides tellurium monoxide, tellurium dioxide, and tellurium trioxide. Polonium's oxidation states are +2 and +4.\nThere are many acids containing chalcogens, including sulfuric acid, sulfurous acid, selenic acid, and telluric acid. All hydrogen chalcogenides are toxic except for water. Oxygen ions often come in the forms of oxide ions (O2\u2212), peroxide ions (O2\u2212\n2), and hydroxide ions (OH\u2212). Sulfur ions generally come in the form of sulfides (S2\u2212), sulfites (SO2\u2212\n3), sulfates (SO2\u2212\n4), and thiosulfates (S\n2O2\u2212\n3). Selenium ions usually come in the form of selenides (Se2\u2212) and selenates (SeO2\u2212\n4). Tellurium ions often come in the form of tellurates (TeO2\u2212\n4). Molecules containing metal bonded to chalcogens are common as minerals. For example, pyrite (FeS2) is an iron ore, and the rare mineral calaverite is the ditelluride (Au, Ag)Te2.\n\nAlthough all group 16 elements of the periodic table, including oxygen, can be defined as chalcogens, oxygen and oxides are usually distinguished from chalcogens and chalcogenides. The term chalcogenide is more commonly reserved for sulfides, selenides, and tellurides, rather than for oxides.\nExcept for polonium, the chalcogens are all fairly similar to each other chemically. They all form X2\u2212 ions when reacting with electropositive metals.\nSulfide minerals and analogous compounds produce gases upon reaction with oxygen.\n\n\n== Compounds ==\n\n\n=== With halogens ===\nChalcogens also form compounds with halogens known as chalcohalides. Such compounds are known as chalcogen halides. The majority of simple chalcogen halides are well-known and widely used as chemical reagents. However, more complicated chalcogen halides, such as sulfenyl, sulfonyl, and sulfuryl halides, are less well-known to science. Out of the compounds consisting purely of chalcogens and halogens, there are a total of 13 chalcogen fluorides, nine chalcogen chlorides, eight chalcogen bromides, and six chalcogen iodides that are known. The heavier chalcogen halides often have significant molecular interactions. Sulfur fluorides with low valences are fairly unstable and little is known about their properties. However, sulfur fluorides with high valences, such as sulfur hexafluoride, are stable and well-known. Sulfur tetrafluoride is also a well-known sulfur fluoride. Certain selenium fluorides, such as selenium difluoride, have been produced in small amounts. The crystal structures of both selenium tetrafluoride and tellurium tetrafluoride are known. Chalcogen chlorides and bromides have also been explored. In particular, selenium dichloride and sulfur dichloride can react to form organic selenium compounds. Dichalcogen dihalides, such as Se2Cl2 also are known to exist. There are also mixed chalcogen-halogen compounds. These include SeSX, with X being chlorine or bromine. Such compounds can form in mixtures of sulfur dichloride and selenium halides. These compounds have been fairly recently structurally characterized, as of 2008. In general, diselenium and disulfur chlorides and bromides are useful chemical reagents. Chalcogen halides with attached metal atoms are soluble in organic solutions. One example of such a compound is MoS2Cl3. Unlike selenium chlorides and bromides, selenium iodides have not been isolated, as of 2008, although it is likely that they occur in solution. Diselenium diiodide, however, does occur in equilibrium with selenium atoms and iodine molecules. Some tellurium halides with low valences, such as Te2Cl2 and Te2Br2, form polymers when in the solid state. These tellurium halides can be synthesized by the reduction of pure tellurium with superhydride and reacting the resulting product with tellurium tetrahalides. Ditellurium dihalides tend to get less stable as the halides become lower in atomic number and atomic mass. Tellurium also forms iodides with even fewer iodine atoms than diiodies. These include TeI and Te2I. These compounds have extended structures in the solid state. Halogens and chalcogens can also form halochalcogenate anions.\n\n\n=== Organic ===\nAlcohols, phenols and other similar compounds contain oxygen. However, in thiols, selenols and tellurols; sulfur, selenium, and tellurium replace oxygen. Thiols are better known than selenols or tellurols. Thiols are the most stable chalcogenols and tellurols are the least stable, being unstable in heat or light. Other organic chalcogen compounds include thioethers, selenoethers and telluroethers. Some of these, such as dimethyl sulfide, diethyl sulfide, and dipropyl sulfide are commercially available. Selenoethers are in the form of R2Se or RSeR. Telluroethers such as dimethyl telluride are typically prepared in the same way as thioethers and selenoethers. Organic chalcogen compounds, especially organic sulfur compounds, have the tendency to smell unpleasant. Dimethyl telluride also smells unpleasant, and selenophenol is renowned for its \"metaphysical stench\". There are also thioketones, selenoketones, and telluroketones. Out of these, thioketones are the most well-studied with 80% of chalcogenoketones papers being about them. Selenoketones make up 16% of such papers and telluroketones make up 4% of them. Thioketones have well-studied non-linear electric and photophysic properties. Selenoketones are less stable than thioketones and telluroketones are less stable than selenoketones. Telluroketones have the highest level of polarity of chalcogenoketones.\n\n\n=== With metals ===\nElemental chalcogens react with certain lanthanide compounds to form lanthanide clusters rich in chalcogens. Uranium(IV) chalcogenol compounds also exist. There are also transition metal chalcogenols which have potential to serve as catalysts and stabilize nanoparticles.\nThere is a very large number of metal chalcogenides. One of the more recent discoveries in this group of compounds is Rb2Te. There are also compounds in which alkali metals and transition metals such as the fourth period transition metals except for copper and zinc. In highly metal-rich metal chalcogenides, such as Lu7Te and Lu8Te have domains of the metal's crystal lattice containing chalcogen atoms. While these compounds do exist, analogous chemicals that contain lanthanum, praseodymium, gadolinium, holmium, terbium, or ytterbium have not been discovered, as of 2008. The boron group metals aluminum, gallium, and indium also form bonds to chalcogens. The Ti3+ ion forms chalcogenide dimers such as TiTl5Se8. Metal chalcogenide dimers also occur as lower tellurides, such as Zr5Te6.\n\n\n=== With pnictogens ===\n\nCompounds with chalcogen-phosphorus bonds have been explored for more than 200 years. These compounds include unsophisticated phosphorus chalcogenides as well as large molecules with biological roles and phosphorus-chalcogen compounds with metal clusters. These compounds have numerous applications, including strike-anywhere matches and quantum dots. A total of 130,000 compounds with at least one phosphorus-sulfur bond, 6000 compounds with at least one phosphorus-selenium bond, and 350 compounds with at least one phosphorus-tellurium bond have been discovered. The decrease in the number of chalcogen-phosphorus compounds further down the periodic table is due to diminishing bond strength. Such compounds tend at least one phosphorus atom in the center, surrounded by four chalcogens and side chains. However, some phosphorus-chalcogen compounds also contain hydrogen (such as secondary phosphine chalcogenides) or nitrogen (such as dichalcogenoimidodiphosphates). Phosphorus selenides are typically harder to handle that phosphorus sulfides, and compounds in the from PxTey have not been discovered. Chalcogens also bond with other pnictogens, such as arsenic, antimony, and bismuth. Heavier chalcogen pnictides tend to form ribbon-like polymers instead of individual molecules. Chemical formulas of these compounds include Bi2S3 and Sb2Se3. Ternary chalcogen pnictides are also known. Examples of these include P4O6Se and P3SbS3. salts containing chalcogens and pnictogens also exist. Almost all chalcogen pnictide salts are typically in the form of [PnxE4x]3\u2212, where Pn is a pnictogen and E is a chalcogen. Tertiary phosphines can react with chalcogens to form compounds in the form of R3PE, where E is a chalcogen. When E is sulfur, these compounds are relatively stable, but they are less so when E is selenium or tellurium. Similarly, secondary phosphines can react with chalcogens to form secondary phosphine chalcogenides. However, these compounds are in a state of equilibrium with chalcogenophosphinous acid. Secondary phosphine chalcogenides are weak acids. Binary compounds consisting of antimony or arsenic and a chalcogen. These compounds tend to be colorful and can be created by a reaction of the constituent elements at temperatures of 500 to 900 \u00b0C (932 to 1,652 \u00b0F).\n\n\n=== Other ===\nChalcogens form single bonds and double bonds with other carbon group elements than carbon, such as silicon, germanium, and tin. Such compounds typically form from a reaction of carbon group halides and chalcogenol salts or chalcogenol bases. Cyclic compounds with chalcogens, carbon group elements, and boron atoms exist, and occur from the reaction of boron dichalcogenates and carbon group metal halides. Compounds in the form of M-E, where M is silicon, germanium, or tin, and E is sulfur, selenium or tellurium have been discovered. These form when carbon group hydrides react or when heavier versions of carbenes react. Sulfur and tellurium can bond with organic compounds containing both silicon and phosphorus.\nAll of the chalcogens form hydrides. In some cases this occurs with chalcogens bonding with two hydrogen atoms. However tellurium hydride and polonium hydride are both volatile and highly labile. Also, oxygen can bond to hydrogen in a 1:1 ratio as in hydrogen peroxide, but this compound is unstable.\nChalcogen compounds form a number of interchalcogens. For instance, sulfur forms the toxic sulfur dioxide and sulfur trioxide. Tellurium also forms oxides. There are some chalcogen sulfides as well. These include selenium sulfide, an ingredient in some shampoos.\nSince 1990, a number of borides with chalcogens bonded to them have been detected. The chalcogens in these compounds are mostly sulfur, although some do contain selenium instead. One such chalcogen boride consists of two molecules of dimethyl sulfide attached to a boron-hydrogen molecule. Other important boron-chalcogen compounds include macropolyhedral systems. Such compounds tend to feature sulfur as the chalcogen. There are also chalcogen borides with two, three, or four chalcogens. Many of these contain sulfur but some, such as Na2B2Se7 contain selenium instead.\n\n\n== History ==\n\n\n=== Early discoveries ===\n\nSulfur has been known since ancient times and is mentioned in the Bible fifteen times. It was known to the ancient Greeks and commonly mined by the ancient Romans. It was also historically used as a component of Greek fire. In the Middle Ages, it was a key part of alchemical experiments. In the 1700s and 1800s, scientists Joseph Louis Gay-Lussac and Louis-Jacques Th\u00e9nard proved sulfur to be a chemical element.\nEarly attempts to separate oxygen from air were hampered by the fact that air was thought of as a single element up to the 17th and 18th centuries. Robert Hooke, Mikhail Lomonosov, Ole Borch, and Pierre Bayden all successfully created oxygen, but did not realize it at the time. Oxygen was discovered by Joseph Priestley in 1774 when he focused sunlight on a sample of mercuric oxide and collected the resulting gas. Carl Wilhelm Scheele had also created oxygen in 1771 by the same method, but Scheele did not publish his results until 1777.\nTellurium was first discovered in 1783 by Franz Joseph M\u00fcller von Reichenstein. He discovered tellurium in a sample of what is now known as calaverite. M\u00fcller assumed at first that the sample was pure antimony, but tests he ran on the sample did not agree with this. Muller then guessed that the sample was bismuth sulfide, but tests confirmed that the sample was not that. For some years, Muller pondered the problem. Eventually he realized that the sample was gold bonded with an unknown element. In 1796, M\u00fcller sent part of the sample to the German chemist Martin Klaproth, who purified the undiscovered element. Klaproth decided to call the element tellurium after the Latin word for earth.\nSelenium was discovered in 1817 by J\u00f6ns Jacob Berzelius. Berzelius noticed a reddish-brown sediment at a sulfuric acid manufacturing plant. The sample was thought to contain arsenic. Berzelius initially thought that the sediment contained tellurium, but came to realize that it also contained a new element, which he named selenium after the Greek moon goddess Selene.\n\n\n=== Periodic table placing ===\n\nThree of the chalcogens (sulfur, selenium, and tellurium) were part of the discovery of periodicity, as they are among a series of triads of elements in the same group that were noted by Johann Wolfgang D\u00f6bereiner as having similar properties. Around 1865 John Newlands produced a series of papers where he listed the elements in order of increasing atomic weight and similar physical and chemical properties that recurred at intervals of eight; he likened such periodicity to the octaves of music. His version included a \"group b\" consisting of oxygen, sulfur, selenium, tellurium, and osmium.\n\nAfter 1869, Dmitri Mendeleev proposed his periodic table placing oxygen at the top of \"group VI\" above sulfur, selenium, and tellurium. Chromium, molybdenum, tungsten, and uranium were sometimes included in this group, but they would be later rearranged as part of group VIB; uranium would later be moved to the actinide series. Oxygen, along with sulfur, selenium, tellurium, and later polonium would be grouped in group VIA, until the group's name was changed to group 16 in 1988.\n\n\n=== Modern discoveries ===\nIn the late 19th century, Marie Curie and Pierre Curie discovered that a sample of pitchblende was emitting four times as much radioactivity as could be explained by the presence of uranium alone. The Curies gathered several tons of pitchblende and refined it for several months until they had a pure sample of polonium. The discovery officially took place in 1898. Prior to the invention of particle accelerators, the only way to create polonium was to extract it over several months from uranium ore.\nThe first attempt at creating livermorium was from 1976 to 1977 at the LBNL, who bombarded curium-248 with calcium-48, but were not successful. After several failed attempts in 1977, 1998, and 1999 by research groups in Russia, Germany, and the USA, livermorium was created successfully in 2000 at the Joint Institute for Nuclear Research by bombarding curium-248 atoms with calcium-48 atoms. The element was known as ununhexium until it was officially named livermorium in 2012.\n\n\n=== Etymology ===\nIn the 19th century, Jons Jacob Berzelius suggested calling the elements in group 16 \"amphigens\", as the elements in the group formed amphid salts (salts of oxyacids) The term received some use in the early 1800s but is now obsolete. The name chalcogen comes from the Greek words \u03c7\u03b1\u03bb\u03ba\u03bf\u03c2 (chalkos, literally \"copper\"), and \u03b3\u03b5\u03bd\u03ad\u03c2 (genes, born, gender, kindle). It was first used in 1932 by Wilhelm Biltz's group at the University of Hanover, where it was proposed by Werner Fischer. The word \"chalcogen\" gained popularity in Germany during the 1930s because the term was analogous to \"halogen\". Although the literal meanings of the Greek words imply that chalcogen means \"copper-former\", this is misleading because the chalcogens have nothing to do with copper in particular. \"Ore-former\" has been suggested as a better translation, as the vast majority of metal ores are chalcogenides and the word \u03c7\u03b1\u03bb\u03ba\u03bf\u03c2 in ancient Greek was associated with metals and metal-bearing rock in general; copper, and its alloy bronze, was one of the first metals to be used by humans.\nOxygen's name comes from the Greek words oxy genes, meaning \"acid-forming\". Sulfur's name comes from either the Latin word sulfurium or the Sanskrit word sulvere; both of those terms are ancient words for sulfur. Selenium is named after the Greek goddess of the moon, Selene, to match the previously-discovered element tellurium, whose name comes from the Latin word telus, meaning earth. Polonium is named after Marie Curie's country of birth, Poland. Livermorium is named for the Lawrence Livermore National Laboratory.\n\n\n== Occurrence ==\nThe four lightest chalcogens (oxygen, sulfur, selenium, and tellurium) are all primordial elements on Earth. Sulfur and oxygen occur as constituent copper ores and selenium and tellurium occur in small traces in such ores. Polonium forms naturally after the decay of other elements, even though it is not primordial. Livermorium does not occur naturally at all.\nOxygen makes up 21% of the atmosphere by weight, 89% of water by weight, 46% of the earth's crust by weight, and 65% of the human body. Oxygen also occurs in many minerals, being found in all oxide minerals and hydroxide minerals, and in numerous other mineral groups. Stars of at least eight times the mass of the sun also produce oxygen in their cores via nuclear fusion. Oxygen is the third-most abundant element in the universe, making up 1% of the universe by weight.\nSulfur makes up 0.035% of the earth's crust by weight, making it the 17th most abundant element there and makes up 0.25% of the human body. It is a major component of soil. Sulfur makes up 870 parts per million of seawater and about 1 part per billion of the atmosphere. Sulfur can be found in elemental form or in the form of sulfide minerals, sulfate minerals, or sulfosalt minerals. Stars of at least 12 times the mass of the sun produce sulfur in their cores via nuclear fusion. Sulfur is the tenth most abundant element in the universe, making up 500 parts per million of the universe by weight.\nSelenium makes up 0.05 parts per million of the earth's crust by weight. This makes it the 67th most abundant element in the earth's crust. Selenium makes up on average 5 parts per million of the soils. Seawater contains around 200 parts per trillion of selenium. The atmosphere contains 1 nanogram of selenium per cubic meter. There are mineral groups known as selenates and selenites, but there are not many of minerals in these groups. Selenium is not produced directly by nuclear fusion. Selenium makes up 30 parts per billion of the universe by weight.\nThere are only 5 parts per billion of tellurium in the earth's crust and 15 parts per billion of tellurium in seawater. Tellurium is one of the eight or nine least abundant elements in the earth's crust. There are a few dozen tellurate minerals and telluride minerals, and tellurium occurs in some minerals with gold, such as sylvanite and calaverite. Tellurium makes up 9 parts per billion of the universe by weight.\nPolonium only occurs in trace amounts on earth, via radioactive decay of uranium and thorium. It is present in uranium ores in concentrations of 100 micrograms per metric ton. Very minute amounts of polonium exist in the soil and thus in most food, and thus in the human body. The earth's crust contains less than 1 part per billion of polonium, making it one of the ten rarest metals on earth.\nLivermorium is always produced artificially in particle accelerators. Even when it is produced, only a small number of atoms at a time are synthesized.\n\n\n=== Chalcophile elements ===\n\nChalcophile elements are those that remain on or close to the surface because they combine readily with chalcogens other than oxygen, forming compounds which do not sink into the core. Chalcophile (\"chalcogen-loving\") elements in this context are those metals and heavier nonmetals that have a low affinity for oxygen and prefer to bond with the heavier chalcogen sulfur as sulfides. Because sulfide minerals are much denser than the silicate minerals formed by lithophile elements, chalcophile elements separated below the lithophiles at the time of the first crystallisation of the Earth's crust. This has led to their depletion in the Earth's crust relative to their solar abundances, though this depletion has not reached the levels found with siderophile elements.\n\n\n== Production ==\nApproximately 100 million metric tons of oxygen are produced yearly. Oxygen is most commonly produced by fractional distillation, in which air is cooled to a liquid, then warmed, allowing all the components of air except for oxygen to turn to gases and escape. Fractionally distilling air several times can produce 99.5% pure oxygen. Another method with which oxygen is produced is to send a stream of dry, clean air through a bed of molecular sieves made of zeolite, which absorbs the nitrogen in the air, leaving 90 to 93% pure oxygen.\n\nSulfur can be mined in its elemental form, although this method is no longer as popular as it used to be. In 1865 a large deposit of elemental sulfur was discovered in the U.S. states of Louisiana and Texas, but it was difficult to extract at the time. In the 1890s, Herman Frasch came up with the solution of liquefying the sulfur with superheated steam and pumping the sulfur up to the surface. These days sulfur is instead more often extracted from oil, natural gas, and tar.\nThe world production of selenium is around 1500 metric tons per year, out of which roughly 10% is recycled. Japan is the largest producer, producing 800 metric tons of selenium per year. Other large producers include Belgium (300 metric tons per year), the United States (over 200 metric tons per year), Sweden (130 metric tons per year), and Russia (100 metric tons per year). Selenium can be extracted from the waste from the process of electrolytically refining copper. Another method of producing selenium is to farm selenium-gathering plants such as milk vetch. This method could produce three kilograms of selenium per acre, but is not commonly practiced.\nTellurium is mostly produced as a by-product of the processing of copper. Tellurium can also be refined by electrolytic reduction of sodium telluride. The world production of tellurium is between 150 and 200 metric tons per year. The United States is one of the largest producers of tellurium, producing around 50 metric tons per year. Peru, Japan, and Canada are also large producers of tellurium.\nUntil the creation of nuclear reactors, all polonium had to be extracted from uranium ore. In modern times, most isotopes of polonium are produced by bombarding bismuth with neutrons. Polonium can also be produced by high neutron fluxes in nuclear reactors. Approximately 100 grams of polonium are produced yearly. All the polonium produced for commercial purposes is made in the Ozersk nuclear reactor in Russia. From there, it is taken to Samara, Russia for purification, and from there to St. Petersburg for distribution. The United States is the largest consumer of polonium.\nAll livermorium is produced artificially in particle accelerators. The first successful production of livermorium was achieved by bombarding curium-248 atoms with calcium-48 atoms. As of 2011, roughly 25 atoms of livermorium had been synthesized.\n\n\n== Applications ==\nSteelmaking is the most important use of oxygen; 55% of all oxygen produced goes to this application. The chemical industry also uses large amounts of oxygen; 25% of all oxygen produced goes to this application. The remaining 20% of oxygen produced is mostly split between medical use, water treatment (as oxygen kills some types of bacteria), rocket fuel (in liquid form), and metal cutting.\nMost sulfur produced is transformed into sulfur dioxide, which is further transformed into sulfuric acid, a very common industrial chemical. Other common uses include being a key ingredient of gunpowder and Greek fire, and being used to change soil pH. Sulfur is also mixed into rubber to vulcanize it. Sulfur is used in some types of concrete and fireworks. 60% of all sulfuric acid produced is used to generate phosphoric acid.\n\nAround 40% of all selenium produced goes to glassmaking. 30% of all selenium produced goes to metallurgy, including manganese production. 15% of all selenium produced goes to agriculture. Electronics such as photovoltaic materials claim 10% of all selenium produced. Pigments account for 5% of all selenium produced. Historically, machines such as photocopiers and light meters used one-third of all selenium produced, but this application is in steady decline.\nTellurium suboxide, a mixture of tellurium and tellurium dioxide, is used in the rewritable data layer of some CD-RW disks and DVD-RW disks. Bismuth telluride is also used in many microelectronic devices, such as photoreceptors. Tellurium is sometimes used as an alternative to sulfur in vulcanized rubber. Cadmium telluride is used as a high-efficiency material in solar panels.\nSome of polonium's applications relate to the element's radioactivity. For instance, polonium is used as an alpha-particle generator for research. Polonium alloyed with beryllium provides an efficient neutron source. Polonium is also used in nuclear batteries. Most polonium is used in antistatic devices. Livermorium does not have any uses whatsoever due to its extreme rarity and short half-life.\nOrganochalcogen compounds are involved in the semiconductor process. These compounds also feature into ligand chemistry and biochemistry. One application of chalcogens themselves is to manipulate redox couples in supramolar chemistry (chemistry involving non-covalent bond interactions). This application leads on to such applications as crystal packing, assembly of large molecules, and biological recognition of patterns. The secondary bonding interactions of the larger chalcogens, selenium and tellurium, can create organic solvent-holding acetylene nanotubes. Chalcogen interactions are useful for conformational analysis and stereoelectronic effects, among other things. Chalcogenides with through bonds also have applications. For instance, divalent sulfur can stabilize carbanions, cationic centers, and radical. Chalcogens can confer upon ligands (such as DCTO) properties such as being able to transform Cu(II) to Cu(I). Studying chalcogen interactions gives access to radical cations, which are used in mainstream synthetic chemistry. Metallic redox centers of biological importance are tunable by interactions of ligands containing chalcogens, such as methionine and selenocysteine. Also, chalcogen through-bonds can provide insight about the process of electron transfer.\n\n\n== Biological role ==\n\nOxygen is needed by almost all organisms for the purpose of generating ATP. It is also a key component of most other biological compounds, such as water, amino acids and DNA. Human blood contains a large amount of oxygen. Human bones contain 28% oxygen. Human tissue contains 16% oxygen. A typical 70-kilogram human contains 43 kilograms of oxygen, mostly in the form of water.\nAll animals need significant amounts of sulfur. Some amino acids, such as cysteine and methionine contain sulfur. Plant roots take up sulfate ions from the soil and reduce it to sulfide ions. Metalloproteins also use sulfur to attach to useful metal atoms in the body and sulfur similarly attaches itself to poisonous metal atoms like cadmium to haul them to the safety of the liver. On average, humans consume 900 milligrams of sulfur each day. Sulfur compounds, such as those found in skunk spray often have strong odors.\nAll animals and some plants need trace amounts of selenium, but only for some specialized enzymes. Humans consume on average between 6 and 200 micrograms of selenium per day. Mushrooms and brazil nuts are especially noted for their high selenium content. Selenium in foods is most commonly found in the form of amino acids such as selenocysteine and selenomethionine. Selenium can protect against heavy metal poisoning.\nTellurium is not known to be needed for animal life, although a few fungi can incorporate it in compounds in place of selenium. Microorganisms also absorb tellurium and emit dimethyl telluride. Most tellurium in the blood stream is excreted slowly in urine, but some is converted to dimethyl telluride and released through the lungs. On average, humans ingest about 600 micrograms of tellurium daily. Plants can take up some tellurium from the soil. Onions and garlic have been found to contain as much as 300 parts per million of tellurium in dry weight.\nPolonium has no biological role, and is highly toxic on account of being radioactive.\n\n\n== Toxicity ==\nOxygen is generally nontoxic, but oxygen toxicity has been reported when it is used in high concentrations. In both elemental gaseous form and as a component of water, it is vital to almost all life on earth. Despite this, liquid oxygen is highly dangerous. Even gaseous oxygen is dangerous in excess. For instance, sports divers have occasionally drowned from convulsions caused by breathing pure oxygen at a depth of more than 10 meters (33 feet) underwater. Oxygen is also toxic to some bacteria. Ozone, an allotrope of oxygen, is toxic to most life. It can cause lesions in the respiratory tract.\nSulfur is generally nontoxic and is even a vital nutrient for humans. However, in its elemental form it can cause redness in the eyes and skin, a burning sensation and a cough if inhaled, a burning sensation and diarrhea if ingested, and can irritate the mucous membranes. An excess of sulfur can be toxic for cows because microbes in the rumens of cows produce toxic hydrogen sulfide upon reaction with sulfur. Many sulfur compounds, such as hydrogen sulfide (H2S) and sulfur dioxide (SO2) are highly toxic.\nSelenium is a trace nutrient required by humans on the order of tens or hundreds of micrograms per day. A dose of over 450 micrograms can be toxic, resulting in bad breath and body odor. Extended, low-level exposure, which can occur at some industries, results in weight loss, anemia, and dermatitis. In many cases of selenium poisoning, selenous acid is formed in the body. Hydrogen selenide (H2Se) is highly toxic.\nExposure to tellurium can produce unpleasant side effects. As little as 10 micrograms of tellurium per cubic meter of air can cause notoriously unpleasant breath, described as smelling like rotten garlic. Acute tellurium poisoning can cause vomiting, gut inflammation, internal bleeding, and respiratory failure. Extended, low-level exposure to tellurium causes tiredness and indigestion. Sodium tellurite (Na2TeO3) is lethal in amounts of around 2 grams.\nPolonium is dangerous both as an alpha particle emitter and because it is chemically toxic. If ingested, polonium-210 is a billion times as toxic as hydrogen cyanide by weight; it has been used as a murder weapon in the past, most famously to kill Alexander Litvinenko. Polonium poisoning can cause nausea, vomiting, anorexia, and lymphopenia. It can also damage hair follicles and white blood cells. Polonium-210 is only dangerous if ingested or inhaled because its alpha particle emissions cannot penetrate human skin. Polonium-209 is also toxic, and can cause leukemia.\n\n\n== See also ==\nChalcogenide\nGold chalcogenides\nHalogen\nInterchalcogen\nPnictogen\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Chalcogen", 
                "title": "Chalcogen"
            }, 
            {
                "snippet": "specified either as a reaction with dissolved carbon dioxide or as a reaction with carbon dioxide gas (see carbonic acid for values and details). At neutral", 
                "pageCategories": "All accuracy disputes\nAll articles with unsourced statements\nArticles containing unverified chemical infoboxes\nArticles with disputed statements from November 2014\nArticles with unsourced statements from January 2013\nArticles with unsourced statements from November 2014\nArticles without KEGG source\nBases (chemistry)\nCS1 maint: Multiple names: authors list\nChemical articles using a fixed chemical formula", 
                "pageContent": "Hydroxide is a diatomic anion with chemical formula OH\u2212. It consists of an oxygen and hydrogen atom held together by a covalent bond, and carries a negative electric charge. It is an important but usually minor constituent of water. It functions as a base, a ligand, a nucleophile and a catalyst. The hydroxide ion forms salts, some of which dissociate in aqueous solution, liberating solvated hydroxide ions. Sodium hydroxide is a multi-million-ton per annum commodity chemical. A hydroxide attached to a strongly electropositive center may itself ionize, liberating a hydrogen cation (H+), making the parent compound an acid.\nThe corresponding electrically neutral compound \u2022HO is the hydroxyl radical. The corresponding covalently-bound group \u2013OH of atoms is the hydroxyl group. Hydroxide ion and hydroxyl group are nucleophiles and can act as a catalyst in organic chemistry.\nMany inorganic substances which bear the word \"hydroxide\" in their names are not ionic compounds of the hydroxide ion, but covalent compounds which contain hydroxyl groups.\n\n\n== Hydroxide ion ==\nThe hydroxide ion is a natural part of water, because of the self-ionization reaction:\nH3O+ + OH\u2212 \u21cc 2H2O\nThe equilibrium constant for this reaction, defined as\nKw = [H+][OH\u2212]\nhas a value close to 10\u221214 at 25 \u00b0C, so the concentration of hydroxide ions in pure water is close to 10\u22127 mol\u2219dm\u22123, in order to satisfy the equal charge constraint. The pH of a solution is equal to the decimal cologarithm of the hydrogen cation concentration; the pH of pure water is close to 7 at ambient temperatures. The concentration of hydroxide ions can be expressed in terms of pOH, which is close to 14 \u2212 pH, so pOH of pure water is also close to 7. Addition of a base to water will reduce the hydrogen cation concentration and therefore increase the hydroxide ion concentration (increase pH, decrease pOH) even if the base does not itself contain hydroxide. For example, ammonia solutions have a pH greater than 7 due to the reaction NH3 + H+ \u21cc NH+\n4, which results in a decrease in hydrogen cation concentration and an increase in hydroxide ion concentration. pOH can be kept at a nearly constant value with various buffer solutions.\n\nIn aqueous solution the hydroxide ion is a base in the Br\u00f8nsted\u2013Lowry sense as it can accept a proton from a Br\u00f8nsted\u2013Lowry acid to form a water molecule. It can also act as a Lewis base by donating a pair of electrons to a Lewis acid. In aqueous solution both hydrogen and hydroxide ions are strongly solvated, with hydrogen bonds between oxygen and hydrogen atoms. Indeed, the bihydroxide ion H\n3O\u2212\n2 has been characterized in the solid state. This compound is centrosymmetric and has a very short hydrogen bond (114.5 pm) that is similar to the length in the bifluoride ion HF\u2212\n2 (114 pm). In aqueous solution the hydroxide ion forms strong hydrogen bonds with water molecules. A consequence of this is that concentrated solutions of sodium hydroxide have high viscosity due to the formation of an extended network of hydrogen bonds as in hydrogen fluoride solutions.\nIn solution, exposed to air, the hydroxide ion reacts rapidly with atmospheric carbon dioxide, acting as an acid, to form, initially, the bicarbonate ion.\nOH\u2212 + CO2 \u21cc HCO\u2212\n3\nThe equilibrium constant for this reaction can be specified either as a reaction with dissolved carbon dioxide or as a reaction with carbon dioxide gas (see carbonic acid for values and details). At neutral or acid pH, the reaction is slow, but is catalyzed by the enzyme carbonic anhydrase, which effectively creates hydroxide ions at the active site.\nSolutions containing the hydroxide ion attack glass. In this case, the silicates in glass are acting as acids. Basic hydroxides, whether solids or in solution, are stored in airtight plastic containers.\nThe hydroxide ion can function as a typical electron-pair donor ligand, forming such complexes as [Al(OH)4]\u2212. It is also often found in mixed-ligand complexes of the type [MLx(OH)y]z+, where L is a ligand. The hydroxide ion often serves as a bridging ligand, donating one pair of electrons to each of the atoms being bridged. As illustrated by [Pb2(OH)]3+, metal hydroxides are often written in a simplified format. It can even act as a 3-electron-pair donor, as in the tetramer [PtMe3(OH)]4.\nWhen bound to a strongly electron-withdrawing metal centre, hydroxide ligands tend to ionise into oxide ligands. For example, the bichromate ion [HCrO4]\u2212 dissociates according to\n[O3CrO\u2013H]\u2212 \u21cc [CrO4]2\u2212 + H+\nwith a pKa of about 5.9.\n\n\n=== Vibrational spectra ===\nThe infrared spectra of compounds containing the OH functional group have strong absorption bands in the region centered around 3500 cm\u22121. The high frequency of molecular vibration is a consequence of the small mass of the hydrogen atom as compared to the mass of the oxygen atom and this makes detection of hydroxyl groups by infrared spectroscopy relatively easy. A band due to an OH group tends to be sharp. However, the band width increases when the OH group is involved in hydrogen bonding. A water molecule has an HOH bending mode at about 1600 cm\u22121, so the absence of this band can be used to distinguish an OH group from a water molecule.\nWhen the OH group is bound to a metal ion in a coordination complex, an M\u2212OH bending mode can be observed. For example, in [Sn(OH)6]2\u2212 it occurs at 1065 cm\u22121. The bending mode for a bridging hydroxide tends to be at a lower frequency as in [(bipyridine)Cu(OH)2Cu(bipyridine)]2+ (955 cm\u22121). M\u2212OH stretching vibrations occur below about 600 cm\u22121. For example, the tetrahedral ion [Zn(OH)4]2\u2212 has bands at 470 cm\u22121 (Raman-active, polarized) and 420 cm\u22121 (infrared). The same ion has a (HO)\u2013Zn\u2013(OH) bending vibration at 300 cm\u22121.\n\n\n== Applications ==\nSodium hydroxide solutions, also known as lye and caustic soda, are used in the manufacture of pulp and paper, textiles, drinking water, soaps and detergents, and as a drain cleaner. Worldwide production in 2004 was approximately 60 million tonnes. The principal method of manufacture is the chlor-alkali process.\nSolutions containing the hydroxide ion are generated when a salt of a weak acid is dissolved in water. Sodium carbonate is used as an alkali, for example, by virtue of the hydrolysis reaction\nCO2\u2212\n3 + H2O \u21cc HCO\u2212\n3 + OH\u2212;      (pKa2 = 10.33 at 25 \u00b0C and zero ionic strength)\nAlthough the base strength of sodium carbonate solutions is lower than a concentrated sodium hydroxide solution, it has the advantage of being a solid. It is also manufactured on a vast scale (42 million tonnes in 2005) by the Solvay process. An example of the use of sodium carbonate as an alkali is when washing soda (another name for sodium carbonate) acts on insoluble esters, such as triglycerides, commonly known as fats, to hydrolyze them and make them soluble.\nBauxite, a basic hydroxide of aluminium, is the principal ore from which the metal is manufactured. Similarly, goethite (\u03b1-FeO(OH)) and lepidocrocite (\u03b3-FeO(OH)), basic hydroxides of iron, are among the principal ores used for the manufacture of metallic iron. Numerous other uses can be found in the articles on individual hydroxides.\n\n\n== Inorganic hydroxides ==\n\n\n=== Alkali metals ===\nAside from NaOH and KOH, which enjoy very large scale applications, the hydroxides of the other alkali metals also are useful. Lithium hydroxide is a strong base, with a pKb of \u22120.36. Lithium hydroxide is used in breathing gas purification systems for spacecraft, submarines, and rebreathers to remove carbon dioxide from exhaled gas.\n2 LiOH + CO2 \u2192 Li2CO3 + H2O\nThe hydroxide of lithium is preferred to that of sodium because of its lower mass. Sodium hydroxide, potassium hydroxide and the hydroxides of the other alkali metals are also strong bases.\n\n\n=== Alkaline earth metals ===\n\nBeryllium hydroxide Be(OH)2 is amphoteric. The hydroxide itself is insoluble in water, with a solubility product log K*sp of \u221211.7. Addition of acid gives soluble hydrolysis products, including the trimeric ion [Be3(OH)3(H2O)6]3+, which has OH groups bridging between pairs of beryllium ions making a 6-membered ring. At very low pH the aqua ion [Be(H2O)4]2+ is formed. Addition of hydroxide to Be(OH)2 gives the soluble tetrahydroxo anion [Be(OH)4]2\u2212.\nThe solubility in water of the other hydroxides in this group increases with increasing atomic number. Magnesium hydroxide Mg(OH)2 is a strong base as are the hydroxides of the heavier alkaline earths, calcium hydroxide, strontium hydroxide and barium hydroxide. A solution/suspension of calcium hydroxide is known as limewater and can be used to test for the weak acid carbon dioxide. The reaction Ca(OH)2 + CO2 \u21cc Ca2+ + HCO\u2212\n3 + OH\u2212 illustrates the strong basicity of calcium hydroxide. Soda lime, which is a mixture of NaOH and Ca(OH)2, is used as a CO2 absorbent.\n\n\n=== Boron group elements ===\n\nThe simplest hydroxide of boron B(OH)3, known as boric acid, is an acid. Unlike the hydroxides of the alkali and alkaline earth hydroxides, it does not dissociate in aqueous solution. Instead, it reacts with water molecules acting as a Lewis acid, releasing protons.\nB(OH)3 + H2O \u21cc B(OH)\u2212\n4 + H+\nA variety of oxyanions of boron are known, which, in the protonated form, contain hydroxide groups.\n\nAluminium hydroxide Al(OH)3 is amphoteric and dissolves in alkaline solution.\nAl(OH)3 (solid) + OH\u2212 (aq) \u21cc Al(OH)\u2212\n4 (aq)\nIn the Bayer process for the production of pure aluminium oxide from bauxite minerals this equilibrium is manipulated by careful control of temperature and alkali concentration. In the first phase, aluminium dissolves in hot alkaline solution as Al(OH)\u2212\n4 but other hydroxides usually present in the mineral, such as iron hydroxides, do not dissolve because they are not amphoteric. After removal of the insolubles, the so-called red mud, pure aluminium hydroxide is made to precipitate by reducing the temperature and adding water to the extract, which, by diluting the alkali, lowers the pH of the solution. Basic aluminium hydroxide AlO(OH), which may be present in bauxite, is also amphoteric.\nIn mildly acidic solutions the hydroxo complexes formed by aluminium are somewhat different from those of boron, reflecting the greater size of Al(III) vs. B(III). The concentration of the species [Al13(OH)32]7+ is very dependent on the total aluminium concentration. Various other hydroxo complexes are found in crystalline compounds. Perhaps the most important is the basic hydroxide AlO(OH), a polymeric material known by the names of the mineral forms boehmite or diaspore, depending on crystal structure. Gallium hydroxide, indium hydroxide and thallium(III) hydroxides are also amphoteric. Thallium(I) hydroxide is a strong base.\n\n\n=== Carbon group elements ===\nCarbon forms no simple hydroxides. The hypothetical compound C(OH)4 (orthocarbonic acid or methanetetraol) is unstable in aqueous solution:\nC(OH)4 \u2192 HCO\u2212\n3 + H3O+\nHCO\u2212\n3 + H+ \u21cc H2CO3\nCarbon dioxide is also known as carbonic anhydride, meaning that it forms by dehydration of carbonic acid H2CO3 (OC(OH)2).\nSilicic acid is the name given to a variety of compounds with a generic formula [SiOx(OH)4\u22122x]n. Orthosilicic acid has been identified in very dilute aqueous solution. It is a weak acid with pKa1 = 9.84, pKa2 = 13.2 at 25 \u00b0C. It is usually written as H4SiO4 but the formula SiO2(OH)2 is generally accepted . Other silicic acids such as metasilicic acid (H2SiO3), disilicic acid (H2Si2O5), and pyrosilicic acid (H6Si2O7) have been characterized. These acids also have hydroxide groups attached to the silicon; the formulas suggest that these acids are protonated forms of polyoxyanions.\nFew hydroxo complexes of germanium have been characterized. Tin(II) hydroxide Sn(OH)2 was prepared in anhydrous media. When tin(II) oxide is treated with alkali the pyramidal hydroxo complex Sn(OH)\u2212\n3 is formed. When solutions containing this ion are acidified the ion [Sn3(OH)4]2+ is formed together with some basic hydroxo complexes. The structure of [Sn3(OH)4]2+ has a triangle of tin atoms connected by bridging hydroxide groups. Tin(IV) hydroxide is unknown but can be regarded as the hypothetical acid from which stannates, with a formula [Sn(OH)6]2\u2212, are derived by reaction with the (Lewis) basic hydroxide ion.\nHydrolysis of Pb2+ in aqueous solution is accompanied by the formation of various hydroxo-containing complexes, some of which are insoluble. The basic hydroxo complex [Pb6O(OH)6]4+ is a cluster of six lead centres with metal\u2013metal bonds surrounding a central oxide ion. The six hydroxide groups lie on the faces of the two external Pb4 tetrahedra. In strongly alkaline solutions soluble plumbate ions are formed, including [Pb(OH)6]2\u2212.\n\n\n=== Other main-group elements ===\nIn the higher oxidation states of the elements in groups 5, 6 and 7 there are oxoacids in which the central atom is attached to oxide ions and hydroxide ions. Examples include phosphoric acid H3PO4, and sulfuric acid H2SO4. In these compounds one or more hydroxide groups can dissociate with the liberation of hydrogen cations as in a standard Br\u00f8nsted\u2013Lowry acid. Many oxoacids of sulfur are known and all feature OH groups that can dissociate.\nTelluric acid is often written with the formula H2TeO4\u00b72H2O but is better described structurally as Te(OH)6.\nOrtho-periodic acid can lose all its protons, eventually forming the periodate ion [IO4]\u2212. It can also be protonated in strongly acidic conditions to give the octahedral ion [I(OH)6]+, completing the isoelectronic series, [E(OH)6]z, E = Sn, Sb, Te, I; z = \u22122, \u22121, 0, +1. Other acids of iodine(VII) that contain hydroxide groups are known, in particular in salts such as the mesoperiodate ion that occurs in K4[I2O8(OH)2]\u00b78H2O.\nAs is common outside of the alkali metals, hydroxides of the elements in lower oxidation states are complicated. For example, phosphorous acid H3PO3 predominantly has the structure OP(H)(OH)2, in equilibrium with a small amount of P(OH)3.\nThe oxoacids of chlorine, bromine and iodine have the formula On\u22121/2A(OH) where n is the oxidation number: +1, +3, +5 or +7, and A = Cl, Br or I. The only oxoacid of fluorine is F(OH). When these acids are neutralized the hydrogen atom is removed from the hydroxide group.\n\n\n=== Transition and post-transition metals ===\nThe hydroxides of the transition metals and post-transition metals usually have the metal in the +2 (M = Mn, Fe, Co, Ni, Cu, Zn) or +3 (M = Fe, Ru, Rh, Ir) oxidation state. None are soluble in water, and many are poorly defined. One complicating feature of the hydroxides is their tendency to undergo further condensation to the oxides, a process called olation. Hydroxides of metals in the +1 oxidation state are also poorly defined or unstable. For example, silver hydroxide Ag(OH) decomposes spontaneously to the oxide (Ag2O). Copper(I) and gold(I) hydroxides are also unstable, although stable adducts of CuOH and AuOH are known. The polymeric compounds M(OH)2 and M(OH)3 are in general prepared by increasing the pH of an aqueous solutions of the corresponding metal cations until the hydroxide precipitates out of solution. On the converse, the hydroxides dissolve in acidic solution. Zinc hydroxide Zn(OH)2 is amphoteric, forming the zincate ion Zn(OH)42\u2212 in strongly alkaline solution.\nNumerous mixed ligand complexes of these metals with the hydroxide ion exist. In fact these are in general better defined than the simpler derivatives. Many can be made by deprotonation of the corresponding metal aquo complex.\nLnM(OH2) + B \u21cc LnM(OH) + BH+ (L = ligand, B = base)\nVanadic acid H3VO4 shows similarities with phosphoric acid H3PO4 though it has a much more complex vanadate oxoanion chemistry. Chromic acid H2CrO4, has similarities with sulfuric acid H2SO4; for example, both form acid salts A+[HMO4]\u2212. Some metals, e.g. V, Cr, Nb, Ta, Mo, W, tend to exist in high oxidation states. Rather than forming hydroxides in aqueous solution, they convert to oxo clusters by the process of olation, forming polyoxometalates.\n\n\n== Basic salts containing hydroxide ==\nIn some cases the products of partial hydrolysis of metal ion, described above, can be found in crystalline compounds. A striking example is found with zirconium(IV). Because of the high oxidation state, salts of Zr4+ are extensively hydrolyzed in water even at low pH. The compound originally formulated as ZrOCl2\u00b78H2O was found to be the chloride salt of a tetrameric cation [Zr4(OH)8(H2O)16]8+ in which there is a square of Zr4+ ions with two hydroxide groups bridging between Zr atoms on each side of the square and with four water molecules attached to each Zr atom.\nThe mineral malachite is a typical example of a basic carbonate. The formula, Cu2CO3(OH)2 shows that it is halfway between copper carbonate and copper hydroxide. Indeed, in the past the formula was written as CuCO3\u00b7Cu(OH)2. The crystal structure is made up of copper, carbonate and hydroxide ions. The mineral atacamite is an example of a basic chloride. It has the formula, Cu2Cl(OH)3. In this case the composition is nearer to that of the hydroxide than that of the chloride CuCl2\u00b73Cu(OH)2. Copper forms hydroxy phosphate (libethenite), arsenate (olivenite), sulfate (brochantite) and nitrate compounds. White lead is a basic lead carbonate, (PbCO3)2\u00b7Pb(OH)2, which has been used as a white pigment because of its opaque quality, though its use is now restricted because it can be a source for lead poisoning.\n\n\n== Structural chemistry ==\nThe hydroxide ion appears to rotate freely in crystals of the heavier alkali metal hydroxides at higher temperatures so as to present itself as a spherical ion, with an effective ionic radius of about 153 pm. Thus, the high-temperature forms of KOH and NaOH have the sodium chloride structure, which gradually freezes in a monocinically distorted sodium chloride structure at temperatures below about 300 \u00b0C. The OH groups still rotate even at room temperature around their symmetry axes and, therefore, cannot be detected by X-ray diffraction. The room-temperature form of NaOH has the thallium iodide structure. LiOH, however, has a layered structure, made up of tetrahedral Li(OH)4 and (OH)Li4 units. This is consistent with the weakly basic character of LiOH in solution, indicating that the Li\u2013OH bond has much covalent character.\nThe hydroxide ion displays cylindrical symmetry in hydroxides of divalent metals Ca, Cd, Mn, Fe, and Co. For example, magnesium hydroxide Mg(OH)2 (brucite) crystallizes with the cadmium iodide layer structure, with a kind of close-packing of magnesium and hydroxide ions.\nThe amphoteric hydroxide Al(OH)3 has four major crystalline forms: gibbsite (most stable), bayerite, nordstrandite and doyleite. All these polymorphs are built up of double layers of hydroxide ions \u2013 the aluminium atoms on two-thirds of the octahedral holes between the two layers \u2013 and differ only in the stacking sequence of the layers. The structures are similar to the brucite structure. However, whereas the brucite structure can be described as a close-packed structure in gibbsite the OH groups on the underside of one layer rest on the groups of the layer below. This arrangement led to the suggestion that there are directional bonds between OH groups in adjacent layers. This is an unusual form of hydrogen bonding since the two hydroxide ion involved would be expected to point away from each other. The hydrogen atoms have been located by neutron diffraction experiments on \u03b1-AlO(OH) (diaspore). The O\u2013H\u2013O distance is very short, at 265 pm; the hydrogen is not equidistant between the oxygen atoms and the short OH bond makes an angle of 12\u00b0 with the O\u2013O line. A similar type of hydrogen bond has been proposed for other amphoteric hydroxides, including Be(OH)2, Zn(OH)2 and Fe(OH)3\nA number of mixed hydroxides are known with stoichiometry A3MIII(OH)6, A2MIV(OH)6 and AMV(OH)6. As the formula suggests these substances contain M(OH)6 octahedral structural units. Layered double hydroxides may be represented by the formula [Mz+\n1\u2212xM3+x(OH)2]q+(Xn\u2212)\u200aq\u2044n\u00b7yH2O. Most commonly, z = 2, and M2+ = Ca2+, Mg2+, Mn2+, Fe2+, Co2+, Ni2+, Cu2+ or Zn2+; hence q = x.\n\n\n== In organic reactions ==\nPotassium hydroxide and sodium hydroxide are two well-known reagents in organic chemistry.\n\n\n=== Base catalysis ===\nThe hydroxide ion may act as a base catalyst. The base abstracts a proton from a weak acid to give an intermediate that goes on to react with another reagent. Common substrates for proton abstraction are alcohols, phenols, amines and carbon acids. The pKa value for dissociation of a C\u2013H bond is extremely high, but the pKa alpha hydrogens of a carbonyl compound are about 3 log units lower. Typical pKa values are 16.7 for acetaldehyde and 19 for acetone. Dissociation can occur in the presence of a suitable base.\nRC(O)CH2R\u2032 + B \u21cc RC(O)CH\u2212R\u2032 + BH+\nThe base should have a pKa value not less than about 4 log units smaller or the equilibrium will lie almost completely to the left.\nThe hydroxide ion by itself is not a strong enough base, but it can be converted in one by adding sodium hydroxide to ethanol\nOH\u2212 + EtOH \u21cc EtO\u2212 + H2O\nto produce the ethoxide ion. The pKa for self-dissociation of ethanol is about 16 so the alkoxide ion is a strong enough base The addition of an alcohol to an aldehyde to form a hemiacetal is an example of a reaction that can be catalyzed by the presence of hydroxide. Hydroxide can also act as a Lewis-base catalyst.\n\n\n=== As a nucleophilic reagent ===\n\nThe hydroxide ion is intermediate in nucleophilicity between the fluoride ion F\u2212, and the amide ion NH\u2212\n2. The hydrolysis of an ester\nR1C(O)OR2 + H2O \u21cc R1C(O)OH + HOR2\nalso known as saponification is an example of a nucleophilic acyl substitution with the hydroxide ion acting as a nucleophile. In this case the leaving group is an alkoxide ion, which immediately removes a proton from a water molecule to form an alcohol. In the manufacture of soap, sodium chloride is added to salt out the sodium salt of the carboxylic acid; this is an example of the application of the common-ion effect.\nOther cases where hydroxide can act as a nucleophilic reagent are amide hydrolysis, the Cannizzaro reaction, nucleophilic aliphatic substitution, nucleophilic aromatic substitution and in elimination reactions. The reaction medium for KOH and NaOH is usually water but with a phase-transfer catalyst the hydroxide anion can be shuttled into an organic solvent as well, for example in the generation of dichlorocarbene.\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Bibliography ==\nHolleman, A.F.; Wiberg, E.; Wiberg, N. (2001). Inorganic Chemistry. Academic press. ISBN 0-12-352651-5. \nHousecroft, C. E.; Sharpe, A. G. (2008). Inorganic Chemistry (3rd ed.). Prentice Hall. ISBN 978-0131755536. \nGreenwood, Norman N.; Earnshaw, Alan (1997). Chemistry of the Elements (2nd ed.). Butterworth-Heinemann. ISBN 0-08-037941-9. \nShriver, D.F; Atkins, P.W (1999). Inorganic Chemistry (3rd ed.). Oxford: Oxford University Press. ISBN 0-19-850330-X. \nWells, A.F (1962). Structural Inorganic Chemistry (3rd. ed.). Oxford: Clarendon Press. ISBN 0-19-855125-8.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Hydroxide", 
                "title": "Hydroxide"
            }, 
            {
                "snippet": "such \"native elements\" are copper, silver, gold, carbon (as coal, graphite, or diamonds), and sulfur. All but a few of the most inert elements, such as noble", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from July 2014\nArticles with unsourced statements from March 2014\nCS1 maint: Uses authors parameter\nChemical elements\nChemistry\nCommons category with local link same as on Wikidata\nUse dmy dates from July 2012\nWikipedia articles with GND identifiers\nWikipedia articles with LCCN identifiers", 
                "pageContent": "A chemical element or element is a species of atoms having the same number of protons in their atomic nuclei (i.e. the same atomic number, Z). There are 118 elements that have been identified, of which the first 94 occur naturally on Earth with the remaining 24 being synthetic elements. There are 80 elements that have at least one stable isotope and 38 that have exclusively radioactive isotopes, which decay over time into other elements. Iron is the most abundant element (by mass) making up Earth, while oxygen is the most common element in the crust of Earth.\nChemical elements constitute all of the ordinary matter of the universe. However astronomical observations suggest that ordinary observable matter is only approximately 15% of the matter in the universe: the remainder is dark matter, the composition of which is unknown, but it is not composed of chemical elements. The two lightest elements, hydrogen and helium were mostly formed in the Big Bang and are the most common elements in the universe. The next three elements (lithium, beryllium and boron) were formed mostly by cosmic ray spallation, and are thus more rare than those that follow. Formation of elements with from six to twenty six protons occurred and continues to occur in main sequence stars via stellar nucleosynthesis. The high abundance of oxygen, silicon, and iron on Earth reflects their common production in such stars. Elements with greater than twenty-six protons are formed by supernova nucleosynthesis in supernovae, which, when they explode, blast these elements far into space as supernova remnants, where they may become incorporated into planets when they are formed.\nThe term \"element\" is used for a kind of atoms with a given number of protons (regardless of whether they are or they are not ionized or chemically bonded, e.g. hydrogen in water) as well as for a pure chemical substance consisting of a single element (e.g. hydrogen gas). For the second meaning, the terms \"elementary substance\" and \"simple substance\" have been suggested, but they have not gained much acceptance in the English-language chemical literature, whereas in some other languages their equivalent is widely used (e.g. French corps simple, Russian \u043f\u0440\u043e\u0441\u0442\u043e\u0435 \u0432\u0435\u0449\u0435\u0441\u0442\u0432\u043e). One element can form multiple substances different by their structure; they are called allotropes of the element.\nWhen different elements are chemically combined, with the atoms held together by chemical bonds, they form chemical compounds. Only a minority of elements are found uncombined as relatively pure minerals. Among the more common of such \"native elements\" are copper, silver, gold, carbon (as coal, graphite, or diamonds), and sulfur. All but a few of the most inert elements, such as noble gases and noble metals, are usually found on Earth in chemically combined form, as chemical compounds. While about 32 of the chemical elements occur on Earth in native uncombined forms, most of these occur as mixtures. For example, atmospheric air is primarily a mixture of nitrogen, oxygen, and argon, and native solid elements occur in alloys, such as that of iron and nickel.\nThe history of the discovery and use of the elements began with primitive human societies that found native elements like carbon, sulfur, copper and gold. Later civilizations extracted elemental copper, tin, lead and iron from their ores by smelting, using charcoal. Alchemists and chemists subsequently identified many more, with almost all of the naturally-occurring elements becoming known by 1900.\nThe properties of the chemical elements are summarized on the periodic table, which organizes the elements by increasing atomic number into rows (\"periods\") in which the columns (\"groups\") share recurring (\"periodic\") physical and chemical properties. Save for unstable radioactive elements with short half-lives, all of the elements are available industrially, most of them in high degrees of purity.\n\n\n== Description ==\nThe lightest chemical elements are hydrogen and helium, both created by Big Bang nucleosynthesis during the first 20 minutes of the universe in a ratio of around 3:1 by mass (or 12:1 by number of atoms), along with tiny traces of the next two elements, lithium and beryllium. Almost all other elements found in nature were made by various natural methods of nucleosynthesis. On Earth, small amounts of new atoms are naturally produced in nucleogenic reactions, or in cosmogenic processes, such as cosmic ray spallation. New atoms are also naturally produced on Earth as radiogenic daughter isotopes of ongoing radioactive decay processes such as alpha decay, beta decay, spontaneous fission, cluster decay, and other rarer modes of decay.\nOf the 94 naturally occurring elements, those with atomic numbers 1 through 82 each have at least one stable isotope (except for technetium, element 43 and promethium, element 61, which have no stable isotopes). Isotopes considered stable are those for which no radioactive decay has yet been observed. Elements with atomic numbers 83 through 94 are unstable to the point that radioactive decay of all isotopes can be detected. Some of these elements, notably bismuth (atomic number 83), thorium (atomic number 90), and uranium (atomic number 92), have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy elements before the formation of our solar system. At over 1.9\u00d71019 years, over a billion times longer than the current estimated age of the universe, bismuth-209 (atomic number 83) has the longest known alpha decay half-life of any naturally occurring element, and is almost always considered on par with the 80 stable elements. The very heaviest elements (those beyond plutonium, element 94) undergo radioactive decay with half-lives so short that they are not found in nature and must be synthesized.\nAs of 2010, there are 118 known elements (in this context, \"known\" means observed well enough, even from just a few decay products, to have been differentiated from other elements). Of these 118 elements, 94 occur naturally on Earth. Six of these occur in extreme trace quantities: technetium, atomic number 43; promethium, number 61; astatine, number 85; francium, number 87; neptunium, number 93; and plutonium, number 94. These 94 elements have been detected in the universe at large, in the spectra of stars and also supernovae, where short-lived radioactive elements are newly being made. The first 94 elements have been detected directly on Earth as primordial nuclides present from the formation of the solar system, or as naturally-occurring fission or transmutation products of uranium and thorium.\nThe remaining 24 heavier elements, not found today either on Earth or in astronomical spectra, have been produced artificially: these are all radioactive, with very short half-lives; if any atoms of these elements were present at the formation of Earth, they are extremely likely, to the point of certainty, to have already decayed, and if present in novae, have been in quantities too small to have been noted. Technetium was the first purportedly non-naturally occurring element synthesized, in 1937, although trace amounts of technetium have since been found in nature (and also the element may have been discovered naturally in 1925). This pattern of artificial production and later natural discovery has been repeated with several other radioactive naturally-occurring rare elements.\nLists of the elements are available by name, by symbol, by atomic number, by density, by melting point, and by boiling point as well as ionization energies of the elements. The nuclides of stable and radioactive elements are also available as a list of nuclides, sorted by length of half-life for those that are unstable. One of the most convenient, and certainly the most traditional presentation of the elements, is in the form of the periodic table, which groups together elements with similar chemical properties (and usually also similar electronic structures).\n\n\n=== Atomic number ===\n\nThe atomic number of an element is equal to the number of protons in each atom, and defines the element. For example, all carbon atoms contain 6 protons in their atomic nucleus; so the atomic number of carbon is 6. Carbon atoms may have different numbers of neutrons; atoms of the same element having different numbers of neutrons are known as isotopes of the element.\nThe number of protons in the atomic nucleus also determines its electric charge, which in turn determines the number of electrons of the atom in its non-ionized state. The electrons are placed into atomic orbitals that determine the atom's various chemical properties. The number of neutrons in a nucleus usually has very little effect on an element's chemical properties (except in the case of hydrogen and deuterium). Thus, all carbon isotopes have nearly identical chemical properties because they all have six protons and six electrons, even though carbon atoms may, for example, have 6 or 8 neutrons. That is why the atomic number, rather than mass number or atomic weight, is considered the identifying characteristic of a chemical element.\nThe symbol for atomic number is Z.\n\n\n=== Isotopes ===\n\nIsotopes are atoms of the same element (that is, with the same number of protons in their atomic nucleus), but having different numbers of neutrons. Most (66 of 94) naturally occurring elements have more than one stable isotope. Thus, for example, there are three main isotopes of carbon. All carbon atoms have 6 protons in the nucleus, but they can have either 6, 7, or 8 neutrons. Since the mass numbers of these are 12, 13 and 14 respectively, the three isotopes of carbon are known as carbon-12, carbon-13, and carbon-14, often abbreviated to 12C, 13C, and 14C. Carbon in everyday life and in chemistry is a mixture of 12C (about 98.9%), 13C (about 1.1%) and about 1 atom per trillion of 14C.\nExcept in the case of the isotopes of hydrogen (which differ greatly from each other in relative mass\u2014enough to cause chemical effects), the isotopes of a given element are chemically nearly indistinguishable.\nAll of the elements have some isotopes that are radioactive (radioisotopes), although not all of these radioisotopes occur naturally. The radioisotopes typically decay into other elements upon radiating an alpha or beta particle. If an element has isotopes that are not radioactive, these are termed \"stable\" isotopes. All of the known stable isotopes occur naturally (see primordial isotope). The many radioisotopes that are not found in nature have been characterized after being artificially made. Certain elements have no stable isotopes and are composed only of radioactive isotopes: specifically the elements without any stable isotopes are technetium (atomic number 43), promethium (atomic number 61), and all observed elements with atomic numbers greater than 82.\nOf the 80 elements with at least one stable isotope, 26 have only one single stable isotope. The mean number of stable isotopes for the 80 stable elements is 3.1 stable isotopes per element. The largest number of stable isotopes that occur for a single element is 10 (for tin, element 50).\n\n\n=== Isotopic mass and atomic mass ===\n\nThe mass number of an element, A, is the number of nucleons (protons and neutrons) in the atomic nucleus. Different isotopes of a given element are distinguished by their mass numbers, which are conventionally written as a superscript on the left hand side of the atomic symbol (e.g., 238U). The mass number is always a simple whole number and has units of \"nucleons.\" An example of a referral to a mass number is \"magnesium-24,\" which is an atom with 24 nucleons (12 protons and 12 neutrons).\nWhereas the mass number simply counts the total number of neutrons and protons and is thus a natural (or whole) number, the atomic mass of a single atom is a real number for the mass of a particular isotope of the element, the unit being u. In general, when expressed in u it differs in value slightly from the mass number for a given nuclide (or isotope) since the mass of the protons and neutrons is not exactly 1 u, since the electrons contribute a lesser share to the atomic mass as neutron number exceeds proton number, and (finally) because of the nuclear binding energy. For example, the atomic mass of chlorine-35 to five significant digits is 34.969 u and that of chlorine-37 is 36.966 u. However, the atomic mass in u of each isotope is quite close to its simple mass number (always within 1%). The only isotope whose atomic mass is exactly a natural number is 12C, which by definition has a mass of exactly 12, because u is defined as 1/12 of the mass of a free neutral carbon-12 atom in the ground state.\nThe relative atomic mass (historically and commonly also called \"atomic weight\") of an element is the average of the atomic masses of all the chemical element's isotopes as found in a particular environment, weighted by isotopic abundance, relative to the atomic mass unit (u). This number may be a fraction that is not close to a whole number, due to the averaging process. For example, the relative atomic mass of chlorine is 35.453 u, which differs greatly from a whole number due to being made of an average of 76% chlorine-35 and 24% chlorine-37. Whenever a relative atomic mass value differs by more than 1% from a whole number, it is due to this averaging effect resulting from significant amounts of more than one isotope being naturally present in the sample of the element in question.\n\n\n=== Chemically pure and isotopically pure ===\nChemists and nuclear scientists have different definitions of a pure element. In chemistry, a pure element means a substance whose atoms all (or in practice almost all) have the same atomic number, or number of protons. Nuclear scientists, however, define a pure element as one that consists of only one stable isotope.\nFor example, a copper wire is 99.99% chemically pure if 99.99% of its atoms are copper, with 29 protons each. However it is not isotopically pure since ordinary copper consists of two stable isotopes, 69% 63Cu and 31% 65Cu, with different numbers of neutrons. However, a pure gold ingot would be both chemically and isotopically pure, since ordinary gold consists only of one isotope, 197Au.\n\n\n=== Allotropes ===\n\nAtoms of chemically pure elements may bond to each other chemically in more than one way, allowing the pure element to exist in multiple structures (spatial arrangements of atoms), known as allotropes, which differ in their properties. For example, carbon can be found as diamond, which has a tetrahedral structure around each carbon atom; graphite, which has layers of carbon atoms with a hexagonal structure stacked on top of each other; graphene, which is a single layer of graphite that is very strong; fullerenes, which have nearly spherical shapes; and carbon nanotubes, which are tubes with a hexagonal structure (even these may differ from each other in electrical properties). The ability of an element to exist in one of many structural forms is known as 'allotropy'.\nThe standard state, also known as reference state, of an element is defined as its thermodynamically most stable state at 1 bar at a given temperature (typically at 298.15 K). In thermochemistry, an element is defined to have an enthalpy of formation of zero in its standard state. For example, the reference state for carbon is graphite, because the structure of graphite is more stable than that of the other allotropes.\n\n\n=== Properties ===\nSeveral kinds of descriptive categorizations can be applied broadly to the elements, including consideration of their general physical and chemical properties, their states of matter under familiar conditions, their melting and boiling points, their densities, their crystal structures as solids, and their origins.\n\n\n==== General properties ====\nSeveral terms are commonly used to characterize the general physical and chemical properties of the chemical elements. A first distinction is between metals, which readily conduct electricity, nonmetals, which do not, and a small group, (the metalloids), having intermediate properties and often behaving as semiconductors.\nA more refined classification is often shown in colored presentations of the periodic table. This system restricts the terms \"metal\" and \"nonmetal\" to only certain of the more broadly defined metals and nonmetals, adding additional terms for certain sets of the more broadly viewed metals and nonmetals. The version of this classification used in the periodic tables presented here includes: actinides, alkali metals, alkaline earth metals, halogens, lanthanides, transition metals, post-transition metals, metalloids, polyatomic nonmetals, diatomic nonmetals, and noble gases. In this system, the alkali metals, alkaline earth metals, and transition metals, as well as the lanthanides and the actinides, are special groups of the metals viewed in a broader sense. Similarly, the polyatomic nonmetals, diatomic nonmetals and the noble gases are nonmetals viewed in the broader sense. In some presentations, the halogens are not distinguished, with astatine identified as a metalloid and the others identified as nonmetals.\n\n\n==== States of matter ====\nAnother commonly used basic distinction among the elements is their state of matter (phase), whether solid, liquid, or gas, at a selected standard temperature and pressure (STP). Most of the elements are solids at conventional temperatures and atmospheric pressure, while several are gases. Only bromine and mercury are liquids at 0 degrees Celsius (32 degrees Fahrenheit) and normal atmospheric pressure; caesium and gallium are solids at that temperature, but melt at 28.4 \u00b0C (83.2 \u00b0F) and 29.8 \u00b0C (85.6 \u00b0F), respectively.\n\n\n==== Melting and boiling points ====\nMelting and boiling points, typically expressed in degrees Celsius at a pressure of one atmosphere, are commonly used in characterizing the various elements. While known for most elements, either or both of these measurements is still undetermined for some of the radioactive elements available in only tiny quantities. Since helium remains a liquid even at absolute zero at atmospheric pressure, it has only a boiling point, and not a melting point, in conventional presentations.\n\n\n==== Densities ====\n\nThe density at a selected standard temperature and pressure (STP) is frequently used in characterizing the elements. Density is often expressed in grams per cubic centimeter (g/cm3). Since several elements are gases at commonly encountered temperatures, their densities are usually stated for their gaseous forms; when liquefied or solidified, the gaseous elements have densities similar to those of the other elements.\nWhen an element has allotropes with different densities, one representative allotrope is typically selected in summary presentations, while densities for each allotrope can be stated where more detail is provided. For example, the three familiar allotropes of carbon (amorphous carbon, graphite, and diamond) have densities of 1.8\u20132.1, 2.267, and 3.515 g/cm3, respectively.\n\n\n==== Crystal structures ====\nThe elements studied to date as solid samples have eight kinds of crystal structures: cubic, body-centered cubic, face-centered cubic, hexagonal, monoclinic, orthorhombic, rhombohedral, and tetragonal. For some of the synthetically produced transuranic elements, available samples have been too small to determine crystal structures.\n\n\n==== Occurrence and origin on Earth ====\nChemical elements may also be categorized by their origin on Earth, with the first 94 considered naturally occurring, while those with atomic numbers beyond 94 have only been produced artificially as the synthetic products of man-made nuclear reactions.\nOf the 94 naturally occurring elements, 84 are considered primordial and either stable or weakly radioactive. The remaining 10 naturally occurring elements possess half lives too short for them to have been present at the beginning of the Solar System, and are therefore considered transient elements. (Plutonium is usually also considered a transient element because primordial plutonium has by now decayed to almost undetectable traces.) Of these 10 transient elements, 5 (polonium, radon, radium, actinium, and protactinium) are relatively common decay products of thorium, uranium, and plutonium. The remaining 6 transient elements (technetium, promethium, astatine, francium, neptunium, and plutonium) occur only rarely, as products of rare decay modes or nuclear reaction processes involving uranium or other heavy elements.\nElements with atomic numbers 1 through 40 are all stable, while those with atomic numbers 41 through 82 (except technetium and promethium) are metastable. The half-lives of these metastable \"theoretical radionuclides\" are so long (at least 100 million times longer than the estimated age of the universe) that their radioactive decay has yet to be detected by experiment. Elements with atomic numbers 83 through 94 are unstable to the point that their radioactive decay can be detected. Three of these elements, bismuth (element 83), thorium (element 90), and uranium (element 92) have one or more isotopes with half-lives long enough to survive as remnants of the explosive stellar nucleosynthesis that produced the heavy elements before the formation of our solar system. For example, at over 1.9\u00d71019 years, over a billion times longer than the current estimated age of the universe, bismuth-209 has the longest known alpha decay half-life of any naturally occurring element. The very heaviest 24 elements (those beyond plutonium, element 94) undergo radioactive decay with short half-lives and cannot be produced as daughters of longer-lived elements, and thus they do not occur in nature at all.\n\n\n=== The periodic table ===\n\nThe properties of the chemical elements are often summarized using the periodic table, which powerfully and elegantly organizes the elements by increasing atomic number into rows (\"periods\") in which the columns (\"groups\") share recurring (\"periodic\") physical and chemical properties. The current standard table contains 118 confirmed elements as of 10 April 2010.\nAlthough earlier precursors to this presentation exist, its invention is generally credited to the Russian chemist Dmitri Mendeleev in 1869, who intended the table to illustrate recurring trends in the properties of the elements. The layout of the table has been refined and extended over time as new elements have been discovered and new theoretical models have been developed to explain chemical behavior.\nUse of the periodic table is now ubiquitous within the academic discipline of chemistry, providing an extremely useful framework to classify, systematize and compare all the many different forms of chemical behavior. The table has also found wide application in physics, geology, biology, materials science, engineering, agriculture, medicine, nutrition, environmental health, and astronomy. Its principles are especially important in chemical engineering.\n\n\n== Nomenclature and symbols ==\n The various chemical elements are formally identified by their unique atomic numbers, by their accepted names, and by their symbols.\n\n\n=== Atomic numbers ===\nThe known elements have atomic numbers from 1 through 118, conventionally presented as Arabic numerals. Since the elements can be uniquely sequenced by atomic number, conventionally from lowest to highest (as in a periodic table), sets of elements are sometimes specified by such notation as \"through\", \"beyond\", or \"from ... through\", as in \"through iron\", \"beyond uranium\", or \"from lanthanum through lutetium\". The terms \"light\" and \"heavy\" are sometimes also used informally to indicate relative atomic numbers (not densities), as in \"lighter than carbon\" or \"heavier than lead\", although technically the weight or mass of atoms of an element (their atomic weights or atomic masses) do not always increase monotonically with their atomic numbers.\n\n\n=== Element names ===\n\nThe naming of various substances now known as elements precedes the atomic theory of matter, as names were given locally by various cultures to various minerals, metals, compounds, alloys, mixtures, and other materials, although at the time it was not known which chemicals were elements and which compounds. As they were identified as elements, the existing names for anciently-known elements (e.g., gold, mercury, iron) were kept in most countries. National differences emerged over the names of elements either for convenience, linguistic niceties, or nationalism. For a few illustrative examples: German speakers use \"Wasserstoff\" (water substance) for \"hydrogen\", \"Sauerstoff\" (acid substance) for \"oxygen\" and \"Stickstoff\" (smothering substance) for \"nitrogen\", while English and some romance languages use \"sodium\" for \"natrium\" and \"potassium\" for \"kalium\", and the French, Italians, Greeks, Portuguese and Poles prefer \"azote/azot/azoto\" (from roots meaning \"no life\") for \"nitrogen\".\nFor purposes of international communication and trade, the official names of the chemical elements both ancient and more recently recognized are decided by the International Union of Pure and Applied Chemistry (IUPAC), which has decided on a sort of international English language, drawing on traditional English names even when an element's chemical symbol is based on a Latin or other traditional word, for example adopting \"gold\" rather than \"aurum\" as the name for the 79th element (Au). IUPAC prefers the British spellings \"aluminium\" and \"caesium\" over the U.S. spellings \"aluminum\" and \"cesium\", and the U.S. \"sulfur\" over the British \"sulphur\". However, elements that are practical to sell in bulk in many countries often still have locally used national names, and countries whose national language does not use the Latin alphabet are likely to use the IUPAC element names.\nAccording to IUPAC, chemical elements are not proper nouns in English; consequently, the full name of an element is not routinely capitalized in English, even if derived from a proper noun, as in californium and einsteinium. Isotope names of chemical elements are also uncapitalized if written out, e.g., carbon-12 or uranium-235. Chemical element symbols (such as Cf for californium and Es for einsteinium), are always capitalized (see below).\nIn the second half of the twentieth century, physics laboratories became able to produce nuclei of chemical elements with half-lives too short for an appreciable amount of them to exist at any time. These are also named by IUPAC, which generally adopts the name chosen by the discoverer. This practice can lead to the controversial question of which research group actually discovered an element, a question that delayed the naming of elements with atomic number of 104 and higher for a considerable amount of time. (See element naming controversy).\nPrecursors of such controversies involved the nationalistic namings of elements in the late 19th century. For example, lutetium was named in reference to Paris, France. The Germans were reluctant to relinquish naming rights to the French, often calling it cassiopeium. Similarly, the British discoverer of niobium originally named it columbium, in reference to the New World. It was used extensively as such by American publications prior to the international standardization (in 1950).\n\n\n=== Chemical symbols ===\n\n\n==== Specific chemical elements ====\nBefore chemistry became a science, alchemists had designed arcane symbols for both metals and common compounds. These were however used as abbreviations in diagrams or procedures; there was no concept of atoms combining to form molecules. With his advances in the atomic theory of matter, John Dalton devised his own simpler symbols, based on circles, to depict molecules.\nThe current system of chemical notation was invented by Berzelius. In this typographical system, chemical symbols are not mere abbreviations\u2014though each consists of letters of the Latin alphabet. They are intended as universal symbols for people of all languages and alphabets.\nThe first of these symbols were intended to be fully universal. Since Latin was the common language of science at that time, they were abbreviations based on the Latin names of metals. Cu comes from Cuprum, Fe comes from Ferrum, Ag from Argentum. The symbols were not followed by a period (full stop) as with abbreviations. Later chemical elements were also assigned unique chemical symbols, based on the name of the element, but not necessarily in English. For example, sodium has the chemical symbol 'Na' after the Latin natrium. The same applies to \"W\" (wolfram) for tungsten, \"Fe\" (ferrum) for iron, \"Hg\" (hydrargyrum) for mercury, \"Sn\" (stannum) for tin, \"K\" (kalium) for potassium, \"Au\" (aurum) for gold, \"Ag\" (argentum) for silver, \"Pb\" (plumbum) for lead, \"Cu\" (cuprum) for copper, and \"Sb\" (stibium) for antimony.\nChemical symbols are understood internationally when element names might require translation. There have sometimes been differences in the past. For example, Germans in the past have used \"J\" (for the alternate name Jod) for iodine, but now use \"I\" and \"Iod\".\nThe first letter of a chemical symbol is always capitalized, as in the preceding examples, and the subsequent letters, if any, are always lower case (small letters). Thus, the symbols for californium or einsteinium are Cf and Es.\n\n\n==== General chemical symbols ====\nThere are also symbols in chemical equations for groups of chemical elements, for example in comparative formulas. These are often a single capital letter, and the letters are reserved and not used for names of specific elements. For example, an \"X\" indicates a variable group (usually a halogen) in a class of compounds, while \"R\" is a radical, meaning a compound structure such as a hydrocarbon chain. The letter \"Q\" is reserved for \"heat\" in a chemical reaction. \"Y\" is also often used as a general chemical symbol, although it is also the symbol of yttrium. \"Z\" is also frequently used as a general variable group. \"E\" is used in organic chemistry to denote an electron-withdrawing group or an electrophile; similarly \"Nu\" denotes a nucleophile. \"L\" is used to represent a general ligand in inorganic and organometallic chemistry. \"M\" is also often used in place of a general metal.\nAt least two additional, two-letter generic chemical symbols are also in informal usage, \"Ln\" for any lanthanide element and \"An\" for any actinide element. \"Rg\" was formerly used for any rare gas element, but the group of rare gases has now been renamed noble gases and the symbol \"Rg\" has now been assigned to the element roentgenium.\n\n\n==== Isotope symbols ====\nIsotopes are distinguished by the atomic mass number (total protons and neutrons) for a particular isotope of an element, with this number combined with the pertinent element's symbol. IUPAC prefers that isotope symbols be written in superscript notation when practical, for example 12C and 235U. However, other notations, such as carbon-12 and uranium-235, or C-12 and U-235, are also used.\nAs a special case, the three naturally occurring isotopes of the element hydrogen are often specified as H for 1H (protium), D for 2H (deuterium), and T for 3H (tritium). This convention is easier to use in chemical equations, replacing the need to write out the mass number for each atom. For example, the formula for heavy water may be written D2O instead of 2H2O.\n\n\n== Origin of the elements ==\n\nOnly about 4% of the total mass of the universe is made of atoms or ions, and thus represented by chemical elements. This fraction is about 15% of the total matter, with the remainder of the matter (85%) being dark matter. The nature of dark matter is unknown, but it is not composed of atoms of chemical elements because it contains no protons, neutrons, or electrons. (The remaining non-matter part of the mass of the universe is composed of the even more mysterious dark energy).\nThe universe's 94 naturally occurring chemical elements are thought to have been produced by at least four cosmic processes. Most of the hydrogen and helium in the universe was produced primordially in the first few minutes of the Big Bang. Three recurrently occurring later processes are thought to have produced the remaining elements. Stellar nucleosynthesis, an ongoing process, produces all elements from carbon through iron in atomic number, but little lithium, beryllium, or boron. Elements heavier in atomic number than iron, as heavy as uranium and plutonium, are produced by explosive nucleosynthesis in supernovas and other cataclysmic cosmic events. Cosmic ray spallation (fragmentation) of carbon, nitrogen, and oxygen is important to the production of lithium, beryllium and boron.\nDuring the early phases of the Big Bang, nucleosynthesis of hydrogen nuclei resulted in the production of hydrogen-1 (protium, 1H) and helium-4 (4He), as well as a smaller amount of deuterium (2H) and very minuscule amounts (on the order of 10\u221210) of lithium and beryllium. Even smaller amounts of boron may have been produced in the Big Bang, since it has been observed in some very old stars, while carbon has not. It is generally agreed that no heavier elements than boron were produced in the Big Bang. As a result, the primordial abundance of atoms (or ions) consisted of roughly 75% 1H, 25% 4He, and 0.01% deuterium, with only tiny traces of lithium, beryllium, and perhaps boron. Subsequent enrichment of galactic halos occurred due to stellar nucleosynthesis and supernova nucleosynthesis. However, the element abundance in intergalactic space can still closely resemble primordial conditions, unless it has been enriched by some means.\n\nOn Earth (and elsewhere), trace amounts of various elements continue to be produced from other elements as products of natural transmutation processes. These include some produced by cosmic rays or other nuclear reactions (see cosmogenic and nucleogenic nuclides), and others produced as decay products of long-lived primordial nuclides. For example, trace (but detectable) amounts of carbon-14 (14C) are continually produced in the atmosphere by cosmic rays impacting nitrogen atoms, and argon-40 (40Ar) is continually produced by the decay of primordially occurring but unstable potassium-40 (40K). Also, three primordially occurring but radioactive actinides, thorium, uranium, and plutonium, decay through a series of recurrently produced but unstable radioactive elements such as radium and radon, which are transiently present in any sample of these metals or their ores or compounds. Three other radioactive elements, technetium, promethium, and neptunium, occur only incidentally in natural materials, produced as individual atoms by natural fission of the nuclei of various heavy elements or in other rare nuclear processes.\nHuman technology has produced various additional elements beyond these first 94, with those through atomic number 118 now known.\n\n\n== Abundance ==\n\nThe following graph (note log scale) shows the abundance of elements in our solar system. The table shows the twelve most common elements in our galaxy (estimated spectroscopically), as measured in parts per million, by mass. Nearby galaxies that have evolved along similar lines have a corresponding enrichment of elements heavier than hydrogen and helium. The more distant galaxies are being viewed as they appeared in the past, so their abundances of elements appear closer to the primordial mixture. As physical laws and processes appear common throughout the visible universe, however, scientist expect that these galaxies evolved elements in similar abundance.\nThe abundance of elements in the Solar System is in keeping with their origin from nucleosynthesis in the Big Bang and a number of progenitor supernova stars. Very abundant hydrogen and helium are products of the Big Bang, but the next three elements are rare since they had little time to form in the Big Bang and are not made in stars (they are, however, produced in small quantities by the breakup of heavier elements in interstellar dust, as a result of impact by cosmic rays). Beginning with carbon, elements are produced in stars by buildup from alpha particles (helium nuclei), resulting in an alternatingly larger abundance of elements with even atomic numbers (these are also more stable). In general, such elements up to iron are made in large stars in the process of becoming supernovas. Iron-56 is particularly common, since it is the most stable element that can easily be made from alpha particles (being a product of decay of radioactive nickel-56, ultimately made from 14 helium nuclei). Elements heavier than iron are made in energy-absorbing processes in large stars, and their abundance in the universe (and on Earth) generally decreases with their atomic number.\nThe abundance of the chemical elements on Earth varies from air to crust to ocean, and in various types of life. The abundance of elements in Earth's crust differs from that in the Solar system (as seen in the Sun and heavy planets like Jupiter) mainly in selective loss of the very lightest elements (hydrogen and helium) and also volatile neon, carbon (as hydrocarbons), nitrogen and sulfur, as a result of solar heating in the early formation of the solar system. Oxygen, the most abundant Earth element by mass, is retained on Earth by combination with silicon. Aluminum at 8% by mass is more common in the Earth's crust than in the universe and solar system, but the composition of the far more bulky mantle, which has magnesium and iron in place of aluminum (which occurs there only at 2% of mass) more closely mirrors the elemental composition of the solar system, save for the noted loss of volatile elements to space, and loss of iron which has migrated to the Earth's core.\nThe composition of the human body, by contrast, more closely follows the composition of seawater\u2014save that the human body has additional stores of carbon and nitrogen necessary to form the proteins and nucleic acids, together with phosphorus in the nucleic acids and energy transfer molecule adenosine triphosphate (ATP) that occurs in the cells of all living organisms. Certain kinds of organisms require particular additional elements, for example the magnesium in chlorophyll in green plants, the calcium in mollusc shells, or the iron in the hemoglobin in vertebrate animals' red blood cells.\n\n\n== History ==\n\n\n=== Evolving definitions ===\nThe concept of an \"element\" as an undivisible substance has developed through three major historical phases: Classical definitions (such as those of the ancient Greeks), chemical definitions, and atomic definitions.\n\n\n==== Classical definitions ====\nAncient philosophy posited a set of classical elements to explain observed patterns in nature. These elements originally referred to earth, water, air and fire rather than the chemical elements of modern science.\nThe term 'elements' (stoicheia) was first used by the Greek philosopher Plato in about 360 BCE in his dialogue Timaeus, which includes a discussion of the composition of inorganic and organic bodies and is a speculative treatise on chemistry. Plato believed the elements introduced a century earlier by Empedocles were composed of small polyhedral forms: tetrahedron (fire), octahedron (air), icosahedron (water), and cube (earth).\nAristotle, c. 350 BCE, also used the term stoicheia and added a fifth element called aether, which formed the heavens. Aristotle defined an element as:\n\nElement \u2013 one of those bodies into which other bodies can decompose, and that itself is not capable of being divided into other.\n\n\n==== Chemical definitions ====\nIn 1661, Robert Boyle proposed his theory of corpuscularism which favoured the analysis of matter as constituted by irreducible units of matter (atoms) and, choosing to side with neither Aristotle's view of the four elements nor Paracelsus' view of three fundamental elements, left open the question of the number of elements. The first modern list of chemical elements was given in Antoine Lavoisier's 1789 Elements of Chemistry, which contained thirty-three elements, including light and caloric. By 1818, J\u00f6ns Jakob Berzelius had determined atomic weights for forty-five of the forty-nine then-accepted elements. Dmitri Mendeleev had sixty-six elements in his periodic table of 1869.\n\nFrom Boyle until the early 20th century, an element was defined as a pure substance that could not be decomposed into any simpler substance. Put another way, a chemical element cannot be transformed into other chemical elements by chemical processes. Elements during this time were generally distinguished by their atomic weights, a property measurable with fair accuracy by available analytical techniques.\n\n\n==== Atomic definitions ====\n\nThe 1913 discovery by English physicist Henry Moseley that the nuclear charge is the physical basis for an atom's atomic number, further refined when the nature of protons and neutrons became appreciated, eventually led to the current definition of an element based on atomic number (number of protons per atomic nucleus). The use of atomic numbers, rather than atomic weights, to distinguish elements has greater predictive value (since these numbers are integers), and also resolves some ambiguities in the chemistry-based view due to varying properties of isotopes and allotropes within the same element. Currently, IUPAC defines an element to exist if it has isotopes with a lifetime longer than the 10\u221214 seconds it takes the nucleus to form an electronic cloud.\nBy 1914, seventy-two elements were known, all naturally occurring. The remaining naturally occurring elements were discovered or isolated in subsequent decades, and various additional elements have also been produced synthetically, with much of that work pioneered by Glenn T. Seaborg. In 1955, element 101 was discovered and named mendelevium in honor of D.I. Mendeleev, the first to arrange the elements in a periodic manner. Most recently, the synthesis of element 118 was reported in October 2006, and the synthesis of element 117 was reported in April 2010.\n\n\n=== Discovery and recognition of various elements ===\n\nTen materials familiar to various prehistoric cultures are now known to be chemical elements: Carbon, copper, gold, iron, lead, mercury, silver, sulfur, tin, and zinc. Three additional materials now accepted as elements, arsenic, antimony, and bismuth, were recognized as distinct substances prior to 1500 AD. Phosphorus, cobalt, and platinum were isolated before 1750.\nMost of the remaining naturally occurring chemical elements were identified and characterized by 1900, including:\nSuch now-familiar industrial materials as aluminium, silicon, nickel, chromium, magnesium, and tungsten\nReactive metals such as lithium, sodium, potassium, and calcium\nThe halogens fluorine, chlorine, bromine, and iodine\nGases such as hydrogen, oxygen, nitrogen, helium, argon, and neon\nMost of the rare-earth elements, including cerium, lanthanum, gadolinium, and neodymium.\nThe more common radioactive elements, including uranium, thorium, radium, and radon\nElements isolated or produced since 1900 include:\nThe three remaining undiscovered regularly occurring stable natural elements: hafnium, lutetium, and rhenium\nPlutonium, which was first produced synthetically in 1940 by Glenn T. Seaborg, but is now also known from a few long-persisting natural occurrences\nThe three incidentally occurring natural elements (neptunium, promethium, and technetium), which were all first produced synthetically but later discovered in trace amounts in certain geological samples\nThree scarce decay products of uranium or thorium, (astatine, francium, and protactinium), and\nVarious synthetic transuranic elements, beginning with americium and curium\n\n\n=== Recently discovered elements ===\nThe first transuranium element (element with atomic number greater than 92) discovered was neptunium in 1940. Since 1999 claims for the discovery of new elements have been considered by the IUPAC/IUPAP Joint Working Party. As of January 2016, all 118 elements have been confirmed as discovered by IUPAC. The discovery of element 112 was acknowledged in 2009, and the name copernicium and the atomic symbol Cn were suggested for it. The name and symbol were officially endorsed by IUPAC on 19 February 2010. The heaviest element that is believed to have been synthesized to date is element 118, ununoctium, on 9 October 2006, by the Flerov Laboratory of Nuclear Reactions in Dubna, Russia. Element 117 was the latest element claimed to be discovered, in 2009. IUPAC officially recognized flerovium and livermorium, elements 114 and 116, in June 2011 and approved their names in May 2012. In December 2015, IUPAC recognized elements 113, 115, 117 and 118, and announced the elements' proposed final names on 8 June 2016. The names, nihonium (113, Nh), moscovium (115, Mc), tennessine (117, Ts), and oganesson (118, Og), are expected to be approved by the end of 2016.\n\n\n== List of the 118 known chemical elements ==\nThe following sortable table shows the 118 known chemical elements.\nAtomic number, name, and symbol all serve independently as unique identifiers.\nNames are those accepted by IUPAC; provisional names for recently produced elements not yet formally named are in parentheses.\nGroup, period, and block refer to an element's position in the periodic table. Group numbers here show the currently accepted numbering; for older alternate numberings, see Group (periodic table).\nState of matter (solid, liquid, or gas) applies at standard temperature and pressure conditions (STP).\nOccurrence distinguishes naturally occurring elements, categorized as either primordial or transient (from decay), and additional synthetic elements that have been produced technologically, but are not known to occur naturally.\nDescription summarizes an element's properties using the broad categories commonly presented in periodic tables: Actinide, alkali metal, alkaline earth metal, lanthanide, post-transition metal, metalloid, noble gas, polyatomic or diatomic nonmetal, and transition metal.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\nBall, P (2004). The Elements: A Very Short Introduction. Oxford University Press. ISBN 0-19-284099-1. \nEmsley, J (2003). Nature's Building Blocks: An A-Z Guide to the Elements. Oxford University Press. ISBN 0-19-850340-7. \nGray, T (2009). The Elements: A Visual Exploration of Every Known Atom in the Universe. Black Dog & Leventhal Publishers Inc. ISBN 1-57912-814-9. \nScerri, ER (2007). The Periodic Table, Its Story and Its Significance. Oxford University Press. \nStrathern, P (2000). Mendeleyev's Dream: The Quest for the Elements. Hamish Hamilton Ltd. ISBN 0-241-14065-X. \nKean, Sam (2011). The Disappearing Spoon: And Other True Tales of Madness, Love, and the History of the World from the Periodic Table of the Elements. Back Bay Books. \nCompiled by A. D. McNaught and A. Wilkinson. (1997). Blackwell Scientific Publications, Oxford, ed. Compendium of Chemical Terminology, 2nd ed. (the \"Gold Book\"). doi:10.1351/goldbook. ISBN 0-9678550-9-8.  CS1 maint: Uses authors parameter (link)\nXML on-line corrected version: created by M. Nic, J. Jirat, B. Kosata; updates compiled by A. Jenkins.\n\n\n== External links ==\nVideos for each element by the University of Nottingham", 
                "titleUrl": "https://en.wikipedia.org/wiki/Chemical_element", 
                "title": "Chemical element"
            }, 
            {
                "snippet": "The rock contains the smallest elements of mica minerals (glauconite), decomposed feldspar and carbon elements. The carbon particles are arranged in clearly", 
                "pageCategories": "Articles containing German-language text\nElbe Sandstone Mountains\nQuarries in Germany\nSandstone", 
                "pageContent": "Cotta Sandstone (German: Cottaer Sandstein, also called Mittelquader), is found in the Elbe Valley and in its numerous tributary valleys. Its main deposit lies in the west of the Elbe Sandstone Mountains, where it runs up to the Bohemian border, ending south of Pirna. It is named after the village of Cotta in the borough of Dohma, an area where the stone is quarried.\n\n\n== Formation and properties ==\nCotta Sandstone was formed in the Cretaceous, in the Lower Turonian age. It is one of the Elbe sandstones and its colours range from whitish to grey and yellowish grey. In the south of the area Cotta Sandstone is medium-grained, whilst, in the north it is fine-grained. Around the village of Cotta itself the grain size is evenly sized at 0.1 to 0.22 millimetres and only very rarely as large as 0.3 millimetres. The rock contains the smallest elements of mica minerals (glauconite), decomposed feldspar and carbon elements. The carbon particles are arranged in clearly recognisable veins. They occasionally resemble marble textures.\nThe technical value of this natural stone varies considerably, because the quartz grains of Cotta Sandstone are frequentlysiliceously bonded, but it has many unevenly divided deposits of the phyllosilicates, illite and kaolinite.\n\n\n== Extraction ==\nThe stone is quarried in Dohma (Gro\u00df-Cotta), Bad Gottleuba-Berggie\u00dfh\u00fcbel (in the villages of Gottleuba and Berggie\u00dfh\u00fcbel), Langhennersdorf, Rottwerndorf, at Neundorf and Lohmgrund south of Pirna, in Gersdorf and Bahretal (Ottendorf), and in the Krippenbach valley. The quarrying of Elbe sandstones is made technically easier because of the separation of the beds with alternating outcrops and fissures, because the fissures are vertical and the beds run roughly at right angles to them. As a result, it is possible to cut rectangular blocks of unfinished stone. The thickness of the quarry-able sandstone beds varies from a \u00bd to 3 metres. The thickness of the deposits of Cotta Sandstone ranges between 50 and 80 metres.\n\n\n== Use ==\n\n\n=== General use ===\nFormerly the sandstone quarried near Langhennersdorf, Berggie\u00dfh\u00fcbel and Gersdorf, which was larger-grained, was cut not only for use as building or sculpting stone, but also for millstones. Today (2008) Cotta Sandstone is used for solid window and door frames, sculpture work and high-profile stonemasonry. It is especially used in restoration, but also in new structures. Its most important use is for sculptures.\n\n\n== Gallery ==\nArt-historic use of Cotta Stone\n\n\n== See also ==\nList of sandstones\nPosta Sandstone\nReinhardtsdorf Sandstone\nWehlen Sandstone\n\n\n== Sources ==\nW. Dienemann und O. Burre: Die nutzbaren Gesteine Deutschlands und ihre Lagerst\u00e4tten mit Ausnahme der Kohlen, Erze und Salze, Enke-Verlag, Stuttgart 1929.\nSiegfried Grunert: Der Elbsandstein: Vorkommen, Verwendung, Eigenschaften. In: Geologica Saxonica Journal of Central European Geology 52/53 (2007), p. 143-204 (Digitalisat)\n\n\n== External links ==\nPictures of Cotta Sandstone\nTechnical data for Cotta Sandstone\nInformation about Elbe Sandstone\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Cotta_Sandstone", 
                "title": "Cotta Sandstone"
            }, 
            {
                "snippet": "of carbonyl olefination.  Other bonds of carbon with elements in the periodic table:    Review: Carbon-Carbon Bond Formations Involving Organochromium(III)", 
                "pageCategories": "Organochromium compounds", 
                "pageContent": "Organochromium chemistry is a branch of organometallic chemistry that deals with organic compounds containing a chromium to carbon bond and their reactions. The field is of some relevance to organic synthesis. The relevant oxidation states for chromium range from -2 to +6. \n\n\n== History ==\nThe first organochromium compound was described in 1919 by Franz Hein. He treated phenylmagnesium bromide with chromium(III) chloride to give a new product (after hydrolysis) which he incorrectly identified as pentaphenyl chromium bromide (Ph5CrBr). Years later, in 1957 H.H. Zeiss et al. repeated Hein's experiments and correctly arrived at a cationic bisarene chromium sandwich compound (Ar2Cr+). Bis(benzene)chromium itself was discovered around the same time in 1956 by Ernst Otto Fischer by reaction of chromium(III) chloride, benzene and aluminum chloride. The related compound chromocene was discovered a few years earlier in 1953 also by Fischer.\n\nIn another development, Anet and Leblanc also in 1957 prepared a benzyl chromium solution from benzyl bromide and chromium(II) perchlorate. This reaction involves one-electron oxidative addition of the carbon-bromine bond, a process which was shown by Kochi to be a case of double single electron transfer, first to give the benzyl free radical and then to the benzyl anion.\n\nG. Wilke et al. introduced tris-(\u03b7-allyl)chromium in 1963 as an early Ziegler-Natta catalyst (but not successful in the long run) Chromocene compounds were first employed in ethylene polymerization in 1972 by Union Carbide and continue to be used today in the industrial production of high-density polyethylene.\nThe organochromium compound (phenylmethoxycarbene)pentacarbonylchromium, Ph(OCH3)C=Cr(CO)5 was the first carbene complex to be crystallographically characterized by Fischer in 1967 (now called a Fischer carbene). The first ever carbyne, this one also containing chromium, made its debut in 1973.\n\nThe first example of a proposed metal-metal quintuple bond is found in a compound of the type [CrAr]2, where Ar is a bulky aryl ligand.\n\n\n== Applications in organic synthesis ==\nAlthough organochromium chemistry is heavily employed in industrial catalysis, relatively few reagents have been developed for applications in organic synthesis. Two are the Nozaki-Hiyama-Kishi reaction (1977) (transmetallation with organonickel intermediate) and the Takai olefination (1986)(oxidation of Cr(II) to Cr(III) while replacing halogens). In a niche exploit, certain tricarbonyl(arene)chromium complexes display benzylic activation.\n\n\n== Organochromium compounds ==\nOrganochromium compounds can be divided into these broad compound classes:\nSandwich compounds: chromocenes Cp2Cr and Bis(benzene)chromium derivatives (ArH)2Cr. More commonly studied are half-sandwich complexes like (ArH)Cr(CO)3.\nChromium carbenes (R1)(R2)C::CrLn and carbynes (R:::CrLn)\nChromium(III) complexes RCrL5.\n\n\n== Ethylene polymerization and oligomerization ==\nChromium catalysts are important in ethylene polymerization. The Phillips catalyst is prepared by impregnating chromium(VI) oxide on silica followed activation in dry air at high temperatures. The bright yellow catalyst becomes reduced by the ethylene to afford a probable Cr(II) species that is catalytically active. A related catalytic systems developed by Union Carbide and DSM are also based on silica with chromocene and other chromium complexes. How these catalysts work is unclear. One model system describes it as coordination polymerization:\n\nWith two THF ligands the catalyst is stable but in dichloromethane one ligand is lost to form a 13 electron chromium intermediate. This enables side-on addition of an ethylene unit and a polymer chain can grow by migratory insertion.\nChromium compounds also catalyse the trimerization of ethylene to produce the monomer 1-hexene.\n\n\n== Comparisons with heavier group 6 organometallics ==\nThe heavier group 6 elements molybdenum and tungsten form organometallic compounds similar to those for chromium but also with differences. Whereas Cr(III) aquo alkyl compounds are well studied, the corresponding Mo(III) and W(III) compounds are not. Whereas chromocene is a stable compound, the related molybdenocene and tungstenocene are highly reactive. On the other hand, Mo and W readily form derivatives of the type Cp2MX2, whereas the smaller Cr does not form such clamshell compounds. Homoleptic alkyl and aryl complexes of the type R4M are rare, and hexamethyltungsten has no analogue in Cr chemistry.\nSimilar are the carbonyls such as molybdenum hexacarbonyl and tungsten hexacarbonyl and the related carbene and carbyne complexes. Compounds of the type [CpM(CO)3]2 are known for all three metals, e.g. Cyclopentadienylmolybdenum tricarbonyl dimer. The chromium compound is however prone to homolysis of the Cr-Cr bond owing to steric crowding.\nIn the Kauffmann olefination, molybdenum(III) chloride and methyllithium form an organometallic complex capable of carbonyl olefination.\n\n\n== See also ==\nOther bonds of carbon with elements in the periodic table:\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Organochromium_chemistry", 
                "title": "Organochromium chemistry"
            }, 
            {
                "snippet": "iron or steel absorbs carbon while the metal is heated in the presence of a carbon bearing material, such as charcoal or carbon monoxide. The intent is", 
                "pageCategories": "Metal heat treatments\nSteelmaking", 
                "pageContent": "Carburizing, carburising (chiefly British English), or carburization is a heat treatment process in which iron or steel absorbs carbon while the metal is heated in the presence of a carbon bearing material, such as charcoal or carbon monoxide. The intent is to make the metal harder. Depending on the amount of time and temperature, the affected area can vary in carbon content. Longer carburizing times and higher temperatures typically increase the depth of carbon diffusion. When the iron or steel is cooled rapidly by quenching, the higher carbon content on the outer surface becomes hard via the transformation from austenite to martensite, while the core remains soft and tough as a ferritic and/or pearlite microstructure.\nThis manufacturing process can be characterized by the following key points: It is applied to low-carbon workpieces; workpieces are in contact with a high-carbon gas, liquid or solid; it produces a hard workpiece surface; workpiece cores largely retain their toughness and ductility; and it produces case hardness depths of up to 0.25 inches (6.4 mm). In some cases it serves as a remedy for undesired decarburization that happened earlier in a manufacturing process.\n\n\n== Method ==\nCarburization of steel involves a heat treatment of the metallic surface using a source of carbon. Carburization can be used to increase the surface hardness of low carbon steel.\nEarly carburization used a direct application of charcoal packed around the sample to be treated (initially referred to as case hardening), but modern techniques use carbon-bearing gases or plasmas (such as carbon dioxide or methane). The process depends primarily upon ambient gas composition and furnace temperature, which must be carefully controlled, as the heat may also impact the microstructure of the remainder of the material. For applications where great control over gas composition is desired, carburization may take place under very low pressures in a vacuum chamber.\nPlasma carburization is increasingly used to improve the surface characteristics (such as wear, corrosion resistance, hardness, load-bearing capacity, in addition to quality-based variables) of various metals, notably stainless steels. The process is environmentally friendly (in comparison to gaseous or solid carburizing). It also provides an even treatment of components with complex geometry (the plasma can penetrate into holes and tight gaps), making it very flexible in terms of component treatment.\nThe process of carburization works via the diffusion of carbon atoms into the surface layers of a metal. As metals are made up of atoms bound tightly into a metallic crystalline lattice, the carbon atoms diffuse into the crystal structure of the metal and either remain in solution (dissolved within the metal crystalline matrix \u2014 this normally occurs at lower temperatures) or react with elements in the host metal to form carbides (normally at higher temperatures, due to the higher mobility of the host metal's atoms). If the carbon remains in solid solution, the steel is then heat treated to harden it. Both of these mechanisms strengthen the surface of the metal, the former by forming pearlite or martensite, and the latter via the formation of carbides. Both of these materials are hard and resist abrasion.\nGas carburizing is normally carried out at a temperature within the range of 900 to 950 \u00b0C.\nIn oxy-acetylene welding, a carburizing flame is one with little oxygen, which produces a sooty, lower-temperature flame. It is often used to anneal metal, making it more malleable and flexible during the welding process.\nA main goal when producing carburized workpieces is to ensure maximum contact between the workpiece surface and the carbon-rich elements. In gas and liquid carburizing, the workpieces are often supported in mesh baskets or suspended by wire. In pack carburizing, the workpiece and carbon are enclosed in a container to ensure that contact is maintained over as much surface area as possible. Pack carburizing containers are usually made of carbon steel coated with aluminum or heat-resisting nickel-chromium alloy and sealed at all openings with fire clay.\n\n\n== Hardening agents ==\nThere are different types of elements or materials that can be used to perform this process, but these mainly consist of high carbon content material. A few typical hardening agents include carbon monoxide gas (CO), sodium cyanide and barium carbonate, or hardwood charcoal. In gas carburizing, the CO is given off by propane or natural gas. In liquid carburizing, the CO is derived from a molten salt composed mainly of sodium cyanide (NaCN) and barium chloride (BaCl2). In pack carburizing, carbon monoxide is given off by coke or hardwood charcoal.\n\n\n== Geometrical possibilities ==\nThere are all sorts of workpieces that can be carburized, which means almost limitless possibilities for the shape of materials that can be carburized. However careful consideration should be given to materials that contain nonuniform or non-symmetric sections. Different cross sections may have different cooling rates which can cause excessive stresses in the material and result in breakage.\n\n\n== Dimensional changes ==\nIt is virtually impossible to have a workpiece undergo carburization without having some dimensional changes. The amount of these changes varies based on the type of material that is used, the carburizing process that the material undergoes and the original size and shape of the work piece. However changes are small compared to heat-treating operations.\n\n\n== Workpiece material ==\nTypically the materials that are carbonized are low-carbon and alloy steels with initial carbon content ranging from 0.2 to 0.3%. The workpiece surface must be free from contaminants, such as oil, oxides, or alkaline solutions, which prevent or impede the diffusion of carbon into the workpiece surface.\n\n\n== Comparing different methods ==\nIn general, pack carburizing equipment can accommodate larger workpieces than liquid or gas carburizing equipment, but liquid or gas carburizing methods are faster and lend themselves to mechanized material handling. Also the advantages of carburizing over carbonitriding are greater case depth (case depths of greater than 0.3 inch are possible), less distortion, and better impact strength. This makes it perfect for high strength and wear applications (e.g. scissors or swords). The disadvantages include added expense, higher working temperatures, and increased time.\n\n\n== Choice of equipment ==\nIn general, gas carburizing is used for parts that are large. Liquid carburizing is used for small and medium parts and pack carburizing can be used for large parts and individual processing of small parts in bulk. Vacuum carburizing (low pressure carburizing or LPC) can be applied across a large spectrum of parts when used in conjunction with either oil or high pressure gas quenching (HPGQ), depending on the alloying elements within the base material.\n\n\n== See also ==\nCarbonitriding\nCase hardening\nCementation process\nCrucible steel\nHarvey armor (also known as Harveyized steel), an early application of carburizing\nHayward A. Harvey, a pioneer in the development of carburizing\nNitridization\n\n\n== References ==\n\n\n== Further reading ==\nGeoffrey Parrish, Carburizing: Microstructures and Properties. ASM International. 1999. pg 11\n\n\n== External links ==\n\"MIL-S-6090A, Military Specification: Process for Steels Used In Aircraft Carburizing and Nitriding\" (PDF). United States Department of Defense. 7 Jun 1971.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Carburizing", 
                "title": "Carburizing"
            }
        ], 
        "phraseCharStart": "1105"
    }, 
    {
        "phraseCharEnd": "1158", 
        "phraseIndex": "T27", 
        "phraseGoldStandardTag": "Task", 
        "phrase": "removal of carbon co-deposits", 
        "wikiSearchResults": [
            {
                "snippet": "commercial reactor. The sputtering rate of tungsten by the plasma fuel ions is orders of magnitude smaller than that of carbon, and tritium is much less incorporated", 
                "pageCategories": "All accuracy disputes\nAll articles containing potentially dated statements\nAll articles needing additional references\nAll articles with dead external links\nAll articles with unsourced statements\nAlternative energy\nArticles containing potentially dated statements from January 2016\nArticles needing additional references from March 2016\nArticles with Wayback Machine links\nArticles with dead external links from August 2014", 
                "pageContent": "Fusion power is the generation of energy by nuclear fusion. Fusion reactions are high energy reactions in which two lighter atomic nuclei fuse to form a heavier nucleus. This major area of plasma physics research is concerned with harnessing this reaction as a source of large scale sustainable energy. There is no question of fusion's scientific feasibility, since stellar nucleosynthesis is the process in which stars transmute matter into energy emitted as radiation.\nIn almost all large scale commercial proposals, heat from neutron scattering in a controlled fusion reaction is used to operate a steam turbine that drives electrical generators, as in existing fossil fuel and nuclear fission power stations. Many different fusion concepts have come in and out of vogue over the years. The current leading designs are the tokamak and inertial confinement fusion (laser) approaches. As of January 2016, these technologies are not yet practically viable, as they are not energetically viable\u2014i.e., it currently takes more energy to initiate and contain a fusion reaction than the reaction then produces.\nThere are also smaller-scale commercial proposals relying on other means of energy transfer, mostly forms of aneutronic fusion\u2014but these are largely considered to be more remote than the large scale neutron scattering approaches.\n\n\n== Background ==\n\n\n=== Mechanism ===\nFusion reactions occur when two (or more) atomic nuclei come close enough for the strong nuclear force pulling them together to exceed the electrostatic force pushing them apart, fusing them into heavier nuclei. For nuclei lighter than iron-56, the reaction is exothermic, releasing energy. For nuclei heavier than iron-56, it is endothermic, requiring an external source of energy. Hence, nuclei smaller than iron-56 are more likely to fuse while those heavier than iron-56 are more likely to break apart.\nTo fuse, nuclei must be brought close enough together for the strong force to act, which occurs only at very short distances. The electrostatic force keeping them apart acts over long distances, so a significant amount of kinetic energy is needed to overcome this \"Coulomb barrier\" before the reaction can take place. There are several ways of doing this, including speeding up atoms in a particle accelerator, or more commonly, heating them to very high temperatures.\nOnce an atom is heated above its ionization energy, its electrons are stripped away, leaving just the bare nucleus (the ion). The result is a hot cloud of ions and the electrons formerly attached to them. This cloud is known as a plasma. Because the charges are separated, plasmas are electrically conductive and magnetically controllable. Many fusion devices take advantage of this to control the particles as they are being heated.\n\n\n=== Cross section ===\n\nA reaction's cross section, denoted \u03c3, is the measure of how likely it is that a fusion reaction will happen. It is a probability, and it depends on the velocity of the two nuclei when they strike one another. If the atoms move faster, fusion is more likely. If the atoms hit head on, fusion is more likely. Cross sections for many different fusion reactions were measured mainly in the 1970s using particle beams. A beam of ions of material A was fired at material B at different speeds, and the amount of neutrons coming off was measured. Neutrons are a key product of most fusion reactions.\nIn most cases, the nuclei are flying around in a hot cloud, with some distribution of velocities. If the plasma is thermalized, then the distribution looks like a bell curve, or maxwellian distribution. In this case, it is useful to take the average cross section over the velocity distribution. This is entered into the volumetric fusion rate:\n\n  \n    \n      \n        \n          P\n          \n            fusion\n          \n        \n        =\n        \n          n\n          \n            A\n          \n        \n        \n          n\n          \n            B\n          \n        \n        \u27e8\n        \u03c3\n        \n          v\n          \n            A\n            ,\n            B\n          \n        \n        \u27e9\n        \n          E\n          \n            fusion\n          \n        \n      \n    \n    {\\displaystyle P_{\\text{fusion}}=n_{A}n_{B}\\langle \\sigma v_{A,B}\\rangle E_{\\text{fusion}}}\n  \nwhere:\n\n  \n    \n      \n        \n          P\n          \n            fusion\n          \n        \n      \n    \n    {\\displaystyle P_{\\text{fusion}}}\n   is the energy made by fusion, per time and volume\nn is the number density of species A or B, the particles in the volume\n\n  \n    \n      \n        \u27e8\n        \u03c3\n        \n          v\n          \n            A\n            ,\n            B\n          \n        \n        \u27e9\n      \n    \n    {\\displaystyle \\langle \\sigma v_{A,B}\\rangle }\n   is the cross section of that reaction, average over all the velocities of the two species v\n\n  \n    \n      \n        \n          E\n          \n            fusion\n          \n        \n      \n    \n    {\\displaystyle E_{\\text{fusion}}}\n   is the energy released by that fusion reaction.\n\n\n=== Lawson criterion ===\nThis equation shows that energy varies with the temperature, density, speed of collision, and fuel used. This equation was central to John Lawsons' analysis of fusion power stations working with a hot plasma. Lawson assumed an energy balance, shown below.\nNet Power = Efficiency * (Fusion - Radiation Loss - Conduction Loss)\nNet Power is the net power for any fusion power station.\nEfficiency how much energy is needed to drive the device and how well it collects power.\nFusion is rate of energy generated by the fusion reactions.\nRadiation is the energy lost as light, leaving the plasma.\nConduction is the energy lost, as momentum leaves the plasma.\nPlasma clouds lose energy through conduction and radiation. Conduction is when ions, electrons or neutrals hit a surface and transfer a portion of their kinetic energy to the atoms of the surface. Radiation is when energy leaves the cloud as light. This can be in the visible, UV, IR, or X-ray light. Radiation increases as the temperature rises. To get net power from fusion, you must overcome these losses.\n\n\n=== Triple product: density, temperature, time ===\nThe Lawson criterion argues that a machine holding a hot thermalized and quasi-neutral plasma has to meet basic criteria to overcome the radiation losses, conduction losses and a power station efficiency of 30 percent. This became known as the \"triple product\": the plasma density and temperature and how long it is held in. For many years, fusion research has focused on achieving the highest triple product possible. The tendency towards maximising the triple product has led to building larger plants in order to reduce conduction and radiation losses, increase temperature, and increase plasma retention time. Building larger plants puts the structural materials further away from the centre of the plasma, which reduces conduction losses, and reduces radiation losses since more of the radiation is internally reflected. This emphasis on \n  \n    \n      \n        (\n        n\n        T\n        \u03c4\n        )\n      \n    \n    {\\displaystyle (nT\\tau )}\n   as a metric of success has hurt other considerations such as cost, size, complexity and efficiency. This has led to larger, more complicated and more expensive machines such as ITER and NIF.\n\n\n=== Plasma behavior ===\nPlasma can be made by fully ionizing a gas. Plasma is an ionized gas which conducts electricity. In bulk, it is modeled using hydrodynamics which is a combination of the Navier-Stokes equations governing fluids and Maxwell's equations governing how magnetic and electric fields behave. Fusion exploits several plasma properties, including:\nSelf-organization plasma conducts electric and magnetic fields. This means that it can self-organize. Its motions can generate fields which can, in turn, self-contain it.\nDiamagnetic plasma can generate its own internal magnetic field. This can reject an externally applied magnetic field, making it diamagnetic.\nMagnetic mirrors Plasma can be reflected when it moves from a low to high density magnetic field.\n\n\n=== Energy capture ===\nThere are several proposals for energy capture. The simplest is using a heat cycle to heat a fluid with fusion reactions. It has been proposed to use the neutrons generated by fusion to re-generate a spent fission fuel. In addition, direct energy conversion, has been developed (at LLNL in the 1980s) as a method to maintain a voltage using the products of a fusion reaction. This has demonstrated an energy capture efficiency of 48 percent.\n\n\n== Possible approaches ==\n\n\n=== Magnetic confinement fusion ===\nThe tokamak is the most well-developed and well-funded approach to fusion energy. As of April 2012 there were an estimated 215 experimental tokamaks either planned, decommissioned or currently operating (35 tokamaks), worldwide. This method races hot plasma around in a magnetically confined ring, with an internal current. When completed, ITER will be the world's largest tokamak.\nSpherical tokamak: A variation on the tokamak with a spherical shape.\nStellarator: These are twisted rings of hot plasma. The stellarator attempts to create a natural twist plasma path, using external magnets; while Tokamaks create those magnetic fields using an internal current. Stellarators were developed by Lyman Spitzer in 1950 and have four designs: Torsatron, Heliotron, Heliac and Helias. One example is Wendelstein 7-X, a German fusion device that produced its first plasma on December 10, 2015. Wendelstein 7-X, the world's largest stellarator-type fusion device, is not intended to produce energy, but will investigate the suitability of this type of device for a power station.\nLevitated Dipole Experiment (LDX): These use a solid superconducting torus. This is magnetically levitated inside the reactor chamber. The superconductor forms an axisymmetric magnetic field that contains the plasma. The LDX was developed between MIT and Columbia University after 2000 by Jay Kesner and Michael E. Mauel.\nMagnetic mirror: Developed by Richard F. Post and teams at LLNL in the 1960s. Magnetic mirrors reflected hot plasma back and forth in a line. Variations included the magnetic bottle and the biconic cusp. A series of well-funded, large, mirror machines were built by the US government in the 1970s and 1980s. Mirror research continues today.\nField-reversed configuration: This device traps plasma in a self-organized quasi-stable structure; where the particle motion makes an internal magnetic field which then traps itself.\nReversed field pinch: Here the plasma moves inside a ring. It has an internal magnetic field. As you move out from the center of this ring, the magnetic field reverses direction.\n\n\n=== Inertial confinement fusion ===\nDirect drive: In this technique, lasers directly blast a pellet of fuel. The goal is to start ignition, a fusion chain reaction. Ignition was first suggested by John Nuckolls, in 1972. Notable direct drive experiments have been conducted at the Laboratory for Laser Energetics, Laser M\u00e9gajoule and the GEKKO XII facilities. Good implosions require fuel pellets with close to a perfect shape in order to generate a symmetrical inward shock wave and to produce the high-density plasma.\nFast ignition: This method uses two laser blasts. The first blast compresses the fusion fuel, while the second high energy pulse ignites it. Experiments have been conducted at the Laboratory for Laser Energetics using the Omega and Omega EP systems and at the GEKKO XII laser at the institute for laser engineering in Osaka Japan.\nIndirect drive: In this technique, lasers blasts a structure around the pellet of fuel. This structure is known as a Hohlraum. As it disintegrates the pellet is bathed in a more uniform x-ray light, creating better compression. The largest system using this method is the National Ignition Facility.\nMagneto-inertial fusion or Magnetized Liner Inertial Fusion: This combines a laser pulse with a magnetic pinch. The pinch community refers to it as magnetized liner Inertial fusion while the ICF community refers to it as magneto-inertial fusion.\nHeavy Ion Beams There are also proposals to do inertial confinement fusion with ion beams instead of laser beams. The main difference is the mass of the beam has momentum, whereas lasers do not.\n\n\n=== Magnetic or electric pinches ===\n\nZ-Pinch: This method sends a strong current (in the z-direction) through the plasma. The current generates a magnetic field that squeezes the plasma to fusion conditions. Pinches were the first method for man-made controlled fusion. Some examples include the Dense plasma focus and the Z machine at Sandia National Laboratories.\nTheta-Pinch: This method sends a current inside a plasma, in the theta direction.\nScrew Pinch: This method combines a theta and z-pinch for improved stabilization.\n\n\n=== Inertial electrostatic confinement ===\nFusor: This method uses an electric field to heat ions to fusion conditions. The machine typically uses two spherical cages, a cathode inside the anode, inside a vacuum. These machines are not considered a viable approach to net power because of their high conduction and radiation losses. They are simple enough to build that amateurs have fused atoms using them.\nPolywell: This designs attempts to combine magnetic confinement with electrostatic fields, to avoid the conduction losses generated by the cage.\n\n\n=== Other ===\nMagnetized target fusion: This method confines hot plasma using a magnetic field and squeezes it using inertia. Examples include LANL FRX-L machine, General Fusion and the plasma liner experiment.\nUncontrolled: Fusion has been initiated by man, using uncontrolled fission explosions to ignite the so-called Hydrogen Bomb. Early proposals for fusion power included using bombs to initiate reactions.\nBeam fusion: A beam of high energy particles can be fired at another beam or target and fusion will occur. This was used in the 1970s and 1980s to study the cross sections of high energy fusion reactions.\nBubble fusion: This was a supposed fusion reaction that was supposed to occur inside extraordinarily large collapsing gas bubbles, created during acoustic liquid cavitation. This approach was discredited.\nCold fusion: This is a hypothetical type of nuclear reaction that would occur at, or near, room temperature. Cold fusion has gained a reputation as Pathological science.\nMuon-catalyzed fusion: Muons allow atoms to get much closer and thus reduce the kinetic energy required to initiate fusion. Muons require more energy to produce than can be obtained from muon-catalysed fusion, making this approach impractical for the generation of power.\nGravitational-confinement fusion (GCF) Direct Photo-Electric Conversion: Also known as Space-Based Solar Power argues that a majority of available fusion fuels exists within the sphere of the Sun where it is gravitationally confined, and an tractable way to accomplish large-scale fusion power is to build very large space-borne platforms that capture energy via photons rather than via a carnot cycle. The theoretical limits of fusion power via this means is a type-2 civilization via a Dyson Sphere.\n\n\n== Common tools ==\n\n\n=== Heating ===\nGas must be first heated to form a plasma. This then needs to be hot enough to start fusion reactions. A number of heating schemes have been explored:\nRadiofrequency Heating A radio wave is applied to the plasma, causing it to oscillate. This is basically the same concept as a microwave oven. This is also known as electron cyclotron resonance heating or Dielectric heating.\nElectrostatic Heating An electric field can do work on charged ions or electrons, heating them.\nNeutral Beam Injection An external source of hydrogen is ionized and accelerated by an electric field to form a charged beam which is shone through a source of neutral hydrogen gas towards the plasma which itself is ionized and contained in the reactor by a magnetic field. Some of the intermediate hydrogen gas is accelerated towards the plasma by collisions with the charged beam while remaining neutral: this neutral beam is thus unaffected by the magnetic field and so shines through it into the plasma. Once inside the plasma the neutral beam transmits energy to the plasma by collisions as a result of which it becomes ionized and thus contained by the magnetic field thereby both heating and refuelling the reactor in one operation. The remainder of the charged beam is diverted by magnetic fields onto cooled beam dumps.\nMagnetic Oscillations\n\n\n=== Measurement ===\nThomson Scattering Light scatters from plasma. This light can be detected and used to reconstruct the plasmas' behavior. This technique can be used to find its density and temperature. It is common in Inertial confinement fusion, Tokamaks and fusors. In ICF systems, this can be done by firing a second beam into a gold foil adjacent to the target. This makes x-rays that scatter or traverse the plasma. In Tokamaks, this can be done using mirrors and detectors to reflect light across a plane (two dimensions) or in a line (one dimension).\nLangmuir probe This is a metal object placed in a plasma. A potential is applied to it, giving it a positive or negative voltage against the surrounding plasma. The metal collects charged particles, drawing a current. As the voltage changes, the current changes. This makes a IV Curve. The IV-curve can be used to determine the local plasma density, potential and temperature.\nGeiger counter Deuterium or tritium fusion produces neutrons. Geiger counters record the rate of neutron production, so they are an essential tool for demonstrating success.\nFlux loop A loop of wire is inserted into the magnetic field. As the field passes through the loop, a current is made. The current is measured and used to find the total magnetic flux through that loop. This has been used on the National Compact Stellarator Experiment, the polywell and the LDX machines.\nX-ray detector All plasma loses energy by emitting light. This covers the whole spectrum: visible, IR, UV, and X-rays. This occurs anytime a particle changes speed, for any reason. If the reason is deflection by a magnetic field, the radiation is Cyclotron radiation at low speeds and Synchrotron radiation at high speeds. If the reason is deflection by another particle, plasma radiates X-rays, known as Bremsstrahlung radiation. X-rays are termed in both hard and soft, based on their energy.\n\n\n=== Power production ===\nSteam turbines It has been proposed  that steam turbines be used to convert the heat from the fusion chamber into electricity. The heat is transferred into a working fluid that turns into steam, driving electric generators.\nNeutron blankets Deuterium and tritium fusion generates neutrons. This varies by technique (NIF has a record of 3E14 neutrons per second while a typical fusor produces 1E5\u20131E9 neutrons per second). It has been proposed to use these neutrons as a way to regenerate spent fission fuel  or as a way to breed tritium from a liquid lithium blanket.\nDirect conversion This is a method where the kinetic energy of a particle is converted into voltage. It was first suggested by Richard F. Post in conjunction with magnetic mirrors, in the late sixties. It has also been suggested for Field-Reversed Configurations. The process takes the plasma, expands it, and converts a large fraction of the random energy of the fusion products into directed motion. The particles are then collected on electrodes at various large electrical potentials. This method has demonstrated an experimental efficiency of 48 percent.\n\n\n== Confinement ==\n\nConfinement refers to all the conditions necessary to keep a plasma dense and hot long enough to undergo fusion. Here are some general principles.\nEquilibrium: The forces acting on the plasma must be balanced for containment. One exception is inertial confinement, where the relevant physics must occur faster than the disassembly time.\nStability: The plasma must be so constructed so that disturbances will not lead to the plasma disassembling.\nTransport or conduction: The loss of material must be sufficiently slow. The plasma carries off energy with it, so rapid loss of material will disrupt any machines power balance. Material can be lost by transport into different regions or conduction through a solid or liquid.\nTo produce self-sustaining fusion, the energy released by the reaction (or at least a fraction of it) must be used to heat new reactant nuclei and keep them hot long enough that they also undergo fusion reactions.\n\n\n=== Unconfined ===\nThe first human-made, large-scale fusion reaction was the test of the hydrogen bomb, Ivy Mike, in 1952. As part of the PACER project, it was once proposed to use hydrogen bombs as a source of power by detonating them in underground caverns and then generating electricity from the heat produced, but such a power station is unlikely ever to be constructed.\n\n\n=== Magnetic confinement ===\nAt the temperatures required for fusion, the fuel is heated to a plasma state. In this state it has a very good electrical conductivity. This opens the possibility of confining the plasma with magnetic fields. This is the case of magnetized plasma, where the magnetic fields and plasma intermix. This is generally known as magnetic confinement. The field lines put a Lorentz force on the plasma. The force works perpendicular to the magnetic fields, so one problem in magnetic confinement is preventing the plasma from leaking out the ends of the field lines. A general measure of magnetic trapping in fusion is the beta ratio:\n\n  \n    \n      \n        \u03b2\n        =\n        \n          \n            p\n            \n              p\n              \n                m\n                a\n                g\n              \n            \n          \n        \n        =\n        \n          \n            \n              n\n              \n                k\n                \n                  B\n                \n              \n              T\n            \n            \n              (\n              \n                B\n                \n                  2\n                \n              \n              \n                /\n              \n              2\n              \n                \u03bc\n                \n                  0\n                \n              \n              )\n            \n          \n        \n      \n    \n    {\\displaystyle \\beta ={\\frac {p}{p_{mag}}}={\\frac {nk_{B}T}{(B^{2}/2\\mu _{0})}}}\n   \nThis is the ratio of the externally applied field to the internal pressure of the plasma. A value of 1 is ideal trapping. Some examples of beta vales include:\nThe START machine: 0.32\nThe Levitated dipole experiment: 0.26\nSpheromaks: \u2248 0.1, Maximum 0.2 based on Mercier limit.\nThe DIII-D machine: 0.126\nThe Gas Dynamic Trap a magnetic mirror: 0.6  for 5E-3 seconds.\nMagnetic Mirror One example of magnetic confinement is with the magnetic mirror effect. If a particle follows the field line and enters a region of higher field strength, the particles can be reflected. There are several devices that try to use this effect. The most famous was the magnetic mirror machines, which was a series of large, expensive devices built at the Lawrence Livermore National Laboratory from the 1960s to mid 1980s. Some other examples include the magnetic bottles and Biconic cusp. Because the mirror machines were straight, they had some advantages over a ring shape. First, mirrors were easier to construct and maintain and second direct conversion energy capture, was easier to implement. As the confinement achieved in experiments was poor, this approach was abandoned.\nMagnetic Loops Another example of magnetic confinement is to bend the field lines back on themselves, either in circles or more commonly in nested toroidal surfaces. The most highly developed system of this type is the tokamak, with the stellarator being next most advanced, followed by the Reversed field pinch. Compact toroids, especially the Field-Reversed Configuration and the spheromak, attempt to combine the advantages of toroidal magnetic surfaces with those of a simply connected (non-toroidal) machine, resulting in a mechanically simpler and smaller confinement area.\n\n\n=== Inertial confinement ===\nInertial confinement is the use of rapidly imploding shell to heat and confine plasma. The shell is imploded using a direct laser blast (direct drive) or a secondary x-ray blast (indirect drive) or heavy ion beams. Theoretically, fusion using lasers would be done using tiny pellets of fuel that explode several times a second. To induce the explosion, the pellet must be compressed to about 30 times solid density with energetic beams. If direct drive is used\u2014the beams are focused directly on the pellet\u2014it can in principle be very efficient, but in practice is difficult to obtain the needed uniformity. The alternative approach, indirect drive, uses beams to heat a shell, and then the shell radiates x-rays, which then implode the pellet. The beams are commonly laser beams, but heavy and light ion beams and electron beams have all been investigated.\n\n\n=== Electrostatic confinement ===\nThere are also electrostatic confinement fusion devices. These devices confine ions using electrostatic fields. The best known is the Fusor. This device has a cathode inside an anode wire cage. Positive ions fly towards the negative inner cage, and are heated by the electric field in the process. If they miss the inner cage they can collide and fuse. Ions typically hit the cathode, however, creating prohibitory high conduction losses. Also, fusion rates in fusors are very low because of competing physical effects, such as energy loss in the form of light radiation. Designs have been proposed to avoid the problems associated with the cage, by generating the field using a non-neutral cloud. These include a plasma oscillating device, a magnetically-shielded-grid a penning trap and the polywell. The technology is relatively immature, however, and many scientific and engineering questions remain.\n\n\n== History of research ==\n\n\n=== 1920s ===\nResearch into nuclear fusion started in the early part of the 20th century. In 1920 the British physicist Francis William Aston discovered that the total mass equivalent of four hydrogen atoms (two protons and two neutrons) are heavier than the total mass of one helium atom (He-4), which implied that net energy can be released by combining hydrogen atoms together to form helium, and provided the first hints of a mechanism by which stars could produce energy in the quantities being measured. Through the 1920s, Arthur Stanley Eddington became a major proponent of the proton\u2013proton chain reaction (PP reaction) as the primary system running the Sun.\n\n\n=== 1930s ===\nA theory was verified by Hans Bethe in 1939 showing that beta decay and quantum tunneling in the Sun's core might convert one of the protons into a neutron and thereby producing deuterium rather than a diproton. The deuterium would then fuse through other reactions to further increase the energy output. For this work, Bethe won the Nobel Prize in Physics.\n\n\n=== 1940s ===\nIn 1942, nuclear fusion research was subsumed into the Manhattan Project when the secrecy surrounding the field obscured by the science. The first patent related to a fusion reactor was registered in 1946 by the United Kingdom Atomic Energy Authority. The inventors were Sir George Paget Thomson and Moses Blackman. This was the first detailed examination of the Z-pinch concept.\nZ-pinch is based on the fact that plasmas are electrically conducting. Running a current through the plasma, will generate a magnetic field around the plasma. This field will, according to Lenz's law, create an inward directed force that causes the plasma to collapse inward, raising its density. Denser plasmas generate denser magnetic fields, increasing the inward force, leading to a chain reaction. If the conditions are correct, this can lead to the densities and temperatures needed for fusion. The difficulty is getting the current into the plasma, which would normally melt any sort of mechanical electrode. A solution emerges again because of the conducting nature of the plasma; by placing the plasma in the middle of an electromagnet, induction can be used to generate the current.\nStarting in 1947, two UK teams carried out small experiments and began building a series of ever-larger experiments. When the Huemul results hit the news (see below), James L. Tuck, a UK physicist working at Los Alamos, introduced the pinch concept in the US and produced a series of machines known as the Perhapsatron. The Soviet Union, unbeknownst to the West, was also building a series of similar machines. All of these devices quickly demonstrated a series of instabilities when the pinch was applied. This broke up the plasma column long before it reached the densities and temperatures required for fusion.\n\n\n=== 1950s ===\n\nThe first successful man-made fusion device was the boosted fission weapon tested in 1951 in the Greenhouse Item test. This was followed by true fusion weapons in 1952's Ivy Mike, and the first practical examples in 1954's Castle Bravo. This was uncontrolled fusion. In these devices, the energy released by the fission explosion is used to compress and heat fusion fuel, starting a fusion reaction. Fusion releases neutrons. These neutrons hit the surrounding fission fuel, causing the atoms to split apart much faster than normal fission processes\u2014almost instantly by comparison. This increases the effectiveness of bombs: normal fission weapons blow themselves apart before all their fuel is used; fusion/fission weapons do not have this practical upper limit.\nIn 1949 an expatriate German, Ronald Richter, proposed the Huemul Project in Argentina, announcing positive results in 1951. These turned out to be fake, but it prompted considerable interest in the concept as a whole. In particular, it prompted Lyman Spitzer to begin considering ways to solve some of the more obvious problems involved in confining a hot plasma, and, unaware of the z-pinch efforts, he developed a new solution to the problem known as the stellarator. Spitzer applied to the US Atomic Energy Commission for funding to build a test device. During this period, Jim Tuck who had worked with the UK teams had been introducing the z-pinch concept to his coworkers at his new job at Los Alamos National Laboratory (LANL). When he heard of Spitzer's pitch for funding, he applied to build a machine of his own, the Perhapsatron.\nSpitzer's idea won funding and he began work on the stellarator under the code name Project Matterhorn. His work led to the creation of the Princeton Plasma Physics Laboratory. Tuck returned to LANL and arranged local funding to build his machine. By this time, however, it was clear that all of the pinch machines were suffering from the same issues involving stability, and progress stalled. In 1953, Tuck and others suggested a number of solutions to the stability problems. This led to the design of a second series of pinch machines, led by the UK ZETA and Sceptre devices.\nSpitzer had planned an aggressive development project of four machines, A, B, C, and D. A and B were small research devices, C would be the prototype of a power-producing machine, and D would be the prototype of a commercial device. A worked without issue, but even by the time B was being used it was clear the stellarator was also suffering from instabilities and plasma leakage. Progress on C slowed as attempts were made to correct for these problems.\nBy the mid-1950s it was clear that the simple theoretical tools being used to calculate the performance of all fusion machines were simply not predicting their actual behavior. Machines invariably leaked their plasma from their confinement area at rates far higher than predicted. In 1954, Edward Teller held a gathering of fusion researchers at the Princeton Gun Club, near the Project Matterhorn (now known as Project Sherwood) grounds. Teller started by pointing out the problems that everyone was having, and suggested that any system where the plasma was confined within concave fields was doomed to fail. Attendees remember him saying something to the effect that the fields were like rubber bands, and they would attempt to snap back to a straight configuration whenever the power was increased, ejecting the plasma. He went on to say that it appeared the only way to confine the plasma in a stable configuration would be to use convex fields, a \"cusp\" configuration.\nWhen the meeting concluded, most of the researchers quickly turned out papers saying why Teller's concerns did not apply to their particular device. The pinch machines did not use magnetic fields in this way at all, while the mirror and stellarator seemed to have various ways out. This was soon followed by a paper by Martin David Kruskal and Martin Schwarzschild discussing pinch machines, however, which demonstrated instabilities in those devices were inherent to the design.\nThe largest \"classic\" pinch device was the ZETA, including all of these suggested upgrades, starting operations in the UK in 1957. In early 1958, John Cockcroft announced that fusion had been achieved in the ZETA, an announcement that made headlines around the world. When physicists in the US expressed concerns about the claims they were initially dismissed. US experiments soon demonstrated the same neutrons, although temperature measurements suggested these could not be from fusion reactions. The neutrons seen in the UK were later demonstrated to be from different versions of the same instability processes that plagued earlier machines. Cockcroft was forced to retract the fusion claims, and the entire field was tainted for years. ZETA ended its experiments in 1968.\nThe first controlled fusion experiment was accomplished using Scylla I at the Los Alamos National Laboratory in 1958. This was a pinch machine, with a cylinder full of deuterium. Electric current shot down the sides of the cylinder. The current made magnetic fields that compressed the plasma to 15 million degrees Celsius, squeezed the gas, fused it and produced neutrons.\nIn 1950\u20131951 I.E. Tamm and A.D. Sakharov in the Soviet Union, first discussed a tokamak-like approach. Experimental research on those designs began in 1956 at the Kurchatov Institute in Moscow by a group of Soviet scientists led by Lev Artsimovich. The tokamak essentially combined a low-power pinch device with a low-power simple stellarator. The key was to combine the fields in such a way that the particles orbited within the reactor a particular number of times, today known as the \"safety factor\". The combination of these fields dramatically improved confinement times and densities, resulting in huge improvements over existing devices.\n\n\n=== 1960s ===\nA key plasma physics text was published by Lyman Spitzer at Princeton in 1963. Spitzer took the ideal gas laws and adopted them to an ionized plasma, developing many of the fundamental equations used to model a plasma.\nLaser fusion was suggested in 1962 by scientists at Lawrence Livermore National Laboratory, shortly after the invention of the laser itself in 1960. At the time, Lasers were low power machines, but low-level research began as early as 1965. Laser fusion, formally known as inertial confinement fusion, involves imploding a target by using laser beams. There are two ways to do this: indirect drive and direct drive. In direct drive, the laser blasts a pellet of fuel. In indirect drive, the lasers blast a structure around the fuel. This makes x-rays that squeeze the fuel. Both methods compress the fuel so that fusion can take place.\nAt the 1964 World's Fair, the public was given its first demonstration of nuclear fusion. The device was a \u03b8-pinch from General Electric. This was similar to the Scylla machine developed earlier at Los Alamos.\nThe magnetic mirror was first published in 1967 by Richard F. Post and many others at the Lawrence Livermore National Laboratory. The mirror consisted of two large magnets arranged so they had strong fields within them, and a weaker, but connected, field between them. Plasma introduced in the area between the two magnets would \"bounce back\" from the stronger fields in the middle.\nThe A.D. Sakharov group constructed the first tokamaks, the most successful being the T-3 and its larger version T-4. T-4 was tested in 1968 in Novosibirsk, producing the world's first quasistationary fusion reaction. When this were first announced, the international community was highly skeptical. A British team was invited to see T-3, however, and after measuring it in depth they released their results that confirmed the Soviet claims. A burst of activity followed as many planned devices were abandoned and new tokamaks were introduced in their place \u2014 the C model stellarator, then under construction after many redesigns, was quickly converted to the Symmetrical Tokamak.\nIn his work with vacuum tubes, Philo Farnsworth observed that electric charge would accumulate in regions of the tube. Today, this effect is known as the Multipactor effect. Farnsworth reasoned that if ions were concentrated high enough they could collide and fuse. In 1962, he filed a patent on a design using a positive inner cage to concentrate plasma, in order to achieve nuclear fusion. During this time, Robert L. Hirsch joined the Farnsworth Television labs and began work on what became the fusor. Hirsch patented the design in 1966 and published the design in 1967.\n\n\n=== 1970s ===\n\nIn 1972, John Nuckolls outlined the idea of ignition. This is a fusion chain reaction. Hot helium made during fusion reheats the fuel and starts more reactions. John argued that ignition would require lasers of about 1 kJ. This turned out to be wrong. Nuckolls's paper started a major development effort. Several laser systems were built at LLNL. These included the argus, the Cyclops, the Janus, the long path, the Shiva laser and the Nova in 1984. This prompted the UK to build the Central Laser Facility in 1976.\nDuring this time, great strides in understanding the tokamak system were made. A number of improvements to the design are now part of the \"advanced tokamak\" concept, which includes non-circular plasma, internal diverters and limiters, often superconducting magnets, and operate in the so-called \"H-mode\" island of increased stability. Two other designs have also become fairly well studied; the compact tokamak is wired with the magnets on the inside of the vacuum chamber, while the spherical tokamak reduces its cross section as much as possible.\nIn 1974 a study of the ZETA results demonstrated an interesting side-effect; after an experimental run ended, the plasma would enter a short period of stability. This led to the reversed field pinch concept, which has seen some level of development since. On May 1, 1974, the KMS fusion company (founded by Kip Siegel) achieves the world's first laser induced fusion in a deuterium-tritium pellet.\nIn the mid-1970s, Project PACER, carried out at Los Alamos National Laboratory (LANL) explored the possibility of a fusion power system that would involve exploding small hydrogen bombs (fusion bombs) inside an underground cavity. As an energy source, the system is the only fusion power system that could be demonstrated to work using existing technology. It would also require a large, continuous supply of nuclear bombs, however, making the economics of such a system rather questionable.\nIn 1976, the two beam Argus laser becomes operational at livermore. In 1977, The 20 beam Shiva laser at Livermore is completed, capable of delivering 10.2 kilojoules of infrared energy on target. At a price of $25 million and a size approaching that of a football field, Shiva is the first of the megalasers. That same year, the JET project is approved by the European Commission and a site is selected.\n\n\n=== 1980s ===\n\nAs a result of advocacy, the cold war, and the 1970s energy crisis a massive magnetic mirror program was funded by the US federal government in the late 1970s and early 1980s. This program resulted in a series of large magnetic mirror devices including: 2X, Baseball I, Baseball II, the Tandem Mirror Experiment, the Tandem mirror experiment upgrade, the Mirror Fusion Test Facility and the MFTF-B. These machines were built and tested at Livermore from the late 1960s to the mid 1980s. A number of institutions collaborated on these machines, conducting experiments. These included the Institute for Advanced Study and the University of Wisconsin\u2013Madison. The last machine, the Mirror Fusion Test Facility cost 372 million dollars and was, at that time, the most expensive project in Livermore history. It opened on February 21, 1986 and was promptly shut down. The reason given was to balance the United States federal budget. This program was supported from within the Carter and early Reagan administrations by Edwin E. Kintner, a US Navy captain, under Alvin Trivelpiece.\nIn Laser fusion progressed: in 1983, the NOVETTE laser was completed. The following December 1984, the ten beam NOVA laser was finished. Five years later, NOVA would produce a maximum of 120 kilojoules of infrared light, during a nanosecond pulse. Meanwhile, efforts focused on either fast delivery or beam smoothness. Both tried to deliver the energy uniformly to implode the target. One early problem was that the light in the infrared wavelength, lost lots of energy before hitting the fuel. Breakthroughs were made at the Laboratory for Laser Energetics at the University of Rochester. Rochester scientists used frequency-tripling crystals to transform the infrared laser beams into ultraviolet beams. In 1985, Donna Strickland and G\u00e9rard Mourou invented a method to amplify lasers pulses by \"chirping\". This method changes a single wavelength into a full spectrum. The system then amplifies the laser at each wavelength and then reconstitutes the beam into one color. Chirp pulsed amplification became instrumental in building the National Ignition Facility and the Omega EP system. Most research into ICF was towards weapons research, because the implosion is relevant to nuclear weapons.\nDuring this time Los Alamos National Laboratory constructed a series of laser facilities. This included Gemini (a two beam system), Helios (eight beams), Antares (24 beams) and Aurora (96 beams). The program ended in the early nineties with a cost on the order of one billion dollars.\nIn 1987, Akira Hasegawa  noticed that in a dipolar magnetic field, fluctuations tended compress the plasma without energy loss. This effect was noticed in data taken by Voyager 2, when it encountered Uranus. This observation would become the basis for a fusion approach known as the Levitated dipole.\nIn Tokamaks, the Tore Supra was under construction over the middle of the eighties (1983 to 1988). This was a Tokamak built in Cadarache, France. In 1983, the JET was completed and first plasmas achieved. In 1985, the Japanese tokamak, JT-60 was completed. In 1988, the T-15 a Soviet tokamak was completed. It was the first industrial fusion reactor to use superconducting magnets to control the plasma. These were Helium cooled.\nIn 1989, Pons and Fleischmann submitted papers to the Journal of Electroanalytical Chemistry claiming that they had observed fusion in a room temperature device and disclosing their work in a press release. Some scientists reported excess heat, neutrons, tritium, helium and other nuclear effects in so-called cold fusion systems, which for a time gained interest as showing promise. Hopes fell when replication failures were weighed in view of several reasons cold fusion is not likely to occur, the discovery of possible sources of experimental error, and finally the discovery that Fleischmann and Pons had not actually detected nuclear reaction byproducts. By late 1989, most scientists considered cold fusion claims dead, and cold fusion subsequently gained a reputation as pathological science. However, a small community of researchers continues to investigate cold fusion claiming to replicate Fleishmann and Pons' results including nuclear reaction byproducts. Claims related to cold fusion are largely disbelieved in the mainstream scientific community. In 1989, the majority of a review panel organized by the US Department of Energy (DOE) found that the evidence for the discovery of a new nuclear process was not persuasive. A second DOE review, convened in 2004 to look at new research, reached conclusions similar to the first.\nIn 1984, Martin Peng of ORNL proposed an alternate arrangement of the magnet coils that would greatly reduce the aspect ratio while avoiding the erosion issues of the compact tokamak: a Spherical tokamak. Instead of wiring each magnet coil separately, he proposed using a single large conductor in the center, and wiring the magnets as half-rings off of this conductor. What was once a series of individual rings passing through the hole in the center of the reactor was reduced to a single post, allowing for aspect ratios as low as 1.2. The ST concept appeared to represent an enormous advance in tokamak design. However, it was being proposed during a period when US fusion research budgets were being dramatically scaled back. ORNL was provided with funds to develop a suitable central column built out of a high-strength copper alloy called \"Glidcop\". However, they were unable to secure funding to build a demonstration machine, \"STX\". Failing to build an ST at ORNL, Peng began a worldwide effort to interest other teams in the ST concept and get a test machine built. One way to do this quickly would be to convert a spheromak machine to the Spherical tokamak layout. Peng's advocacy also caught the interest of Derek Robinson, of the United Kingdom Atomic Energy Authority fusion center at Culham. Robinson was able to gather together a team and secure funding on the order of 100,000 pounds to build an experimental machine, the Small Tight Aspect Ratio Tokamak, or START. Several parts of the machine were recycled from earlier projects, while others were loaned from other labs, including a 40 keV neutral beam injector from ORNL. Construction of START began in 1990, it was assembled rapidly and started operation in January 1991.\n\n\n=== 1990s ===\n\nIn 1991 the Preliminary Tritium Experiment at the Joint European Torus in England achieved the world\u2019s first controlled release of fusion power.\nIn 1992, a major article was published in Physics Today by Robert McCory at the Laboratory for laser energetics outlying the current state of ICF and advocating for a national ignition facility. This was followed up by a major review article, from John Lindl in 1995, advocating for NIF. During this time a number of ICF subsystems were developing, including target manufacturing, cryogenic handling systems, new laser designs (notably the NIKE laser at NRL) and improved diagnostics like time of flight analyzers and Thomson scattering. This work was done at the NOVA laser system, General Atomics, Laser M\u00e9gajoule and the GEKKO XII system in Japan. Through this work and lobbying by groups like the fusion power associates and John Sethian at NRL, a vote was made in congress, authorizing funding for the NIF project in the late nineties.\nIn the early nineties, theory and experimental work regarding fusors and polywells was published. In response, Todd Rider at MIT developed general models of these devices. Rider argued that all plasma systems at thermodynamic equilibrium were fundamentally limited. In 1995, William Nevins published a criticism  arguing that the particles inside fusors and polywells would build up angular momentum, causing the dense core to degrade.\nIn 1995, the University of Wisconsin\u2013Madison built a large fusor, known as HOMER, which is still in operation. Meanwhile, Dr George H. Miley at Illinois, built a small fusor that has produced neutrons using deuterium gas  and discovered the \"star mode\" of fusor operation. The following year, the first \"US-Japan Workshop on IEC Fusion\", was conducted. At this time in Europe, an IEC device was developed as a commercial neutron source by Daimler-Chrysler and NSD Fusion.\nIn 1996, the Z-machine was upgraded and opened to the public by the US Army in August 1998 in Scientific American. The key attributes of Sandia\u2019s Z machine are its 18 million amperes and a discharge time of less than 100 nanoseconds. This generates a magnetic pulse, inside a large oil tank, this strikes an array of tungsten wires called a liner. Firing the Z-machine has become a way to test very high energy, high temperature (2 billion degrees) conditions. In 1996, the Tore Supra creates a plasma for two minutes with a current of almost 1 million amperes driven non-inductively by 2.3 MW of lower hybrid frequency waves. This is 280 MJ of injected and extracted energy. This result was possible because of the actively cooled plasma-facing components\nIn 1997, JET produced a peak of 16.1MW of fusion power (65% of heat to plasma), with fusion power of over 10MW sustained for over 0.5 sec. Its successor, the International Thermonuclear Experimental Reactor (ITER), was officially announced as part of a seven-party consortium (six countries and the EU). ITER is designed to produce ten times more fusion power than the power put into the plasma. ITER is currently under construction in Cadarache, France.\nIn the late nineties, a team at Columbia University and MIT developed the Levitated dipole a fusion device which consisted of a superconducting electromagnet, floating in a saucer shaped vacuum chamber. Plasma swirled around this donut and fused along the center axis.\n\n\n=== 2000s ===\n\nIn the March 8, 2002 issue of the peer-reviewed journal Science, Rusi P. Taleyarkhan and colleagues at the Oak Ridge National Laboratory (ORNL) reported that acoustic cavitation experiments conducted with deuterated acetone (C3D6O) showed measurements of tritium and neutron output consistent with the occurrence of fusion. Taleyarkhan was later found guilty of misconduct, the Office of Naval Research debarred him for 28 months from receiving Federal Funding, and his name was listed in the 'Excluded Parties List'.\n\"Fast ignition\" was developed in the late nineties, and was part of a push by the Laboratory for Laser Energetics for building the Omega EP system. This system was finished in 2008. Fast ignition showed such dramatic power savings that ICF appears to be a useful technique for energy production. There are even proposals to build an experimental facility dedicated to the fast ignition approach, known as HiPER.\nIn April 2005, a team from UCLA announced it had devised a way of producing fusion using a machine that \"fits on a lab bench\", using lithium tantalate to generate enough voltage to smash deuterium atoms together. The process, however, does not generate net power (see Pyroelectric fusion). Such a device would be useful in the same sort of roles as the fusor. In 2006, China's EAST test reactor is completed. This was the first tokamak to use superconducting magnets to generate both the toroidal and poloidal fields.\nIn the early 2000s, Researchers at LANL reasoned that a plasma oscillating could be at local thermodynamic equilibrium. This prompted the POPS and Penning trap designs. At this time, researchers at MIT became interested in fusors for space propulsion and powering space vehicles. Specifically, researchers developed fusors with multiple inner cages. Greg Piefer graduated from Madison and founded Phoenix Nuclear Labs, a company that developed the fusor into a neutron source for the mass production of medical isotopes. Robert Bussard began speaking openly about the Polywell in 2006. He attempted to generate interest in the research, before his death. In 2008, Taylor Wilson achieved notoriety for achieving nuclear fusion at 14, with a homemade fusor.\nIn 2009, a high-energy laser system, the National Ignition Facility (NIF), was finished in the US, which can heat hydrogen atoms to temperatures only existing in nature in the cores of stars. The new laser is expected to have the ability to produce, for the first time, more energy from controlled, inertially confined nuclear fusion than was required to initiate the reaction.\n\n\n=== 2010s ===\n\nIn 2010, NIF researchers were conducting a series of \"tuning\" shots to determine the optimal target design and laser parameters for high-energy ignition experiments with fusion fuel in the following months. Two firing tests were performed on October 31, 2010 and November 2, 2010. In early 2012, NIF director Mike Dunne expected the laser system to generate fusion with net energy gain by the end of 2012. However, it was delayed and not achieved by that date.\nInertial (laser) confinement is being developed at the United States National Ignition Facility (NIF) based at Lawrence Livermore National Laboratory in California, the French Laser M\u00e9gajoule, and the planned European Union High Power laser Energy Research (HiPER) facility. NIF reached initial operational status in 2010 and has been in the process of increasing the power and energy of its \"shots\", with fusion ignition tests to follow. A three-year goal announced in 2009 to produce net energy from fusion by 2012 was missed; in September 2013, however, the facility announced a significant milestone from an August 2013 test that produced more energy from the fusion reaction than had been provided to the fuel pellet. This was reported as the first time this had been accomplished in fusion power research. The facility reported that their next step involved improving the system to prevent the hohlraum from either breaking up asymmetrically or too soon.\nA 2012 paper demonstrated that a dense plasma focus had achieved temperatures of 1.8 billion degrees Celsius, sufficient for boron fusion, and that fusion reactions were occurring primarily within the contained plasmoid, a necessary condition for net power. The focus consists of two coaxial cylindrical electrodes made from copper or beryllium and housed in a vacuum chamber containing a low-pressure fusible gas. An electrical pulse is applied across the electrodes, heating the gas into a plasma. The current forms into a minuscule vortex along the axis of the machine, which then kinks into a cage of current with an associated magnetic field. The cage of current and magnetic-field-entrapped plasma is called a plasmoid. The acceleration of the electrons about the magnetic field lines heats the nuclei within the plasmoid to fusion temperatures.\nIn April 2014, Lawrence Livermore National Laboratory ended the Laser Inertial Fusion Energy (LIFE) program and redirected their efforts towards NIF. In August 2014, Phoenix Nuclear Labs announced the sale of a high-yield neutron generator that could sustain 5\u00d71011 deuterium fusion reactions per second over a 24-hour period. In October 2014, Lockheed Martin's Skunk Works announced the development of a high-beta fusion reactor that they hope to yield a functioning 100-megawatt prototype by 2017 and to be ready for regular operation by 2022.\nDeep-space exploration, as well as higher-velocity lower-cost space transport services in general would be enabled by this compact fusion reactor technology.\nIn January 2015, the polywell was presented at Microsoft Research.\nIn August, 2015, MIT announced a tokamak it named ARC fusion reactor design using rare-earth barium-copper oxide (REBCO) superconducting tapes to produce high-magnetic field coils that it claimed produce comparable magnetic field strength in a smaller configuration than other designs.\nIn October 2015, researchers at the Max Planck Institute of Plasma Physics completed building the largest stellarator to date, named Wendelstein 7-X. On December 10, they successfully produced the first helium plasma, and on February 3, 2016 produced the device's first Hydrogen plasma. With plasma discharges lasting up to 30 minutes, Wendelstein 7-X will try to demonstrate the essential stellarator attribute: continuous operation of a high-temperature hydrogen plasma.\n\n\n== Fuels ==\nBy firing particle beams at targets, many fusion reactions have been tested, while the fuels considered for power have all been light elements like the isotopes of hydrogen\u2014deuterium and tritium. Other reactions like the deuterium and Helium3 reaction or the Helium3 and Helium3 reactions, would require a supply of Helium3. This can either come from other nuclear reactions or from extraterrestrial sources. Finally, researchers hope to do the p-11B reaction, because it does not directly produce neutrons, though side reactions can.\n\n\n=== Deuterium, tritium ===\n\nThe easiest nuclear reaction, at the lowest energy, is:\n2\n1D + 3\n1T \u2192 4\n2He + 1\n0n\nThis reaction is common in research, industrial and military applications, usually as a convenient source of neutrons. Deuterium is a naturally occurring isotope of hydrogen and is commonly available. The large mass ratio of the hydrogen isotopes makes their separation easy compared to the difficult uranium enrichment process. Tritium is a natural isotope of hydrogen, but because it has a short half-life of 12.32 years, it is hard to find, store, produce, and is expensive. Consequently, the deuterium-tritium fuel cycle requires the breeding of tritium from lithium using one of the following reactions:\n1\n0n + 6\n3Li \u2192 3\n1T + 4\n2He\n1\n0n + 7\n3Li \u2192 3\n1T + 4\n2He + 1\n0n\nThe reactant neutron is supplied by the D-T fusion reaction shown above, and the one that has the greatest yield of energy. The reaction with 6Li is exothermic, providing a small energy gain for the reactor. The reaction with 7Li is endothermic but does not consume the neutron. At least some 7Li reactions are required to replace the neutrons lost to absorption by other elements. Most reactor designs use the naturally occurring mix of lithium isotopes.\nSeveral drawbacks are commonly attributed to D-T fusion power:\nIt produces substantial amounts of neutrons that result in the neutron activation of the reactor materials.\nOnly about 20% of the fusion energy yield appears in the form of charged particles with the remainder carried off by neutrons, which limits the extent to which direct energy conversion techniques might be applied.\nIt requires the handling of the radioisotope tritium. Similar to hydrogen, tritium is difficult to contain and may leak from reactors in some quantity. Some estimates suggest that this would represent a fairly large environmental release of radioactivity.\nThe neutron flux expected in a commercial D-T fusion reactor is about 100 times that of current fission power reactors, posing problems for material design. After a series of D-T tests at JET, the vacuum vessel was sufficiently radioactive that remote handling was required for the year following the tests.\nIn a production setting, the neutrons would be used to react with lithium in order to create more tritium. This also deposits the energy of the neutrons in the lithium, which would then be transferred to drive electrical production. The lithium neutron absorption reaction protects the outer portions of the reactor from the neutron flux. Newer designs, the advanced tokamak in particular, also use lithium inside the reactor core as a key element of the design. The plasma interacts directly with the lithium, preventing a problem known as \"recycling\". The advantage of this design was demonstrated in the Lithium Tokamak Experiment.\n\n\n=== Deuterium ===\n\nThis is the second easiest fusion reaction, fusing deuterium with itself. The reaction has two branches that occur with nearly equal probability:\n\nThis reaction is also common in research. The optimum energy to initiate this reaction is 15 keV, only slightly higher than the optimum for the D-T reaction. The first branch does not produce neutrons, but it does produce tritium, so that a D-D reactor will not be completely tritium-free, even though it does not require an input of tritium or lithium. Unless the tritons can be quickly removed, most of the tritium produced would be burned before leaving the reactor, which would reduce the handling of tritium, but would produce more neutrons, some of which are very energetic. The neutron from the second branch has an energy of only 2.45 MeV (0.393 pJ), whereas the neutron from the D-T reaction has an energy of 14.1 MeV (2.26 pJ), resulting in a wider range of isotope production and material damage. When the tritons are removed quickly while allowing the 3He to react, the fuel cycle is called \"tritium suppressed fusion\" The removed tritium decays to 3He with a 12.5 year half life. By recycling the 3He produced from the decay of tritium back into the fusion reactor, the fusion reactor does not require materials resistant to fast 14.1 MeV (2.26 pJ) neutrons.\nAssuming complete tritium burn-up, the reduction in the fraction of fusion energy carried by neutrons would be only about 18%, so that the primary advantage of the D-D fuel cycle is that tritium breeding would not be required. Other advantages are independence from scarce lithium resources and a somewhat softer neutron spectrum. The disadvantage of D-D compared to D-T is that the energy confinement time (at a given pressure) must be 30 times longer and the power produced (at a given pressure and volume) would be 68 times less .\nAssuming complete removal of tritium and recycling of 3He, only 6% of the fusion energy is carried by neutrons. The tritium-suppressed D-D fusion requires an energy confinement that is 10 times longer compared to D-T and a plasma temperature that is twice as high.\n\n\n=== Deuterium, helium 3 ===\nA second-generation approach to controlled fusion power involves combining helium-3 (3He) and deuterium (2H):\n\nThis reaction produces a helium-4 nucleus (4He) and a high-energy proton. As with the p-11B aneutronic fusion fuel cycle, most of the reaction energy is released as charged particles, reducing activation of the reactor housing and potentially allowing more efficient energy harvesting (via any of several speculative technologies). In practice, D-D side reactions produce a significant number of neutrons, resulting in p-11B being the preferred cycle for aneutronic fusion.\n\n\n=== Proton, boron 11 ===\nIf aneutronic fusion is the goal, then the most promising candidate may be the Hydrogen-1 (proton)/boron reaction, which releases alpha (helium) particles, but does not rely on neutron scattering for energy transfer.\n1H + 11B \u2192 3 4He\nUnder reasonable assumptions, side reactions will result in about 0.1% of the fusion power being carried by neutrons. At 123 keV, the optimum temperature for this reaction is nearly ten times higher than that for the pure hydrogen reactions, the energy confinement must be 500 times better than that required for the D-T reaction, and the power density will be 2500 times lower than for D-T.\nBecause the confinement properties of conventional approaches to fusion such as the tokamak and laser pellet fusion are marginal, most proposals for aneutronic fusion are based on radically different confinement concepts, such as the Polywell and the Dense Plasma Focus. Results have been extremely promising:\n\"In the October 2013 edition of Nature Communications, a research team led by Christine Labaune at \u00c9cole Polytechnique in Palaiseau, France, reported a new record fusion rate: an estimated 80 million fusion reactions during the 1.5 nanoseconds that the laser fired, which is at least 100 times more than any previous proton-boron experiment. \" \n\n\n== Material selection ==\n\n\n=== Considerations ===\nAny power station using hot plasma, is going to have plasma facing walls. In even the simplest plasma approaches, the material will get blasted with matter and energy. This leads to a minimum list of considerations, including dealing with:\nA heating and cooling cycle, up to a 10 MW/m\u00b2 thermal load.\nNeutron radiation, which over time leads to neutron activation and embrittlement.\nHigh energy ions leaving at tens to hundreds of electronvolts.\nAlpha particles leaving at millions of electronvolts.\nElectrons leaving at high energy.\nLight radiation (IR, visible, UV, X-ray).\nDepending on the approach, these effects may be higher or lower than typical fission reactors like the pressurized water reactor (PWR). One estimate put the radiation at 100 times the (PWR). Materials need to be selected or developed that can withstand these basic conditions. Depending on the approach, however, there may be other considerations such as electrical conductivity, magnetic permeability and mechanical strength. There is also a need for materials whose primary components and impurities do not result in long-lived radioactive wastes.\n\n\n=== Durability ===\nFor long term use, each atom in the wall is expected to be hit by a neutron and displaced about a hundred times before the material is replaced. High-energy neutrons will produce hydrogen and helium by way of various nuclear reactions that tends to form bubbles at grain boundaries and result in swelling, blistering or embrittlement.\n\n\n=== Selection ===\nOne can choose either a low-Z material, such as graphite or beryllium, or a high-Z material, usually tungsten with molybdenum as a second choice. Use of liquid metals (lithium, gallium, tin) has also been proposed, e.g., by injection of 1\u20135 mm thick streams flowing at 10 m/s on solid substrates.\nIf graphite is used, the gross erosion rates due to physical and chemical sputtering would be many meters per year, so one must rely on redeposition of the sputtered material. The location of the redeposition will not exactly coincide with the location of the sputtering, so one is still left with erosion rates that may be prohibitive. An even larger problem is the tritium co-deposited with the redeposited graphite. The tritium inventory in graphite layers and dust in a reactor could quickly build up to many kilograms, representing a waste of resources and a serious radiological hazard in case of an accident. The consensus of the fusion community seems to be that graphite, although a very attractive material for fusion experiments, cannot be the primary PFC material in a commercial reactor.\nThe sputtering rate of tungsten by the plasma fuel ions is orders of magnitude smaller than that of carbon, and tritium is much less incorporated into redeposited tungsten, making this a more attractive choice. On the other hand, tungsten impurities in a plasma are much more damaging than carbon impurities, and self-sputtering of tungsten can be high, so it will be necessary to ensure that the plasma in contact with the tungsten is not too hot (a few tens of eV rather than hundreds of eV). Tungsten also has disadvantages in terms of eddy currents and melting in off-normal events, as well as some radiological issues.\n\n\n== Safety and the environment ==\n\n\n=== Accident potential ===\nNuclear fusion is unlike nuclear fission: fusion requires extremely precise and controlled temperature, pressure and magnetic field parameters for any net energy to be produced. If a reactor suffers damage or loses even a small degree of required control, fusion reactions and heat generation would rapidly cease. Additionally, fusion reactors contain relatively small amounts of fuel, enough to \"burn\" for minutes, or in some cases, microseconds. Unless they are actively refueled, the reactions will quickly end. Therefore, fusion reactors are considered extremely safe.\nRunaway reactions cannot occur in a fusion reactor. The plasma is burnt at optimal conditions, and any significant change will quench the reactions. The reaction process is so delicate that this level of safety is inherent. Although the plasma in a fusion power station is expected to have a volume of 1,000 cubic metres (35,000 cu ft) or more, the plasma density is low and the total amount of fusion fuel in the vessel typically only a few grams. If the fuel supply is closed, the reaction stops within seconds. In comparison, a fission reactor is typically loaded with enough fuel for several months or years, and no additional fuel is necessary to continue the reaction. It is this large amount of fuel that gives rise to the possibility of a meltdown; nothing analogous exists in a fusion reactor.\nIn the magnetic approach, strong fields are developed in coils that are held in place mechanically by the reactor structure. Failure of this structure could release this tension and allow the magnet to \"explode\" outward. The severity of this event would be similar to any other industrial accident or an MRI machine quench/explosion, and could be effectively stopped with a containment building similar to those used in existing (fission) nuclear generators. The laser-driven inertial approach is generally lower-stress because of the increased size of the reaction chamber. Although failure of the reaction chamber is possible, simply stopping fuel delivery would prevent any sort of catastrophic failure.\nMost reactor designs rely on liquid hydrogen as both a coolant and a method for converting stray neutrons from the reaction into tritium, which is fed back into the reactor as fuel. Hydrogen is highly flammable, and in the case of a fire it is possible that the hydrogen stored on-site could be burned up and escape. In this case, the tritium contents of the hydrogen would be released into the atmosphere, posing a radiation risk. Calculations suggest that at about 1 kg the total amount of tritium and other radioactive gases in a typical power station would be so small that they would have diluted to legally acceptable limits by the time they blew as far as the station's perimeter fence.\nThe likelihood of small industrial accidents including the local release of radioactivity and injury to staff cannot be estimated yet. These would include accidental releases of lithium or tritium or mis-handling of decommissioned radioactive components of the reactor itself.\n\n\n=== Magnet quench ===\nA quench is an abnormal termination of magnet operation that occurs when part of the superconducting coil enters the normal (resistive) state. This can occur because the field inside the magnet is too large, the rate of change of field is too large (causing eddy currents and resultant heating in the copper support matrix), or a combination of the two.\nMore rarely a defect in the magnet can cause a quench. When this happens, that particular spot is subject to rapid Joule heating from the enormous current, which raises the temperature of the surrounding regions. This pushes those regions into the normal state as well, which leads to more heating in a chain reaction. The entire magnet rapidly becomes normal (this can take several seconds, depending on the size of the superconducting coil). This is accompanied by a loud bang as the energy in the magnetic field is converted to heat, and rapid boil-off of the cryogenic fluid. The abrupt decrease of current can result in kilovolt inductive voltage spikes and arcing. Permanent damage to the magnet is rare, but components can be damaged by localized heating, high voltages, or large mechanical forces.\nIn practice, magnets usually have safety devices to stop or limit the current when the beginning of a quench is detected. If a large magnet undergoes a quench, the inert vapor formed by the evaporating cryogenic fluid can present a significant asphyxiation hazard to operators by displacing breathable air.\nA large section of the superconducting magnets in CERN's Large Hadron Collider unexpectedly quenched during start-up operations in 2008, necessitating the replacement of a number of magnets. In order to mitigate against potentially destructive quenches, the superconducting magnets that form the LHC are equipped with fast-ramping heaters which are activated once a quench event is detected by the complex quench protection system. As the dipole bending magnets are connected in series, each power circuit includes 154 individual magnets, and should a quench event occur, the entire combined stored energy of these magnets must be dumped at once. This energy is transferred into dumps that are massive blocks of metal which heat up to several hundreds of degrees Celsius\u2014because of resistive heating\u2014in a matter of seconds. Although undesirable, a magnet quench is a \"fairly routine event\" during the operation of a particle accelerator.\n\n\n=== Effluents ===\nThe natural product of the fusion reaction is a small amount of helium, which is completely harmless to life. Of more concern is tritium, which, like other isotopes of hydrogen, is difficult to retain completely. During normal operation, some amount of tritium will be continually released.\nAlthough tritium is volatile and biologically active, the health risk posed by a release is much lower than that of most radioactive contaminants, because of tritium's short half-life (12.32 years) and very low decay energy (~14.95 keV), and because it does not bioaccumulate (instead being cycled out of the body as water, with a biological half-life of 7 to 14 days). Current ITER designs are investigating total containment facilities for any tritium.\n\n\n=== Waste management ===\nThe large flux of high-energy neutrons in a reactor will make the structural materials radioactive. The radioactive inventory at shut-down may be comparable to that of a fission reactor, but there are important differences.\nThe half-life of the radioisotopes produced by fusion tends to be less than those from fission, so that the inventory decreases more rapidly. Unlike fission reactors, whose waste remains radioactive for thousands of years, most of the radioactive material in a fusion reactor would be the reactor core itself, which would be dangerous for about 50 years, and low-level waste for another 100. Although this waste will be considerably more radioactive during those 50 years than fission waste, the very short half-life makes the process very attractive, as the waste management is fairly straightforward. By 500 years the material would have the same radiotoxicity as coal ash.\nAdditionally, the choice of materials used in a fusion reactor is less constrained than in a fission design, where many materials are required for their specific neutron cross-sections. This allows a fusion reactor to be designed using materials that are selected specifically to be \"low activation\", materials that do not easily become radioactive. Vanadium, for example, would become much less radioactive than stainless steel. Carbon fiber materials are also low-activation, as well as being strong and light, and are a promising area of study for laser-inertial reactors where a magnetic field is not required.\nIn general terms, fusion reactors would create far less radioactive material than a fission reactor, the material it would create is less damaging biologically, and the radioactivity \"burns off\" within a time period that is well within existing engineering capabilities for safe long-term waste storage.\n\n\n=== Nuclear proliferation ===\n\nAlthough fusion power uses nuclear technology, the overlap with nuclear weapons would be limited. A huge amount of tritium could be produced by a fusion power station; tritium is used in the trigger of hydrogen bombs and in a modern boosted fission weapon, but it can also be produced by nuclear fission. The energetic neutrons from a fusion reactor could be used to breed weapons-grade plutonium or uranium for an atomic bomb (for example by transmutation of U238 to Pu239, or Th232 to U233).\nA study conducted 2011 assessed the risk of three scenarios:\nUse in small-scale fusion station: As a result of much higher power consumption, heat dissipation and a more recognizable design compared to enrichment gas centrifuges this choice would be much easier to detect and therefore implausible.\nModifications to produce weapon-usable material in a commercial facility: The production potential is significant. But no fertile or fissile substances necessary for the production of weapon-usable materials needs to be present at a civil fusion system at all. If not shielded, a detection of these materials can be done by their characteristic gamma radiation. The underlying redesign could be detected by regular design information verifications. In the (technically more feasible) case of solid breeder blanket modules, it would be necessary for incoming components to be inspected for the presence of fertile material, otherwise plutonium for several weapons could be produced each year.\nPrioritizing a fast production of weapon-grade material regardless of secrecy: The fastest way to produce weapon usable material was seen in modifying a prior civil fusion power station. Unlike in some nuclear power stations, there is no weapon compatible material during civil use. Even without the need for covert action this modification would still take about 2 months to start the production and at least an additional week to generate a significant amount for weapon production. This was seen as enough time to detect a military use and to react with diplomatic or military means. To stop the production, a military destruction of inevitable parts of the facility leaving out the reactor itself would be sufficient. This, together with the intrinsic safety of fusion power would only bear a low risk of radioactive contamination.\nAnother study concludes that \"[..]large fusion reactors \u2013 even if not designed for fissile material breeding \u2013 could easily produce several hundred kg Pu per year with high weapon quality and very low source material requirements.\" It was emphasized that the implementation of features for intrinsic proliferation resistance might only be possible at this phase of research and development. The theoretical and computational tools needed for hydrogen bomb design are closely related to those needed for inertial confinement fusion, but have very little in common with the more scientifically developed magnetic confinement fusion.\n\n\n=== Energy source ===\nLarge-scale reactors using neutronic fuels (e.g. ITER) and thermal power production (turbine based) are most comparable to fission power from an engineering and economics viewpoint. Both fission and fusion power stations involve a relatively compact heat source powering a conventional steam turbine-based power station, while producing enough neutron radiation to make activation of the station materials problematic. The main distinction is that fusion power produces no high-level radioactive waste (though activated station materials still need to be disposed of). There are some power station ideas that may significantly lower the cost or size of such stations; however, research in these areas is nowhere near as advanced as in tokamaks.\nFusion power commonly proposes the use of deuterium, an isotope of hydrogen, as fuel and in many current designs also use lithium. Assuming a fusion energy output equal to the 1995 global power output of about 100 EJ/yr (= 1 \u00d7 1020 J/yr) and that this does not increase in the future, which is unlikely, then the known current lithium reserves would last 3000 years. Lithium from sea water would last 60 million years, however, and a more complicated fusion process using only deuterium from sea water would have fuel for 150 billion years. To put this in context, 150 billion years is close to 30 times the remaining lifespan of the sun, and more than 10 times the estimated age of the universe.\n\n\n== Economics ==\nWhile fusion power is still in early stages of development, substantial sums have been and continue to be invested in research. In the EU almost \u20ac10 billion was spent on fusion research up to the end of the 1990s, and the new ITER reactor alone is budgeted at \u20ac6.6 billion total for the timeframe between 2008 and 2020.\nIt is estimated that up to the point of possible implementation of electricity generation by nuclear fusion, R&D will need further promotion totalling around \u20ac60\u201380 billion over a period of 50 years or so (of which \u20ac20\u201330 billion within the EU) based on a report from 2002. Nuclear fusion research receives \u20ac750 million (excluding ITER funding) from the European Union, compared with \u20ac810 million for sustainable energy research, putting research into fusion power well ahead of that of any single rivaling technology. Indeed, the size of the investments and time frame of the expected results mean that fusion research is almost exclusively publicly funded, while research in other forms of energy can be done by the private sector. In spite of that, a number of start-up companies active in the field of fusion power have managed to attract private money.\n\n\n== Advantages ==\nFusion power would provide more energy for a given weight of fuel than any fuel-consuming energy source currently in use, and the fuel itself (primarily deuterium) exists abundantly in the Earth's ocean: about 1 in 6500 hydrogen atoms in seawater is deuterium. Although this may seem a low proportion (about 0.015%), because nuclear fusion reactions are so much more energetic than chemical combustion and seawater is easier to access and more plentiful than fossil fuels, fusion could potentially supply the world's energy needs for millions of years.\nDespite being technically non-renewable, fusion power has many of the benefits of renewable energy sources (such as being a long-term energy supply and emitting no greenhouse gases) as well as some of the benefits of the resource-limited energy sources as hydrocarbons and nuclear fission (without reprocessing). Like these currently dominant energy sources, fusion could provide very high power-generation density and uninterrupted power delivery (because it is not dependent on the weather, unlike wind and solar power).\nAnother aspect of fusion energy is that the cost of production does not suffer from diseconomies of scale. The cost of water and wind energy, for example, goes up as the optimal locations are developed first, while further generators must be sited in less ideal conditions. With fusion energy the production cost will not increase much even if large numbers of stations are built, because the raw resource (seawater) is abundant and widespread.\nSome problems that are expected to be an issue in this century, such as fresh water shortages, can alternatively be regarded as problems of energy supply. For example, in desalination stations, seawater can be purified through distillation or reverse osmosis. Nonetheless, these processes are energy intensive. Even if the first fusion stations are not competitive with alternative sources, fusion could still become competitive if large-scale desalination requires more power than the alternatives are able to provide.\nA scenario has been presented of the effect of the commercialization of fusion power on the future of human civilization. ITER and later Demo are envisioned to bring online the first commercial nuclear fusion energy reactor by 2050. Using this as the starting point and the history of the uptake of nuclear fission reactors as a guide, the scenario depicts a rapid take up of nuclear fusion energy starting after the middle of this century.\nFusion power could be used in interstellar space, where solar energy is not available.\n\n\n== Criticism ==\nBecause commercial fusion projects are very large and complex, and ongoing funding is a political issue, such projects usually involve cost overruns and missed deadlines. For example, the construction of the National Ignition Facility cost $5 billion and took seven years longer than expected. ITER's expected cost has gone from $5 billion to $20 billion, and the date for full power operation has been put back to 2027, from the original estimate of 2016.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Bibliography ==\nChen, Francis (2011). An Indispensable Truth: How Fusion Power Can Save the Planet. New York: Springer. ISBN 978-1441978196\nClery, Daniel (2013). A Piece of the Sun. New York: Overlook. ISBN 978-1468304930\nDean, Stephen (2013). Search for the Ultimate Energy Source: A History of the U.S. Fusion Energy Program. New York: Springer. ISBN 978-1461460367\nMolina, Andr\u00e9s de Bustos (2013) Kinetic Simulations of Ion Transport in Fusion Devices. New York: Springer. ISBN 978-3319004211\nVoss, David (March 1, 1999). \"What Ever Happened to Cold Fusion\". Physics World. ISSN 0953-8585. Retrieved 1 May 2008. \nKruglinksi, Susan (2006-03-03). \"Whatever Happened To... Cold Fusion?\". Discover Magazine. ISSN 0274-7529. Retrieved 20 June 2008. \nChoi, Charles (2005). \"Back to Square One\". Scientific American. Retrieved 25 November 2008. \nFeder, Toni (January 2005). \"Cold Fusion Gets Chilly Encore\". Physics Today. 58: 31. Bibcode:2005PhT....58a..31F. doi:10.1063/1.1881896. \nHagelstein, Peter L.; McKubre, Michael; Nagel, David; Chubb, Talbot; Hekman, Randall (2004), New Physical Effects in Metal Deuterides (PDF), Washington: US Department of Energy, archived from the original (PDF) on January 6, 2007  (manuscript)\nU.S. Department of Energy (2004), Report of the Review of Low Energy Nuclear Reactions (PDF), Washington, DC: U.S. Department of Energy, archived from the original (PDF) on 2008-02-26, retrieved 2008-07-19 \nGoodstein, David (1994), \"Whatever happened to cold fusion?\", American Scholar, Phi Beta Kappa Society, 63 (4): 527\u2013541, ISSN 0003-0937, retrieved 2008-05-25 \nClose, Frank E. (1992), Too Hot to Handle: The Race for Cold Fusion (2 ed.), London: Penguin, ISBN 0-14-015926-6 \nBeaudette, Charles G. (2002), Excess Heat & Why Cold Fusion Research Prevailed, South Bristol, Maine: Oak Grove Press, ISBN 0-9678548-3-0 \nVan Noorden, R. (April 2007), \"Cold fusion back on the menu\", Chemistry World, ISSN 1473-7604, retrieved 2008-05-25 \nTaubes, Gary (1993). Bad Science: The Short Life and Weird Times of Cold Fusion. New York: Random House. ISBN 0-394-58456-2. \nBrowne, M. (May 3, 1989), \"Physicists Debunk Claim Of a New Kind of Fusion\", New York Times, retrieved 2008-05-25 \nAdam, David (24 March 2005), Rusbringer, Alan, ed., \"In from the cold\", The Guardian, London, retrieved 2008-05-25 \nPlatt, Charles (1998), \"What if Cold Fusion is Real?\", Wired Magazine (6.11), retrieved 2008-05-25 \nHutchinson, Alex (January 8, 2006), \"The Year in Science: Physics\", Discover Magazine (online), ISSN 0274-7529, retrieved 2008-06-20 \nAdam, David (24 March 2005), Rusbringer, Alan, ed., \"In from the cold\", The Guardian, London, retrieved 2008-05-25 \nAlfred, Randy (2009-03-23). \"March 23, 1989: Cold Fusion Gets Cold Shoulder\". Wired. Archived from the original on January 4, 2014. \n\n\n== External links ==\nUltimate Energy: Fusion Reactor Research\nFusion as an Energy Source\nU.S. Fusion Energy Science Program\nEURATOM/UKAEA Fusion Association\nITER\nEUROfusion\nA Central Site for Fusion Energy Links\nInstitute for Plasma Focus Studies\nMontage of 60 years of fusion research history", 
                "titleUrl": "https://en.wikipedia.org/wiki/Fusion_power", 
                "title": "Fusion power"
            }
        ], 
        "phraseCharStart": "1129"
    }, 
    {
        "phraseCharEnd": "1230", 
        "phraseIndex": "T28", 
        "phraseGoldStandardTag": "Task", 
        "phrase": "plasma density control", 
        "wikiSearchResults": [
            {
                "snippet": "{\\displaystyle n_{e}}    is the number density of electrons.  See also: Nonthermal plasma and Anisothermal plasma Plasma temperature is commonly measured in", 
                "pageCategories": "Articles containing video clips\nArticles with Wayback Machine links\nAstrophysics\nCS1 maint: Uses editors parameter\nConcepts in physics\nElectrical conductors\nGases\nGood articles\nPhases of matter\nPlasma physics", 
                "pageContent": "Plasma (from Greek \u03c0\u03bb\u03ac\u03c3\u03bc\u03b1, \"anything formed\") is one of the four fundamental states of matter, the others being solid, liquid, and gas. A plasma has properties unlike those of the other states.\nA plasma can be created by heating a gas or subjecting it to a strong electromagnetic field, applied with a laser or microwave generator at temperatures above 5000 Celsius. This decreases or increases the number of electrons, creating positive or negative charged particles called ions, and is accompanied by the dissociation of molecular bonds, if present.\nThe presence of a significant number of charge carriers makes plasma electrically conductive so that it responds strongly to electromagnetic fields. Like gas, plasma does not have a definite shape or a definite volume unless enclosed in a container. Unlike gas, under the influence of a magnetic field, it may form structures such as filaments, beams and double layers.\nPlasma is the most abundant form of ordinary matter in the Universe (of the forms proven to exist; the more abundant dark matter is hypothetical and may or may not be explained by ordinary matter), most of which is in the rarefied intergalactic regions, particularly the intracluster medium, and in stars, including the Sun. A common form of plasma on Earth is produced in neon signs.\nMuch of the understanding of plasma has come from the pursuit of controlled nuclear fusion and fusion power, for which plasma physics provides the scientific foundation.\n\n\n== Properties and parameters ==\n\n\n=== Definition ===\nPlasma is an electrically neutral medium of unbound positive and negative particles (i.e. the overall charge of a plasma is roughly zero). It is important to note that although the particles are unbound, they are not \u2018free\u2019 in the sense of not experiencing forces. When a charged particle moves, it generates an electric current with magnetic fields; in plasma, the movement of a charged particle affects and is affected by the general field created by the movement of other charges. This governs collective behavior with many degrees of variation. Three factors are listed in the definition of a plasma stream:\nThe plasma approximation: Charged particles must be close enough together that each particle influences many nearby charged particles, rather than just interacting with the closest particle (these collective effects are a distinguishing feature of a plasma). The plasma approximation is valid when the number of charge carriers within the sphere of influence (called the Debye sphere whose radius is the Debye screening length) of a particular particle is higher than unity to provide collective behavior of the charged particles. The average number of particles in the Debye sphere is given by the plasma parameter, \"\u039b\" (the Greek uppercase letter Lambda).\nBulk interactions: The Debye screening length (defined above) is short compared to the physical size of the plasma. This criterion means that interactions in the bulk of the plasma are more important than those at its edges, where boundary effects may take place. When this criterion is satisfied, the plasma is quasineutral.\nPlasma frequency: The electron plasma frequency (measuring plasma oscillations of the electrons) is large compared to the electron-neutral collision frequency (measuring frequency of collisions between electrons and neutral particles). When this condition is valid, electrostatic interactions dominate over the processes of ordinary gas kinetics.\n\n\n=== Ranges of parameters ===\nThe factors of a plasma stream can vary by many orders of magnitude, but the properties of plasmas with apparently disparate parameters may be very similar (see plasma scaling). The following chart considers only conventional atomic plasmas and not exotic phenomena like quark gluon plasmas:\n\n\n=== Degree of ionization ===\nFor plasma to exist, ionization is necessary. The term \"plasma density\" by itself usually refers to the \"electron density\", that is, the number of free electrons per unit volume. The degree of ionization of a plasma is the proportion of atoms that have lost or gained electrons, and is controlled mostly by the temperature. Even a partially ionized gas in which as little as 1% of the particles are ionized can have the characteristics of a plasma (i.e., response to magnetic fields and high electrical conductivity). The degree of ionization, \n  \n    \n      \n        \u03b1\n      \n    \n    {\\displaystyle \\alpha }\n  , is defined as \n  \n    \n      \n        \u03b1\n        =\n        \n          \n            \n              n\n              \n                i\n              \n            \n            \n              \n                n\n                \n                  i\n                \n              \n              +\n              \n                n\n                \n                  n\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\alpha ={\\frac {n_{i}}{n_{i}+n_{n}}}}\n  , where \n  \n    \n      \n        \n          n\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle n_{i}}\n   is the number density of ions and \n  \n    \n      \n        \n          n\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle n_{n}}\n   is the number density of neutral atoms. The electron density is related to this by the average charge state \n  \n    \n      \n        \u27e8\n        Z\n        \u27e9\n      \n    \n    {\\displaystyle \\langle Z\\rangle }\n   of the ions through \n  \n    \n      \n        \n          n\n          \n            e\n          \n        \n        =\n        \u27e8\n        Z\n        \u27e9\n        \n          n\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle n_{e}=\\langle Z\\rangle n_{i}}\n  , where \n  \n    \n      \n        \n          n\n          \n            e\n          \n        \n      \n    \n    {\\displaystyle n_{e}}\n   is the number density of electrons.\n\n\n=== Temperatures ===\n\nPlasma temperature is commonly measured in kelvins or electronvolts and is, informally, a measure of the thermal kinetic energy per particle. High temperatures are usually needed to sustain ionization, which is a defining feature of a plasma. The degree of plasma ionization is determined by the electron temperature relative to the ionization energy (and more weakly by the density), in a relationship called the Saha equation. At low temperatures, ions and electrons tend to recombine into bound states\u2014atoms\u2014and the plasma will eventually become a gas.\nIn most cases the electrons are close enough to thermal equilibrium that their temperature is relatively well-defined, even when there is a significant deviation from a Maxwellian energy distribution function, for example, due to UV radiation, energetic particles, or strong electric fields. Because of the large difference in mass, the electrons come to thermodynamic equilibrium amongst themselves much faster than they come into equilibrium with the ions or neutral atoms. For this reason, the ion temperature may be very different from (usually lower than) the electron temperature. This is especially common in weakly ionized technological plasmas, where the ions are often near the ambient temperature.\n\n\n==== Thermal vs. nonthermal plasmas ====\nBased on the relative temperatures of the electrons, ions and neutrals, plasmas are classified as \"thermal\" or \"non-thermal\". Thermal plasmas have electrons and the heavy particles at the same temperature, i.e. they are in thermal equilibrium with each other. Nonthermal plasmas on the other hand have the ions and neutrals at a much lower temperature (sometimes room temperature), whereas electrons are much \"hotter\" (\n  \n    \n      \n        \n          T\n          \n            e\n          \n        \n        \u226b\n        \n          T\n          \n            n\n          \n        \n      \n    \n    {\\displaystyle T_{e}\\gg T_{n}}\n  ).\n\n\n==== Complete vs. incomplete ionization ====\nA plasma is sometimes referred to as being \"hot\" if it is nearly fully ionized, or \"cold\" if only a small fraction (for example 1%) of the gas molecules are ionized, but other definitions of the terms \"hot plasma\" and \"cold plasma\" are common. Even in a \"cold\" plasma, the electron temperature is still typically several thousand degrees Celsius. Plasmas utilized in \"plasma technology\" (\"technological plasmas\") are usually cold plasmas in the sense that only a small fraction of the gas molecules are ionized.\n\n\n=== Plasma potential ===\n\nSince plasmas are very good electrical conductors, electric potentials play an important role. The potential as it exists on average in the space between charged particles, independent of the question of how it can be measured, is called the \"plasma potential\", or the \"space potential\". If an electrode is inserted into a plasma, its potential will generally lie considerably below the plasma potential due to what is termed a Debye sheath. The good electrical conductivity of plasmas makes their electric fields very small. This results in the important concept of \"quasineutrality\", which says the density of negative charges is approximately equal to the density of positive charges over large volumes of the plasma (\n  \n    \n      \n        \n          n\n          \n            e\n          \n        \n        =\n        \u27e8\n        Z\n        \u27e9\n        \n          n\n          \n            i\n          \n        \n      \n    \n    {\\displaystyle n_{e}=\\langle Z\\rangle n_{i}}\n  ), but on the scale of the Debye length there can be charge imbalance. In the special case that double layers are formed, the charge separation can extend some tens of Debye lengths.\nThe magnitude of the potentials and electric fields must be determined by means other than simply finding the net charge density. A common example is to assume that the electrons satisfy the Boltzmann relation:\n\n  \n    \n      \n        \n          n\n          \n            e\n          \n        \n        \u221d\n        \n          e\n          \n            e\n            \u03a6\n            \n              /\n            \n            \n              k\n              \n                B\n              \n            \n            \n              T\n              \n                e\n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle n_{e}\\propto e^{e\\Phi /k_{B}T_{e}}.}\n  \nDifferentiating this relation provides a means to calculate the electric field from the density:\n\n  \n    \n      \n        \n          \n            \n              E\n              \u2192\n            \n          \n        \n        =\n        (\n        \n          k\n          \n            B\n          \n        \n        \n          T\n          \n            e\n          \n        \n        \n          /\n        \n        e\n        )\n        (\n        \u2207\n        \n          n\n          \n            e\n          \n        \n        \n          /\n        \n        \n          n\n          \n            e\n          \n        \n        )\n        .\n      \n    \n    {\\displaystyle {\\vec {E}}=(k_{B}T_{e}/e)(\\nabla n_{e}/n_{e}).}\n  \nIt is possible to produce a plasma that is not quasineutral. An electron beam, for example, has only negative charges. The density of a non-neutral plasma must generally be very low, or it must be very small, otherwise it will be dissipated by the repulsive electrostatic force.\nIn astrophysical plasmas, Debye screening prevents electric fields from directly affecting the plasma over large distances, i.e., greater than the Debye length. However, the existence of charged particles causes the plasma to generate, and be affected by, magnetic fields. This can and does cause extremely complex behavior, such as the generation of plasma double layers, an object that separates charge over a few tens of Debye lengths. The dynamics of plasmas interacting with external and self-generated magnetic fields are studied in the academic discipline of magnetohydrodynamics.\n\n\n=== Magnetization ===\nPlasma with a magnetic field strong enough to influence the motion of the charged particles is said to be magnetized. A common quantitative criterion is that a particle on average completes at least one gyration around the magnetic field before making a collision, i.e., \n  \n    \n      \n        \n          \u03c9\n          \n            \n              c\n              e\n            \n          \n        \n        \n          /\n        \n        \n          v\n          \n            \n              c\n              o\n              l\n              l\n            \n          \n        \n        >\n        1\n      \n    \n    {\\displaystyle \\omega _{\\mathrm {ce} }/v_{\\mathrm {coll} }>1}\n  , where \n  \n    \n      \n        \n          \u03c9\n          \n            \n              c\n              e\n            \n          \n        \n      \n    \n    {\\displaystyle \\omega _{\\mathrm {ce} }}\n   is the \"electron gyrofrequency\" and \n  \n    \n      \n        \n          v\n          \n            \n              c\n              o\n              l\n              l\n            \n          \n        \n      \n    \n    {\\displaystyle v_{\\mathrm {coll} }}\n   is the \"electron collision rate\". It is often the case that the electrons are magnetized while the ions are not. Magnetized plasmas are anisotropic, meaning that their properties in the direction parallel to the magnetic field are different from those perpendicular to it. While electric fields in plasmas are usually small due to the high conductivity, the electric field associated with a plasma moving in a magnetic field is given by \n  \n    \n      \n        \n          E\n        \n        =\n        \u2212\n        v\n        \u00d7\n        \n          B\n        \n      \n    \n    {\\displaystyle \\mathbf {E} =-v\\times \\mathbf {B} }\n   (where \n  \n    \n      \n        \n          E\n        \n      \n    \n    {\\displaystyle \\mathbf {E} }\n   is the electric field, \n  \n    \n      \n        \n          v\n        \n      \n    \n    {\\displaystyle \\mathbf {v} }\n   is the velocity, and \n  \n    \n      \n        \n          B\n        \n      \n    \n    {\\displaystyle \\mathbf {B} }\n   is the magnetic field), and is not affected by Debye shielding.\n\n\n=== Comparison of plasma and gas phases ===\nPlasma is often called the fourth state of matter after solid, liquids and gases, despite plasma typically being an ionized gas. It is distinct from these and other lower-energy states of matter. Although it is closely related to the gas phase in that it also has no definite form or volume, it differs in a number of ways, including the following:\n\n\n== Common plasmas ==\n\nPlasmas are by far the most common phase of ordinary matter in the universe, both by mass and by volume. Essentially, all of the visible light from space comes from stars, which are plasmas with a temperature such that they radiate strongly at visible wavelengths. Most of the ordinary (or baryonic) matter in the universe, however, is found in the intergalactic medium, which is also a plasma, but much hotter, so that it radiates primarily as X-rays.\nIn 1937, Hannes Alfv\u00e9n argued that if plasma pervaded the universe, it could then carry electric currents capable of generating a galactic magnetic field. After winning the Nobel Prize, he emphasized that:\n\nIn order to understand the phenomena in a certain plasma region, it is necessary to map not only the magnetic but also the electric field and the electric currents. Space is filled with a network of currents which transfer energy and momentum over large or very large distances. The currents often pinch to filamentary or surface currents. The latter are likely to give space, as also interstellar and intergalactic space, a cellular structure.\n\nBy contrast the current scientific consensus is that about 96% of the total energy density in the universe is not plasma or any other form of ordinary matter, but a combination of cold dark matter and dark energy. Our Sun, and all stars, are made of plasma, much of interstellar space is filled with a plasma, albeit a very sparse one, and intergalactic space too. Even black holes, which are not directly visible, are thought to be fuelled by accreting ionising matter (i.e. plasma), and they are associated with astrophysical jets of luminous ejected plasma, such as M87's jet that extends 5,000 light-years.\nIn our solar system, interplanetary space is filled with the plasma of the Solar Wind that extends from the Sun out to the heliopause. However, the density of ordinary matter is much higher than average and much higher than that of either dark matter or dark energy. The planet Jupiter accounts for most of the non-plasma within the orbit of Pluto (about 0.1% by mass, or 10\u221215% by volume).\nDust and small grains within a plasma will also pick up a net negative charge, so that they in turn may act like a very heavy negative ion component of the plasma (see dusty plasmas).\n\n\n== Complex plasma phenomena ==\nAlthough the underlying equations governing plasmas are relatively simple, plasma behavior is extraordinarily varied and subtle: the emergence of unexpected behavior from a simple model is a typical feature of a complex system. Such systems lie in some sense on the boundary between ordered and disordered behavior and cannot typically be described either by simple, smooth, mathematical functions, or by pure randomness. The spontaneous formation of interesting spatial features on a wide range of length scales is one manifestation of plasma complexity. The features are interesting, for example, because they are very sharp, spatially intermittent (the distance between features is much larger than the features themselves), or have a fractal form. Many of these features were first studied in the laboratory, and have subsequently been recognized throughout the universe. Examples of complexity and complex structures in plasmas include:\n\n\n=== Filamentation ===\nStriations or string-like structures, also known as birkeland currents, are seen in many plasmas, like the plasma ball, the aurora, lightning, electric arcs, solar flares, and supernova remnants. They are sometimes associated with larger current densities, and the interaction with the magnetic field can form a magnetic rope structure. High power microwave breakdown at atmospheric pressure also leads to the formation of filamentary structures. (See also Plasma pinch)\nFilamentation also refers to the self-focusing of a high power laser pulse. At high powers, the nonlinear part of the index of refraction becomes important and causes a higher index of refraction in the center of the laser beam, where the laser is brighter than at the edges, causing a feedback that focuses the laser even more. The tighter focused laser has a higher peak brightness (irradiance) that forms a plasma. The plasma has an index of refraction lower than one, and causes a defocusing of the laser beam. The interplay of the focusing index of refraction, and the defocusing plasma makes the formation of a long filament of plasma that can be micrometers to kilometers in length. One interesting aspect of the filamentation generated plasma is the relatively low ion density due to defocusing effects of the ionized electrons. (See also Filament propagation)\n\n\n=== Shocks or double layers ===\nPlasma properties change rapidly (within a few Debye lengths) across a two-dimensional sheet in the presence of a (moving) shock or (stationary) double layer. Double layers involve localized charge separation, which causes a large potential difference across the layer, but does not generate an electric field outside the layer. Double layers separate adjacent plasma regions with different physical characteristics, and are often found in current carrying plasmas. They accelerate both ions and electrons.\n\n\n=== Electric fields and circuits ===\nQuasineutrality of a plasma requires that plasma currents close on themselves in electric circuits. Such circuits follow Kirchhoff's circuit laws and possess a resistance and inductance. These circuits must generally be treated as a strongly coupled system, with the behavior in each plasma region dependent on the entire circuit. It is this strong coupling between system elements, together with nonlinearity, which may lead to complex behavior. Electrical circuits in plasmas store inductive (magnetic) energy, and should the circuit be disrupted, for example, by a plasma instability, the inductive energy will be released as plasma heating and acceleration. This is a common explanation for the heating that takes place in the solar corona. Electric currents, and in particular, magnetic-field-aligned electric currents (which are sometimes generically referred to as \"Birkeland currents\"), are also observed in the Earth's aurora, and in plasma filaments.\n\n\n=== Cellular structure ===\nNarrow sheets with sharp gradients may separate regions with different properties such as magnetization, density and temperature, resulting in cell-like regions. Examples include the magnetosphere, heliosphere, and heliospheric current sheet. Hannes Alfv\u00e9n wrote: \"From the cosmological point of view, the most important new space research discovery is probably the cellular structure of space. As has been seen in every region of space accessible to in situ measurements, there are a number of 'cell walls', sheets of electric currents, which divide space into compartments with different magnetization, temperature, density, etc.\"\n\n\n=== Critical ionization velocity ===\nThe critical ionization velocity is the relative velocity between an ionized plasma and a neutral gas, above which a runaway ionization process takes place. The critical ionization process is a quite general mechanism for the conversion of the kinetic energy of a rapidly streaming gas into ionization and plasma thermal energy. Critical phenomena in general are typical of complex systems, and may lead to sharp spatial or temporal features.\n\n\n=== Ultracold plasma ===\nUltracold plasmas are created in a magneto-optical trap (MOT) by trapping and cooling neutral atoms, to temperatures of 1 mK or lower, and then using another laser to ionize the atoms by giving each of the outermost electrons just enough energy to escape the electrical attraction of its parent ion.\nOne advantage of ultracold plasmas are their well characterized and tunable initial conditions, including their size and electron temperature. By adjusting the wavelength of the ionizing laser, the kinetic energy of the liberated electrons can be tuned as low as 0.1 K, a limit set by the frequency bandwidth of the laser pulse. The ions inherit the millikelvin temperatures of the neutral atoms, but are quickly heated through a process known as disorder induced heating (DIH). This type of non-equilibrium ultracold plasma evolves rapidly, and displays many other interesting phenomena.\nOne of the metastable states of a strongly nonideal plasma is Rydberg matter, which forms upon condensation of excited atoms.\n\n\n=== Non-neutral plasma ===\nThe strength and range of the electric force and the good conductivity of plasmas usually ensure that the densities of positive and negative charges in any sizeable region are equal (\"quasineutrality\"). A plasma with a significant excess of charge density, or, in the extreme case, is composed of a single species, is called a non-neutral plasma. In such a plasma, electric fields play a dominant role. Examples are charged particle beams, an electron cloud in a Penning trap and positron plasmas.\n\n\n=== Dusty plasma/grain plasma ===\nA dusty plasma contains tiny charged particles of dust (typically found in space). The dust particles acquire high charges and interact with each other. A plasma that contains larger particles is called grain plasma. Under laboratory conditions, dusty plasmas are also called complex plasmas.\n\n\n=== Impermeable plasma ===\nImpermeable plasma is a type of thermal plasma which acts like an impermeable solid with respect to gas or cold plasma and can be physically pushed. Interaction of cold gas and thermal plasma was briefly studied by a group led by Hannes Alfv\u00e9n in 1960s and 1970s for its possible applications in insulation of fusion plasma from the reactor walls. However, later it was found that the external magnetic fields in this configuration could induce kink instabilities in the plasma and subsequently lead to an unexpectedly high heat loss to the walls. In 2013, a group of materials scientists reported that they have successfully generated stable impermeable plasma with no magnetic confinement using only an ultrahigh-pressure blanket of cold gas. While spectroscopic data on the characteristics of plasma were claimed to be difficult to obtain due to the high pressure, the passive effect of plasma on synthesis of different nanostructures clearly suggested the effective confinement. They also showed that upon maintaining the impermeability for a few tens of seconds, screening of ions at the plasma-gas interface could give rise to a strong secondary mode of heating (known as viscous heating) leading to different kinetics of reactions and formation of complex nanomaterials.\n\n\n== Mathematical descriptions ==\n\nTo completely describe the state of a plasma, we would need to write down all the particle locations and velocities and describe the electromagnetic field in the plasma region. However, it is generally not practical or necessary to keep track of all the particles in a plasma. Therefore, plasma physicists commonly use less detailed descriptions, of which there are two main types:\n\n\n=== Fluid model ===\nFluid models describe plasmas in terms of smoothed quantities, like density and averaged velocity around each position (see Plasma parameters). One simple fluid model, magnetohydrodynamics, treats the plasma as a single fluid governed by a combination of Maxwell's equations and the Navier\u2013Stokes equations. A more general description is the two-fluid plasma picture, where the ions and electrons are described separately. Fluid models are often accurate when collisionality is sufficiently high to keep the plasma velocity distribution close to a Maxwell\u2013Boltzmann distribution. Because fluid models usually describe the plasma in terms of a single flow at a certain temperature at each spatial location, they can neither capture velocity space structures like beams or double layers, nor resolve wave-particle effects.\n\n\n=== Kinetic model ===\nKinetic models describe the particle velocity distribution function at each point in the plasma and therefore do not need to assume a Maxwell\u2013Boltzmann distribution. A kinetic description is often necessary for collisionless plasmas. There are two common approaches to kinetic description of a plasma. One is based on representing the smoothed distribution function on a grid in velocity and position. The other, known as the particle-in-cell (PIC) technique, includes kinetic information by following the trajectories of a large number of individual particles. Kinetic models are generally more computationally intensive than fluid models. The Vlasov equation may be used to describe the dynamics of a system of charged particles interacting with an electromagnetic field. In magnetized plasmas, a gyrokinetic approach can substantially reduce the computational expense of a fully kinetic simulation.\n\n\n== Artificial plasmas ==\nMost artificial plasmas are generated by the application of electric and/or magnetic fields through a gas. Plasma generated in a laboratory setting and for industrial use can be generally categorized by:\nThe type of power source used to generate the plasma\u2014DC, RF and microwave\nThe pressure they operate at\u2014vacuum pressure (< 10 mTorr or 1 Pa), moderate pressure (~ 1 Torr or 100 Pa), atmospheric pressure (760 Torr or 100 kPa)\nThe degree of ionization within the plasma\u2014fully, partially, or weakly ionized\nThe temperature relationships within the plasma\u2014thermal plasma (\n  \n    \n      \n        \n          T\n          \n            e\n          \n        \n        =\n        \n          T\n          \n            i\n          \n        \n        =\n        \n          T\n          \n            g\n            a\n            s\n          \n        \n      \n    \n    {\\displaystyle T_{e}=T_{i}=T_{gas}}\n  ), non-thermal or \"cold\" plasma (\n  \n    \n      \n        \n          T\n          \n            e\n          \n        \n        \u226b\n        \n          T\n          \n            i\n          \n        \n        =\n        \n          T\n          \n            g\n            a\n            s\n          \n        \n      \n    \n    {\\displaystyle T_{e}\\gg T_{i}=T_{gas}}\n  )\nThe electrode configuration used to generate the plasma\nThe magnetization of the particles within the plasma\u2014magnetized (both ion and electrons are trapped in Larmor orbits by the magnetic field), partially magnetized (the electrons but not the ions are trapped by the magnetic field), non-magnetized (the magnetic field is too weak to trap the particles in orbits but may generate Lorentz forces)\n\n\n=== Generation of artificial plasma ===\n\nJust like the many uses of plasma, there are several means for its generation, however, one principle is common to all of them: there must be energy input to produce and sustain it. For this case, plasma is generated when an electric current is applied across a dielectric gas or fluid (an electrically non-conducting material) as can be seen in the image to the right, which shows a discharge tube as a simple example (DC used for simplicity).\nThe potential difference and subsequent electric field pull the bound electrons (negative) toward the anode (positive electrode) while the cathode (negative electrode) pulls the nucleus. As the voltage increases, the current stresses the material (by electric polarization) beyond its dielectric limit (termed strength) into a stage of electrical breakdown, marked by an electric spark, where the material transforms from being an insulator into a conductor (as it becomes increasingly ionized). The underlying process is the Townsend avalanche, where collisions between electrons and neutral gas atoms create more ions and electrons (as can be seen in the figure on the right). The first impact of an electron on an atom results in one ion and two electrons. Therefore, the number of charged particles increases rapidly (in the millions) only \"after about 20 successive sets of collisions\", mainly due to a small mean free path (average distance travelled between collisions).\n\n\n==== Electric arc ====\n\nWith ample current density and ionization, this forms a luminous electric arc (a continuous electric discharge similar to lightning) between the electrodes. Electrical resistance along the continuous electric arc creates heat, which dissociates more gas molecules and ionizes the resulting atoms (where degree of ionization is determined by temperature), and as per the sequence: solid-liquid-gas-plasma, the gas is gradually turned into a thermal plasma. A thermal plasma is in thermal equilibrium, which is to say that the temperature is relatively homogeneous throughout the heavy particles (i.e. atoms, molecules and ions) and electrons. This is so because when thermal plasmas are generated, electrical energy is given to electrons, which, due to their great mobility and large numbers, are able to disperse it rapidly and by elastic collision (without energy loss) to the heavy particles.\n\n\n=== Examples of industrial/commercial plasma ===\nBecause of their sizable temperature and density ranges, plasmas find applications in many fields of research, technology and industry. For example, in: industrial and extractive metallurgy, surface treatments such as plasma spraying (coating), etching in microelectronics, metal cutting and welding; as well as in everyday vehicle exhaust cleanup and fluorescent/luminescent lamps, while even playing a part in supersonic combustion engines for aerospace engineering.\n\n\n==== Low-pressure discharges ====\nGlow discharge plasmas: non-thermal plasmas generated by the application of DC or low frequency RF (<100 kHz) electric field to the gap between two metal electrodes. Probably the most common plasma; this is the type of plasma generated within fluorescent light tubes.\nCapacitively coupled plasma (CCP): similar to glow discharge plasmas, but generated with high frequency RF electric fields, typically 13.56 MHz. These differ from glow discharges in that the sheaths are much less intense. These are widely used in the microfabrication and integrated circuit manufacturing industries for plasma etching and plasma enhanced chemical vapor deposition.\nCascaded Arc Plasma Source: a device to produce low temperature (~1eV) high density plasmas (HDP).\nInductively coupled plasma (ICP): similar to a CCP and with similar applications but the electrode consists of a coil wrapped around the chamber where plasma is formed.\nWave heated plasma: similar to CCP and ICP in that it is typically RF (or microwave). Examples include helicon discharge and electron cyclotron resonance (ECR).\n\n\n==== Atmospheric pressure ====\nArc discharge: this is a high power thermal discharge of very high temperature (~10,000 K). It can be generated using various power supplies. It is commonly used in metallurgical processes. For example, it is used to smelt minerals containing Al2O3 to produce aluminium.\nCorona discharge: this is a non-thermal discharge generated by the application of high voltage to sharp electrode tips. It is commonly used in ozone generators and particle precipitators.\nDielectric barrier discharge (DBD): this is a non-thermal discharge generated by the application of high voltages across small gaps wherein a non-conducting coating prevents the transition of the plasma discharge into an arc. It is often mislabeled 'Corona' discharge in industry and has similar application to corona discharges. It is also widely used in the web treatment of fabrics. The application of the discharge to synthetic fabrics and plastics functionalizes the surface and allows for paints, glues and similar materials to adhere.\nCapacitive discharge: this is a nonthermal plasma generated by the application of RF power (e.g., 13.56 MHz) to one powered electrode, with a grounded electrode held at a small separation distance on the order of 1 cm. Such discharges are commonly stabilized using a noble gas such as helium or argon.\n\"Piezoelectric direct discharge plasma:\" is a nonthermal plasma generated at the high-side of a piezoelectric transformer (PT). This generation variant is particularly suited for high efficient and compact devices where a separate high voltage power supply is not desired.\n\n\n== History ==\nPlasma was first identified in a Crookes tube, and so described by Sir William Crookes in 1879 (he called it \"radiant matter\"). The nature of the Crookes tube \"cathode ray\" matter was subsequently identified by British physicist Sir J.J. Thomson in 1897. The term \"plasma\" was coined by Irving Langmuir in 1928, perhaps because the glowing discharge molds itself to the shape of the Crookes tube (Gr. \u03c0\u03bb\u03ac\u03c3\u03bc\u03b1 \u2013 a thing moulded or formed). Langmuir described his observations as:\n\nExcept near the electrodes, where there are sheaths containing very few electrons, the ionized gas contains ions and electrons in about equal numbers so that the resultant space charge is very small. We shall use the name plasma to describe this region containing balanced charges of ions and electrons.\n\n\n== Research ==\n\nPlasmas are the object of study of the academic field of plasma science or plasma physics, including sub-disciplines such as space plasma physics. There are multiple journals devoted to the subject (see list). It involves the following fields of active research (see also list of articles):\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\nFree plasma physics books and notes\nPlasmas: the Fourth State of Matter\nPlasma Science and Technology\nPlasma on the Internet \u2013 a list of plasma related links.\nIntroduction to Plasma Physics: Graduate course given by Richard Fitzpatrick|M.I.T. Introduction by I.H.Hutchinson\nPlasma Material Interaction\nHow to make a glowing ball of plasma in your microwave with a grape|More (Video)\nHow to make plasma in your microwave with only one match (video)\nOpenPIC3D \u2013 3D Hybrid Particle-In-Cell simulation of plasma dynamics\nPlasma Formulary Interactive", 
                "titleUrl": "https://en.wikipedia.org/wiki/Plasma_(physics)", 
                "title": "Plasma (physics)"
            }, 
            {
                "snippet": "An inductively coupled plasma (ICP) or transformer coupled plasma (TCP) is a type of plasma source in which the energy is supplied by electric currents", 
                "pageCategories": "Electrodynamics\nIon source\nPlasma physics\nSpectroscopy", 
                "pageContent": "An inductively coupled plasma (ICP) or transformer coupled plasma (TCP) is a type of plasma source in which the energy is supplied by electric currents which are produced by electromagnetic induction, that is, by time-varying magnetic fields.\n\n\n== Operation ==\nThere are three types of ICP geometries: planar (Fig. 2 (a)), cylindrical  (Fig. 2 (b)), and half-toroidal (Fig. 2 (c)).\n\nIn planar geometry, the electrode is a length of flat metal wound like a spiral (or coil). In cylindrical geometry, it is like a helical spring. In half-toroidal geometry, it is toroidal solenoid cut along its main diameter to two equal halves.\nWhen a time-varying electric current is passed through the coil, it creates a time-varying magnetic field around it, which in turn induces azimuthal electric field in the rarefied gas, leading to the formation of the figure-8 electron trajectories providing a plasma generation (see Hamilton-Jacobi equation in electromagnetic fields). Argon is one example of a commonly used rarefied gas.\n\n\n== Applications ==\nPlasma electron temperatures can range between ~6 000 K and ~10 000 K (~6 eV - ~100 eV), comparable to the surface of the sun. ICP discharges are of relatively high electron density, on the order of 1015 cm\u22123. As a result, ICP discharges have wide applications where a high-density plasma (HDP) is needed.\nICP-AES, a type of atomic emission spectroscopy.\nICP-MS, a type of mass spectrometry.\nICP-RIE, a type of reactive-ion etching.\nExciting a beam of noble gas to the metastable state.\nAnother benefit of ICP discharges is that they are relatively free of contamination because the electrodes are completely outside the reaction chamber. By contrast, in a capacitively coupled plasma (CCP), the electrodes are often placed inside the reactor and are thus exposed to the plasma and subsequent reactive chemical species.\n\n\n== See also ==\nPulsed inductive thruster\nInduction plasma technology\nList of plasma (physics) articles\nCapacitively coupled plasma\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Inductively_coupled_plasma", 
                "title": "Inductively coupled plasma"
            }, 
            {
                "snippet": "directed flow of plasma. The plasma jet can be used for applications including plasma cutting, plasma arc welding, plasma spraying, and plasma gasification", 
                "pageCategories": "Plasma physics\nPlasma processing\nSustainable technologies", 
                "pageContent": "A plasma torch (also known as a plasma arc, plasma gun, or plasma cutter) is a device for generating a directed flow of plasma. The plasma jet can be used for applications including plasma cutting, plasma arc welding, plasma spraying, and plasma gasification for waste disposal.\n\n\n== Types of thermal plasma torches ==\nThermal plasmas are generated in plasma torches by direct current (DC), alternating current (AC), radio-frequency (RF) and other discharges. DC torches are the most commonly used and researched, because when compared to AC: \u201cthere is less flicker generation and noise, a more stable operation, better control, a minimum of two electrodes, lower electrode consumption, slightly lower refractory [heat] wear and lower power consumption\u201d.\n\n\n=== Thermal plasma DC torches, non-transferred arc, based on hot cathode ===\n\nIn a DC torch, the electric arc is formed between the electrodes (which can be made of copper, tungsten, graphite, molybdenum, silver etc.), and the thermal plasma is formed from the continual input of carrier/working gas, projecting outward as a plasma jet/flame (as can be seen on the right). In DC torches, the carrier gas can be, for example, either oxygen, nitrogen, argon, helium, air, hydrogen; and although termed as such, it does not have to be a gas (thus, better termed a carrier fluid).\nFor example, a research plasma torch at the Institute of Plasma Physics (IPP) in Prague, Czech Republic, functions with an H2O vortex (as well as a small addition of argon to ignite the arc), and produces a high temperature/velocity plasma flame. In fact, early studies of arc stabilization employed a water-vortex. Overall, the electrode materials and carrier fluids have to be specifically matched to avoid excessive electrode corrosion or oxidation (and contamination of materials to be treated), while maintaining ample power and function.\nFurthermore, the flow-rate of the carrier gas can be raised to promote a larger, more projecting plasma jet, provided that the arc current is sufficiently increased; and vice versa.\nThe plasma flame of a real plasma torch is a few inches long at most; it is to be distinguished from fictional long-range plasma weapons.\n\n\n==== Transferred vs. non-transferred ====\nIt is important to note that there are two types of DC torches: non-transferred and transferred. In non-transferred DC torches, the electrodes are inside the body/housing of the torch itself (creating the arc there). Whereas in transferred \u2014 one electrode is outside (and is usually the conductive material to be treated), allowing the arc to form outside of the torch over a larger distance.\nA benefit of transferred DC torches is that the plasma arc is formed outside the water-cooled body, preventing heat loss \u2014 as is the case with non-transferred torches, where their electrical-to-thermal efficiency can be as low as 50%, but the hot water can itself be utilized. Furthermore, transferred DC torches can be used in a twin-torch setup, where one torch is cathodic and the other anodic, which has the earlier benefit of a regular transferred single-torch system, but allows their use with non-conductive materials, as there is no need for it to form the other electrode. The electrodes of non-transferred torches are larger, because they suffer more wear by the plasma arc.\nThe quality of plasma produced is a function of density (pressure), temperature and torch power (the greater the better). With regards to the efficiency of the torch itself \u2014 this can vary among manufacturers and torch technology; though for example, Leal-Quir\u00f3s reports that for Westinghouse Plasma Corp. torches \u201ca thermal efficiency of 90% is easily possible; the efficiency represents the percentage of arc power that exits the torch and enters the process\u201d.\n\n\n== See also ==\nPlasma (physics)\nList of plasma (physics) applications articles\nPlasma source\n\n\n== Images ==\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Plasma_torch", 
                "title": "Plasma torch"
            }, 
            {
                "snippet": "\"Plasma gun\" redirects here. For the science-fiction weapon, see plasma weapon. A dense plasma focus (DPF) is a machine that produces, by electromagnetic", 
                "pageCategories": "Articles with Wayback Machine links\nCS1 errors: dates\nFusion power\nNeutron sources\nPlasma physics", 
                "pageContent": "A dense plasma focus (DPF) is a machine that produces, by electromagnetic acceleration and compression, a short-lived plasma that is hot and dense enough to cause nuclear fusion and the emission of X-rays and neutrons. The electromagnetic compression of the plasma is called a pinch. It was invented in 1954 by N.V. Filippov and also independently by J.W. Mather in the early 1960s. The plasma focus is similar to the high-intensity plasma gun device (HIPGD) (or just plasma gun), which ejects plasma in the form of a plasmoid, without pinching it. A comprehensive review of the Dense Plasma Focus and its diverse applications has been made by Krishnan in 2012.\n\n\n== Applications ==\nWhen operated using deuterium, intense bursts of X-rays and charged particles are emitted, as are nuclear fusion byproducts including neutrons. There is ongoing research that demonstrates potential applications as a soft X-ray source for next-generation microelectronics lithography, surface micromachining, pulsed X-ray and neutron source for medical and security inspection applications and materials modification, among others.\nFor nuclear weapons applications, dense plasma focus devices can be used as an external neutron source. Other applications include simulation of nuclear explosions (for testing of the electronic equipment) and a short and intense neutron source useful for non-contact discovery or inspection of nuclear materials (uranium, plutonium).\n\n\n== Positive characteristics ==\nAn important characteristic of the dense plasma focus is that the energy density of the focused plasma is practically a constant over the whole range of machines, from sub-kilojoule machines to megajoule machines, when these machines are tuned for optimal operation. This means that a small table-top-sized plasma focus machine produces essentially the same plasma characteristics (temperature and density) as the largest plasma focus. Of course the larger machine will produce the larger volume of focused plasma with a corresponding longer lifetime and more radiation yield.\nEven the smallest plasma focus has essentially the same dynamic characteristics as larger machines, producing the same plasma characteristics and the same radiation products. This is due to the scalability of plasma phenomena.\nSee also plasmoid, the self-contained magnetic plasma ball that may be produced by a dense plasma focus.\n\n\n== Operation ==\nA charged bank of electrical capacitors is switched onto the anode. The gas within the reaction chamber breaks down and a rapidly rising electric current flows across the backwall electrical insulator, axisymmetrically, as depicted by the path (labeled 1) as shown in Fig. 1. The axisymmetric sheath of plasma current lifts off the insulator due to the interaction of the current with its own magnetic field (Lorentz force). The plasma sheath is accelerated axially, to position 2, and then to position 3, ending the axial phase of the device.\nThe whole process proceeds at many times the speed of sound in the ambient gas. As the current sheath continues to move axially, the portion in contact with the anode slides across the face of the anode, axisymmetrically. When the imploding front of the shock wave coalesces onto the axis, a reflected shock front emanates from the axis until it meets the driving current sheath which then forms the axisymmetric boundary of the pinched, or focused, hot plasma column.\nThe dense plasma column (akin to the Z-pinch) rapidly pinches and undergoes instabilities and breaks up. The intense electromagnetic radiation and particle bursts, collectively referred to as multi-radiation occur during the dense plasma and breakup phases. These critical phases last typically tens of nanoseconds for a small (kJ, 100 kA) focus machine to around a microsecond for a large (MJ, several MA) focus machine.\nThe whole process, including axial and radial phases, may last, for the Mather DPF machine, a few microseconds (for a small focus) to 10 microseconds for a larger focus machine. A Filippov focus machine has a very short axial phase compared to a Mather focus.\n\n\n== Design parameters ==\nThe fact that the plasma energy density is constant throughout the range of plasma focus devices, from big to small, is related to the value of a design parameter that needs to be kept at a certain value if the plasma focus is to operate efficiently.\nThe critical 'speed' design parameter for neutron-producing devices is \n  \n    \n      \n        \n          \n            I\n            \n              a\n              \n                \n                  p\n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\frac {I}{a{\\sqrt {p}}}}}\n  , where \n  \n    \n      \n        I\n      \n    \n    {\\displaystyle I}\n   is the current, \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n   is the anode radius, and \n  \n    \n      \n        p\n      \n    \n    {\\displaystyle p}\n   is the gas density or pressure.\nFor example for neutron-optimised operation in deuterium the value of this critical parameter, experimentally observed over a range of machines from kilojoules to hundreds of kilojoules, is: 9 kA/(mm\u00b7Torr0.5), or 780 kA/(m\u00b7Pa0.5), with a remarkably small deviation of 10% over such a large range of sizes of machines.\nThus if we have a peak current of 180 kA we require an anode radius of 10 mm with a deuterium fill pressure of 4 Torr (530 Pa). The length of the anode has then to be matched to the risetime of the capacitor current in order to allow an average axial transit speed of the current sheath of just over 50 mm/\u03bcs. Thus a capacitor risetime of 3 \u03bcs requires a matched anode length of 160 mm.\nThe above example of peak current of 180 kA rising in 3 \u00b5s, anode radius and length of respectively 10 and 160 mm are close to the design parameters of the UNU/ICTP PFF (United Nations University/International Centre for Theoretical Physics Plasma Fusion Facility). This small table-top device was designed as a low-cost integrated experimental system for training and transfer to initiate/strengthen experimental plasma research in developing countries.\nIt can be noted that the square of the drive parameter is a measure of the \"plasma energy density\".\nOn the other hand, another proposed, so called \u201cenergy density parameter\u201d \n  \n    \n      \n        \n          \n            \n              28\n              E\n            \n            \n              a\n              \n                3\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {28E \\over a^{3}}}\n  , where E is the energy stored in the capacitor bank and a is the anode radius, for neutron-optimised operation in deuterium the value of this critical parameter, experimentally observed over a range of machines from tens of joules to hundreds of kilojoules, is in the order of \n  \n    \n      \n        \n          5\n          \u22c5\n          \n            10\n            \n              10\n            \n          \n        \n      \n    \n    {\\displaystyle {5\\cdot 10^{10}}}\n   J/m3. For example for a capacitor bank of 3kJ, the anode radius is in the order of 12mm. This parameter has a range of 3.6x10^9 to 7.6x10^11 for the machines surveyed by Soto. The wide range of this parameter is because it is a \"storage energy density\" which translates into plasma energy density with different efficiency depending on the widely differing performance of different machines. Thus to result in the necessary plasma energy density (which is found to be a near constant for optimized neutron production) requires widely differing initial storage density.\n\n\n== Current research ==\nA network of ten identical DPF machines operates in eight countries around the world. This network produces research papers on topics including machine optimization & diagnostics (soft x-rays, neutrons, electron and ion beams), applications (microlithography, micromachining, materials modification and fabrication, imaging & medical, astrophysical simulation) as well as modeling & computation. The network was organized by Sing Lee in 1986 and is coordinated by the Asian African Association for Plasma Training, AAAPT. A simulation package, the Lee Model, has been developed for this network but is applicable to all plasma focus devices. The code typically produces excellent agreement between computed and measured results, and is available for downloading as a Universal Plasma Focus Laboratory Facility. The Institute for Plasma Focus Studies IPFS was founded on 25 February 2008 to promote correct and innovative use of the Lee Model code and to encourage the application of plasma focus numerical experiments. IPFS research has already extended numerically-derived neutron scaling laws to multi-megajoule experiments. These await verification. Numerical experiments with the code have also resulted in the compilation of a global scaling law indicating that the well-known neutron saturation effect is better correlated to a scaling deterioration mechanism. This is due to the increasing dominance of the axial phase dynamic resistance as capacitor bank impedance decreases with increasing bank energy (capacitance). In principle, the resistive saturation could be overcome by operating the pulse power system at a higher voltage.\nThe International Centre for Dense Magnetised Plasmas (ICDMP) in Warsaw Poland, operates several plasma focus machines for an international research and training programme. Among these machines is one with energy capacity of 1 MJ making it one of the largest plasma focus devices in the world.\nIn Argentina there is an Inter-institutional Program for Plasma Focus Research since 1996, coordinated by a National Laboratory of Dense Magnetized Plasmas (www.pladema.net) in Tandil, Buenos Aires. The Program also cooperates with the Chilean Nuclear Energy Commission, and networks the Argentine National Energy Commission, the Scientific Council of Buenos Aires, the University of Center, the University of Mar del Plata, The University of Rosario, and the Institute of Plasma Physics of the University of Buenos Aires. The program operates six Plasma Focus Devices, developing applications, in particular ultra-short tomography and substance detection by neutron pulsed interrogation. PLADEMA also contributed during the last decade with several mathematical models of Plasma Focus. The thermodynamic model was able to develop for the first time design maps combining geometrical and operational parameters, showing that there is always an optimum gun length and charging pressure which maximize the neutron emission. Currently there is a complete finite-elements code validated against numerous experiments, which can be used confidently as a design tool for Plasma Focus.\nIn Chile, at the Chilean Nuclear Energy Commission the plasma focus experiments have been extended to sub-kilojoules devices and the scales rules have been stretched up to region less than one joule    . Their studies have contributes to know that is possible to scale the plasma focus in a wide range of energies and sizes keeping the same value of ion density, magnetic field, plasma sheath velocity, Alfv\u00e9n speed and the quantity of energy per particle. Therefore, fusion reactions are even possible to be obtained in ultraminiature devices (driven by generators of 0.1J for example), as they are in the bigger devices (driven by generators of 1MJ). However, the stability of the plasma pinch highly depends on the size and energy of the device. A rich plasma phenomenology it has been observed in the table-top plasma focus devices developed at the Chilean Nuclear Energy Commission: filamentary structures, toroidal singularities, plasma bursts  and plasma jets generations. In addition, possible applications are explored using these kind of small plasma devices: development of portable generator as non-radioactive sources of neutrons and x-rays for field applications, pulsed radiation applied to biological studies, plasma focus as neutron source for nuclear fusion-fission hybrid reactors, and the use of plasma focus devices as plasma accelerators for studies of materials under intense fusion-relevant pulses. In addition, Chilean Nuclear Energy Commission currently operates the facility SPEED-2, the largest Plasma Focus facility of the southern hemisphere.\nSince the beginning of 2009, a number of new plasma focus machines have been/are being commissioned including the INTI Plasma Focus in Malaysia, the NX3 in Singapore, the first plasma focus to be commissioned in a US university in recent times, the KSU Plasma Focus at Kansas State University which recorded its first fusion neutron emitting pinch on New Year's Eve 2009 and the IR-MPF-100 plasma focus (115kJ) in Iran.\n\n\n=== Fusion power ===\nSeveral groups proposed that fusion power based on the DPF could be economically viable, possibly even with low-neutron fuel cycles like p-B11. The feasibility of net power from p-B11 in the DPF requires that the bremsstrahlung losses be reduced by quantum mechanical effects induced by an applied powerful magnetic field. The high magnetic field also results in a high rate of emission of cyclotron radiation, but at the densities envisioned, where the plasma frequency is larger than the cyclotron frequency, most of this power will be reabsorbed before being lost from the plasma. Another advantage claimed is the capability of direct conversion of the energy of the fusion products into electricity, with an efficiency potentially above 70%.\nExperiments and computer simulations to investigate the capability of DPF for fusion power are underway at Lawrenceville Plasma Physics (LPP) under the direction of Eric Lerner, who explained his \"Focus Fusion\" approach in a 2007 Google Tech Talk. On November 14, 2008, Lerner received funding for continued research, to test the scientific feasibility of Focus Fusion. On October 15, 2009, the DPF device \"Focus Fusion-1\" achieved its first pinch. On January 28, 2011, LPP published initial results including experimental shots with considerably higher fusion yields than the historical DPF trend. In March, 2012, the company announced that it had achieved temperatures of 1.8 billion degrees, beating the old record of 1.1 billion that had survived since 1978. In 2016 the company announced that it had achieved a fusion yield of .25 joules.\n\n\n== See also ==\nList of plasma (physics) articles\n\n\n== History ==\n1958: \u041f\u0435\u0442\u0440\u043e\u0432 \u0414.\u041f., \u0424\u0438\u043b\u0438\u043f\u043f\u043e\u0432 \u041d.\u0412., \u0424\u0438\u043b\u0438\u043f\u043f\u043e\u0432\u0430 \u0422.\u0418., \u0425\u0440\u0430\u0431\u0440\u043e\u0432 \u0412.\u0410. \"\u041c\u043e\u0449\u043d\u044b\u0439 \u0438\u043c\u043f\u0443\u043b\u044c\u0441\u043d\u044b\u0439 \u0433\u0430\u0437\u043e\u0432\u044b\u0439 \u0440\u0430\u0437\u0440\u044f\u0434 \u0432 \u043a\u0430\u043c\u0435\u0440\u0430\u0445 \u0441 \u043f\u0440\u043e\u0432\u043e\u0434\u044f\u0449\u0438\u043c\u0438 \u0441\u0442\u0435\u043d\u043a\u0430\u043c\u0438\". \u0412 \u0441\u0431. \u0424\u0438\u0437\u0438\u043a\u0430 \u043f\u043b\u0430\u0437\u043c\u044b \u0438 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u044b \u0443\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u043c\u044b\u0445 \u0442\u0435\u0440\u043c\u043e\u044f\u0434\u0435\u0440\u043d\u044b\u0445 \u0440\u0435\u0430\u043a\u0446\u0438\u0439. \u0418\u0437\u0434. \u0410\u041d \u0421\u0421\u0421\u0420, 1958, \u0442. 4, \u0441. 170-181.\n1958: Hannes Alfv\u00e9n: Proceedings of the Second International Conference on Peaceful Uses of Atomic Energy (United Nations), 31, 3\n1960: H Alfven, L Lindberg and P Mitlid, \"Experiments with plasma rings\" (1961) Journal of Nuclear Energy. Part C, Plasma Physics, Accelerators, Thermonuclear Research, Volume 1, Issue 3, pp. 116\u2013120\n1960: Lindberg, L., E. Witalis and C. T. Jacobsen, \"Experiments with plasma rings\" (1960) Nature 185:452.\n1961: Hannes Alfv\u00e9n: Plasma Ring Experiment in \"On the Origin of Cosmic Magnetic Fields\" (1961) Astrophysical Journal, vol. 133, p. 1049\n1961: Lindberg, L. & Jacobsen, C., \"On the Amplification of the Poloidal Magnetic Flux in a Plasma\" (1961) Astrophysical Journal, vol. 133, p. 1043\n1962: Filippov. N.V., et al., \"Dense, High-Temperature Plasma in a Noncylindrical 2-pinch Compression\" (1962) 'Nuclear Fusion Supplement'. Pt. 2, 577\n1969: Buckwald, Robert Allen, \"Dense Plasma Focus Formation by Disk Symmetry\" (1969) Thesis, Ohio State University.\n\n\n== Notes ==\n\n\n== External links ==\nInstitute for Plasma Focus Studies (IPFS).\nResearch papers published in 2011 by IPFS staff. [2]\nThe Plasma Focus-Trending into the Future([3])\nDimensions and Lifetime of the Plasma Focus ([4])\nPlasma Radiation Source Lab at the National Institute of Education in Singapore\nPlasma Focus Laboratory, International Centre for Dense Magnetised Plasmas, Warsaw, Poland\nOptics and Plasma Physics Group,Pontificia Universidad Cat\u00f3lica de Chile\nPaper by Leopoldo Soto (Chilean Nuclear Energy Commission, Thermonucluar Plasma Department): New trends and future perspectives on plasma focus research\nFocus Fusion Society\nAbdus Salam ICTP Plasma Focus Laboratory. [5]\nNumerical Simulation Package: Universal Plasma Focus Laboratory Facility at INTI-UC. [6]\nDense Plasma Focus Network in Argentina.\nResearch papers published in 2011 by IPFS staff. [7]\nFusion Energy Site with links.\nGoogle talk by Eric J. Lerner, President of Lawrenceville Plasma Physics and Executive Director of the Focus Fusion Society", 
                "titleUrl": "https://en.wikipedia.org/wiki/Dense_plasma_focus", 
                "title": "Dense plasma focus"
            }, 
            {
                "snippet": "magnetosphere through the magnetotail. As cold, dense plasma moves outward, it is replaced by hot, low-density plasma (temperature 20\u00a0keV (200\u00a0million\u00a0K) or higher)", 
                "pageCategories": "Astronomical objects discovered in 1973\nCommons category with local link same as on Wikidata\nFeatured articles\nJupiter\nMagnetospheres\nPlanetary science", 
                "pageContent": "The magnetosphere of Jupiter is the cavity created in the solar wind by the planet's magnetic field. Extending up to seven million kilometers in the Sun's direction and almost to the orbit of Saturn in the opposite direction, Jupiter's magnetosphere is the largest and most powerful of any planetary magnetosphere in the Solar System, and by volume the largest known continuous structure in the Solar System after the heliosphere. Wider and flatter than the Earth's magnetosphere, Jupiter's is stronger by an order of magnitude, while its magnetic moment is roughly 18,000 times larger. The existence of Jupiter's magnetic field was first inferred from observations of radio emissions at the end of the 1950s and was directly observed by the Pioneer 10 spacecraft in 1973.\nJupiter's internal magnetic field is generated by electrical currents in the planet's outer core, which is composed of liquid metallic hydrogen. Volcanic eruptions on Jupiter's moon Io eject large amounts of sulfur dioxide gas into space, forming a large torus around the planet. Jupiter's magnetic field forces the torus to rotate with the same angular velocity and direction as the planet. The torus in turn loads the magnetic field with plasma, in the process stretching it into a pancake-like structure called a magnetodisk. In effect, Jupiter's magnetosphere is shaped by Io's plasma and its own rotation, rather than by the solar wind like Earth's magnetosphere. Strong currents in the magnetosphere generate permanent aurorae around the planet's poles and intense variable radio emissions, which means that Jupiter can be thought of as a very weak radio pulsar. Jupiter's aurorae have been observed in almost all parts of the electromagnetic spectrum, including infrared, visible, ultraviolet and soft X-rays.\nThe action of the magnetosphere traps and accelerates particles, producing intense belts of radiation similar to Earth's Van Allen belts, but thousands of times stronger. The interaction of energetic particles with the surfaces of Jupiter's largest moons markedly affects their chemical and physical properties. Those same particles also affect and are affected by the motions of the particles within Jupiter's tenuous planetary ring system. Radiation belts present a significant hazard for spacecraft and potentially to human space travellers.\n\n\n== Structure ==\nJupiter's magnetosphere is a complex structure comprising a bow shock, magnetosheath, magnetopause, magnetotail, magnetodisk, and other components. The magnetic field around Jupiter emanates from a number of different sources, including fluid circulation at the planet's core (the internal field), electrical currents in the plasma surrounding Jupiter and the currents flowing at the boundary of the planet's magnetosphere. The magnetosphere is embedded within the plasma of the solar wind, which carries the interplanetary magnetic field.\n\n\n=== Internal magnetic field ===\nThe bulk of Jupiter's magnetic field, like Earth's, is generated by an internal dynamo supported by the circulation of a conducting fluid in its outer core. But whereas Earth's core is made of molten iron and nickel, Jupiter's is composed of metallic hydrogen. As with Earth's, Jupiter's magnetic field is mostly a dipole, with north and south magnetic poles at the ends of a single magnetic axis. However, on Jupiter the north pole of the dipole is located in the planet's northern hemisphere and the south pole of the dipole lies in its southern hemisphere, opposite to the Earth, whose north pole lies in the southern hemisphere and south pole lies in the northern hemisphere. Jupiter's field also has quadrupole, octupole and higher components, though they are less than one tenth as strong as the dipole component.\nThe dipole is tilted roughly 10\u00b0 from Jupiter's axis of rotation; the tilt is similar to that of the Earth (11.3\u00b0). Its equatorial field strength is about 428 \u03bcT (4.28 G), which corresponds to a dipole magnetic moment of about 1.56 \u00d7 1020 T\u00b7m3. This makes Jupiter's magnetic field 10 times stronger than Earth's, and its magnetic moment about 18,000 times larger. Jupiter's magnetic field rotates at the same speed as the region below its atmosphere, with a period of 9 h 55 m. No changes in its strength or structure have been observed since the first measurements were taken by the Pioneer spacecraft in the mid-1970s.\n\n\n=== Size and shape ===\nJupiter's internal magnetic field prevents the solar wind, a stream of ionized particles emitted by the Sun, from interacting directly with its atmosphere, and instead diverts it away from the planet, effectively creating a cavity in the solar wind flow, called a magnetosphere, composed of a plasma different from that of the solar wind. The Jovian (i.e. pertaining to Jupiter) magnetosphere is so large that the Sun and its visible corona would fit inside it with room to spare. If one could see it from Earth, it would appear five times larger than the full moon in the sky despite being nearly 1700 times farther away.\nAs with Earth's magnetosphere, the boundary separating the denser and colder solar wind's plasma from the hotter and less dense one within Jupiter's magnetosphere is called the magnetopause. The distance from the magnetopause to the center of the planet is from 45 to 100 RJ (where RJ=71,492 km is the radius of Jupiter) at the subsolar point\u2014the unfixed point on the surface at which the Sun would appear directly overhead to an observer. The position of the magnetopause depends on the pressure exerted by the solar wind, which in turn depends on solar activity. In front of the magnetopause (at a distance from 80 to 130 RJ from the planet's center) lies the bow shock, a wake-like disturbance in the solar wind caused by its collision with the magnetosphere. The region between the bow shock and magnetopause is called the magnetosheath.\n\nAt the opposite side of the planet, the solar wind stretches Jupiter's magnetic field lines into a long, trailing magnetotail, which sometimes extends well beyond the orbit of Saturn. The structure of Jupiter's magnetotail is similar to Earth's. It consists of two lobes (blue areas in the figure), with the magnetic field in the southern lobe pointing toward Jupiter, and that in the northern lobe pointing away from it. The lobes are separated by a thin layer of plasma called the tail current sheet (orange layer in the middle). Like Earth's, the Jovian tail is a channel through which solar plasma enters the inner regions of the magnetosphere, where it is heated and forms the radiation belts at distances closer than 10 RJ from Jupiter.\nThe shape of Jupiter's magnetosphere described above is sustained by the neutral sheet current (also known as the magnetotail current), which flows with Jupiter's rotation through the tail plasma sheet, the tail currents, which flow against Jupiter's rotation at the outer boundary of the magnetotail, and the magnetopause currents (or Chapman-Ferraro currents), which flow against rotation along the dayside magnetopause. These currents create the magnetic field that cancels the internal field outside the magnetosphere. They also interact substantially with the solar wind.\nJupiter's magnetosphere is traditionally divided into three parts: the inner, middle and outer magnetosphere. The inner magnetosphere is located at distances closer than 10 RJ from the planet. The magnetic field within it remains approximately dipole, because contributions from the currents flowing in the magnetospheric equatorial plasma sheet are small. In the middle (between 10 and 40 RJ) and outer (further than 40 RJ) magnetospheres, the magnetic field is not a dipole, and is seriously disturbed by its interaction with the plasma sheet (see magnetodisk below).\n\n\n=== Role of Io ===\n\nAlthough overall the shape of Jupiter's magnetosphere resembles that of the Earth's, closer to the planet its structure is very different. Jupiter's volcanically active moon Io is a strong source of plasma in its own right, and loads Jupiter's magnetosphere with as much as 1,000 kg of new material every second. Strong volcanic eruptions on Io emit huge amounts of sulfur dioxide, a major part of which is dissociated into atoms and ionized by the solar ultraviolet radiation, producing ions of sulfur and oxygen: S+, O+, S2+ and O2+. These ions escape from the satellite's atmosphere and form the Io plasma torus: a thick and relatively cool ring of plasma encircling Jupiter, located near Io's orbit. The plasma temperature within the torus is 10\u2013100 eV (100,000\u20131,000,000 K), which is much lower than that of the particles in the radiation belts\u201410 keV (100 million K). The plasma in the torus is forced into co-rotation with Jupiter, meaning both share the same period of rotation. The Io torus fundamentally alters the dynamics of the Jovian magnetosphere.\nAs a result of several processes\u2014diffusion and interchange instability being the main escape mechanisms\u2014the plasma slowly leaks away from Jupiter. As the plasma moves further from the planet, the radial currents flowing within it gradually increase its velocity, maintaining co-rotation. These radial currents are also the source of the magnetic field's azimuthal component, which as a result bends back against the rotation. The particle number density of the plasma decreases from around 2,000 cm\u22123 in the Io torus to about 0.2 cm\u22123 at a distance of 35 RJ. In the middle magnetosphere, at distances greater than 20 RJ from Jupiter, co-rotation gradually breaks down and the plasma begins to rotate more slowly than the planet. Eventually at the distances greater than 40 RJ (in the outer magnetosphere) this plasma escapes the magnetic field completely and leaves the magnetosphere through the magnetotail. As cold, dense plasma moves outward, it is replaced by hot, low-density plasma (temperature 20 keV (200 million K) or higher) moving from the outer magnetosphere. This plasma, adiabatically heated as it approaches Jupiter, forms the radiation belts in Jupiter's inner magnetosphere.\n\n\n=== Magnetodisk ===\nWhile Earth's magnetic field is roughly teardrop-shaped, Jupiter's is flatter, more closely resembling a disk, and \"wobbles\" periodically about its axis. The main reasons for this disk-like configuration are the centrifugal force from the co-rotating plasma and thermal pressure of hot plasma, both of which act to stretch Jupiter's magnetic field lines, forming a flattened pancake-like structure, known as the magnetodisk, at the distances greater than 20 RJ from the planet. The magnetodisk has a thin current sheet at the middle plane, approximately near the magnetic equator. The magnetic field lines point away from Jupiter above the sheet and towards Jupiter below it. The load of plasma from Io greatly expands the size of the Jovian magnetosphere, because the magnetodisk creates an additional internal pressure which balances the pressure of the solar wind. In the absence of Io the distance from the planet to the magnetopause at the subsolar point would be no more than 42 RJ, whereas it is actually 75 RJ on average.\nThe configuration of the magnetodisk's field is maintained by the azimuthal ring current (not an analog of Earth's ring current), which flows with rotation through the equatorial plasma sheet. The Lorentz force resulting from the interaction of this current with the planetary magnetic field creates a centripetal force, which keeps the co-rotating plasma from escaping the planet. The total ring current in the equatorial current sheet is estimated at 90\u2013160 million amperes.\n\n\n== Dynamics ==\n\n\n=== Co-rotation and radial currents ===\n\nThe main driver of Jupiter's magnetosphere is the planet's rotation. In this respect Jupiter is similar to a device called a Unipolar generator. When Jupiter rotates, its ionosphere moves relatively to the dipole magnetic field of the planet. Because the dipole magnetic moment points in the direction of the rotation, the Lorentz force, which appears as a result of this motion, drives negatively charged electrons to the poles, while positively charged ions are pushed towards the equator. As a result, the poles become negatively charged and the regions closer to the equator become positively charged. Since the magnetosphere of Jupiter is filled with highly conductive plasma, the electrical circuit is closed through it. A current called the direct current flows along the magnetic field lines from the ionosphere to the equatorial plasma sheet. This current then flows radially away from the planet within the equatorial plasma sheet and finally returns to the planetary ionosphere from the outer reaches of the magnetosphere along the field lines connected to the poles. The currents that flow along the magnetic field lines are generally called field-aligned or Birkeland currents. The radial current interacts with the planetary magnetic field, and the resulting Lorentz force accelerates the magnetospheric plasma in the direction of planetary rotation. This is the main mechanism that maintains co-rotation of the plasma in Jupiter's magnetosphere.\nThe current flowing from the ionosphere to the plasma sheet is especially strong when the corresponding part of the plasma sheet rotates slower than the planet. As mentioned above, co-rotation breaks down in the region located between 20 and 40 RJ from Jupiter. This region corresponds to the magnetodisk, where the magnetic field is highly stretched. The strong direct current flowing into the magnetodisk originates in a very limited latitudinal range of about 16 \u00b1 1\u00b0 from the Jovian magnetic poles. These narrow circular regions correspond to Jupiter's main auroral ovals. (See below.) The return current flowing from the outer magnetosphere beyond 50 RJ enters the Jovian ionosphere near the poles, closing the electrical circuit. The total radial current in the Jovian magnetosphere is estimated at 60 million\u2013140 million amperes.\nThe acceleration of the plasma into the co-rotation leads to the transfer of energy from the Jovian rotation to the kinetic energy of the plasma. In that sense, the Jovian magnetosphere is powered by the planet's rotation, whereas the Earth's magnetosphere is powered mainly by the solar wind.\n\n\n=== Interchange instability and reconnection ===\nThe main problem encountered in deciphering the dynamics of the Jovian magnetosphere is the transport of heavy cold plasma from the Io torus at 6 RJ to the outer magnetosphere at distances of more than 50 RJ. The precise mechanism of this process is not known, but it is hypothesized to occur as a result of plasma diffusion due to interchange instability. The process is similar to the Rayleigh-Taylor instability in hydrodynamics. In the case of the Jovian magnetosphere, centrifugal force plays the role of gravity; the heavy liquid is the cold and dense Ionian (i.e. pertaining to Io) plasma, and the light liquid is the hot, much less dense plasma from the outer magnetosphere. The instability leads to an exchange between the outer and inner parts of the magnetosphere of flux tubes filled with plasma. The buoyant empty flux tubes move towards the planet, while pushing the heavy tubes, filled with the Ionian plasma, away from Jupiter. This interchange of flux tubes is a form of magnetospheric turbulence.\n\nThis highly hypothetical picture of the flux tube exchange was partly confirmed by the Galileo spacecraft, which detected regions of sharply reduced plasma density and increased field strength in the inner magnetosphere. These voids may correspond to the almost empty flux tubes arriving from the outer magnetosphere. In the middle magnetosphere, Galileo detected so-called injection events, which occur when hot plasma from the outer magnetosphere impacts the magnetodisk, leading to increased flux of energetic particles and a strengthened magnetic field. No mechanism is yet known to explain the transport of cold plasma outward.\nWhen flux tubes loaded with the cold Ionian plasma reach the outer magnetosphere, they go through a reconnection process, which separates the magnetic field from the plasma. The former returns to the inner magnetosphere in the form of flux tubes filled with hot and less dense plasma, while the latter are probably ejected down the magnetotail in the form of plasmoids\u2014large blobs of plasma. The reconnection processes may correspond to the global reconfiguration events also observed by the Galileo probe, which occurred regularly every 2\u20133 days. The reconfiguration events usually included rapid and chaotic variation of the magnetic field strength and direction, as well as abrupt changes in the motion of the plasma, which often stopped co-rotating and began flowing outward. They were mainly observed in the dawn sector of the night magnetosphere. The plasma flowing down the tail along the open field lines is called the planetary wind.\nThe reconnection events are analogues to the magnetic substorms in the Earth's magnetosphere. The difference seems to be their respective energy sources: terrestrial substorms involve storage of the solar wind's energy in the magnetotail followed by its release through a reconnection event in the tail's neutral current sheet. The latter also creates a plasmoid which moves down the tail. Conversely, in Jupiter's magnetosphere the rotational energy is stored in the magnetodisk and released when a plasmoid separates from it.\n\n\n=== Influence of the solar wind ===\n\nWhereas the dynamics of Jovian magnetosphere mainly depend on internal sources of energy, the solar wind probably has a role as well, particularly as a source of high-energy protons. The structure of the outer magnetosphere shows some features of a solar wind-driven magnetosphere, including a significant dawn\u2013dusk asymmetry. In particular, magnetic field lines in the dusk sector are bent in the opposite direction to those in the dawn sector. In addition, the dawn magnetosphere contains open field lines connecting to the magnetotail, whereas in the dusk magnetosphere, the field lines are closed. All these observations indicate that a solar wind driven reconnection process, known on Earth as the Dungey cycle, may also be taking place in the Jovian magnetosphere.\nThe extent of the solar wind's influence on the dynamics of Jupiter's magnetosphere is currently unknown; however, it could be especially strong at times of elevated solar activity. The auroral radio, optical and X-ray emissions, as well as synchrotron emissions from the radiation belts all show correlations with solar wind pressure, indicating that the solar wind may drive plasma circulation or modulate internal processes in the magnetosphere.\n\n\n== Emissions ==\n\n\n=== Aurorae ===\n\nJupiter demonstrates bright, persistent aurorae around both poles. Unlike Earth's aurorae, which are transient and only occur at times of heightened solar activity, Jupiter's aurorae are permanent, though their intensity varies from day to day. They consist of three main components: the main ovals, which are bright, narrow (less than 1000 km in width) circular features located at approximately 16\u00b0 from the magnetic poles; the satellites' auroral spots, which correspond to the footprints of the magnetic field lines connecting Jupiter's ionosphere with those of its largest moons, and transient polar emissions situated within the main ovals. Whereas the auroral emissions were detected in almost all parts of the electromagnetic spectrum from radio waves to X-rays (up to 3 keV), they are brightest in the mid-infrared (wavelength 3\u20134 \u03bcm and 7\u201314 \u03bcm) and deep ultraviolet spectral regions (wavelength 80\u2013180 nm).\nThe main ovals are the dominant part of the Jovian aurorae. They have stable shapes and locations, but their intensities are strongly modulated by the solar wind pressure\u2014the stronger solar wind, the weaker the aurorae. As mentioned above, the main ovals are maintained by the strong influx of electrons accelerated by the electric potential drops between the magnetodisk plasma and the Jovian ionosphere. These electrons carry field aligned currents, which maintain the plasma's co-rotation in the magnetodisk. The potential drops develop because the sparse plasma outside the equatorial sheet can only carry a current of a limited strength without those currents. The precipitating electrons have energy in the range 10\u2013100 keV and penetrate deep into the atmosphere of Jupiter, where they ionize and excite molecular hydrogen causing ultraviolet emission. The total energy input into the ionosphere is 10\u2013100 TW. In addition, the currents flowing in the ionosphere heats it by the process known as Joule heating. This heating, which produces up to 300 TW of power, is responsible for the strong infrared radiation from the Jovian aurorae and partially for the heating of the thermosphere of Jupiter.\nSpots were found to correspond to three Galilean moons: Io, Europa and Ganymede. They develop because the co-rotation of the plasma is slowed in the vicinity of moons. The brightest spot belongs to Io, which is the main source of the plasma in the magnetosphere (see above). The Ionian auroral spot is thought to be related to Alfv\u00e9n currents flowing from the Jovian to Ionian ionosphere. Europa's and Ganymede's spots are much dimmer, because these moons are weak plasma sources, because of sublimation of the water ice from their surfaces.\nBright arcs and spots sporadically appear within the main ovals. These transient phenomena are thought to be related to interaction with the solar wind. The magnetic field lines in this region are believed to be open or to map onto the magnetotail. The secondary ovals observed inside the main oval may be related to the boundary between open and closed magnetic field lines or to the polar cusps. The polar auroral emissions are similar to those observed around Earth's poles: both appear when electrons are accelerated towards the planet by potential drops, during reconnection of solar magnetic field with that of the planet. The regions within both main ovals emit most of auroral X-rays. The spectrum of the auroral X-ray radiation consists of spectral lines of highly ionized oxygen and sulfur, which probably appear when energetic (hundreds of kiloelectronvolts) S and O ions precipitate into the polar atmosphere of Jupiter. The source of this precipitation remains unknown.\n\n\n=== Jupiter as a pulsar ===\nJupiter is a powerful source of radio waves in the spectral region stretching from several kilohertz to tens of megahertz. Radio waves with frequencies of less than about 0.3 MHz (and thus wavelengths longer than 1 km) are called the Jovian kilometric radiation or KOM. Those with frequencies in the interval of 0.3\u20133 MHz (with wavelengths of 100\u20131000 m) are called the hectometric radiation or HOM, while emissions in the range 3\u201340 MHz (with wavelengths of 10\u2013100 m) are referred to as the decametric radiation or DAM. The latter radiation was the first to be observed from Earth, and its approximately 10-hour periodicity helped to identify it as originating from Jupiter. The strongest part of decametric emission, which is related to Io and to the Io\u2013Jupiter current system, is called Io-DAM.\n\nThe majority of these emissions are thought to be produced by a mechanism called Cyclotron Maser Instability, which develops close to the auroral regions, when electrons bounce back and forth between the poles. The electrons involved in the generation of radio waves are probably those carrying currents from the poles of the planet to the magnetodisk. The intensity of Jovian radio emissions usually varies smoothly with time; however, Jupiter periodically emits short and powerful bursts (S bursts), which can outshine all other components. The total emitted power of the DAM component is about 100 GW, while the power of all other HOM/KOM components is about 10 GW. In comparison, the total power of Earth's radio emissions is about 0.1 GW.\nJupiter's radio and particle emissions are strongly modulated by its rotation, which makes the planet somewhat similar to a pulsar. This periodical modulation is probably related to asymmetries in the Jovian magnetosphere, which are caused by the tilt of the magnetic moment with respect to the rotational axis as well as by high-latitude magnetic anomalies. The physics governing Jupiter's radio emissions is similar to that of radio pulsars. They differ only in the scale, and Jupiter can be considered a very small radio pulsar too. In addition, Jupiter's radio emissions strongly depend on solar wind pressure and, hence, on solar activity.\nIn addition to relatively long-wavelength radiation, Jupiter also emits synchrotron radiation (also known as the Jovian decimetric radiation or DIM radiation) with frequencies in the range of 0.1\u201315 GHz (wavelength from 3 m to 2 cm), which is the bremsstrahlung radiation of the relativistic electrons trapped in the inner radiation belts of the planet. The energy of the electrons that contribute to the DIM emissions is from 0.1 to 100 MeV, while the leading contribution comes from the electrons with energy in the range 1\u201320 MeV. This radiation is well-understood and was used since the beginning of the 1960s to study the structure of the planet's magnetic field and radiation belts. The particles in the radiation belts originate in the outer magnetosphere and are adiabatically accelerated, when they are transported to the inner magnetosphere.\nJupiter's magnetosphere ejects streams of high-energy electrons and ions (energy up to tens megaelectronvolts), which travel as far as Earth's orbit. These streams are highly collimated and vary with the rotational period of the planet like the radio emissions. In this respect as well, Jupiter shows similarity to a pulsar.\n\n\n== Interaction with rings and moons ==\n\nJupiter's extensive magnetosphere envelops its ring system and the orbits of all four Galilean satellites. Orbiting near the magnetic equator, these bodies serve as sources and sinks of magnetospheric plasma, while energetic particles from the magnetosphere alter their surfaces. The particles sputter off material from the surfaces and create chemical changes via radiolysis. The plasma's co-rotation with the planet means that the plasma preferably interacts with the moons' trailing hemispheres, causing noticeable hemispheric asymmetries. In addition, the large internal magnetic fields of the moons contribute to the Jovian magnetic field.\n\nClose to Jupiter, the planet's rings and small moons absorb high-energy particles (energy above 10 keV) from the radiation belts. This creates noticeable gaps in the belts' spatial distribution and affects the decimetric synchrotron radiation. In fact, the existence of Jupiter's rings was first hypothesized on the basis of data from the Pioneer 11 spacecraft, which detected a sharp drop in the number of high-energy ions close to the planet. The planetary magnetic field strongly influences the motion of sub-micrometer ring particles as well, which acquire an electrical charge under the influence of solar ultraviolet radiation. Their behavior is similar to that of co-rotating ions. The resonant interaction between the co-rotation and the orbital motion is thought to be responsible for the creation of Jupiter's innermost halo ring (located between 1.4 and 1.71 RJ), which consists of sub-micrometer particles on highly inclined and eccentric orbits. The particles originate in the main ring; however, when they drift toward Jupiter, their orbits are modified by the strong 3:2 Lorentz resonance located at 1.71 RJ, which increases their inclinations and eccentricities. Another 2:1 Lorentz resonance at 1.4 Rj defines the inner boundary of the halo ring.\nAll Galilean moons have thin atmospheres with surface pressures in the range 0.01\u20131 nbar, which in turn support substantial ionospheres with electron densities in the range of 1,000\u201310,000 cm\u22123. The co-rotational flow of cold magnetospheric plasma is partially diverted around them by the currents induced in their ionospheres, creating wedge-shaped structures known as Alfv\u00e9n wings. The interaction of the large moons with the co-rotational flow is similar to the interaction of the solar wind with the non-magnetized planets like Venus, although the co-rotational speed is usually subsonic (the speeds vary from 74 to 328 km/s), which prevents the formation of a bow shock. The pressure from the co-rotating plasma continuously strips gases from the moons' atmospheres (especially from that of Io), and some of these atoms are ionized and brought into co-rotation. This process creates gas and plasma tori in the vicinity of moons' orbits with the Ionian torus being the most prominent. In effect, the Galilean moons (mainly Io) serve as the principal plasma sources in Jupiter's inner and middle magnetosphere. Meanwhile, the energetic particles are largely unaffected by the Alfv\u00e9n wings and have free access to the moons' surfaces (except Ganymede's).\n\nThe icy Galilean moons, Europa, Ganymede and Callisto, all generate induced magnetic moments in response to changes in Jupiter's magnetic field. These varying magnetic moments create dipole magnetic fields around them, which act to compensate for changes in the ambient field. The induction is thought to take place in subsurface layers of salty water, which are likely to exist in all of Jupiter's large icy moons. These underground oceans can potentially harbor life, and evidence for their presence was one of the most important discoveries made in the 1990s by spacecraft.\nThe interaction of the Jovian magnetosphere with Ganymede, which has an intrinsic magnetic moment, differs from its interaction with the non-magnetized moons. Ganymede's internal magnetic field carves a cavity inside Jupiter's magnetosphere with a diameter of approximately two Ganymede diameters, creating a mini-magnetosphere within Jupiter's magnetosphere. Ganymede's magnetic field diverts the co-rotating plasma flow around its magnetosphere. It also protects the moon's equatorial regions, where the field lines are closed, from energetic particles. The latter can still freely strike Ganymede's poles, where the field lines are open. Some of the energetic particles are trapped near the equator of Ganymede, creating mini-radiation belts. Energetic electrons entering its thin atmosphere are responsible for the observed Ganymedian polar aurorae.\nCharged particles have a considerable influence on the surface properties of Galilean moons. Plasma originating from Io carries sulfur and sodium ions farther from the planet, where they are implanted preferentially on the trailing hemispheres of Europa and Ganymede. On Callisto however, for unknown reasons, sulfur is concentrated on the leading hemisphere. Plasma may also be responsible for darkening the moons' trailing hemispheres (again, except Callisto's). Energetic electrons and ions, with the flux of the latter being more isotropic, bombard surface ice, sputtering atoms and molecules off and causing radiolysis of water and other chemical compounds. The energetic particles break water into oxygen and hydrogen, maintaining the thin oxygen atmospheres of the icy moons (since the hydrogen escapes more rapidly). The compounds produced radiolytically on the surfaces of Galilean moons also include ozone and hydrogen peroxide. If organics or carbonates are present, carbon dioxide, methanol and carbonic acid can be produced as well. In the presence of sulfur, likely products include sulfur dioxide, hydrogen disulfide and sulfuric acid. Oxidants produced by radiolysis, like oxygen and ozone, may be trapped inside the ice and carried downward to the oceans over geologic time intervals, thus serving as a possible energy source for life.\n\n\n== Discovery ==\n\nThe first evidence for the existence of Jupiter's magnetic field came in 1955, with the discovery of the decametric radio emission or DAM. As the DAM's spectrum extended up to 40 MHz, astronomers concluded that Jupiter must possess a magnetic field with a strength of about 1 milliteslas (10 gauss).\nIn 1959, observations in the microwave part of the electromagnetic (EM) spectrum (0.1\u201310 GHz) led to the discovery of the Jovian decimetric radiation (DIM) and the realization that it was synchrotron radiation emitted by relativistic electrons trapped in the planet's radiation belts. These synchrotron emissions were used to estimate the number and energy of the electrons around Jupiter and led to improved estimates of the magnetic moment and its tilt.\nBy 1973 the magnetic moment was known within a factor of two, whereas the tilt was correctly estimated at about 10\u00b0. The modulation of Jupiter's DAM by Io (the so-called Io-DAM) was discovered in 1964, and allowed Jupiter's rotation period to be precisely determined. The definitive discovery of the Jovian magnetic field occurred in December 1973, when the Pioneer 10 spacecraft flew near the planet.\n\n\n== Exploration after 1970 ==\nAs of 2009 a total of eight spacecraft have flown around Jupiter and all have contributed to the present knowledge of the Jovian magnetosphere. The first space probe to reach Jupiter was Pioneer 10 in December 1973, which passed within 2.9 RJ from the center of the planet. Its twin Pioneer 11 visited Jupiter a year later, traveling along a highly inclined trajectory and approaching the planet as close as 1.6 RJ.\nPioneer provided the best coverage available of the inner magnetic field. The level of radiation at Jupiter was ten times more powerful than Pioneer's designers had predicted, leading to fears that the probe would not survive; however, with a few minor glitches, it managed to pass through the radiation belts, saved in large part by the fact that Jupiter's magnetosphere had \"wobbled\" slightly upward at that point, moving away from the spacecraft. However, Pioneer 11 did lose most images of Io, as the radiation had caused its imaging photo polarimeter to receive a number of spurious commands. The subsequent and far more technologically advanced Voyager spacecraft had to be redesigned to cope with the massive radiation levels.\nVoyagers 1 and 2 arrived to Jupiter in 1979\u20131980 and traveled almost in its equatorial plane. Voyager 1, which passed within 5 RJ from the planet's center, was first to encounter the Io plasma torus. Voyager 2 passed within 10 RJ and discovered the current sheet in the equatorial plane. The next probe to approach Jupiter was Ulysses in 1992, which investigated the planet's polar magnetosphere.\nThe Galileo spacecraft, which orbited Jupiter from 1995 to 2003, provided a comprehensive coverage of Jupiter's magnetic field near the equatorial plane at distances up to 100 RJ. The regions studied included the magnetotail and the dawn and dusk sectors of the magnetosphere. While Galileo successfully survived in the harsh radiation environment of Jupiter, it still experienced a few technical problems. In particular, the spacecraft's gyroscopes often exhibited increased errors. Several times electrical arcs occurred between rotating and non-rotating parts of the spacecraft, causing it to enter safe mode, which led to total loss of the data from the 16th, 18th and 33rd orbits. The radiation also caused phase shifts in Galileo's ultra-stable quartz oscillator.\nWhen the Cassini spacecraft flew by Jupiter in 2000, it conducted coordinated measurements with Galileo. New Horizons passed close to Jupiter in 2007, carrying out a unique investigation of the Jovian magnetotail, traveling as far as 2500 RJ along its length. In July 2016 Juno was inserted into Jupiter orbit, its scientific objectives include exploration of Jupiter's polar magnetosphere. The coverage of Jupiter's magnetosphere remains much poorer than for Earth's magnetic field. Further study is important to further understand the Jovian magnetosphere's dynamics.\nIn 2003, NASA conducted a conceptual study called \"Human Outer Planets Exploration\" (HOPE) regarding the future human exploration of the outer solar system. The possibility was mooted of building a surface base on Callisto, because of the low radiation levels at the moon's distance from Jupiter and its geological stability. Callisto is the only one of Jupiter's Galilean satellites for which human exploration is feasible. The levels of ionizing radiation on Io, Europa and Ganymede are inimical to human life, and adequate protective measures have yet to be devised.\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Cited sources ==\n\n\n== Further reading ==\n\n\n== External links ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Magnetosphere_of_Jupiter", 
                "title": "Magnetosphere of Jupiter"
            }, 
            {
                "snippet": "sources for hadron therapy. Plasma accelerators generally use wakefields generated by plasma density waves. However, plasma accelerators can operate in", 
                "pageCategories": "Accelerator physics\nPlasma physics", 
                "pageContent": "Plasma acceleration is a technique for accelerating charged particles, such as electrons, positrons and ions, using an electric field associated with electron plasma wave or other high-gradient plasma structures (like shock and sheath fields). The plasma acceleration structures are created either using ultra-short laser pulses or energetic particle beams that are matched to the plasma parameters. These techniques offer a way to build high performance particle accelerators of much smaller size than conventional devices. The basic concepts of plasma acceleration and its possibilities were originally conceived by Toshiki Tajima and Prof. John M. Dawson of UCLA in 1979. Initial designs of experiment for \"wakefield\" were conceived at UCLA by the group of Prof. Chan Joshi. Current experimental devices show accelerating gradients several orders of magnitude better than current particle accelerators.\nPlasma accelerators have immense promise for innovation of affordable and compact accelerators for various applications ranging from high energy physics to medical and industrial applications. Medical applications include betatron and free-electron light sources for diagnostics or radiation therapy and protons sources for hadron therapy. Plasma accelerators generally use wakefields generated by plasma density waves. However, plasma accelerators can operate in many different regimes depending upon the characteristics of the plasmas used.\nFor example, an experimental laser plasma accelerator at Lawrence Berkeley National Laboratory accelerates electrons to 1 GeV over about 3.3 cm (5.4x1020 gn), and one at the SLAC conventional accelerator (highest electron energy accelerator) requires 64 m to reach the same energy. Similarly, using plasmas an energy gain of more than 40 GeV was achieved using the SLAC SLC beam (42 GeV) in just 85 cm using a plasma wakefield accelerator (8.9x1020 gn). Once fully developed, the technology could replace many of the traditional RF accelerators currently found in particle colliders, hospitals and research facilities.\nThe Texas Petawatt laser facility at the University of Texas at Austin accelerated electrons to 2 GeV over about 2 cm (1.6x1021 gn). This record was broken (by more than 2x) in 2014 by the scientists at the BELLA (laser) Center at the Lawrence Berkeley National Laboratory, when they produced electron beams up to 4.25 GeV.\nIn late 2014, researchers from SLAC National Accelerator Laboratory using the Facility for Advanced Accelerator Experimental Tests (FACET) published proof of the viability of plasma acceleration technology. It was shown to be able to achieve 400 to 500 times higher energy transfer compared to a general linear accelerator design. \nA proof-of-principle plasma wakefield accelerator experiment using a 400 GeV proton beam from the Super Proton Synchrotron is currently under construction at CERN. The experiment, named AWAKE, is scheduled for start-up at the end of 2016.\n\n\n== Concept ==\nA plasma consists of fluid of positive and negative charged particles, generally created by heating or photo-ionizing (direct / tunneling / multi-photon / barrier-suppression) a dilute gas. Under normal conditions the plasma will be macroscopically neutral (or quasi-neutral), an equal mix of electrons and ions in equilibrium. However, if a strong enough external electric or electromagnetic field is applied, the plasma electrons, which are very light in comparison to the background ions (by a factor of at least 1836), will separate spatially from the massive ions creating a charge imbalance in the perturbed region. A particle injected into such a plasma would be accelerated by the charge separation field, but since the magnitude of this separation is generally similar to that of the external field, apparently nothing is gained in comparison to a conventional system that simply applies the field directly to the particle. But, the plasma medium acts as the most efficient transformer (currently known) of the transverse field of an electromagnetic wave into longitudinal fields of a plasma wave. In existing accelerator technology various appropriately designed materials are used to convert from transverse propagating extremely intense fields into longitudinal fields that the particles can get a kick from. This process is achieved using two approaches: standing-wave structures (such as resonant cavities) or traveling-wave structures such as disc-loaded waveguides etc. But, the limitation of materials interacting with higher and higher fields is that they eventually get destroyed through ionization and breakdown. Here the plasma accelerator science provides the breakthrough to generate, sustain, and exploit the highest fields ever produced by science in the laboratory.\nWhat makes the system useful is the possibility of introducing waves of very high charge separation that propagate through the plasma similar to the traveling-wave concept in the conventional accelerator. The accelerator thereby phase-locks a particle bunch on a wave and this loaded space-charge wave accelerates them to higher velocities while retaining the bunch properties. Currently, plasma wakes are excited by appropriately shaped laser pulses or electron bunches. Plasma electrons are driven out and away from the center of wake by the ponderomotive force or the electrostatic fields from the exciting fields (electron or laser). Plasma ions are too massive to move significantly and are assumed to be stationary at the time-scales of plasma electron response to the exciting fields. As the exciting fields pass through the plasma, the plasma electrons experience a massive attractive force back to the center of the wake by the positive plasma ions chamber, bubble or column that have remained positioned there, as they were originally in the unexcited plasma. This forms a full wake of an extremely high longitudinal (accelerating) and transverse (focusing) electric field. The positive charge from ions in the charge-separation region then creates a huge gradient between the back of the wake, where there are many electrons, and the middle of the wake, where there are mostly ions. Any electrons in between these two areas will be accelerated (in self-injection mechanism). In the external bunch injection schemes the electrons are strategically injected to arrive at the evacuated region during maximum excursion or expulsion of the plasma electrons.\nA beam-driven wake can be created by sending a relativistic proton or electron bunch into an appropriate plasma or gas. In some cases, the gas can be ionized by the electron bunch, so that the electron bunch both creates the plasma and the wake. This requires an electron bunch with relatively high charge and thus strong fields. The high fields of the electron bunch then push the plasma electrons out from the center, creating the wake.\nSimilar to a beam-driven wake, a laser pulse can be used to excite the plasma wake. As the pulse travels through the plasma, the electric field of the light separates the electrons and nucleons in the same way that an external field would.\nIf the fields are strong enough, all of the ionized plasma electrons can be removed from the center of the wake: this is known as the \"blowout regime\". Although the particles are not moving very quickly during this period, macroscopically it appears that a \"bubble\" of charge is moving through the plasma at close to the speed of light. The bubble is the region cleared of electrons that is thus positively charged, followed by the region where the electrons fall back into the center and is thus negatively charged. This leads to a small area of very strong potential gradient following the laser pulse.\nIn the linear regime, plasma electrons aren't completely removed from the center of the wake. In this case, the linear plasma wave equation can be applied. However, the wake appears very similar to the blowout regime, and the physics of acceleration is the same.\n\nIt is this \"wakefield\" that is used for particle acceleration. A particle injected into the plasma near the high-density area will experience an acceleration toward (or away) from it, an acceleration that continues as the wakefield travels through the column, until the particle eventually reaches the speed of the wakefield. Even higher energies can be reached by injecting the particle to travel across the face of the wakefield, much like a surfer can travel at speeds much higher than the wave they surf on by traveling across it. Accelerators designed to take advantage of this technique have been referred to colloquially as \"surfatrons\".\n\n\n== Comparison with RF acceleration ==\nThe advantage of plasma acceleration is that its acceleration field can be much stronger than that of conventional radio-frequency (RF) accelerators. In RF accelerators, the field has an upper limit determined by the threshold for dielectric breakdown of the acceleration tube. This limits the amount of acceleration over any given area, requiring very long accelerators to reach high energies. In contrast, the maximum field in a plasma is defined by mechanical qualities and turbulence, but is generally several orders of magnitude stronger than with RF accelerators. It is hoped that a compact particle accelerator can be created based on plasma acceleration techniques or accelerators for much higher energy can be built, if long accelerators are realizable with an accelerating field of 10 GV/m.\nPlasma acceleration is categorized into several types according to how the electron plasma wave is formed:\nplasma wakefield acceleration (PWFA): The electron plasma wave is formed by an electron or proton bunch.\nlaser wakefield acceleration (LWFA): A laser pulse is introduced to form an electron plasma wave.\nlaser beat-wave acceleration (LBWA): The electron plasma wave arises based on different frequency generation of two laser pulses. The \"Surfatron\" is an improvement on this technique.\nself-modulated laser wakefield acceleration (SMLWFA): The formation of an electron plasma wave is achieved by a laser pulse modulated by stimulated Raman forward scattering instability.\nThe first experimental demonstration of wakefield acceleration, which was performed with PWFA, was reported by a research group at Argonne National Laboratory in 1988.\n\n\n== Formula ==\nThe acceleration gradient for a linear plasma wave is:\n\n  \n    \n      \n        E\n        =\n        c\n        \u22c5\n        \n          \n            \n              \n                \n                  m\n                  \n                    e\n                  \n                \n                \u22c5\n                \u03c1\n              \n              \n                \u03b5\n                \n                  0\n                \n              \n            \n          \n        \n        .\n      \n    \n    {\\displaystyle E=c\\cdot {\\sqrt {\\frac {m_{e}\\cdot \\rho }{\\varepsilon _{0}}}}.}\n  \nIn this equation, \n  \n    \n      \n        E\n      \n    \n    {\\displaystyle E}\n   is the electric field, \n  \n    \n      \n        c\n      \n    \n    {\\displaystyle c}\n   is the speed of light in vacuum, \n  \n    \n      \n        \n          m\n          \n            e\n          \n        \n      \n    \n    {\\displaystyle m_{e}}\n   is the mass of the electron, \n  \n    \n      \n        \u03c1\n      \n    \n    {\\displaystyle \\rho }\n   is the plasma density (in particles per metre cubed), and \n  \n    \n      \n        \n          \u03b5\n          \n            0\n          \n        \n      \n    \n    {\\displaystyle \\varepsilon _{0}}\n   is the permittivity of free space.\n\n\n== Experimental laboratories ==\nCurrently plasma-based particle accelerators are in the proof of concept phase at the following institutions:\nArgonne National Laboratory\nMax Planck Institute for Quantum Optics\nHelmholtz Institute Jena\nLawrence Berkeley National Laboratory\nSLAC National Accelerator Laboratory\nUCLA\nRutherford Appleton Laboratory\nLawrence Livermore National Laboratory\nUnited States Naval Research Laboratory\nBudker Institute of Nuclear Physics\nUniversity of Michigan\nChalk River Laboratories\nTexas Petawatt Laser, University of Texas at Austin\nAdvanced Laser-Plasma High-energy Accelerators towards X-rays (ALPHA-X) beam line at the University of Strathclyde\nScottish Centre for the Application of Plasma-based Accelerators (SCAPA)\nLund University\nLaboratoire d'Optique Appliqu\u00e9e\nCERN\nDESY/Universit\u00e4t Hamburg\n\n\n== See also ==\nDielectric wall accelerator\nList of plasma (physics) articles\n\n\n== References ==\n\nC. Joshi, \"Plasma Accelerators,\" Scientific American (February 2006), 294, 40\u201347\nKatsouleas, T. \"Accelerator physics: Electrons hang ten on laser wake\" Nature (September 2004), 431, 515\u2013516, doi:10.1038/431515a\nJoshi, C. & Katsouleas, T., \"Plasma accelerators at the energy frontier and on tabletops\", Physics Today 56, No. 6, 47\u201351 (2003), doi:10.1063/1.1595054.\nJoshi, C. & Malka, V. (2010). \"Focus on Laser- and Beam-Driven Plasma Accelerators\". New Journal of Physics. \n\n\n== External links ==\nPlasma Wakefield Acceleration - A Guide\nRiding the Plasma Wave of the Future", 
                "titleUrl": "https://en.wikipedia.org/wiki/Plasma_acceleration", 
                "title": "Plasma acceleration"
            }, 
            {
                "snippet": "can be better controlled. The non transferred plasma arc possesses comparatively less energy density as compared to a transferred arc plasma and it is employed", 
                "pageCategories": "All articles needing additional references\nArc welding\nArticles needing additional references from November 2009\nPlasma processing", 
                "pageContent": "Plasma arc welding (PAW) is an arc welding process similar to gas tungsten arc welding (GTAW). The electric arc is formed between an electrode (which is usually but not always made of sintered tungsten) and the workpiece. The key difference from GTAW is that in PAW, by positioning the electrode within the body of the torch, the plasma arc can be separated from the shielding gas envelope. The plasma is then forced through a fine-bore copper nozzle which constricts the arc and the plasma exits the orifice at high velocities (approaching the speed of sound) and a temperature approaching 28,000 \u00b0C (50,000 \u00b0F) or higher.\nJust as oxy-fuel torches can be used for either welding or cutting, so too can plasma torches, which can achieve plasma arc welding or plasma cutting.\nArc plasma is the temporary state of a gas. The gas gets ionized after passage of electric current through it and it becomes a conductor of electricity. In ionized state atoms break into electrons (\u2212) and cations (+) and the system contains a mixture of ions, electrons and highly excited atoms. The degree of ionization may be between 1% and greater than 100% i.e.; double and triple degrees of ionization. Such states exist as more electrons are pulled from their orbits.\nThe energy of the plasma jet and thus the temperature is dependent upon the electrical power employed to create arc plasma. A typical value of temperature obtained in a plasma jet torch may be of the order of 28000 \u00b0C(50000 \u00b0F ) against about 5500 \u00b0C (10000 \u00b0F) in ordinary electric welding arc. Actually all welding arcs are (partially ionized) plasmas, but the one in plasma arc welding is a constricted arc plasma.\n\n\n== Concept ==\nPlasma arc welding is an arc welding process wherein coalescence is produced by the heat obtained from a constricted arc setup between a tungsten/alloy tungsten electrode and the water-cooled (constricting) nozzle (non-transferred arc) or between a tungsten/alloy tungsten electrode and the job (transferred arc). The process employs two inert gases, one forms the arc plasma and the second shields the arc plasma. Filler metal may or may not be added.\n\n\n== History ==\nThe plasma arc welding and cutting process was invented by Robert M. Gage in 1953 and patented in 1957. The process was unique in that it could achieve precision cutting and welding on both thin and thick metals. It was also capable of spray coating hardening metals onto other metals. One example was the spray coating of the turbine blades of the moon bound Saturn rocket.\n\n\n== Principle of operation ==\nPlasma arc welding is a constricted arc process. The arc is constricted with the help of a water-cooled small diameter nozzle which squeezes the arc, increases its pressure, temperature and heat intensely and thus improves arc stability, arc shape and heat transfer characteristics. Plasma arc welding processes can be divided into two basic types:\nNon-transferred arc process\nThe arc is formed between the electrode(-) and the water cooled constricting nozzle(+). Arc plasma comes out of the nozzle as a flame. The arc is independent of the work piece and the work piece does not form a part of the electrical circuit. Just like an arc flame (as in atomic hydrogen welding), it can be moved from one place to another and can be better controlled. The non transferred plasma arc possesses comparatively less energy density as compared to a transferred arc plasma and it is employed for welding and in applications involving ceramics or metal plating (spraying). High density metal coatings can be produced by this process. A non-transferred arc is initiated by using a high frequency unit in the circuit.\nTransferred arc process\nThe arc is formed between the electrode(-) and the work piece(+). In other words, arc is transferred from the electrode to the work piece. A transferred arc possesses high energy density and plasma jet velocity. For this reason it is employed to cut and melt metals. Besides carbon steels this process can cut stainless steel and nonferrous metals where an oxyacetylene torch does not succeed. Transferred arc can also be used for welding at high arc travel speeds. For initiating a transferred arc, a current limiting resistor is put in the circuit, which permits a flow of about 50 amps, between the nozzle and electrode and a pilot arc is established between the electrode and the nozzle. As the pilot arc touches the job main current starts flowing between electrode and job, thus igniting the transferred arc. The pilot arc initiating unit gets disconnected and pilot arc extinguishes as soon as the arc between the electrode and the job is started. The temperature of a constricted plasma arc may be of the order of 8000 - 250000C.\n\n\n== Equipment ==\nThe equipment needed in plasma arc welding along with their functions are as follows:\n\n\n=== Power Supply ===\nA direct current power source (generator or rectifier) having drooping characteristics and open circuit voltage of 70 volts or above is suitable for plasma arc welding. Rectifiers are generally preferred over DC generators. Working with helium as an inert gas needs open circuit voltage above 70 volts. This higher voltage can be obtained by series operation of two power sources; or the arc can be initiated with argon at normal open circuit voltage and then helium can be switched on.\nTypical welding parameters for plasma arc welding are as follows:\nCurrent 50 to 350 amps, voltage 27 to 31 volts, gas flow rates 2 to 40 liters/minute (lower range for orifice gas and higher range for outer shielding gas), direct current electrode negative (DCEN) is normally employed for plasma arc welding except for the welding of aluminum in which cases water cooled electrode is preferable for reverse polarity welding, i.e. direct current electrode positive (DCEP).\n\n\n=== High frequency generator and current limiting resistors ===\nA high frequency generator and current limiting resistors are used for arc ignition. The arc starting system may be separate or built into the system.\n\n\n=== Plasma Torch ===\nIt is either transferred arc or non transferred arc typed. It is hand operated or mechanized. At present, almost all applications require automated system. The torch is water cooled to increase the life of the nozzle and the electrode. The size and the type of nozzle tip are selected depending upon the metal to be welded, weld shapes and desired penetration depth.\n\n\n=== Shielding gases ===\nTwo inert gases or gas mixtures are employed. The orifice gas at lower pressure and flow rate forms the plasma arc. The pressure of the orifice gas is intentionally kept low to avoid weld metal turbulence, but this low pressure is not able to provide proper shielding of the weld pool. To have suitable shielding protection same or another inert gas is sent through the outer shielding ring of the torch at comparatively higher flow rates. Most of the materials can be welded with argon, helium, argon+hydrogen and argon+helium, as inert gases or gas mixtures. Argon is very commonly used. Helium is preferred where a broad heat input pattern and flatter cover pass is desired without key hole mode weld. A mixture of argon and hydrogen supplies heat energy higher than when only argon is used and thus permits keyhole mode welds in nickel base alloys, copper base alloys and stainless steels.\nFor cutting purposes a mixture of argon and hydrogen (10-30%) or that of nitrogen may be used. Hydrogen, because of its dissociation into atomic form and thereafter recombination generates temperatures above those attained by using argon or helium alone. In addition, hydrogen provides a reducing atmosphere, which helps in preventing oxidation of the weld and its vicinity. (Care must be taken, as hydrogen diffusing into the metal can lead to embrittlement in some metals and steels.)\n\n\n=== Voltage control ===\nVoltage control is required in contour welding. In normal key hole welding a variation in arc length up to 1.5 mm does not affect weld bead penetration or bead shape to any significant extent and thus a voltage control is not considered essential.\n\n\n=== Current and gas decay control ===\nIt is necessary to close the key hole properly while terminating the weld in the structure.\n\n\n=== Fixture ===\nIt is required to avoid atmospheric contamination of the molten metal under bead.\n\n\n== Process Description ==\nTechnique of work piece cleaning and filler metal addition is similar to that in TIG welding. Filler metal is added at the leading edge of the weld pool. Filler metal is not required in making root pass weld.\nType of Joints: For welding work piece up to 25 mm thick, joints like square butt, J or V are employed. Plasma welding is used to make both key hole and non-key hole types of welds.\nMaking a non-key hole weld: The process can make non key hole welds on work pieces having thickness 2.4 mm and under.\nMaking a keyhole welds: An outstanding characteristics of plasma arc welding, owing to exceptional penetrating power of plasma jet, is its ability to produce keyhole welds in work piece having thickness from 2.5 mm to 25 mm. A keyhole effect is achieved through right selection of current, nozzle orifice diameter and travel speed, which create a forceful plasma jet to penetrate completely through the work piece. Plasma jet in no case should expel the molten metal from the joint. The major advantages of keyhole technique are the ability to penetrate rapidly through relatively thick root sections and to produce a uniform under bead without mechanical backing. Also, the ratio of the depth of penetration to the width of the weld is much higher, resulting narrower weld and heat-affected zone. As the weld progresses, base metal ahead the keyhole melts, flow around the same solidifies and forms the weld bead. Key holing aids deep penetration at faster speeds and produces high quality bead. While welding thicker pieces, in laying others than root run, and using filler metal, the force of plasma jet is reduced by suitably controlling the amount of orifice gas.\nPlasma arc welding is an advancement over the GTAW process. This process uses a non-consumable tungsten electrode and an arc constricted through a fine-bore copper nozzle. PAW can be used to join all metals that are weldable with GTAW (i.e., most commercial metals and alloys). Difficult-to-weld in metals by PAW include bronze, cast iron, lead and magnesium. Several basic PAW process variations are possible by varying the current, plasma gas flow rate, and the orifice diameter, including:\nMicro-plasma (< 15 Amperes)\nMelt-in mode (15\u2013100 Amperes)\nKeyhole mode (>100 Amperes)\nPlasma arc welding has a greater energy concentration as compared to GTAW.\nA deep, narrow penetration is achievable, with a maximum depth of 12 to 18 mm (0.47 to 0.71 in) depending on the material.\nGreater arc stability allows a much longer arc length (stand-off), and much greater tolerance to arc length changes.\nPAW requires relatively expensive and complex equipment as compared to GTAW; proper torch maintenance is critical\nWelding procedures tend to be more complex and less tolerant to variations in fit-up, etc.\nOperator skill required is slightly greater than for GTAW.\nOrifice replacement is necessary.\n\n\n== Process variables ==\n\n\n=== Gases ===\nAt least two separate (and possibly three) flows of gas are used in PAW:\nPlasma gas \u2013 flows through the orifice and becomes ionized.\nShielding gas \u2013 flows through the outer nozzle and shields the molten weld from the atmosphere\nBack-purge and trailing gas \u2013 required for certain materials and applications.\nThese gases can all be same, or of differing composition.\n\n\n=== Key process variables ===\nCurrent Type and Polarity\nDCEN from a CC source is standard\nAC square-wave is common on aluminum and magnesium\nWelding current and pulsing - Current can vary from 0.5 A to 1200 A; Current can be constant or pulsed at frequencies up to 20 kHz\nGas flow rate (This critical variable must be carefully controlled based upon the current, orifice diameter and shape, gas mixture, and the base material and thickness.)\n\n\n== Other plasma arc processes ==\nDepending upon the design of the torch (e.g., orifice diameter), electrode design, gas type and velocities, and the current levels, several variations of the plasma process are achievable, including:\nPlasma arc cutting (PAC)\nPlasma arc gouging\nPlasma arc surfacing\nPlasma arc spraying\n\n\n=== Plasma arc cutting ===\nWhen used for cutting, the plasma gas flow is increased so that the deeply penetrating plasma jet cuts through the material and molten material is removed as cutting dross. PAC differs from oxy-fuel cutting in that the plasma process operates by using the arc to melt the metal whereas in the oxy-fuel process, the oxygen oxidizes the metal and the heat from the exothermic reaction melts the metal. Unlike oxy-fuel cutting, the PAC process can be applied to cutting metals which form refractory oxides such as stainless steel, cast iron, aluminum, and other non-ferrous alloys. Since PAC was introduced by Praxair Inc. at the American Welding Society show in 1954, many process refinements, gas developments, and equipment improvements have occurred.\n\n\n== References ==\n\n\n=== Bibliography ===\nOberg, Erik; Jones, Franklin D.; Horton, Holbrook L.; Ryffel, Henry H. (2000), Machinery's Handbook (26th ed.), New York: Industrial Press Inc., ISBN 0-8311-2635-3. \n\n\n== Further reading ==\nAmerican Welding Society, Welding Handbook, Volume 2 (8th Ed.)\n\n\n== External links ==\nPlasma Arc Welding\nhttp://mewelding.com/plasma-arc-welding-paw/\nMicroplasma welding\nhttp://www.youtube.com/watch?v=T8g1lULZryk\nhttp://www.youtube.com/user/multiplazslovenia#p/u/6/SWbUJh4XuMQ\nArc spray welding\nhttp://www.youtube.com/watch?v=BtsywbmjKIE&NR=1\nhttp://www.youtube.com/watch?v=ibPPbQC5LeE&feature=related", 
                "titleUrl": "https://en.wikipedia.org/wiki/Plasma_arc_welding", 
                "title": "Plasma arc welding"
            }, 
            {
                "snippet": "confused with cell walls.           The cell membrane (also known as the plasma membrane or cytoplasmic membrane) is a biological membrane that separates", 
                "pageCategories": "Cell anatomy\nCommons category with local link same as on Wikidata\nMembrane biology\nOrganelles\nWikipedia articles needing clarification from October 2012\nWikipedia pages semi-protected against vandalism", 
                "pageContent": "The cell membrane (also known as the plasma membrane or cytoplasmic membrane) is a biological membrane that separates the interior of all cells from the outside environment. The cell membrane is selectively permeable to ions and organic molecules and controls the movement of substances in and out of cells. The basic function of the cell membrane is to protect the cell from its surroundings.\nIt consists of the phospholipid bilayer with embedded proteins. Cell membranes are involved in a variety of cellular processes such as cell adhesion, ion conductivity and cell signalling and serve as the attachment surface for several extracellular structures, including the cell wall, glycocalyx, and intracellular cytoskeleton. Cell membranes can be artificially reassembled.\n\n\n== History ==\nThe structure has been variously referred to by different writers as the ectoplast (de Vries, 1885), Plasmahaut (plasma skin, Pfeffer, 1877, 1891), Hautschicht (skin layer, Pfeffer, 1886; used with a different meaning by Hofmeister, 1867), plasmatic membrane (Pfeffer, 1900), plasma membrane, cytoplasmic membrane, cell envelope and cell membrane.\nSome authors that did not believe that there was a functional permeable boundary at the surface of the cell preferred to use the term plasmalemma (coined by Mast, 1924) to the extern region of the cell.\n\n\n== Function ==\n\nThe cell membrane (or plasma membrane or plasmalemma) surrounds the cytoplasm of living cells, physically separating the intracellular components from the extracellular environment. Fungi, bacteria and plants have a cell wall in addition, which provides a mechanical support to the cell and precludes the passage of larger molecules. The cell membrane also plays a role in anchoring the cytoskeleton to provide shape to the cell, and in attaching to the extracellular matrix and other cells to hold them together to form tissues.\nThe cell membrane is selectively permeable and able to regulate what enters and exits the cell, thus facilitating the transport of materials needed for survival. The movement of substances across the membrane can be either \"passive\", occurring without the input of cellular energy, or \"active\", requiring the cell to expend energy in transporting it. The membrane also maintains the cell potential. The cell membrane thus works as a selective filter that allows only certain things to come inside or go outside the cell. The cell employs a number of transport mechanisms that involve biological membranes:\n1. Passive osmosis and diffusion: Some substances (small molecules, ions) such as carbon dioxide (CO2) and oxygen (O2), can move across the plasma membrane by diffusion, which is a passive transport process. Because the membrane acts as a barrier for certain molecules and ions, they can occur in different concentrations on the two sides of the membrane. Such a concentration gradient across a semipermeable membrane sets up an osmotic flow for the water.\n2. Transmembrane protein channels and transporters: Nutrients, such as sugars or amino acids, must enter the cell, and certain products of metabolism must leave the cell. Such molecules diffuse passively through protein channels such as aquaporins (in the case of water (H2O)) in facilitated diffusion or are pumped across the membrane by transmembrane transporters. Protein channel proteins, also called permeases, are usually quite specific, recognizing and transporting only a limited food group of chemical substances, often even only a single substance.\n3. Endocytosis: Endocytosis is the process in which cells absorb molecules by engulfing them. The plasma membrane creates a small deformation inward, called an invagination, in which the substance to be transported is captured. The deformation then pinches off from the membrane on the inside of the cell, creating a vesicle containing the captured substance. Endocytosis is a pathway for internalizing solid particles (\"cell eating\" or phagocytosis), small molecules and ions (\"cell drinking\" or pinocytosis), and macromolecules. Endocytosis requires energy and is thus a form of active transport.\n4. Exocytosis: Just as material can be brought into the cell by invagination and formation of a vesicle, the membrane of a vesicle can be fused with the plasma membrane, extruding its contents to the surrounding medium. This is the process of exocytosis. Exocytosis occurs in various cells to remove undigested residues of substances brought in by endocytosis, to secrete substances such as hormones and enzymes, and to transport a substance completely across a cellular barrier. In the process of exocytosis, the undigested waste-containing food vacuole or the secretory vesicle budded from Golgi apparatus, is first moved by cytoskeleton from the interior of the cell to the surface. The vesicle membrane comes in contact with the plasma membrane. The lipid molecules of the two bilayers rearrange themselves and the two membranes are, thus, fused. A passage is formed in the fused membrane and the vesicles discharges its contents outside the cell.\n\n\n== Prokaryotes ==\nGram-negative bacteria have both a plasma membrane and an outer membrane separated by periplasm. Other prokaryotes have only a plasma membrane. Prokaryotic cells are also surrounded by a cell wall composed of peptidoglycan (amino acids and sugars). Some eukaryotic cells also have cell walls, but none that are made of peptidoglycan. The outer membrane of gram negative microbes is rich in lipopolysaccharide and thus is different from cell membrane of the microbes. The outer membrane can bleb out into periplasmic protrusions under stess conditions or upon virulence requirements while encountering a host target cell, and thus such blebs may work as virulence organelles.\n\n\n== Structures ==\n\n\n=== Fluid mosaic model ===\nAccording to the fluid mosaic model of S. J. Singer and G. L. Nicolson (1972), which replaced the earlier model of Davson and Danielli, biological membranes can be considered as a two-dimensional liquid in which lipid and protein molecules diffuse more or less easily. Although the lipid bilayers that form the basis of the membranes do indeed form two-dimensional liquids by themselves, the plasma membrane also contains a large quantity of proteins, which provide more structure. Examples of such structures are protein-protein complexes, pickets and fences formed by the actin-based cytoskeleton, and potentially lipid rafts.\n\n\n=== Lipid bilayer ===\n\nLipid bilayers form through the process of self-assembly. The cell membrane consists primarily of a thin layer of amphipathic phospholipids that spontaneously arrange so that the hydrophobic \"tail\" regions are isolated from the surrounding water while the hydrophilic \"head\" regions interact with the intracellular (cytosolic) and extracellular faces of the resulting bilayer. This forms a continuous, spherical lipid bilayer. Hydrophobic interactions (also known as the hydrophobic effect) are the major driving forces in the formation of lipid bilayers. An increase in interactions between hydrophobic molecules (causing clustering of hydrophobic regions) allows water molecules to bond more freely with each other, increasing the entropy of the system. This complex interaction can include noncovalent interactions such as van der Waals, electrostatic and hydrogen bonds.\nLipid bilayers are generally impermeable to ions and polar molecules. The arrangement of hydrophilic heads and hydrophobic tails of the lipid bilayer prevent polar solutes (ex. amino acids, nucleic acids, carbohydrates, proteins, and ions) from diffusing across the membrane, but generally allows for the passive diffusion of hydrophobic molecules. This affords the cell the ability to control the movement of these substances via transmembrane protein complexes such as pores, channels and gates.\nFlippases and scramblases concentrate phosphatidyl serine, which carries a negative charge, on the inner membrane. Along with NANA, this creates an extra barrier to charged moieties moving through the membrane.\nMembranes serve diverse functions in eukaryotic and prokaryotic cells. One important role is to regulate the movement of materials into and out of cells. The phospholipid bilayer structure (fluid mosaic model) with specific membrane proteins accounts for the selective permeability of the membrane and passive and active transport mechanisms. In addition, membranes in prokaryotes and in the mitochondria and chloroplasts of eukaryotes facilitate the synthesis of ATP through chemiosmosis.\n\n\n=== Membrane polarity ===\n\nThe apical membrane of a polarized cell is the surface of the plasma membrane that faces inward to the lumen. This is particularly evident in epithelial and endothelial cells, but also describes other polarized cells, such as neurons. The basolateral membrane of a polarized cell is the surface of the plasma membrane that forms its basal and lateral surfaces. It faces outwards, towards the interstitium, and away from the lumen. Basolateral membrane is a compound phrase referring to the terms \"basal (base) membrane\" and \"lateral (side) membrane\", which, especially in epithelial cells, are identical in composition and activity. Proteins (such as ion channels and pumps) are free to move from the basal to the lateral surface of the cell or vice versa in accordance with the fluid mosaic model. Tight junctions join epithelial cells near their apical surface to prevent the migration of proteins from the basolateral membrane to the apical membrane. The basal and lateral surfaces thus remain roughly equivalent to one another, yet distinct from the apical surface.\n\n\n=== Membrane structures ===\nCell membrane can form different types of \"supramembrane\" structures such as caveola, postsynaptic density, podosome, invadopodium, focal adhesion, and different types of cell junctions. These structures are usually responsible for cell adhesion, communication, endocytosis and exocytosis. They can be visualized by electron microscopy or fluorescence microscopy. They are composed of specific proteins, such as integrins and cadherins.\n\n\n=== Cytoskeleton ===\nThe cytoskeleton is found underlying the cell membrane in the cytoplasm and provides a scaffolding for membrane proteins to anchor to, as well as forming organelles that extend from the cell. Indeed, cytoskeletal elements interact extensively and intimately with the cell membrane. Anchoring proteins restricts them to a particular cell surface \u2014 for example, the apical surface of epithelial cells that line the vertebrate gut \u2014 and limits how far they may diffuse within the bilayer. The cytoskeleton is able to form appendage-like organelles, such as cilia, which are microtubule-based extensions covered by the cell membrane, and filopodia, which are actin-based extensions. These extensions are ensheathed in membrane and project from the surface of the cell in order to sense the external environment and/or make contact with the substrate or other cells. The apical surfaces of epithelial cells are dense with actin-based finger-like projections known as microvilli, which increase cell surface area and thereby increase the absorption rate of nutrients. Localized decoupling of the cytoskeleton and cell membrane results in formation of a bleb.\n\n\n== Composition ==\nCell membranes contain a variety of biological molecules, notably lipids and proteins. Material is incorporated into the membrane, or deleted from it, by a variety of mechanisms:\nFusion of intracellular vesicles with the membrane (exocytosis) not only excretes the contents of the vesicle but also incorporates the vesicle membrane's components into the cell membrane. The membrane may form blebs around extracellular material that pinch off to become vesicles (endocytosis).\nIf a membrane is continuous with a tubular structure made of membrane material, then material from the tube can be drawn into the membrane continuously.\nAlthough the concentration of membrane components in the aqueous phase is low (stable membrane components have low solubility in water), there is an exchange of molecules between the lipid and aqueous phases.\n\n\n=== Lipids ===\n\nThe cell membrane consists of three classes of amphipathic lipids: phospholipids, glycolipids, and sterols. The amount of each depends upon the type of cell, but in the majority of cases phospholipids are the most abundant. In RBC studies, 30% of the plasma membrane is lipid.\nThe fatty chains in phospholipids and glycolipids usually contain an even number of carbon atoms, typically between 16 and 20. The 16- and 18-carbon fatty acids are the most common. Fatty acids may be saturated or unsaturated, with the configuration of the double bonds nearly always \"cis\". The length and the degree of unsaturation of fatty acid chains have a profound effect on membrane fluidity as unsaturated lipids create a kink, preventing the fatty acids from packing together as tightly, thus decreasing the melting temperature (increasing the fluidity) of the membrane. The ability of some organisms to regulate the fluidity of their cell membranes by altering lipid composition is called homeoviscous adaptation.\nThe entire membrane is held together via non-covalent interaction of hydrophobic tails, however the structure is quite fluid and not fixed rigidly in place. Under physiological conditions phospholipid molecules in the cell membrane are in the liquid crystalline state. It means the lipid molecules are free to diffuse and exhibit rapid lateral diffusion along the layer in which they are present. However, the exchange of phospholipid molecules between intracellular and extracellular leaflets of the bilayer is a very slow process. Lipid rafts and caveolae are examples of cholesterol-enriched microdomains in the cell membrane. Also, a fraction of the lipid in direct contact with integral membrane proteins, which is tightly bound to the protein surface is called annular lipid shell; it behaves as a part of protein complex.\nIn animal cells cholesterol is normally found dispersed in varying degrees throughout cell membranes, in the irregular spaces between the hydrophobic tails of the membrane lipids, where it confers a stiffening and strengthening effect on the membrane.\n\n\n=== Phospholipids forming lipid vesicles ===\nLipid vesicles or liposomes are circular pockets that are enclosed by a lipid bilayer. These structures are used in laboratories to study the effects of chemicals in cells by delivering these chemicals directly to the cell, as well as getting more insight into cell membrane permeability. Lipid vesicles and liposomes are formed by first suspending a lipid in an aqueous solution then agitating the mixture through sonication, resulting in a vesicle. By measuring the rate of efflux from that of the inside of the vesicle to the ambient solution, allows researcher to better understand membrane permeability. Vesicles can be formed with molecules and ions inside the vesicle by forming the vesicle with the desired molecule or ion present in the solution. Proteins can also be embedded into the membrane through solubilizing the desired proteins in the presence of detergents and attaching them to the phospholipids in which the liposome is formed. These provide researchers with a tool to examine various membrane protein functions.\n\n\n=== Carbohydrates ===\nPlasma membranes also contain carbohydrates, predominantly glycoproteins, but with some glycolipids (cerebrosides and gangliosides). For the most part, no glycosylation occurs on membranes within the cell; rather generally glycosylation occurs on the extracellular surface of the plasma membrane. The glycocalyx is an important feature in all cells, especially epithelia with microvilli. Recent data suggest the glycocalyx participates in cell adhesion, lymphocyte homing, and many others. The penultimate sugar is galactose and the terminal sugar is sialic acid, as the sugar backbone is modified in the Golgi apparatus. Sialic acid carries a negative charge, providing an external barrier to charged particles.\n\n\n=== Proteins ===\nThe cell membrane has large content of proteins, typically around 50% of membrane volume These proteins are important for cell because they are responsible for various biological activities. Approximately a third of the genes in yeast code specifically for them, and this number is even higher in multicellular organisms.\nThe cell membrane, being exposed to the outside environment, is an important site of cell\u2013cell communication. As such, a large variety of protein receptors and identification proteins, such as antigens, are present on the surface of the membrane. Functions of membrane proteins can also include cell\u2013cell contact, surface recognition, cytoskeleton contact, signaling, enzymatic activity, or transporting substances across the membrane.\nMost membrane proteins must be inserted in some way into the membrane. For this to occur, an N-terminus \"signal sequence\" of amino acids directs proteins to the endoplasmic reticulum, which inserts the proteins into a lipid bilayer. Once inserted, the proteins are then transported to their final destination in vesicles, where the vesicle fuses with the target membrane.\n\n\n== Variation ==\nThe cell membrane has different lipid and protein compositions in distinct types of cells and may have therefore specific names for certain cell types:\nSarcolemma in myocytes\nOolemma in oocytes\nAxolemma in neuronal processes - axons\nHistorically, the plasma membrane was also referred to as the plasmalemma\n\n\n== Permeability ==\nThe permeability of a membrane is the rate of passive diffusion of molecules through the membrane. These molecules are known as permeant molecules. Permeability depends mainly on the electric charge and polarity of the molecule and to a lesser extent the molar mass of the molecule. Due to the cell membrane's hydrophobic nature, small electrically neutral molecules pass through the membrane more easily than charged, large ones. The inability of charged molecules to pass through the cell membrane results in pH partition of substances throughout the fluid compartments of the body.\n\n\n== See also ==\n\n\n== Notes and references ==\n\n\n== External links ==\nLipids, Membranes and Vesicle Trafficking - The Virtual Library of Biochemistry and Cell Biology\nCell membrane protein extraction protocol\nMembrane homeostasis, tension regulation, mechanosensitive membrane exchange and membrane traffic\n3D structures of proteins associated with plasma membrane of eukaryotic cells\nLipid composition and proteins of some eukariotic membranes\n[6]", 
                "titleUrl": "https://en.wikipedia.org/wiki/Cell_membrane", 
                "title": "Cell membrane"
            }, 
            {
                "snippet": " antigens, adhesins, and toxins are lipoproteins. Examples include the plasma lipoprotein particles classified under HDL, LDL, IDL, VLDL and ULDL (commonly", 
                "pageCategories": "All articles needing additional references\nAll articles with unsourced statements\nArticles needing additional references from October 2013\nArticles with unsourced statements from August 2014\nArticles with unsourced statements from May 2010\nLipids\nLipoproteins\nWikipedia articles with GND identifiers", 
                "pageContent": "A Lipoprotein is a biochemical assembly whose purpose is to transport hydrophobic lipids through water, as in blood or ECF. They have a single layer phospholipid membrane, with their hydrophilic portions out to the water and lipophilic parts in to the lipids within. Apolipoproteins are embedded in the membrane, both stabilising the complex and giving it functional identity determining its fate. Thus the complex serves to emulsify the fats. Many enzymes, transporters, structural proteins, antigens, adhesins, and toxins are lipoproteins. Examples include the plasma lipoprotein particles classified under HDL, LDL, IDL, VLDL and ULDL (commonly called chylomicron) lipoproteins, which enable fats to be carried in the blood stream (an example of emulsification), the transmembrane proteins of the mitochondrion and the chloroplast, and bacterial lipoproteins.\n\n\n== Scope ==\n\n\n=== Transmembrane lipoproteins ===\nThe lipids are often an essential part of the complex, even if they seem to have no catalytic activity by themselves. To isolate transmembrane lipoproteins from their associated biological membranes, detergents are often needed.\n\n\n=== Plasma lipoprotein particles ===\nThe role of lipoprotein particles is to transport triacylglycerols (a.k.a. triglycerides) and cholesterol in the blood between all the tissues of the body. The most common being the liver and the adipocytes of adipose tissue. Particles are synthesized in the small intestine and the liver, but interestingly not in the adipocytes.\nAll cells use and rely on fats and cholesterol as building-blocks to create the multiple membranes that cells use both to control internal water content and internal water-soluble elements and to organize their internal structure and protein enzymatic systems.\nThe lipoprotein particles have hydrophilic groups of phospholipids, cholesterol, and apoproteins directed outward. Such characteristics make them soluble in the salt water-based blood pool. Triglyceride-fats and cholesteryl esters are carried internally, shielded from the water by the phospholipid monolayer and the apoproteins.\nThe interaction of the proteins forming the surface of the particles (with enzymes in the blood; with each other; and with specific proteins on the surfaces of cells) determines whether triglycerides and cholesterol will be added to or removed from the lipoprotein transport particles.\nRegarding atheroma development and progression as opposed to regression, the key issue has always been cholesterol transport patterns, not cholesterol concentration itself.\n\n\n== Function ==\nThe handling of lipoprotein particles in the body is referred to as lipoprotein particle metabolism. It is divided into two pathways, exogenous and endogenous, depending in large part on whether the lipoprotein particles in question are composed chiefly of dietary (exogenous) lipids or whether they originated in the liver (endogenous), through de novo synthesis of triacylglycerols.\nThe hepatocytes are the main platform for the handling of triacylglycerols and cholesterol; the liver can also store certain amounts of glycogen and triacylglycerols. While adipocytes are the main storage cells for triacylglycerols, they do not produce any lipoproteins.\n\n\n=== Exogenous pathway ===\n\nBile emulsifies fats contained in the chyme, then pancreatic lipase cleaves triacylglycerol molecules into two fatty acids and one 2-monoacylglycerol. Enterocytes readily absorb these small molecules from the chymus. Inside of the enterocytes, fatty acids and monoacylglycerides are transformed again into triacylglycerides. Then these lipids (i.e. triacylglycerols, phospholipids, cholesterol, and cholesteryl esters) are assembled with apolipoprotein B-48 into nascent chylomicrons. These particles are then secreted into the lacteals in a process that depends heavily on apolipoprotein B-48. As they circulate through the lymphatic vessels, nascent chylomicrons bypass the liver circulation and are drained via the thoracic duct into the bloodstream.\nIn the blood stream, nascent chylomicron particles interact with HDL particles resulting in HDL donatation of apolipoprotein C-II and apolipoprotein E to the nascent chylomicron. The chylomicron at this stage is then considered mature. Via apolipoprotein C-II, mature chylomicrons activate lipoprotein lipase (LPL), an enzyme on endothelial cells lining the blood vessels. LPL catalyzes the hydrolysis of triacylglycerol (glycerol covalently joined to three fatty acids) that ultimately releases glycerol and fatty acids from the chylomicrons. Glycerol and fatty acids can then be absorbed in peripheral tissues, especially adipose and muscle, for energy and storage.\nThe hydrolyzed chylomicrons are now called chylomicron remnants. The chylomicron remnants continue circulating the bloodstream until they interact via apolipoprotein E with chylomicron remnant receptors, found chiefly in the liver. This interaction causes the endocytosis of the chylomicron remnants, which are subsequently hydrolyzed within lysosomes. Lysosomal hydrolysis releases glycerol and fatty acids into the cell, which can be used for energy or stored for later use.\n\n\n=== Endogenous pathway ===\nThe liver is the central platform for the handling of lipids: it is able to store glycerols and fats in its cells, the hepatocytes. Hepatocytes are also able to create triacylglycerols via de novo synthesis. They also produce the bile from cholesterol.\nIn the hepatocytes, triacylglycerols and cholesteryl esters are assembled with apolipoprotein B-100 to form nascent VLDL particles. Nascent VLDL particles are released into the bloodstream via a process that depends upon apolipoprotein B-100.\nIn the blood stream, nascent VLDL particles bump with HDL particles; as a result, HDL particles donate apolipoprotein C-II and apolipoprotein E to the nascent VLDL particle; Once loaded with apolipoproteins C-II and E, the nascent VLDL particle is considered mature.\nAgain, like chylomicrons, VLDL particles circulate and encounter LPL expressed on endothelial cells. Apolipoprotein C-II activates LPL, causing hydrolysis of the VLDL particle and the release of glycerol and fatty acids. These products can be absorbed from the blood by peripheral tissues, principally adipose and muscle. The hydrolyzed VLDL particles are now called VLDL remnants or intermediate-density lipoproteins (IDLs). VLDL remnants can circulate and, via an interaction between apolipoprotein E and the remnant receptor, be absorbed by the liver, or they can be further hydrolyzed by hepatic lipase.\nHydrolysis by hepatic lipase releases glycerol and fatty acids, leaving behind IDL remnants, called low-density lipoproteins (LDL), which contain a relatively high cholesterol content (see native LDL structure at 37\u00b0C on YouTube). LDL circulates and is absorbed by the liver and peripheral cells. Binding of LDL to its target tissue occurs through an interaction between the LDL receptor and apolipoprotein B-100 on the LDL particle. Absorption occurs through endocytosis, and the internalized LDL particles are hydrolyzed within lysosomes, releasing lipids, chiefly cholesterol.\n\n\n== Classification ==\n\n\n=== By density ===\nLipoproteins may be classified as follows, listed from larger and less dense to smaller and denser. Lipoproteins are larger and less dense when the fat to protein ratio is increased. They are classified on the basis of electrophoresis and ultracentrifugation.\nChylomicrons carry triglycerides (fat) from the intestines to the liver, to skeletal muscle, and to adipose tissue.\nVery-low-density lipoproteins (VLDL) carry (newly synthesised) triglycerides from the liver to adipose tissue.\nIntermediate-density lipoproteins (IDL) are intermediate between VLDL and LDL. They are not usually detectable in the blood when fasting.\nLow-density lipoproteins (LDL) carry 3,000 to 6,000 fat molecules (phospholipids, cholesterol, triglycerides, etc.) around the body. LDL particles are sometimes referred to as \"bad\" lipoprotein because concentrations, dose related, correlate with atherosclerosis progression.\nlarge buoyant LDL (lb LDL) particles\nsmall dense LDL (sd LDL) particles\nLipoprotein(a) is a lipoprotein particle of a certain phenotype\n\nHigh-density lipoproteins (HDL) collect fat molecules (phospholipids, cholesterol, triglycerides, etc.) from the body's cells/tissues, and take it back to the liver. HDLs are sometimes referred to as \"good\" lipoprotein because higher concentrations correlate with low rates of atherosclerosis progression and/or regression.\nFor young healthy research subjects, ~70 kg, 154 lb, the following applies:\n However, this data is not reliable for the general clinical population.\n\n\n=== Alpha and beta ===\nIt is also possible to classify lipoproteins as \"alpha\" and \"beta\", according to the classification of proteins in serum protein electrophoresis. This terminology is sometimes used in describing lipid disorders such as Abetalipoproteinemia.\n\n\n=== Lipoprotein(a) ===\n\n\n== Studies ==\nAtherosclerosis is the leading cause of coronary artery disease, which is the leading cause of mortality in the world. Since the 1980s, many studies have examined possible correlations between the incidence of the disease and plasma lipoprotein particle concentrations in the blood. Hypotheses exist for possible causations. Studies have shown correlation between atherosclerosis and concentrations of particles. Further studies looked for correlations between nutrition and concentration of the distinguishable lipoprotein particles, e.g. whether the ratio of dietary fat raises or lowers levels of LDL particles in the blood. Studies have shown that different phenotypes do exist regarding the amount of particles and reaction to diet composition.\n\n\n== See also ==\nApolipoprotein\nLipid anchored protein\nReverse cholesterol transport\nVertical Auto Profile\n\n\n== References ==\n\nFurther reading\nLusis, Aldons J; Pajukanta, P\u00e4ivi. \"A treasure trove for lipoprotein biology\". Nature Genetics. 40 (2): 129\u2013130. doi:10.1038/ng0208-129. PMID 18227868.  including Figure 1 - The primary pathways for the metabolism of human plasma lipoproteins are summarized\n\n\n== External links ==\nDatabase of bacterial lipoproteins at mrc-lmb.cam.ac.uk\nOverview and diagram at washington.edu\nLipoprotein research at the Medical University of Vienna\nLipoprotein assembly at wisc.edu\nLipoprotein circulation at purdue.edu\nVarious types of lipoprotein in Medscape\nCholesterol, Lipoproteins and the Liver\nRemnant Lipoproteins\nLipoproteins at the US National Library of Medicine Medical Subject Headings (MeSH)\nProteolipids at the US National Library of Medicine Medical Subject Headings (MeSH)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Lipoprotein", 
                "title": "Lipoprotein"
            }, 
            {
                "snippet": "Inductively coupled plasma atomic emission spectroscopy (ICP-AES), also referred to as inductively coupled plasma optical emission spectrometry (ICP-OES)", 
                "pageCategories": "Analytical chemistry\nEmission spectroscopy\nLaboratory equipment\nScientific techniques", 
                "pageContent": "Inductively coupled plasma atomic emission spectroscopy (ICP-AES), also referred to as inductively coupled plasma optical emission spectrometry (ICP-OES), is an analytical technique used for the detection of trace metals. It is a type of emission spectroscopy that uses the inductively coupled plasma to produce excited atoms and ions that emit electromagnetic radiation at wavelengths characteristic of a particular element. It is a flame technique with a flame temperature in a range from 6000 to 10000 K. The intensity of this emission is indicative of the concentration of the element within the sample.\n\n\n== Mechanism ==\n\nThe ICP-AES is composed of two parts: the ICP and the optical spectrometer. The ICP torch consists of 3 concentric quartz glass tubes. The output or \"work\" coil of the radio frequency (RF) generator surrounds part of this quartz torch. Argon gas is typically used to create the plasma.\nWhen the torch is turned on, an intense electromagnetic field is created within the coil by the high power radio frequency signal flowing in the coil. This RF signal is created by the RF generator which is, effectively, a high power radio transmitter driving the \"work coil\" the same way a typical radio transmitter drives a transmitting antenna. Typical instruments run at either 27 or 40 MHz. The argon gas flowing through the torch is ignited with a Tesla unit that creates a brief discharge arc through the argon flow to initiate the ionization process. Once the plasma is \"ignited\", the Tesla unit is turned off.\nThe argon gas is ionized in the intense electromagnetic field and flows in a particular rotationally symmetrical pattern towards the magnetic field of the RF coil. A stable, high temperature plasma of about 7000 K is then generated as the result of the inelastic collisions created between the neutral argon atoms and the charged particles.\nA peristaltic pump delivers an aqueous or organic sample into an analytical nebulizer where it is changed into mist and introduced directly inside the plasma flame. The sample immediately collides with the electrons and charged ions in the plasma and is itself broken down into charged ions. The various molecules break up into their respective atoms which then lose electrons and recombine repeatedly in the plasma, giving off radiation at the characteristic wavelengths of the elements involved.\nIn some designs, a shear gas, typically nitrogen or dry compressed air is used to 'cut' the plasma at a specific spot. One or two transfer lenses are then used to focus the emitted light on a diffraction grating where it is separated into its component wavelengths in the optical spectrometer. In other designs, the plasma impinges directly upon an optical interface which consists of an orifice from which a constant flow of argon emerges, deflecting the plasma and providing cooling while allowing the emitted light from the plasma to enter the optical chamber. Still other designs use optical fibers to convey some of the light to separate optical chambers.\nWithin the optical chamber(s), after the light is separated into its different wavelengths (colours), the light intensity is measured with a photomultiplier tube or tubes physically positioned to \"view\" the specific wavelength(s) for each element line involved, or, in more modern units, the separated colors fall upon an array of semiconductor photodetectors such as charge coupled devices (CCDs). In units using these detector arrays, the intensities of all wavelengths (within the system's range) can be measured simultaneously, allowing the instrument to analyze for every element to which the unit is sensitive all at once. Thus, samples can be analyzed very quickly.\nThe intensity of each line is then compared to previously measured intensities of known concentrations of the elements, and their concentrations are then computed by interpolation along the calibration lines.\nIn addition, special software generally corrects for interferences caused by the presence of different elements within a given sample matrix.\n\n\n== Applications ==\nExamples of the application of ICP-AES include the determination of metals in wine, arsenic in food, and trace elements bound to proteins.\nICP-OES is widely used in minerals processing to provide the data on grades of various streams, for the construction of mass balances.\nIn 2008, the technique was used at Liverpool University to demonstrate that a Chi Rho amulet found in Shepton Mallet and previously believed to be among the earliest evidence of Christianity in England, only dated to the nineteenth century.\nICP-AES is often used for analysis of trace elements in soil, and it is for that reason it is often used in forensics to ascertain the origin of soil samples found at crime scenes or on victims etc. Taking one sample from a control and determining the metal composition and taking the sample obtained from evidence and determine that metal composition allows a comparison to be made. While soil evidence may not stand alone in court it certainly strengthens other evidence.\nIt is also fast becoming the analytical method of choice for the determination of nutrient levels in agricultural soils. This information is then used to calculate the amount of fertiliser required to maximise crop yield and quality.\nICP-AES is used for motor oil analysis. Analyzing used motor oil reveals a great deal about how the engine is operating. Parts that wear in the engine will deposit traces in the oil which can be detected with ICP-AES. ICP-AES analysis can help to determine whether parts are failing. In addition, ICP-AES can determine what amount of certain oil additives remain and therefore indicate how much service life the oil has remaining. Oil analysis is often used by fleet manager or automotive enthusiasts who have an interest in finding out as much about their engine's operation as possible. ICP-AES is also used during the production of motor oils (and other lubricating oils) for quality control and compliance with production and industry specifications.\n\n\n== See also ==\nAtomic emission spectroscopy\nAtomic absorption spectroscopy\nInductively coupled plasma mass spectrometry\nAshing\nList of plasma (physics) articles\n\n\n== References ==\n\n\n== External links ==\nInductively Coupled Plasma/Optical Emission Spectrometry in Encyclopedia of Analytical Chemistry\nInductively-Coupled Plasma (ICP) Excitation Source [User name and password required]", 
                "titleUrl": "https://en.wikipedia.org/wiki/Inductively_coupled_plasma_atomic_emission_spectroscopy", 
                "title": "Inductively coupled plasma atomic emission spectroscopy"
            }
        ], 
        "phraseCharStart": "1208"
    }, 
    {
        "phraseCharEnd": "1236", 
        "phraseIndex": "T29", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "dust", 
        "wikiSearchResults": [
            {
                "snippet": "uses, see Dust (disambiguation).           Dust consists of particles in the atmosphere that come from various sources such as soil, dust lifted by weather", 
                "pageCategories": "All articles with unsourced statements\nArticles containing video clips\nArticles with Wayback Machine links\nArticles with unsourced statements from October 2008\nArticles with unsourced statements from September 2015\nCS1 errors: dates\nCleaning\nCommons category with local link same as on Wikidata\nParticulates\nWikipedia articles with GND identifiers", 
                "pageContent": "Dust consists of particles in the atmosphere that come from various sources such as soil, dust lifted by weather (an aeolian process), volcanic eruptions, and pollution. Dust in homes, offices, and other human environments contains small amounts of plant pollen, human and animal hairs, textile fibers, paper fibers, minerals from outdoor soil, human skin cells, burnt meteorite particles, and many other materials which may be found in the local environment.\n\n\n== Domestic dust and humans ==\n\nHouse dust mites are present indoors wherever humans live. Positive tests for dust mite allergies are extremely common among people with asthma. Dust mites are microscopic arachnids whose primary food is dead human skin cells, but they do not live on living people. They and their feces and other allergens they produce are major constituents of house dust, but because they are so heavy they are not suspended for long in the air. They are generally found on the floor and other surfaces until disturbed (by walking, for example). It could take somewhere between twenty minutes and two hours for dust mites to settle back down out of the air.\nDust mites are a nesting species that prefers a dark, warm, and humid climate. They flourish in mattresses, bedding, upholstered furniture, and carpets. Their feces include enzymes that are released upon contact with a moist surface, which can happen when a person inhales, and these enzymes can kill cells within the human body. House dust mites did not become a problem until humans began to use textiles, such as western style blankets and clothing.\n\n\n== Atmospheric dust ==\n\nAtmospheric or wind-borne dust, also known as aeolian dust, comes from arid and dry regions where high velocity winds are able to remove mostly silt-sized material, deflating susceptible surfaces. This includes areas where grazing, ploughing, vehicle use, and other human activities have further destabilized the land, though not all source areas have been largely affected by anthropogenic impacts. One-third of the global land area is covered by dust-producing surfaces, made up of hyper-arid regions like the Sahara which covers 0.9 billion hectares, and drylands which occupy 5.2 billion hectares.\nDust in the atmosphere is produced by saltation and sandblasting of sand-sized grains, and it is transported through the troposphere. This airborne dust is considered an aerosol and once in the atmosphere, it can produce strong local radiative forcing. Saharan dust in particular can be transported and deposited as far as the Caribbean and Amazonia, and may affect air temperatures, cause ocean cooling, and alter rainfall amounts.\n\n\n=== Middle East ===\nDust in the Middle East has been a historic phenomenon. Recently, because of climate change and the escalating process of desertification, the problem has worsened dramatically. As a multi-factor phenomenon, there is not yet a clear consensus on the sources or potential solutions to the problem.\nIn Iran, the dust is already affecting more than 5 million people directly, and has emerged as a serious government issue in recent years. In the province of Khuzestan it has led to the severe reduction of air quality. The amount of pollutants in the air has surpassed more than 50 times the normal level several times in a year. Recently, initiatives such as Project-Dust have been established to directly study the Middle Eastern dust.\n\n\n=== Road dust ===\n\nDust kicked up by vehicles traveling on roads may make up 33% of air pollution. Road dust consists of deposits of vehicle exhausts and industrial exhausts, particles from tire and brake wear, dust from paved roads or potholes, and dust from construction sites. Road dust is a significant source contributing to the generation and release of particulate matter into the atmosphere. Control of road dust is a significant challenge in urban areas, and also in other locations with high levels of vehicular traffic upon unsealed roads, such as mines and landfill dumps.\nRoad dust may be suppressed by mechanical methods like street sweeper vehicles equipped with vacuum cleaners, vegetable oil sprays, or with water sprayers. Improvements in automotive engineering have reduced the amount of PM10s produced by road traffic; the proportion representing re-suspension of existing particulates has, increased as a result.\n\n\n== Coal dust ==\nCoal dust is responsible for the lung disease known as pneumoconiosis, including black lung disease that occurs among coal miners. The danger of coal dust resulted in environmental legislation regulating work place air quality in some jurisdictions. In addition, if enough coal dust is dispersed within the air in a given area, in very rare circumstances, it can create an explosion hazard under certain circumstances. These circumstances are typically within confined spaces.\n\n\n== Dust control ==\n\n\n=== Control of atmospheric dust ===\nMost governmental EPAs, including the United States Environmental Protection Agency (EPA) mandate that facilities that generate dust, minimize or mitigate the production of dust in their operation. The most frequent dust control violations occur at new residential housing developments in urban areas. United States Federal law requires that construction sites obtain permits to conduct earth moving, clearing of areas, to include plans to control dust emissions when the work is being carried out. Control measures include such simple practices as spraying construction and demolition sites with water, and preventing the tracking of dust onto adjacent roads.\nSome of the issues include:\nReducing dust related health risks that include allergic reactions, pneumonia and asthmatic attacks.\nImproving visibility and road safety.\nProviding cleaner air, cleaner vehicles and cleaner homes and promoting better health.\nImproving crop productivity in agriculture.\nReducing vehicle maintenance costs by lowering the levels of dust that clog filters, bearings and machinery.\nReducing driver fatigue, maintenance on suspension systems and improving fuel economy.\nIncreasing cumulative effect - each new application builds on previous residuals reducing re-application rate *while improving performance.\nUS federal laws require dust control on sources such as vacant lots, unpaved parking lots, and unpaved roads. Dust in such places may be suppressed by mechanical methods, including paving or laying down gravel, or stabilizing the surface with water, vegetable oils or other dust suppressants, or by using water misters to suppress dust that is already airborne.\n\n\n=== Control of domestic dust ===\n\nDust control is the suppression of solid particles with diameters less than 500 micrometers. Dust poses a health threat to children, older people, and those with respiratory illnesses.\nHouse dust can become airborne easily. Care is required when removing dust to avoid causing the dust to become airborne. A feather duster tends to agitate the dust so it lands elsewhere. Products like Pledge and Swiffer are specifically made for removing dust by trapping it with sticky chemicals.\nCertified HEPA (tested to MIL STD 282) can effectively trap 99.97% of dust at 0.3 micrometers. Not all HEPA (type/media) filters can effectively stop dust; while vacuum cleaners with HEPA (type/media) filters, water, or cyclones may filter more effectively than without, they may still exhaust millions of particles per cubic foot of air circulated. Central vacuum cleaners can be effective in removing dust, especially if they are exhausted directly to the outdoors.\nAir filtering appliances differ greatly in their effectiveness. Laser particle counters are an effective way to measure filter effectiveness, medical grade instruments can test for particles as small as 0.3 micrometers. In order to test for dust in the air, there are several options available. Pre weighted filter and matched weight filters made from polyvinyl chloride or mixed cellulose ester are suitable for respirable dust (less than 10 micrometers in diameter).\n\n\n=== Control of dust resistance on surfaces ===\nA dust resistant surface is a state of prevention against dust contamination or damage, by a design or treatment of materials and items in manufacturing or through a repair process. A reduced tacticity of a synthetic layer or covering can protect surfaces and release small molecules that could have remained attached. A panel, container or enclosure with seams may feature types of strengthened rigidity or sealant to vulnerable edges and joins.\n\n\n== Dust in other contexts ==\nDust accelerates snowmelt in the San Juan Mountains\n\n\n=== Dust in outer space ===\nCosmic dust is widely present in space, where gas and dust clouds are primary precursors for planetary systems. The zodiacal light, as seen in a dark night sky, is produced by sunlight reflected from particles of dust in orbit around the Sun. The tails of comets are produced by emissions of dust and ionized gas from the body of the comet. Dust also covers solid planetary bodies, and vast dust storms occur on Mars that cover almost the entire planet. Interstellar dust is found between the stars, and high concentrations produce diffuse nebulae and reflection nebulae.\nDust is widely present in the galaxy. Ambient radiation heats dust and re-emits radiation into the microwave band, which may distort the cosmic microwave background power spectrum. Dust in this regime has a complicated emission spectrum, and includes both thermal dust emission and spinning dust emission.\nDust samples returned from outer space may provide information about conditions in the early solar system. Several spacecraft have sought to gather samples of dust and other materials. Among these craft was Stardust, which flew past Comet Wild 2 in 2004, and returned a capsule of the comet's remains to Earth in January 2006. In 2010 the Japanese Hayabusa spacecraft returned samples of dust from the surface of an asteroid.\n\n\n== Examples of atmospheric dust ==\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\nHolmes, Hannah; (2001)The Secret Life of Dust. Wiley. ISBN 0-471-37743-0\nSteedman, Carolyn; (2002) Dust. Manchester University Press. ISBN 978-0-7190-6015-1\n\n\n== External links ==\nThe Bibliography of Aeolian Research", 
                "titleUrl": "https://en.wikipedia.org/wiki/Dust", 
                "title": "Dust"
            }, 
            {
                "snippet": "Dust to Dust may refer to: \"Earth to earth, ashes to ashes, dust to dust\", a phrase from the funeral service in the Book of Common Prayer   Dust to Dust", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Dust_to_Dust", 
                "title": "Dust to Dust"
            }, 
            {
                "snippet": "\"Space dust\" redirects here. For other uses, see Space dust (disambiguation). For the jazz band, see Cosmic Dust (band).      Cosmic dust is dust which", 
                "pageCategories": "All articles with unsourced statements\nArticles with Wayback Machine links\nArticles with unsourced statements from March 2011\nAstrochemistry\nCosmic dust\nExtragalactic astronomy\nGalactic astronomy\nPlanetary science", 
                "pageContent": "Cosmic dust is dust which exists in outer space. Most cosmic dust particles are between a few molecules to 0.1 \u00b5m in size. A smaller fraction of all dust in space consists of larger refractory minerals that condensed as matter left the stars. It is called \"stardust\" and is included in a separate section below. The dust density falling to Earth is approximately 10\u22126/m3 with each grain having a mass between 10\u221216kg and 10\u22124 kg.\nCosmic dust can be further distinguished by its astronomical location: intergalactic dust, interstellar dust, interplanetary dust (such as in the zodiacal cloud) and circumplanetary dust (such as in a planetary ring). In the Solar System, interplanetary dust causes the zodiacal light. Sources of Solar System dust include comet dust, asteroidal dust, dust from the Kuiper belt, and interstellar dust passing through the Solar System. The terminology has no specific application for describing materials found on the planet Earth except for dust that has demonstrably fallen to Earth. By one estimate, as much as 40,000 tons of cosmic dust reaches the Earth's surface every year. In October 2011, scientists reported that cosmic dust contains complex organic matter (amorphous organic solids with a mixed aromatic\u2013aliphatic structure) that could be created naturally, and rapidly, by stars.\nOn August 14, 2014, scientists announced the collection of possible interstellar dust particles from the Stardust spacecraft since returning to Earth in 2006.\n\n\n== Study and importance ==\n\nCosmic dust was once solely an annoyance to astronomers, as it obscures objects they wish to observe. When infrared astronomy began, those dust particles were observed to be significant and vital components of astrophysical processes. Their analysis can reveal information about phenomena like the formation of the Solar System. For example, cosmic dust can drive the mass loss when a star is nearing the end of its life, play a part in the early stages of star formation, and form planets. In the Solar System, dust plays a major role in the zodiacal light, Saturn's B Ring spokes, the outer diffuse planetary rings at Jupiter, Saturn, Uranus and Neptune, and comets.\n\nThe study of dust is a many-faceted research topic that brings together different scientific fields: physics (solid-state, electromagnetic theory, surface physics, statistical physics, thermal physics), fractal mathematics, chemistry (chemical reactions on grain surfaces), meteoritics, as well as every branch of astronomy and astrophysics. These disparate research areas can be linked by the following theme: the cosmic dust particles evolve cyclically; chemically, physically and dynamically. The evolution of dust traces out paths in which the Universe recycles material, in processes analogous to the daily recycling steps with which many people are familiar: production, storage, processing, collection, consumption, and discarding. Observations and measurements of cosmic dust in different regions provide an important insight into the Universe's recycling processes; in the clouds of the diffuse interstellar medium, in molecular clouds, in the circumstellar dust of young stellar objects, and in planetary systems such as the Solar System, where astronomers consider dust as in its most recycled state. The astronomers accumulate observational \u2018snapshots\u2019 of dust at different stages of its life and, over time, form a more complete movie of the Universe's complicated recycling steps.\nParameters such as the particle's initial motion, material properties, intervening plasma and magnetic field determined the dust particle's arrival at the dust detector. Slightly changing any of these parameters can give significantly different dust dynamical behavior. Therefore, one can learn about where that object came from, and what is (in) the intervening medium.\n\n\n== Detection methods ==\n\nCosmic dust can be detected by indirect methods utilizing the radiative properties of cosmic dust.\nCosmic dust can also be detected directly ('in-situ') using a variety of collection methods and from a variety of collection locations. Estimates of the daily influx of extraterrestrial material entering the Earth's atmosphere range between 5 and 300 tonnes. The Earth-falling dust particles are collected in the Earth's atmosphere using plate collectors under the wings of stratospheric-flying NASA airplanes and collected from surface deposits on the large Earth ice-masses (Antarctica and Greenland/the Arctic) and in deep-sea sediments. Don Brownlee at the University of Washington in Seattle first reliably identified the extraterrestrial nature of collected dust particles in the later 1970s. Another source is the meteorites, which contain stardust extracted from them (see below). Stardust grains are solid refractory pieces of individual presolar stars. They are recognized by their extreme isotopic compositions, which can only be isotopic compositions within evolved stars, prior to any mixing with the interstellar medium. These grains condensed from the stellar matter as it cooled while leaving the star.\n\nIn interplanetary space, dust detectors on planetary spacecraft have been built and flown, some are presently flying, and more are presently being built to fly. The large orbital velocities of dust particles in interplanetary space (typically 10\u201340 km/s) make intact particle capture problematic. Instead, in-situ dust detectors are generally devised to measure parameters associated with the high-velocity impact of dust particles on the instrument, and then derive physical properties of the particles (usually mass and velocity) through laboratory calibration (i.e. impacting accelerated particles with known properties onto a laboratory replica of the dust detector). Over the years dust detectors have measured, among others, the impact light flash, acoustic signal and impact ionisation. Recently the dust instrument on Stardust captured particles intact in low-density aerogel.\nDust detectors in the past flew on the HEOS-2, Helios, Pioneer 10, Pioneer 11, Giotto, and Galileo space missions, on the Earth-orbiting LDEF, EURECA, and Gorid satellites, and some scientists have utilized the Voyager 1 and 2 spacecraft as giant Langmuir probes to directly sample the cosmic dust. Presently dust detectors are flying on the Ulysses, Cassini, Proba, Rosetta, Stardust, and the New Horizons spacecraft. The collected dust at Earth or collected further in space and returned by sample-return space missions is then analyzed by dust scientists in their respective laboratories all over the world. One large storage facility for cosmic dust exists at the NASA Houston JSC.\nInfrared light can penetrate the cosmic dust clouds, allowing us to peer into regions of star formation and the centers of galaxies. NASA's Spitzer Space Telescope is the largest infrared telescope ever launched into space. The Spitzer Space Telescope (formerly SIRTF, the Space Infrared Telescope Facility) was launched into space by a Delta rocket from Cape Canaveral, Florida on 25 August 2003. During its mission, Spitzer will obtain images and spectra by detecting the infrared energy, or heat, radiated by objects in space between wavelengths of 3 and 180 micrometres. Most of this infrared radiation is blocked by the Earth's atmosphere and cannot be observed from the ground. The findings from the Spitzer already revitalized the studies of cosmic dust. A recent report from a Spitzer team shows some evidence that cosmic dust is formed near a supermassive black hole.\nAnother detection mechanism is polarimetry. Dust grains are not spherical and tend to align to interstellar magnetic fields, preferentially polarising starlight that passes through dust clouds. In nearby interstellar space, where cosmic reddening is not sensitive enough to be detected, high precision optical polarimetry has been used to glean the structure of dust within the Local Bubble.\n\n\n== Radiative properties of cosmic dust ==\n\nA dust particle interacts with electromagnetic radiation in a way that depends on its cross section, the wavelength of the electromagnetic radiation, and on the nature of the grain: its refractive index, size, etc. The radiation process for an individual grain is called its emissivity, dependent on the grain's efficiency factor. Furthermore, we have to specify whether the emissivity process is extinction, scattering, absorption, or polarisation. In the radiation emission curves, several important signatures identify the composition of the emitting or absorbing dust particles.\nDust particles can scatter light nonuniformly. Forward-scattered light means that light is redirected slightly by diffraction off its path from the star/sunlight, and back-scattered light is reflected light.\nThe scattering and extinction (\"dimming\") of the radiation gives useful information about the dust grain sizes. For example, if the object(s) in one's data is many times brighter in forward-scattered visible light than in back-scattered visible light, then we know that a significant fraction of the particles are about a micrometer in diameter.\nThe scattering of light from dust grains in long exposure visible photographs is quite noticeable in reflection nebulae, and gives clues about the individual particle's light-scattering properties. In X-ray wavelengths, many scientists are investigating the scattering of X-rays by interstellar dust, and some have suggested that astronomical X-ray sources would possess diffuse haloes, due to the dust.\n\n\n== Stardust ==\n\nStardust grains (also called presolar grains by meteoriticists ) are contained within meteorites, from which they are extracted in terrestrial laboratories. Stardust was a component of the dust in the interstellar medium before its incorporation into meteorites. The meteorites have stored those stardust grains ever since the meteorites first assembled within the planetary accretion disk more than four billion years ago. So-called carbonaceous chondrites are especially fertile reservoirs of stardust. Each stardust grain existed before the Earth was formed. Stardust is a scientific term referring to refractory dust grains that condensed from cooling ejected gases from individual presolar stars and incorporated into the cloud from which the Solar System condensed.\nMany different types of stardust have been identified by laboratory measurements of the highly unusual isotopic composition of the chemical elements that comprise each stardust grain. These refractory mineral grains may earlier have been coated with volatile compounds, but those are lost in the dissolving of meteorite matter in acids, leaving only insoluble refractory minerals. Finding the grain cores without dissolving most of the meteorite has been possible, but difficult and labor-intensive (see presolar grains).\nMany new aspects of nucleosynthesis have been discovered from the isotopic ratios within the stardust grains. An important property of stardust is the hard, refractory, high-temperature nature of the grains. Prominent are silicon carbide, graphite, aluminium oxide, aluminium spinel, and other such grains that would condense at high temperature from a cooling gas, such as in stellar winds or in the decompression of the inside of a supernova. They differ greatly from the solids formed at low temperature within the interstellar medium.\nAlso important are their extreme isotopic compositions, which are expected to exist nowhere in the interstellar medium. This also suggests that the stardust condensed from the gases of individual stars before the isotopes could be diluted by mixing with the interstellar medium. These allow the source stars to be identified. For example, the heavy elements within the silicon carbide (SiC) grains are almost pure S-process isotopes, fitting their condensation within AGB star red giant winds inasmuch as the AGB stars are the main source of S-process nucleosynthesis and have atmospheres observed by astronomers to be highly enriched in dredged-up s process elements.\nAnother dramatic example is given by the so-called supernova condensates, usually shortened by acronym to SUNOCON (from SUperNOva CONdensate) to distinguish them from other stardust condensed within stellar atmospheres. SUNOCONs contain in their calcium an excessively large abundance of 44Ca, demonstrating that they condensed containing abundant radioactive 44Ti, which has a 65-year half-life. The outflowing 44Ti nuclei were thus still \"alive\" (radioactive) when the SUNOCON condensed near one year within the expanding supernova interior, but would have become an extinct radionuclide (specifically 44Ca) after the time required for mixing with the interstellar gas. Its discovery proved the prediction from 1975 that it might be possible to identify SUNOCONs in this way. The SiC SUNOCONs (from supernovae) are only about 1% as numerous as are SiC stardust from AGB stars.\nStardust itself (SUNOCONs and AGB grains that come from specific stars) is but a modest fraction of the condensed cosmic dust, forming less than 0.1% of the mass of total interstellar solids. The high interest in stardust derives from new information that it has brought to the sciences of stellar evolution and nucleosynthesis.\nLaboratories have studied solids that existed before the Earth existed. This was once thought impossible, especially in the 1970s when cosmochemists were confident that the Solar System began as a hot gas  virtually devoid of any remaining solids, which would have been vaporized by high temperature. The existence of stardust proved this historic picture incorrect.\n\n\n== Some bulk properties of cosmic dust ==\n\nCosmic dust is made of dust grains and aggregates of dust grains. These particles are irregularly shaped, with porosity ranging from fluffy to compact. The composition, size, and other properties depends on where the dust is found, and conversely, a compositional analysis of a dust particle can reveal much about the dust particle's origin. General diffuse interstellar medium dust, dust grains in dense clouds, planetary rings dust, and circumstellar dust, are each different in their characteristics. For example, grains in dense clouds have acquired a mantle of ice and on average are larger than dust particles in the diffuse interstellar medium. Interplanetary dust particles (IDPs) are generally larger still.\n\nMost of the influx of extraterrestrial matter that falls onto the Earth is dominated by meteoroids with diameters in the range 50 to 500 micrometers, of average density 2.0 g/cm\u00b3 (with porosity about 40%). The densities of most IDPs captured in the Earth's stratosphere range between 1 and 3 g/cm\u00b3, with an average density at about 2.0 g/cm\u00b3.\nOther specific dust properties:\nIn circumstellar dust, astronomers have found molecular signatures of CO, silicon carbide, amorphous silicate, polycyclic aromatic hydrocarbons, water ice, and polyformaldehyde, among others (in the diffuse interstellar medium, there is evidence for silicate and carbon grains).\nCometary dust is generally different (with overlap) from asteroidal dust. Asteroidal dust resembles carbonaceous chondritic meteorites. Cometary dust resembles interstellar grains which can include silicates, polycyclic aromatic hydrocarbons, and water ice.\n\n\n== Dust grain formation ==\nThe large grains in interstellar space are probably complex, with refractory cores that condensed within stellar outflows topped by layers acquired subsequently during incursions into cold dense interstellar clouds. That cyclic process of growth and destruction outside of the clouds has been modeled  to demonstrate that the cores live much longer than the average lifetime of dust mass. Those cores mostly start with silicate particles condensing in the atmospheres of cool oxygen rich red-giant stars and carbon grains condensing in the atmospheres of cool carbon stars. The red-giant stars have evolved off the main sequence and have entered the giant phase of their evolution and are the major source of refractory dust grain cores in galaxies. Those refractory cores are also called Stardust (section above), which is a scientific term for the small fraction of cosmic dust that condensed thermally within stellar gases as they were ejected from the stars. Several percent of refractory grain cores have condensed within expanding interiors of supernovae, a type of cosmic decompression chamber. And meteoriticists that study this refractory stardust extracted from meteorites often call it presolar grains, although the refractory stardust that they study is actually only a small fraction of all presolar dust. Stardust condenses within the stars via considerably different condensation chemistry than that of the bulk of cosmic dust, which accretes cold onto preexisting dust in dark molecular clouds of the galaxy. Those molecular clouds are very cold, typically less than 50K, so that ices of many kinds may accrete onto grains, perhaps to be destroyed later. Finally, when the Solar System formed, interstellar dust grains were further modified by chemical reactions within the planetary accretion disk. So the history of the complex grains in the early Solar System is complicated and only partially understood.\nAstronomers know that the dust is formed in the envelopes of late-evolved stars from specific observational signatures. In infrared light, emission at 9.7 micrometres is a signature of silicate dust in cool evolved oxygen-rich giant stars. Emission at 11.5 micrometres indicates the presence of silicon carbide dust in cool evolved carbon-rich giant stars. These help provide evidence that the small silicate particles in space came from the ejected outer envelopes of these stars.\nConditions in interstellar space are generally not suitable for the formation of silicate cores. This would take excessive time to accomplish, even if it might be possible. The arguments are that: given an observed typical grain diameter a, the time for a grain to attain a, and given the temperature of interstellar gas, it would take considerably longer than the age of the Universe for interstellar grains to form. On the other hand, grains are seen to have recently formed in the vicinity of nearby stars, in nova and supernova ejecta, and in R Coronae Borealis variable stars which seem to eject discrete clouds containing both gas and dust. So mass loss from stars is unquestionably where the refractory cores of grains formed.\nMost dust in the Solar System is highly processed dust, recycled from the material out of which the Solar System formed and subsequently collected in the planetesimals, and leftover solid material such as comets and asteroids, and reformed in each of those bodies' collisional lifetimes. During the Solar System's formation history, the most abundant element was (and still is) H2. The metallic elements: magnesium, silicon, and iron, which are the principal ingredients of rocky planets, condensed into solids at the highest temperatures of the planetary disk. Some molecules such as CO, N2, NH3, and free oxygen, existed in a gas phase. Some molecules, for example, graphite (C) and SiC would condense into solid grains in the planetary disk; but carbon and SiC grains found in meteorites are presolar based on their isotopic compositions, rather than from the planetary disk formation. Some molecules also formed complex organic compounds and some molecules formed frozen ice mantles, of which either could coat the \"refractory\" (Mg, Si, Fe) grain cores. Stardust once more provides an exception to the general trend, as it appears to be totally unprocessed since its thermal condensation within stars as refractory crystalline minerals. The condensation of graphite occurs within supernova interiors as they expand and cool, and do so even in gas containing more oxygen than carbon, a surprising carbon chemistry made possible by the intense radioactive environment of supernovae. This special example of dust formation has merited specific review.\nPlanetary disk formation of precursor molecules was determined, in large part, by the temperature of the solar nebula. Since the temperature of the solar nebula decreased with heliocentric distance, scientists can infer a dust grain's origin(s) with knowledge of the grain's materials. Some materials could only have been formed at high temperatures, while other grain materials could only have been formed at much lower temperatures. The materials in a single interplanetary dust particle often show that the grain elements formed in different locations and at different times in the solar nebula. Most of the matter present in the original solar nebula has since disappeared; drawn into the Sun, expelled into interstellar space, or reprocessed, for example, as part of the planets, asteroids or comets.\nDue to their highly processed nature, IDPs (interplanetary dust particles) are fine-grained mixtures of thousands to millions of mineral grains and amorphous components. We can picture an IDP as a \"matrix\" of material with embedded elements which were formed at different times and places in the solar nebula and before the solar nebula's formation. Examples of embedded elements in cosmic dust are GEMS, chondrules, and CAIs.\n\n\n== From the solar nebula to Earth ==\n\nThe arrows in the adjacent diagram show one possible path from a collected interplanetary dust particle back to the early stages of the solar nebula.\nWe can follow the trail to the right in the diagram to the IDPs that contain the most volatile and primitive elements. The trail takes us first from interplanetary dust particles to chondritic interplanetary dust particles. Planetary scientists classify chondritic IDPs in terms of their diminishing degree of oxidation so that they fall into three major groups: the carbonaneous, the ordinary, and the enstatite chondrites. As the name implies, the carbonaceous chondrites are rich in carbon, and many have anomalies in the isotopic abundances of H, C, N, and O (Jessberger, 2000). From the carbonaceous chondrites, we follow the trail to the most primitive materials. They are almost completely oxidized and contain the lowest condensation temperature elements (\"volatile\" elements) and the largest amount of organic compounds. Therefore, dust particles with these elements are thought to be formed in the early life of the Solar System. The volatile elements have never seen temperatures above about 500 K, therefore, the IDP grain \"matrix\" consists of some very primitive Solar System material. Such a scenario is true in the case of comet dust. The provenance of the small fraction that is stardust (see above) is quite different; these refractory interstellar minerals thermally condense within stars, become a small component of interstellar matter, and therefore remain in the presolar planetary disk. Nuclear damage tracks are caused by the ion flux from solar flares. Solar wind ions impacting on the particle's surface produce amorphous radiation damaged rims on the particle's surface. And spallogenic nuclei are produced by galactic and solar cosmic rays. A dust particle that originates in the Kuiper Belt at 40 AU would have many more times the density of tracks, thicker amorphous rims and higher integrated doses than a dust particle originating in the main-asteroid belt.\nBased on 2012 computer model studies, the complex organic molecules necessary for life may have formed in the protoplanetary disk of dust grains surrounding the Sun before the formation of the Earth. According to the computer studies, this same process may also occur around other stars that acquire planets. (Also see Extraterrestrial organic molecules.)\nIn September 2012, NASA scientists reported that polycyclic aromatic hydrocarbons (PAHs), subjected to interstellar medium (ISM) conditions, are transformed, through hydrogenation, oxygenation and hydroxylation, to more complex organics - \"a step along the path toward amino acids and nucleotides, the raw materials of proteins and DNA, respectively\". Further, as a result of these transformations, the PAHs lose their spectroscopic signature which could be one of the reasons \"for the lack of PAH detection in interstellar ice grains, particularly the outer regions of cold, dense clouds or the upper molecular layers of protoplanetary disks.\"\nIn February 2014, NASA announced a greatly upgraded database for detecting and monitoring polycyclic aromatic hydrocarbons (PAHs) in the universe. According to NASA scientists, over 20% of the carbon in the Universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed shortly after the Big Bang, are abundant in the Universe, and are associated with new stars and exoplanets.\nIn March 2015, NASA scientists reported that, for the first time, complex DNA and RNA organic compounds of life, including uracil, cytosine and thymine, have been formed in the laboratory under outer space conditions, using starting chemicals, such as pyrimidine, found in meteorites. Pyrimidine, like polycyclic aromatic hydrocarbons (PAHs), the most carbon-rich chemical found in the Universe, may have been formed in red giants or in interstellar dust and gas clouds, according to the scientists.\n\n\n== Some \"dusty\" clouds in the universe ==\nThe Solar System has its own interplanetary dust cloud, as do extrasolar systems.\nThere are different types of nebulae with different physical causes and processes. One might see these classifications:\ndiffuse nebula\ninfrared (IR) reflection nebula\nsupernova remnant\nmolecular cloud\nHII regions\nphotodissociation regions\nDark Nebula\nDistinctions between those types of nebula are that different radiation processes are at work. For example, H II regions, like the Orion Nebula, where a lot of star-formation is taking place, are characterized as thermal emission nebulae. Supernova remnants, on the other hand, like the Crab Nebula, are characterized as nonthermal emission (synchrotron radiation).\nSome of the better known dusty regions in the Universe are the diffuse nebulae in the Messier catalog, for example: M1, M8, M16, M17, M20, M42, M43.\nSome larger dust catalogs are:\nSharpless (1959) A Catalogue of HII Regions\nLynds (1965) Catalogue of Bright Nebulae\nLunds (1962) Catalogue of Dark Nebulae\nvan den Bergh (1966) Catalogue of Reflection Nebulae\nGreen (1988) Rev. Reference Cat. of Galactic SNRs\nThe National Space Sciences Data Center (NSSDC)\nCDS Online Catalogs\n\n\n== Interstellar dust sample return ==\nIn the spring of 2014, the recovery of particles of interstellar dust from the Discovery program's Stardust mission was announced.\n\n\n== Images ==\n\n\n== See also ==\n\n\n== Further reading ==\nEvans, Aneurin (1994). The Dusty Universe. Ellis Horwood. \n\n\n== References ==\n\n\n== External links ==\nCosmic Dust Group\nEvidence for interstellar origin of seven dust particles collected by the Stardust spacecraft", 
                "titleUrl": "https://en.wikipedia.org/wiki/Cosmic_dust", 
                "title": "Cosmic dust"
            }, 
            {
                "snippet": "Angel dust may refer to: A common name for the drug phencyclidine (PCP) Angel dusting, a misleading marketing practice Angel Dust (wrestler), American", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Angel_dust", 
                "title": "Angel dust"
            }, 
            {
                "snippet": "character, see Dust Storm (Transformers). \"Black blizzard\" redirects here. For the Yoshihiro Tatsumi manga, see Black Blizzard (manga). A dust storm is a", 
                "pageCategories": "All articles lacking reliable references\nAll articles with dead external links\nAll articles with unsourced statements\nAll pages needing factual verification\nArticles lacking reliable references from May 2016\nArticles with dead external links from May 2016\nArticles with unsourced statements from May 2016\nDust storms\nRoad hazards\nWeather hazards", 
                "pageContent": "A dust storm is a meteorological phenomenon common in arid and semi-arid regions. Dust storms arise when a gust front or other strong wind blows loose sand and dirt from a dry surface. Particles are transported by saltation and suspension, a process that moves soil from one place and deposits it in another.\nDrylands around North Africa and the Arabian peninsula are the main terrestrial sources of airborne dust. Also with some contributions from Iran, Pakistan and India into the Arabian Sea, and China's significant storms deposit dust in the Pacific. It has been argued that recently, poor management of the Earth's drylands, such as neglecting the fallow system, are increasing dust storms size and frequency from desert margins and changing both the local and global climate, and also impacting local economies.\nThe term sandstorm is used most often in the context of desert sandstorms, especially in the Sahara Desert, or places where sand is a more prevalent soil type than dirt or rock, when, in addition to fine particles obscuring visibility, a considerable amount of larger sand particles are blown closer to the surface. The term dust storm is more likely to be used when finer particles are blown long distances, especially when the dust storm affects urban areas.\n\n\n== Causes ==\nAs the force of wind passing over loosely held particles increases, particles of sand first start to vibrate, then to saltate (\"leaps\"). As they repeatedly strike the ground, they loosen and break off smaller particles of dust which then begin to travel in suspension. At wind speeds above that which causes the smallest to suspend, there will be a population of dust grains moving by a range of mechanisms: suspension, saltation and creep.\n\nA study from 2008 finds that the initial saltation of sand particles induces a static electric field by friction. Saltating sand acquires a negative charge relative to the ground which in turn loosens more sand particles which then begin saltating. This process has been found to double the number of particles predicted by previous theories.\nParticles become loosely held mainly due to drought or arid conditions, and varied wind causes. Gust fronts may be produced by the outflow of rain-cooled air from an intense thunderstorm. Or, the wind gusts may be produced by a dry cold front, that is, a cold front that is moving into a dry air mass and is producing no precipitation\u2014the type of dust storm which was common during the Dust Bowl years in the U.S. Following the passage of a dry cold front, convective instability resulting from cooler air riding over heated ground can maintain the dust storm initiated at the front.\nIn desert areas, dust and sand storms are most commonly caused by either thunderstorm outflows, or by strong pressure gradients which cause an increase in wind velocity over a wide area. The vertical extent of the dust or sand that is raised is largely determined by the stability of the atmosphere above the ground as well as by the weight of the particulates. In some cases, dust and sand may be confined to a relatively shallow layer by a low-lying temperature inversion. In other instances, dust (but not sand) may be lifted as high as 20,000 feet (6,100 m) high.\nDrought and wind contribute to the emergence of dust storms, as do poor farming and grazing practices by exposing the dust and sand to the wind.\nOne poor farming practice which contributes to dust storms is dryland farming. Particularly poor dryland farming techniques are intensive tillage or not having established crops or cover crops when storms strike at particularly vulnerable times prior to revegetation. In a semi-arid climate, these practices increase susceptibility to dust storms. However, soil conservation practices may be implemented to control wind erosion.\n\n\n== Physical and environmental effects ==\n\nA sandstorm can transport and carry large volumes of sand unexpectedly. Dust storms can carry large amounts of dust, with the leading edge being composed of a wall of thick dust as much as 1.6 km (0.99 mi) high. Dust and sand storms which come off the Sahara Desert are locally known as a simoom or simoon (s\u00eem\u016bm, s\u00eem\u016bn). The haboob (h\u0259b\u016bb) is a sandstorm prevalent in the region of Sudan around Khartoum, with occurrences being most common in the summer.\nThe Sahara desert is a key source of dust storms, particularly the Bod\u00e9l\u00e9 Depression and an area covering the confluence of Mauritania, Mali, and Algeria.\nSaharan dust storms have increased approximately 10-fold during the half-century since the 1950s, causing topsoil loss in Niger, Chad, northern Nigeria, and Burkina Faso. In Mauritania there were just two dust storms a year in the early 1960s, but there are about 80 a year today, according to Andrew Goudie, a professor of geography at Oxford University. Levels of Saharan dust coming off the east coast of Africa in June (2007) were five times those observed in June 2006, and were the highest observed since at least 1999, which may have cooled Atlantic waters enough to slightly reduce hurricane activity in late 2007.\nDust storms have also been shown to increase the spread of disease across the globe. Virus spores in the ground are blown into the atmosphere by the storms with the minute particles then acting like urban smog or acid rain.\n\nProlonged and unprotected exposure of the respiratory system in a dust storm can also cause silicosis which, if left untreated, will lead to asphyxiation; silicosis is an incurable condition that may also lead to lung cancer. There is also the danger of keratoconjunctivitis sicca (\"dry eyes\") which, in severe cases without immediate and proper treatment, can lead to blindness.\n\n\n=== Economic impact ===\nDust storms cause soil loss from the dry lands, and worse, they preferentially remove organic matter and the nutrient-rich lightest particles, thereby reducing agricultural productivity. Also the abrasive effect of the storm damages young crop plants. Dust storms also reduced visibility affecting aircraft and road transportation. In addition dust storms also create problems due to complications of breathing in dust.\nDust can also have beneficial effects where it deposits: Central and South American rain forests get most of their mineral nutrients from the Sahara; iron-poor ocean regions get iron; and dust in Hawaii increases plantain growth. In northern China as well as the mid-western U.S., ancient dust storm deposits known as loess are highly fertile soils, but they are also a significant source of contemporary dust storms when soil-securing vegetation is disturbed.\n\n\n== Extraterrestrial dust storms ==\n\nDust storms are not limited to Earth and have been known to form on other planets such as Mars. These dust storms can extend over larger areas than those on Earth, sometimes encircling the planet, with wind speeds as high as 60 miles per hour (97 km/h). However, given Mars' much lower atmospheric pressure (roughly 1% that of Earth's), the intensity of Mars' storms could never reach the kind of hurricane-force winds experienced on Earth. Martian dust storms are formed when solar heating warms the Martian atmosphere and causes the air to move, lifting dust off the ground. The chance for storms is increased when there are great temperature variations like those seen at the equator during the Martian summer.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\n12-hour U.S. map of surface dust concentrations Mouse-over an hour block on the row for 'Surface Dust Concentrations'\nSlide Show of a Dust Storm in Lubbock, Texas on December 15, 2003\nDust in the Wind\nPhotos of the April 14 1935 and September 2 1934 dust storms in the Texas Panhandle hosted by the Portal to Texas History.\nThe Bibliography of Aeolian Research\nUniversity of Arizona Dust Model Page\nPhotos of a sandstorm in Riyadh in 2009 from the BBC Newsbeat website\nDust storm in Phoenix Arizona via YouTube", 
                "titleUrl": "https://en.wikipedia.org/wiki/Dust_storm", 
                "title": "Dust storm"
            }, 
            {
                "snippet": "Red Dust may refer to: Red Dust (1932 film), an American romantic drama directed by Victor Fleming and starring Clark Gable and Jean Harlow Red Dust (1990", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Red_Dust", 
                "title": "Red Dust"
            }, 
            {
                "snippet": "Band song) Dust (Screaming Trees album), 1996 Dust (DJ Muggs album), 2003 Dust (Dust album), 1971 Dust (Peatbog Faeries album), 2011 Dust (Peter Murphy", 
                "pageCategories": "All article disambiguation pages\nAll disambiguation pages\nDisambiguation pages", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Dust_(disambiguation)", 
                "title": "Dust (disambiguation)"
            }, 
            {
                "snippet": "Dust of Dreams (\u5922\u306e\u7c89\u672b, Yume no Funmatsu?) is an album by the Japanese noise musician Merzbow.  All music composed by Masami Akita.  Masami Akita    \"Merzbow", 
                "pageCategories": "2005 albums\nArticles containing Japanese-language text\nArticles with hAudio microformats\nMerzbow albums", 
                "pageContent": "Dust of Dreams (\u5922\u306e\u7c89\u672b, Yume no Funmatsu) is an album by the Japanese noise musician Merzbow.\n\n\n== Track listing ==\nAll music composed by Masami Akita.\n\n\n== Personnel ==\nMasami Akita\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Dust_of_Dreams_(album)", 
                "title": "Dust of Dreams (album)"
            }, 
            {
                "snippet": "Intergalactic dust is cosmic dust in between galaxies in intergalactic space. Evidence for intergalactic dust has been suggested as early as 1949, and", 
                "pageCategories": "Cosmic dust\nExtragalactic astronomy\nIntergalactic media", 
                "pageContent": "Intergalactic dust is cosmic dust in between galaxies in intergalactic space. Evidence for intergalactic dust has been suggested as early as 1949, and study of it grew throughout the late 20th century. There are large variations in the distribution of intergalactic dust. The dust may affect intergalactic distance measurements, such as to supernova and quasars in other galaxies.\nIntergalactic dust can be a part of intergalactic dust clouds, shown to exist around some other galaxies since the 1960s. By the 1980s, at least four intergalactic dust clouds were discovered within several megaparsec (Mpc) of the Milky Way galaxy, and an example of this is the Okroy cloud.\nIn February 2014, NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. According to scientists, more than 20% of the carbon in the universe may be associated with PAHs, possible starting materials for the formation of life. PAHs seem to have been formed as early as two billion years after the big bang, are widespread throughout the universe, and are associated with new stars and exoplanets.\n\n\n== See also ==\nAstrochemistry\nAtomic and molecular astrophysics\nCosmochemistry\nExtragalactic astronomy\nExtraterrestrial materials\nHypervelocity star\nIntergalactic space\nIntergalactic medium\nWarm\u2013hot intergalactic medium\nIntergalactic star\nInterstellar medium\nList of interstellar and circumstellar molecules\n\n\n== References ==\n\n\n== External links ==\nNasa database", 
                "titleUrl": "https://en.wikipedia.org/wiki/Intergalactic_dust", 
                "title": "Intergalactic dust"
            }, 
            {
                "snippet": "administrative subdivision, see Dust Mohammad Rural District. For the nearby villages, see Dust Mohammad-e Lashkaran and Dust Mohammad-e Shah Gol Pahlavan", 
                "pageCategories": "All stub articles\nArticles containing Persian-language text\nCities in Iran\nCities in Sistan and Baluchestan Province\nCoordinates on Wikidata\nHirmand County geography stubs\nPopulated places in Hirmand County", 
                "pageContent": "Dust Mohammad (Persian: \u062f\u0648\u0633\u062a \u0645\u062d\u0645\u062f\u200e\u200e, also Romanized as D\u016bst Mo\u1e29ammad; also known as D\u016bst Mo\u1e29ammad Kh\u0101n) is a city in and the capital of Hirmand County, Sistan and Baluchestan Province, Iran. At the 2006 census, its population was 6,902, in 1,328 families.\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Dust_Mohammad", 
                "title": "Dust Mohammad"
            }
        ], 
        "phraseCharStart": "1232"
    }, 
    {
        "phraseCharEnd": "1427", 
        "phraseIndex": "T30", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "ITER", 
        "wikiSearchResults": [
            {
                "snippet": "43\u00b042\u203217.84\u2033N 5\u00b046\u20329.1\u2033E\ufeff / \ufeff43.7049556\u00b0N 5.769194\u00b0E\ufeff / 43.7049556; 5.769194 ITER (International Thermonuclear Experimental Reactor, and is also Latin for", 
                "pageCategories": "All articles with dead external links\nAll articles with unsourced statements\nArticles with French-language external links\nArticles with dead external links from August 2012\nArticles with unsourced statements from April 2008\nArticles with unsourced statements from December 2015\nArticles with unsourced statements from February 2007\nArticles with unsourced statements from March 2016\nBuildings and structures in Bouches-du-Rh\u00f4ne\nCommons category with local link same as on Wikidata", 
                "pageContent": "ITER (International Thermonuclear Experimental Reactor, and is also Latin for \"the way\") is an international nuclear fusion research and engineering megaproject, which will be the world's largest magnetic confinement plasma physics experiment. It is an experimental tokamak nuclear fusion reactor that is being built next to the Cadarache facility in Saint-Paul-l\u00e8s-Durance, south of France.\nThe ITER project aims to make the long-awaited transition from experimental studies of plasma physics to full-scale electricity-producing fusion power stations. The ITER fusion reactor has been designed to produce 500 megawatts of output power for several seconds while needing 50 megawatts to operate. Thereby the machine aims to demonstrate the principle of producing more energy from the fusion process than is used to initiate it, something that has not yet been achieved in any fusion reactor.\nThe project is funded and run by seven member entities\u2014the European Union, India, Japan, China, Russia, South Korea, and the United States. The EU, as host party for the ITER complex, is contributing about 45 percent of the cost, with the other six parties contributing approximately 9 percent each.\nConstruction of the ITER Tokamak complex started in 2013 and the building costs are now over US$14 billion as of June 2015. The facility is expected to finish its construction phase in 2019 and will start commissioning the reactor that same year and initiate plasma experiments in 2020 with full deuterium\u2013tritium fusion experiments starting in 2027. If ITER becomes operational, it will become the largest magnetic confinement plasma physics experiment in use, surpassing the Joint European Torus. The first commercial demonstration fusion power station, named DEMO, is proposed to follow on from the ITER project.\n\n\n== Background ==\nFusion power has the potential to provide sufficient energy to satisfy mounting demand, and to do so sustainably, with a relatively small impact on the environment.\nNuclear fusion has many potential attractions. Firstly, its hydrogen isotope fuels are relatively abundant \u2013 one of the necessary isotopes, deuterium, can be extracted from seawater, while the other fuel, tritium, would be bred from a lithium blanket using neutrons produced in the fusion reaction itself. Furthermore, a fusion reactor would produce virtually no CO2 or atmospheric pollutants, and its other radioactive waste products would be very short-lived compared to those produced by conventional nuclear reactors.\nOn 21 November 2006, the seven participants formally agreed to fund the creation of a nuclear fusion reactor. The program is anticipated to last for 30 years \u2013 10 for construction, and 20 of operation. ITER was originally expected to cost approximately \u20ac5billion, but the rising price of raw materials and changes to the initial design have seen that amount almost triple to \u20ac13billion. The reactor is expected to take 10 years to build with completion scheduled for 2019. Site preparation has begun in Cadarache, France, and procurement of large components has started.\nITER is designed to produce approximately 500 MW of fusion power sustained for up to 1,000 seconds (compared to JET's peak of 16 MW for less than a second) by the fusion of about 0.5 g of deuterium/tritium mixture in its approximately 840 m3 reactor chamber. Although ITER is expected to produce (in the form of heat) 10 times more energy than the amount consumed to heat up the plasma to fusion temperatures, the generated heat will not be used to generate any electricity.\nITER was originally an acronym for International Thermonuclear Experimental Reactor, but that title was eventually dropped due to the negative popular connotations of the word \"thermonuclear\", especially when used in conjunction with \"experimental\". \"Iter\" also means \"journey\", \"direction\" or \"way\" in Latin, reflecting ITER's potential role in harnessing nuclear fusion as a peaceful power source.\n\n\n== Organization history ==\n\nITER began in 1985 as a Reagan\u2013Gorbachev initiative with the equal participation of the Soviet Union, European Union (through European Atomic Energy Community), the United States, and Japan through the 1988\u20131998 initial design phases. Preparations for the first Gorbachev-Reagan Summit showed that there were no tangible agreements in the works for the summit.\nOne energy research project, however, was being considered quietly by two physicists, Alvin Trivelpiece and Evgeny Velikhov. The project involved collaboration on the next phase of magnetic fusion research \u2014 the construction of a demonstration model. At the time, magnetic fusion research was ongoing in Japan, Europe, the Soviet Union and the US. Velikhov and Trivelpiece believed that taking the next step in fusion research would be beyond the budget of any of the key nations and that collaboration would be useful internationally.\nA major bureaucratic fight erupted in the US government over the project. One argument against collaboration was that the Soviets would use it to steal US technology and know-how. A second was symbolic \u2014 the Soviet physicist Andrei Sakharov was in internal exile and the US was pushing the Soviet Union on its human rights record. The United States National Security Council convened a meeting under the direction of William Flynn Martin that resulted in a consensus that the US should go forward with the project.\nMartin and Velikhov concluded the agreement that was agreed at the summit and announced in the last paragraph of this historic summit meeting, \"... The two leaders emphasized the potential importance of the work aimed at utilizing controlled thermonuclear fusion for peaceful purposes and, in this connection, advocated the widest practicable development of international cooperation in obtaining this source of energy, which is essentially inexhaustible, for the benefit for all mankind.\"\nConceptual and engineering design phases carried out under the auspices of the IAEA led to an acceptable, detailed design in 2001, underpinned by US$650 million worth of research and development by the \"ITER Parties\" to establish its practical feasibility. These parties, namely EU, Japan, Russian Federation (replacing the Soviet Union), and United States (which opted out of the project in 1999 and returned in 2003), were joined in negotiations by China, South Korea, and Canada (who then terminated its participation at the end of 2003). India officially became part of ITER on December 2005.\nOn 28 June 2005, it was officially announced that ITER would be built in the European Union in Southern France. The negotiations that led to the decision ended in a compromise between the EU and Japan, in that Japan was promised 20% of the research staff on the French location of ITER, as well as the head of the administrative body of ITER. In addition, another research facility for the project will be built in Japan, and the European Union has agreed to contribute about 50% of the costs of this institution.\nOn 21 November 2006, an international consortium signed a formal agreement to build the reactor. On 24 September 2007, the People's Republic of China became the seventh party to deposit the ITER Agreement to the IAEA. Finally, on 24 October 2007, the ITER Agreement entered into force and the ITER Organization legally came into existence.\n\n\n== Objectives ==\nITER's mission is to demonstrate the feasibility of fusion power, and prove that it can work without negative impact. Specifically, the project aims:\nTo momentarily produce ten times more thermal energy from fusion heating than is supplied by auxiliary heating (a Q value equals 10).\nTo produce a steady-state plasma with a Q value greater than 5. (Q = 1 is breakeven.)\nTo maintain a fusion pulse for up to 8 minutes.\nTo ignite a \"burning\" (self-sustaining) plasma. (i.e. 'ignition' see Lawson criterion)\nTo develop technologies and processes needed for a fusion power station \u2014 including superconducting magnets and remote handling (maintenance by robot).\nTo verify tritium breeding concepts.\nTo refine neutron shield/heat conversion technology (most of the energy in the D+T fusion reaction is released in the form of fast neutrons).\n\n\n== Timeline and current status ==\nIn 1978, the EC, Japan, USA, and USSR joined in the International Tokamak Reactor (INTOR) Workshop, under the auspices of the International Atomic Energy Agency (IAEA), to assess the readiness of magnetic fusion to move forward to the experimental power reactor (EPR) stage, to identify the additional R&D that must be undertaken, and to define the characteristics of such an EPR by means of a conceptual design.\nHundreds of fusion scientists and engineers in each participating country took part in a detailed assessment of the then present status of the tokamak confinement concept vis-a-vis the requirements of an EPR, identified the required R&D by early 1980, and produced a conceptual design by mid-1981.\nTimeline:\n1985. At the Geneva summit meeting in 1985, Mikhail Gorbachev suggested to Ronald Reagan that the two countries jointly undertake the construction of a tokamak EPR as proposed by the INTOR Workshop. The ITER project was initiated in 1988.\n1988. Conceptual design activities ran from 1988 to 1990.\n1992. Engineering design activities started.\n1998. In June, the 'Final design' from the Engineering Design Activities was approved.\n2001. In July, the \"cost-cutting\" 'ITER-FEAT' design was agreed.\n2006. The ITER project was formally agreed to and funded with a cost estimate of \u20ac10 billion ($12.8 billion) projecting the start of construction in 2008 and completion a decade later.\n2007. In September, fourteen major design changes were agreed to the 2001 design.\n2013. The project had run into many delays and budget overruns. The facility is not expected to begin operations at the schedule initially anticipated.\n2014. In February, The New Yorker published the ITER Management Assessment report, listing 11 essential recommendations, for example: \"Create a Project Culture\", \"Instill a Nuclear Safety Culture\", \"Develop a realistic ITER Project Schedule\" and \"Simplify and Reduce the IO Bureaucracy\". The USA considered withdrawal, but is still participating in ITER.\n2015. In November a project review concludes that the schedule may need extending by at least six years; (e.g. first plasma in 2026).\n2016. Atomic Energy Organization of Iran completed the preliminary work for Iran to join ITER\n\n\n== Reactor overview ==\n\nWhen deuterium and tritium fuse, two nuclei come together to form a helium nucleus (an alpha particle), and a high-energy neutron.\n2\n1D + 3\n1T \u2192 4\n2He + 1\n0n + 6988281983061712000\u266017.6 MeV\nWhile nearly all stable isotopes lighter on the periodic table than iron-56 and nickel-62, which have the highest binding energy per nucleon, will fuse with some other isotope and release energy, deuterium and tritium are by far the most attractive for energy generation as they require the lowest activation energy (thus lowest temperature) to do so, while producing among the most energy per unit weight.\nAll proto- and mid-life stars radiate enormous amounts of energy generated by fusion processes. Mass for mass, the deuterium\u2013tritium fusion process releases roughly three times as much energy as uranium-235 fission, and millions of times more energy than a chemical reaction such as the burning of coal. It is the goal of a fusion power station to harness this energy to produce electricity.\nActivation energies for fusion reactions are generally high because the protons in each nucleus will tend to strongly repel one another, as they each have the same positive charge. A heuristic for estimating reaction rates is that nuclei must be able to get within 100 femtometer (1 \u00d7 10\u221213 meter) of each other, where the nuclei are increasingly likely to undergo quantum tunneling past the electrostatic barrier and the turning point where the strong nuclear force and the electrostatic force are equally balanced, allowing them to fuse. In ITER, this distance of approach is made possible by high temperatures and magnetic confinement. High temperatures give the nuclei enough energy to overcome their electrostatic repulsion (see Maxwell\u2013Boltzmann distribution). For deuterium and tritium, the optimal reaction rates occur at temperatures on the order of 100,000,000 K. The plasma is heated to a high temperature by ohmic heating (running a current through the plasma). Additional heating is applied using neutral beam injection (which cross magnetic field lines without a net deflection and will not cause a large electromagnetic disruption) and radio frequency (RF) or microwave heating.\nAt such high temperatures, particles have a large kinetic energy, and hence velocity. If unconfined, the particles will rapidly escape, taking the energy with them, cooling the plasma to the point where net energy is no longer produced. A successful reactor would need to contain the particles in a small enough volume for a long enough time for much of the plasma to fuse. In ITER and many other magnetic confinement reactors, the plasma, a gas of charged particles, is confined using magnetic fields. A charged particle moving through a magnetic field experiences a force perpendicular to the direction of travel, resulting in centripetal acceleration, thereby confining it to move in a circle or helix around the lines of magnetic flux.\nA solid confinement vessel is also needed, both to shield the magnets and other equipment from high temperatures and energetic photons and particles, and to maintain a near-vacuum for the plasma to populate. The containment vessel is subjected to a barrage of very energetic particles, where electrons, ions, photons, alpha particles, and neutrons constantly bombard it and degrade the structure. The material must be designed to endure this environment so that a power station would be economical. Tests of such materials will be carried out both at ITER and at IFMIF (International Fusion Materials Irradiation Facility).\nOnce fusion has begun, high energy neutrons will radiate from the reactive regions of the plasma, crossing magnetic field lines easily due to charge neutrality (see neutron flux). Since it is the neutrons that receive the majority of the energy, they will be ITER's primary source of energy output. Ideally, alpha particles will expend their energy in the plasma, further heating it.\nBeyond the inner wall of the containment vessel one of several test blanket modules will be placed. These are designed to slow and absorb neutrons in a reliable and efficient manner, limiting damage to the rest of the structure, and breeding tritium for fuel from lithium and the incoming neutrons. Energy absorbed from the fast neutrons is extracted and passed into the primary coolant. This heat energy would then be used to power an electricity-generating turbine in a real power station; in ITER this generating system is not of scientific interest, so instead the heat will be extracted and disposed of.\n\n\n== Technical design ==\n\n\n=== Vacuum vessel ===\nThe vacuum vessel is the central part of the ITER machine: a double walled steel container in which the plasma is contained by means of magnetic fields.\nThe ITER vacuum vessel will be twice as large and 16 times as heavy as any previously manufactured fusion vessel: each of the nine torus shaped sectors will weigh between 390 and 430 tonnes. When all the shielding and port structures are included, this adds up to a total of 5,116 tonnes. Its external diameter will measure 19.4 metres (64 ft), the internal 6.5 metres (21 ft). Once assembled, the whole structure will be 11.3 metres (37 ft) high.\nThe primary function of the vacuum vessel is to provide a hermetically sealed plasma container. Its main components are the main vessel, the port structures and the supporting system. The main vessel is a double walled structure with poloidal and toroidal stiffening ribs between 60-millimetre-thick (2.4 in) shells to reinforce the vessel structure. These ribs also form the flow passages for the cooling water. The space between the double walls will be filled with shield structures made of stainless steel. The inner surfaces of the vessel will act as the interface with breeder modules containing the breeder blanket component. These modules will provide shielding from the high-energy neutrons produced by the fusion reactions and some will also be used for tritium breeding concepts.\nThe vacuum vessel has 18 upper, 17 equatorial and 9 lower ports that will be used for remote handling operations, diagnostic systems, neutral beam injections and vacuum pumping.\n\n\n=== Breeder blanket ===\nOwing to very limited terrestrial resources of tritium, a key component of the ITER reactor design is the breeder blanket. This component, located adjacent to the vacuum vessel, serves to produce tritium through reaction of 6Li isotopes with high energy neutrons from the plasma. Concepts for the breeder blanket include helium cooled lithium lead (HCLL) and helium cooled pebble bed (HCPB) methods. Test blanket modules based on both concepts will be tested in ITER and will share a common box geometry. Materials for use as breeder pebbles in the HCPB concept include lithium metatitanate and lithium orthosilicate. Requirements of breeder materials include good tritium production and extraction, mechanical stability and low activation levels.\n\n\n=== Magnet system ===\nThe central solenoid coil will use superconducting niobium-tin to carry 46 kA and produce a field of up to 13.5 teslas. The 18 toroidal field coils will also use niobium-tin. At their maximum field strength of 11.8 teslas, they will be able to store 41 gigajoules. They have been tested at a record 80 kA. Other lower field ITER magnets (PF and CC) will use niobium-titanium for their superconducting elements.\n\n\n=== Additional heating ===\nThere will be three types of external heating in ITER:\nTwo Heating Neutral Beam injectors (HNB), each providing about 17MW to the burning plasma, with the possibility to add a third one. The requirements in terms of deuterium beam energy (1MeV), total current (40A) and beam pulse duration (up to 1h). The prototype is being built at the a Neutral Beam Test Facility (NBTF) prototype is being constructed in Padova\nIon Cyclotron Resonance Heating (ICRH)\nElectron Cyclotron Resonance Heating (ECRH)\n\n\n=== Cryostat ===\nThe cryostat is a large 3,800-tonne stainless steel structure surrounding the vacuum vessel and the superconducting magnets, in order to provide a super-cool vacuum environment. Its thickness ranging from 50 to 250 mm will allow it to withstand the atmospheric pressure on the area of a volume of 8,500 cubic meters. The total of 54 modules of the cryostat will be engineered, procured, manufactured, and installed by Larsen & Toubro Heavy Engineering.\n\n\n=== Cooling systems ===\nThe ITER tokamak will use three interconnected cooling systems. Most of the heat will be removed by a primary water cooling loop, itself cooled by water through a heat exchanger within the tokamak building's secondary confinement. The secondary cooling loop will be cooled by a larger complex, comprising a cooling tower, a 5 km pipeline supplying water from Canal de Provence, and basins that allow cooling water to be cooled and tested for chemical contamination and tritium before being released into the Durance River. This system will need to dissipate an average power of 450 MW during the tokamak's operation. A liquid nitrogen system will provide a further 1,300 kW of cooling to 80 kelvins, and a liquid helium system will provide 75 kW of cooling to 4.5 K. The liquid helium system will be designed, manufactured, installed and commissioned by Air Liquide.\n\n\n== Location ==\n\nThe process of selecting a location for ITER was long and drawn out. The most likely sites were Cadarache in Provence-Alpes-C\u00f4te-d'Azur, France, and Rokkasho, Aomori, Japan. Additionally, Canada announced a bid for the site in Clarington in May 2001, but withdrew from the race in 2003. Spain also offered a site at Vandell\u00f2s on 17 April 2002, but the EU decided to concentrate its support solely behind the French site in late November 2003. From this point on, the choice was between France and Japan. On 3 May 2005, the EU and Japan agreed to a process which would settle their dispute by July.\nAt the final meeting in Moscow on 28 June 2005, the participating parties agreed to construct ITER at Cadarache in Provence-Alpes-C\u00f4te-d'Azur, France. Construction of the ITER complex began in 2007, while assembly of the tokamak itself is scheduled to begin in 2015.\nFusion for Energy, the EU agency in charge of the European contribution to the project, is located in Barcelona, Spain. Fusion for Energy (F4E) is the European Union's Joint Undertaking for ITER and the Development of Fusion Energy. According to the agency's website:\n\n\"F4E is responsible for providing Europe's contribution to ITER, the world's largest scientific partnership that aims to demonstrate fusion as a viable and sustainable source of energy. [...] F4E also supports fusion research and development initiatives [...]\"\n\nThe ITER Neutral Beam Test Facility aimed at developing and optimizing the neutral beam injector prototype, is being constructed in Padova. It will be the only ITER facility out of the site in Cadarache.\n\n\n== Participants ==\n\nCurrently there are seven parties participating in the ITER program: the European Union (through the legally distinct organisation EURATOM), India, Japan, China, Russia, South Korea, and the United States. Canada was previously a full member, but has since pulled out due to a lack of funding from the federal government. The lack of funding also resulted in Canada withdrawing from its bid for the ITER site in 2003. The host member of the ITER project, and hence the member contributing most of the costs, is the EU.\nIn 2007, it was announced that participants in the ITER will consider Kazakhstan's offer to join the program and in March 2009, Switzerland, an associate member of EURATOM since 1979, also ratified the country's accession to the European Domestic Agency Fusion for Energy as a third country member.\nITER's work is supervised by the ITER Council, which has the authority to appoint senior staff, amend regulations, decide on budgeting issues, and allow additional states or organizations to participate in ITER. The present Chairman of the ITER Council is Dr Hideyuki Takatsu \nParticipating countries\n\n\n== Funding ==\nAs of 2016, the total price of constructing the experiment is expected to be in excess of \u20ac20 billion, an increase of \u20ac4.6 billion of its 2010 estimate, and of \u20ac9.6 billion from the 2009 estimate. Prior to that, the proposed costs for ITER were \u20ac5 billion for the construction and \u20ac5 billion for maintenance and the research connected with it during its 35-year lifetime. At the June 2005 conference in Moscow the participating members of the ITER cooperation agreed on the following division of funding contributions: 45% by the hosting member, the European Union, and the rest split between the non-hosting members \u2013 China, India, Japan, South Korea, the Russian Federation and the USA. During the operation and deactivation phases, Euratom will contribute to 34% of the total costs.\nAlthough Japan's financial contribution as a non-hosting member is one-eleventh of the total, the EU agreed to grant it a special status so that Japan will provide for two-elevenths of the research staff at Cadarache and be awarded two-elevenths of the construction contracts, while the European Union's staff and construction components contributions will be cut from five-elevenths to four-elevenths.\nIt was reported in December 2010 that the European Parliament had refused to approve a plan by member states to reallocate \u20ac1.4 billion from the budget to cover a shortfall in ITER building costs in 2012\u201313. The closure of the 2010 budget required this financing plan to be revised, and the European Commission (EC) was forced to put forward an ITER budgetary resolution proposal in 2011.\nThe U.S. withdrew from the ITER consortium in 2000. In 2006, Congress voted to rejoin, and again contribute financially. In June 2015, it appeared that the U.S. Senate might vote to stop the scheduled U.S. contribution of $150 million in the 2015\u20132016 fiscal year.\n\n\n== Criticism ==\n\nA technical concern is that the 14 MeV neutrons produced by the fusion reactions will damage the materials from which the reactor is built. Research is in progress to determine whether and how reactor walls can be designed to last long enough to make a commercial power station economically viable in the presence of the intense neutron bombardment. The damage is primarily caused by high energy neutrons knocking atoms out of their normal position in the crystal lattice. A related problem for a future commercial fusion power station is that the neutron bombardment will induce radioactivity in the reactor material itself. Maintaining and decommissioning a commercial reactor may thus be difficult and expensive. Another problem is that superconducting magnets are damaged by neutron fluxes. A new special research facility, IFMIF, is planned to investigate this problem.\nAnother source of concern comes from the recent tokamak parameters database interpolation which says that power load on tokamak divertors will be five times the expected value for ITER and much more for actual electricity-generating reactors. Given that the projected power load on the ITER divertor is already very high, these new findings mean that new divertor designs should be urgently tested. However, the corresponding test facility (ADX) still has not received any funding.\nA number of fusion researchers working on non-tokamak systems, such as Robert Bussard and Eric Lerner, have been critical of ITER for diverting funding from what they believe could be a potentially more viable and/or cost-effective path to fusion power, such as the polywell reactor. Many critics accuse ITER researchers of being unwilling to face up to the technical and economic potential problems posed by Tokamak fusion schemes. The expected cost of ITER has risen from $5 billion USD to $20 billion USD, and the timeline for operation at full power was moved from the original estimate of 2016 to 2027.\nA French association including about 700 anti-nuclear groups, Sortir du nucl\u00e9aire (Get Out of Nuclear Energy), claimed that ITER was a hazard because scientists did not yet know how to manipulate the high-energy deuterium and tritium hydrogen isotopes used in the fusion process.\nRebecca Harms, Green/EFA member of the European Parliament's Committee on Industry, Research and Energy, said: \"In the next 50 years, nuclear fusion will neither tackle climate change nor guarantee the security of our energy supply.\" Arguing that the EU's energy research should be focused elsewhere, she said: \"The Green/EFA group demands that these funds be spent instead on energy research that is relevant to the future. A major focus should now be put on renewable sources of energy.\" French Green party lawmaker No\u00ebl Mam\u00e8re claims that more concrete efforts to fight present-day global warming will be neglected as a result of ITER: \"This is not good news for the fight against the greenhouse effect because we're going to put ten billion euros towards a project that has a term of 30\u201350 years when we're not even sure it will be effective.\"\nITER is not designed to produce electricity, but made as a proof of concept reactor for the later DEMO project.\n\n\n=== Responses to criticism ===\nProponents believe that much of the ITER criticism is misleading and inaccurate, in particular the allegations of the experiment's \"inherent danger.\" The stated goals for a commercial fusion power station design are that the amount of radioactive waste produced should be hundreds of times less than that of a fission reactor, and that it should produce no long-lived radioactive waste, and that it is impossible for any such reactor to undergo a large-scale runaway chain reaction. A direct contact of the plasma with ITER inner walls would contaminate it, causing it to cool immediately and stop the fusion process. In addition, the amount of fuel contained in a fusion reactor chamber (one half gram of deuterium/tritium fuel) is only sufficient to sustain the fusion burn pulse from minutes up to an hour at most, whereas a fission reactor usually contains several years' worth of fuel. Moreover, some detritiation systems will be implemented, so that at a fuel cycle inventory level of about 2 kg, ITER will eventually need to recycle large amounts of tritium and at turnovers orders of magnitude higher than any preceding tritium facility worldwide.\nIn the case of an accident (or sabotage), it is expected that a fusion reactor might release far less radioactive pollution than would an ordinary fission nuclear station. Furthermore, ITER's type of fusion power has little in common with nuclear weapons technology, and does not produce the fissile materials necessary for the construction of a weapon. Proponents note that large-scale fusion power would be able to produce reliable electricity on demand, and with virtually zero pollution (no gaseous CO2, SO2, or NOx by-products are produced).\nAccording to researchers at a demonstration reactor in Japan, a fusion generator should be feasible in the 2030s and no later than the 2050s. Japan is pursuing its own research program with several operational facilities that are exploring several fusion paths.\nIn the United States alone, electricity accounts for US$210 billion in annual sales. Asia's electricity sector attracted US$93 billion in private investment between 1990 and 1999. These figures take into account only current prices. Proponents of ITER contend that an investment in research now should be viewed as an attempt to earn a far greater future return. Also, worldwide investment of less than US$1 billion per year into ITER is not incompatible with concurrent research into other methods of power generation, which in 2007 totaled US$16.9 billion.\nSupporters of ITER emphasize that the only way to test ideas for withstanding the intense neutron flux is to experimentally subject materials to that flux, which is one of the primary missions of ITER and the IFMIF, and both facilities will be vitally important to that effort. The purpose of ITER is to explore the scientific and engineering questions that surround potential fusion power stations. It is nearly impossible to acquire satisfactory data for the properties of materials expected to be subject to an intense neutron flux, and burning plasmas are expected to have quite different properties from externally heated plasmas. Supporters contend that the answer to these questions requires the ITER experiment, especially in the light of the monumental potential benefits.\nFurthermore, the main line of research via tokamaks has been developed to the point that it is now possible to undertake the penultimate step in magnetic confinement plasma physics research with a self-sustained reaction. In the tokamak research program, recent advances devoted to controlling the configuration of the plasma have led to the achievement of substantially improved energy and pressure confinement, which reduces the projected cost of electricity from such reactors by a factor of two to a value only about 50% more than the projected cost of electricity from advanced light-water reactors. In addition, progress in the development of advanced, low activation structural materials supports the promise of environmentally benign fusion reactors and research into alternate confinement concepts is yielding the promise of future improvements in confinement. Finally, supporters contend that other potential replacements to the fossil fuels have environmental issues of their own. Solar, wind, and hydroelectric power all have a relatively low power output per square kilometer compared to ITER's successor DEMO which, at 2,000 MW, would have an energy density that exceeds even large fission power stations.\n\n\n== Similar projects ==\nPrecursors to ITER were JET and Tore Supra. Other planned and proposed fusion reactors include DEMO, Wendelstein 7-X, NIF, HiPER, and MAST, as well as CFETR (China Fusion Engineering Test Reactor), a 200 MW tokamak.\n\n\n== See also ==\n\nTokamak\nITER Neutral Beam Test Facility, the facility dedicated to the development of the ITER neutral beam injector prototype\nFusion for Energy, the Domestic Agency in charge of managing EU contributions to the ITER project\nInternational Fusion Materials Irradiation Facility, proposed, construction not started\nJT-60/JT-60SA\nEAST (Experimental Advanced Superconducting Tokamak)\nNational Ignition Facility, inertial confinement using lasers\nNuclear power in France\nWendelstein 7-X (German experimental fusion reactor) - a stellarator\nFusenet, European Fusion Education Network, 2008-2013\n\n\n== References ==\n\n\n== External links ==\nOfficial website\nThe New Yorker, Mar. 3 2014, Star in a Bottle, by Raffi Khatchadourian\nArchival material collected by Prof. McCray relating to ITER\u2019s early phase (1979\u20131989) can be consulted at the Historical Archives of the European Union in Florence\n\"Way to New Energy\" video (23:24) at YouTube, by RT, on May 6, 2014.\nThe roles of the Host and the non-Host for the ITER Project. June 2005 The broader approach agreement with Japan.\nFusion Electricity - A roadmap to the realisation of fusion energy EFDA 2012 - 8 missions, ITER, project plan with dependancies, ...", 
                "titleUrl": "https://en.wikipedia.org/wiki/ITER", 
                "title": "ITER"
            }, 
            {
                "snippet": "Iter-pi\u0161a, inscribed in cuneiform as i-te-er-pi/pi4-\u0161a and meaning \"Her command is surpassing\", ca. 1769\u20131767 BC (short chronology) or ca. 1833\u20131831 BC", 
                "pageCategories": "18th-century BC rulers\nBabylonian kings", 
                "pageContent": "Iter-pi\u0161a, inscribed in cuneiform as i-te-er-pi/pi4-\u0161a and meaning \"Her command is surpassing\", ca. 1769\u20131767 BC (short chronology) or ca. 1833\u20131831 BC (middle chronology), was the 12th king of Isin during the Old Babylonian period. The Sumerian King List tells us that \"the divine Iter-pi\u0161a ruled for 4 years.\" The Ur-Isin King List which was written in the 4th year of the reign of Damiq-ili\u0161u gives a reign of just 3 years.\n\n\n== Biography ==\nHe was a contemporary of Warad-Sin (ca. 1770 BC to 1758 BC) the king of Larsa, whose brother and successor, Rim-Sin I would eventually come to overthrow the dynasty, ending the cities' bitter rivalry around 40 years later. He is only known from Kings lists and year-name date formulae.\nA letter from Iter-pi\u0161a to a deity was excavated in a scribal school, \"House F,\" in Nippur during the 1951\u201352 dig season. The scribal school had operated during the 1740s, early in the reign of king Samsu-iluna and the piece had become a belle letter.\n\n\n== External links ==\nIter-pi\u0161a year-names at CDLI, but note the tablet reference BM 85384 in year-name (b) is incorrect.\n\n\n== Inscriptions ==\n\n\n== Notes ==\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Iter-pisha", 
                "title": "Iter-pisha"
            }, 
            {
                "snippet": "European Union (EU) organisation responsible for Europe\u2019s contribution to ITER, the world\u2019s largest scientific partnership aiming to demonstrate fusion", 
                "pageCategories": "Agencies of the European Union\nAll articles needing additional references\nArticles needing additional references from October 2015\nEuropean Atomic Energy Community\nWikipedia articles with possible conflicts of interest from October 2015", 
                "pageContent": "Fusion for Energy (F4E) is the European Union (EU) organisation responsible for Europe\u2019s contribution to ITER, the world\u2019s largest scientific partnership aiming to demonstrate fusion as a viable and sustainable source of energy. The organisation \u2013 formally known as the European Joint Undertaking for ITER and the Development of Fusion Energy \u2013was created under article 45 of the Euratom Treaty by the decision of the Council of the European Union on 27 March 2007 for a period of 35 years.\nF4E counts 400 members of staff and its offices are located in Barcelona, in Spain. One of its main tasks is to work together with European industry and research organisations to develop and provide a wide range of high technology components for the ITER project. The European Union is the host party for the ITER project. Its contribution amounts to 45%, while the other six parties have an in-kind contribution of approximately 9% each. Since 2008, F4E has been collaborating with at least 250 companies and more than 50 R&D organisations.\n\n\n== Mission and governance ==\nF4E\u2019s primary mission is to manage the European contribution to the ITER project; therefore it provides financial funds, which mostly come from the European Community budget. Among other tasks, F4E oversees the preparation of the ITER construction site in Saint-Paul-l\u00e8s-Durance, in France. F4E is formed by Euratom (represented by the European Commission), the Member States of the European Union and Switzerland, which participates as a third country. To ensure the overall supervision of its activities, the members sit on a governing board, which has a wide range of responsibilities including appointing the director.\n\n\n== Fusion energy ==\nFusion is the process which powers the sun, producing energy by fusing together light atoms such as hydrogen at extremely high pressures and temperatures. Fusion reactors use two forms of hydrogen, deuterium and tritium, as fuel.\nThe benefits of fusion energy are that it is an inherently safe process and it does not create greenhouse gases or long-lasting radioactive waste.\n\n\n== The ITER project ==\nITER, meaning \u201cthe way\u201d in Latin, is an international experiment aiming to demonstrate the scientific and technical feasibility of fusion as an energy source. The machine is being constructed in Saint-Paul-l\u00e8s-Durance in the South of France and is funded by seven parties: China, the European Union, India, Japan, Russia, South Korea and the United States. Collectively, the parties taking part in the ITER project represent over one half of the world\u2019s population and 80% of the global GDP.\n\n\n== The Broader Approach activities ==\nThe Broader Approach (BA) activities are three research projects carried out under an agreement between the European Atomic Energy Community (Euratom) and Japan, which contribute equally financially. They are meant to complement the ITER project and accelerate the development of fusion energy through R&D by cooperating on a number of projects of mutual interest.\nThis agreement entered into force on 1 June 2007 and runs for at least 10 years. The Broader Approach consists of three main projects located in Japan: the Satellite Tokamak Programme project JT-60SA (super advanced), the International Fusion Materials Irradiation Facility - Engineering Validation and Engineering Design Activities (IFMIF/EVEDA) and the International Fusion Energy Research Centre (IFERC).\n\n\n== The DEMO project ==\nF4E also aims to contribute to DEMO (Demonstration Power Plant). This experiment is supposed to generate significant amounts of electricity over extended periods and will be self-sufficient in tritium, one of the necessary gases to create fusion. The first commercial fusion electricity power plants are set to be established following DEMO, which is set to be larger in size than ITER and to produce significantly larger fusion power over long periods: a continuous production of up to 500 megawatts of electricity.\n\n\n== Management difficulties ==\nA report by the consultancy Ernst & Young published in 2013 by the European Parliament's Budgetary Control Committee found that F4E has suffered from significant management difficulties. According to the report, \"the organisation faced a series of internal problems that have only been gradually addressed, notably an organisational structure ill-adapted for project-oriented activities.\" From 2010, a host of reforms were undertaken within F4E, including a reshuffling and reorientation of the governance and management structures, as well as a cost-savings programme.\n\n\n== See also ==\nITER\nFusenet\nFusion power\nEuratom\n\n\n== References ==\n\n\n== External links ==\nFusion for Energy, the agency's home page.\nFusion for Energy: Understanding Fusion\nEuratom/fusion, the Fusion page of the EURATOM\n[1], the Broader Approach agreement", 
                "titleUrl": "https://en.wikipedia.org/wiki/Fusion_for_Energy", 
                "title": "Fusion for Energy"
            }, 
            {
                "snippet": "build upon the ITER experimental nuclear fusion reactor. The objectives of DEMO are usually understood to lie somewhere between those of ITER and a \"first", 
                "pageCategories": "All articles containing potentially dated statements\nAll articles with unsourced statements\nArticles containing potentially dated statements from 2016\nArticles with unsourced statements from August 2011\nFusion power\nInterlanguage link template link number\nProposed fusion reactors\nProposed nuclear power stations\nTokamaks\nUse dmy dates from August 2011", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/DEMO", 
                "title": "DEMO"
            }, 
            {
                "snippet": "Iter.Viator is the debut solo album of ex-Peccatum member, (and wife of Ihsahn) Ihriel. Translated from Latin \"Iter Viator\" literally means \"road traveler\"", 
                "pageCategories": "2002 debut albums\nArticles with album ratings that need to be turned into prose\nArticles with hAudio microformats", 
                "pageContent": "Iter.Viator is the debut solo album of ex-Peccatum member, (and wife of Ihsahn) Ihriel. Translated from Latin \"Iter Viator\" literally means \"road traveler\".\n\n\n== Track listing ==\nAll Songs Written & Arranged By Ihriel.\n\"Chasm Blue\" \u2013 1:44\n\"Sanies\" \u2013 7:03\n\"Beautiful As Torment\" \u2013 6:38\n\"Death Salutes Atropos\" \u2013 5:27\n\"The Nudity Of Light\" \u2013 3:26\n\"Odie Et Amo\" \u2013 7:14\n\"In The Throws Of Guilt\" \u2013 11:00\n\n\n== Personnel ==\n\n\n=== Star of Ash ===\nHeidi S. Tveitan: Vocals, Keyboards, programming\n\n\n=== Additional Personnel ===\nVegard Sverre Tveitan: Guitar on all songs except \"Sanies\", bass on tracks 3\u20137, vocals on 4 & 7.\nEinar Solberg: Vocals on track 3.\nJostein Thomassen: Guitar on track 3 & 6.\nKnut Aalefj\u00e6r: Drums & percussion on tracks 3\u20136.\nKenneth Lia Solberg: Guitar on tracks 4 & 6.\nKris G. Rygg: Vocals on tracks 5 & 7.\nThe Star Of Ash Choir on track 7: Kaia Lia (conductor), Sanne Anundsk\u00e5s, Astrid Marie Lia, Inger Bronken, Elisabeth Lia, Marit B\u00f8e, Heidi S. Tveitan, P\u00e5l Solberg, Knut Bendik Breistein, Einar Solberg, Vegard Tveitan, Kenneth Lia Solberg\n\n\n== Production ==\nProduced By Heidi S. Tveitan, V. Tveitan, Tore Ylwizaker & Kris G. Rygg\nRecorded, Engineered & Mixed By Kristoffer G. Rygg, Tore Ylwizaker & Ihriel\nMastered By Tom Kvalsvoll\n\n\n== External links ==\n\"Iter.Viator\" at discogs", 
                "titleUrl": "https://en.wikipedia.org/wiki/Iter.Viator", 
                "title": "Iter.Viator"
            }, 
            {
                "snippet": "fusion energy which will be pertinent to the ITER fusion project as part of that country's contribution to the ITER effort. The project was approved in 1995", 
                "pageCategories": "Interlanguage link template link number\nTokamaks", 
                "pageContent": "The KSTAR, or Korea Superconducting Tokamak Advanced Research is a magnetic fusion device being built at the National Fusion Research Institute in Daejeon, South Korea. It is intended to study aspects of magnetic fusion energy which will be pertinent to the ITER fusion project as part of that country's contribution to the ITER effort. The project was approved in 1995 but construction was delayed by the East Asian financial crisis which weakened the South Korean economy considerably; however the construction phase of the project was completed on September 14, 2007. First plasma occurred on July 15, 2008. or more likely on June 30 2008.\nKSTAR will be one of the first research tokamaks in the world to feature fully superconducting magnets, which again will be of great relevance to ITER as this will also use SC magnets. The KSTAR magnet system consists of 16 niobium-tin direct current toroidal field magnets, 10 niobium-tin alternating current poloidal field magnets and 4 niobium-titanium alternating current poloidal field magnets. It is planned that the reactor will study plasma pulses of up to 20 seconds duration until 2011, when it will be upgraded to study pulses of up to 300 seconds duration. The reactor vessel will have a major radius of 1.8 m, a minor radius of 0.5 m, a maximum toroidal field of 3.5 tesla, and a maximum plasma current of 2 megaampere. As with other tokamaks, heating and current drive will be initiated using neutral beam injection, ion cyclotron resonance heating (ICRH), radio frequency heating and electron cyclotron resonance heating (ECRH). Initial heating power will be 8 megawatt from neutral beam injection upgradeable to 24 MW, 6 MW from ICRH upgradeable to 12 MW, and at present undetermined heating power from ECRH and RF heating. The experiment will use both hydrogen and deuterium fuels but not the deuterium-tritium mix which will be studied in ITER.\nIn 2012, it succeeded in maintaining high-temperature plasma (about 50 million degrees Celsius) for 17 seconds.\n\n\n== Timeline ==\nThe design was based on Tokomak Physics Experiment which was based on Compact Ignition Tokamak design - See Robert J. Goldston.\n1995 - Started Project KSTAR\n1997 - JET of EU emits 17 MW energy from itself.\n1998 - JT-60U went beyond energy junction successfully, and acknowledged possibility of commercialization of nuclear fusion.\n2006 - Life span of 3 Fusion Reactors (JT-60U, JET, and DIII-D) are terminated.\n2007, September - KSTAR's major devices are constructed.\n2008, July - First plasma occurred. Maintenance time: 0.865 seconds, Temperature: 2\u00d7106 K\n2009 - Maintained 320,000A plasma for 3.6 seconds.\n2010, November - First H-mode plasma run.\n2011 - Maintained high-temperature plasma for 5.2 seconds, Temperature: ~50\u00d7106 K, successfully fully deterred ELM (Edge-Localized Mode), first ever in the World.\n2012 - Maintained high-temperature plasma for 17 seconds, Temperature: 50\u00d7106 K\n2013 - Maintained high-temperature plasma for 20 seconds, Temperature: 50\u00d7106 K\n2014 - Maintained high-temperature plasma for 48 seconds, and successfully fully deterred ELM for 5 seconds.\n\n\n== References ==\n\n\n== External links ==\nKSTAR homepage\nEnglish KSTAR homepage\nKSTAR parameters re ITER and other tokamaks\n\nKSTAR Project Status PDF (undated - seems to be 2001. Includes slide-13 construction schedule to end 2004 and slide-16 operation from 2005 with upgrade planned 2010-11.)\nKSTAR Assembly Status, October 2006 PDF\nStatus and Result of the KSTAR Upgrade for the 2010\u2019s Campaign\nKSTAR ICRF transmission line system upgrade for load resilient operation. Jan 2013", 
                "titleUrl": "https://en.wikipedia.org/wiki/KSTAR", 
                "title": "KSTAR"
            }, 
            {
                "snippet": "journey from Rome to Brundisium. It is thus also known as the Iter Brundisium or Iter ad Brundisium. Alluding to a famous satire in which Horace\u2019s poetic", 
                "pageCategories": "1st-century BC Latin books\nAll articles lacking in-text citations\nArticles containing Latin-language text\nArticles lacking in-text citations from October 2012\nPoetry by Horace\nSatirical works", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Satires_(Horace)", 
                "title": "Satires (Horace)"
            }, 
            {
                "snippet": "series of experiments. Compared to the ITER international project, IGNITOR would be much smaller and cheaper: the ITER reactor is 19,000 tons[vague] in weight", 
                "pageCategories": "All Wikipedia articles needing clarification\nArticles with Italian-language external links\nFusion power\nInterlanguage link template link number\nNuclear research institutes\nResearch projects\nTokamaks\nWikipedia articles needing clarification from January 2010", 
                "pageContent": "IGNITOR is the Italian name for a nuclear research project of magnetic confinement fusion, developed by ENEA Laboratories in Frascati. Construction (in Russia) is not complete.\nThe project theory is based on ignited plasma in tokamak. Started in 1977 by Prof. Bruno Coppi of MIT, IGNITOR is based on the 1970s Alcator machine at MIT which pioneered the high magnetic field approach to plasma magnetic confinement, continued with the Alcator C/C-Mod at MIT and the FT/FTU series of experiments.\nCompared to the ITER international project, IGNITOR would be much smaller and cheaper: the ITER reactor is 19,000 tons in weight while the IGNITOR is only 500 tons in weight. IGNITOR is designed to produce approximately 100 MW of fusion power (and ITER to produce ~500 MW fusion power).\n\n\n== Development ==\nAt a meeting with the scientific attach\u00e9s of the European embassies in Moscow in early February 2010 Mikhail Kovalchuk, Director of the Kurchatov Institute, announced that an initiative aimed at developing a fast paced joint research programme in nuclear fusion research was strongly supported by the Governments of Russia and Italy.\nThe original proposal had been initiated earlier by Evgeny Velikhov (President of the Kurchatov Institute) and Bruno Coppi (Head of the High Energy Plasmas Undertaking, MIT) during the early developments of the Alcator C-Mod programme at MIT, where well known scientists of the Kurchatov Institute made key contributions to experiments that identified the unique confinement and purity properties of the high density plasmas produced by the high field Alcator machine. In effects this investigated, for the first time, physical processes leading to attain self-sustained fusion burning plasmas.\nThe collaboration with the Kurchatov Institute is directed at the construction of the Ignitor machine, the first experiment proposed to achieve ignition conditions by nuclear fusion reactions on the basis of existing knowledge of plasma physics and available technologies. Ignitor is part of the line of research on high magnetic field, experiments producing high density plasmas that began with the Alcator and the Frascati Torus programs at MIT and in Italy, respectively. It remains, at the world level, the only experiment capable of reaching ignition by the magnetic field confinement approach. However, several fusion scientists have contested the claim made for IGNITOR that it is a bigger step towards fusion power than the international ITER project.\nAccording to existing plans, Ignitor will be installed at the Triniti site at Troitsk near Moscow that has facilities which can be upgraded to house and operate the machine. This site will become open and made to be easily accessible to scientists of all nations. The management of the relevant research programme will involve Italy and Russia only to facilitate the success of the enterprise. The proponents have suggested that the US become an Associate Member of this effort with a similar arrangement to that made with CERN for its participation in the LHC (Large Hadron Collider) Programme.\nThe goal to produce meaningful fusion reactors in a reasonable time leads to pursuing the achievement of ignition conditions in the near term in order to understand the plasma physical regimes needed for a net power producing reactor. In addition, an objective other than ignition that can be envisioned for the relatively near term is that of high flux neutron sources for material testing involving compact, high density fusion machines. This has been one of the incentives that have led the Ignitor Project to adopt magnesium diboride (MgB2) superconducting cables in the machine design, a first in fusion research. Accordingly, the largest coils (about 5 m diameter) of the machine will be made entirely of MgB2 cables.\nIn the context of the Italy-Russia summit meeting held in Milan on 26 April 2010 the agreement to proceed with the proposed joint Ignitor program has been signed. The participants, from the Russian side, have included the Prime Minister Vladimir Putin, the Deputy Prime Minister Igor Sechin, the Energy Minister Sergei Shmatko, and the Vice Minister of Education and Research Sergey Mazurenko. Participants from the Italian side have included Prime Minister Silvio Berlusconi, the Foreign Affairs Advisor to the Prime Minister Valentino Valentini (who had a key role in forging the agreement on the Ignitor program), and the Minister of Education and Research Mariastella Gelmini who, together with Sergey Mazurenko, signed the agreement in the presence of the two Prime Ministers.\n\n\n== Progress on construction ==\nSome components have been built in Italy.\n\n\n== See also ==\nList of plasma (physics) articles\n\n\n== External links ==\nIGNITOR website\nFact sheet says \"Construction on the reactor is projected to be complete in 2014\"\n\n(English) IGNITOR technical specs on ENEA Laboratories in Frascati\n(Italian) Paolo Detragiache, Technical presentation of the project\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/IGNITOR", 
                "title": "IGNITOR"
            }, 
            {
                "snippet": "Britain and Roman roads in Britain      The British section is known as the Iter Britanniarum, and can be described as the 'road map' of Roman Britain. There", 
                "pageCategories": "2nd-century Latin books\nArticles containing Latin-language text\nGeography of England\nGeography of Wales\nLatin prose texts\nMaps\nNerva\u2013Antonine dynasty\nRoman Britain\nRoman itineraries", 
                "pageContent": "The Antonine Itinerary (Latin: Itinerarium Antonini Augusti, lit. \"The Itinerary of the Emperor Antoninus\") is a famous itinerarium, a register of the stations and distances along various roads. Seemingly based on official documents, possibly from a survey carried out under Augustus, it describes the roads of the Roman Empire. Owing to the scarcity of other extant records of this type, it is a valuable historical record. Almost nothing is known of its date or author. Scholars consider it likely that the original edition was prepared at the beginning of the 3rd century: although it is traditionally ascribed to the patronage of the 2nd-century Antoninus Pius, the oldest extant copy has been assigned to the time of Diocletian and the most likely imperial patron\u2014if the work had one\u2014would have been Caracalla.\n\n\n== Iter Britanniarum ==\n\nThe British section is known as the Iter Britanniarum, and can be described as the 'road map' of Roman Britain. There are 15 such itineraries in the document applying to different geographic areas.\nThe itinerary measures distances in Roman miles, where 1,000 Roman paces equals one Roman mile. A Roman pace was two steps, left plus right. Roman paces were not everywhere the same, and conversion to modern units is imprecise, but 1 Roman mile approximately equals 4,690 feet, or 1430 m.\n\n\n=== Examples ===\nBelow is the original Latin ablative forms for sites along route 13, followed by a translation with a possible (but not necessarily authoritative) name for the modern sites. A transcriber omitted an entry, so that the total number of paces does not equal the sum of paces between locations.\nBelow is the original Latin for route 14 followed by a translation with a possible (but not necessarily authoritative) name for the modern site.\n\n\n=== A confounding factor ===\nDe Situ Britanniae (made available c.\u20091749, published 1757) was a forgery that provided much spurious information on Roman Britain, including \"itineraries\" that overlapped the legitimate Antonine Itineraries, sometimes with contradicting information. Its authenticity was not seriously challenged until 1845, and it was still cited as an authoritative source until the late nineteenth century. By then, its false data had infected almost every account of ancient British history, and been adopted into the Ordnance Survey maps, as General Roy and his successors believed it to be a legitimate source of information, on a par with the Antonine Itineraries. While the document is no longer cited, since its authenticity became indefensible, its data has not been systematically removed from past and present works.\nSome authors, such as Thomas Reynolds, without challenging the authenticity of the forgery, took care to note its discrepancies and challenge the quality of its information. This was not always so, even after the forgery was debunked.\nGonzalo Arias (died 2008) proposed that some of the distance anomalies in the British section of the Antonine Itinerary resulted from the loss of Latin grammatical endings, as these had marked junctions heading towards places, as distinct from the places themselves. However, Arias may not have taken account of earlier work indicating that distances were measured between the edges of administrative areas of named settlements as opposed to centre-to-centre, thereby explaining supposed distance shortfalls and providing additional useful data on the approximate sizes of such areas.\n\n\n== Hispania ==\n\n\n== Citations ==\n\n\n== Bibliography ==\n\n\n== External links ==\nThe Antonine Itinerary: Iter Britanniarum - The British Section\nAnalysis of the Itinerary\nItinerarium Antonini Augusti (the Balkanic roads) at SOLTDM.COM\nRoman Roads in Britain", 
                "titleUrl": "https://en.wikipedia.org/wiki/Antonine_Itinerary", 
                "title": "Antonine Itinerary"
            }, 
            {
                "snippet": "cross-section. Pioneering experiments followed, including for example the ITER-relevant tests of magnetic field correction with saddle coils for RMPs experiment", 
                "pageCategories": "All NPOV disputes\nCommons category without a link on Wikidata\nFusion power\nNPOV disputes from October 2016\nTokamaks", 
                "pageContent": "Tokamak COMPASS (COMPact ASSembly)[1][2] is the main experimental facility of Tokamak department of Institute of Plasma Physics[3] of the Academy of Sciences of the Czech Republic since 2006. It was designed in the 1980s in the British Culham Science Centre as a flexible research facility dedicated mostly to plasma physics studies in circular and D shaped plasmas.\nThe first plasma in COMPASS \"broke down\" in 1989 in a C-shaped vacuum vessel, i.e., in a simpler vessel with a circular cross-section. Pioneering experiments followed, including for example the ITER-relevant tests of magnetic field correction with saddle coils for RMPs experiment (Resonant magnetic perturbations) or experiments with non-inductive current drive in plasma.\nThe operation of tokamak restarted with a D-shaped vacuum vessel in 1992. The operation mode with high plasma confinement (H-mode) was achieved, which represents a reference operation (\"standard scenario\") for the ITER tokamak. The COMPASS tokamak with its size (major radius 0.6 m and height of the vessel approx. 0.7 m) ranks to smaller tokamaks capable of the H-mode operation. Importantly, due to its size and shape the COMPASS plasmas correspond to one tenth (in the linear scale) of the ITER plasmas. At present, besides COMPASS there are only two operational tokamaks in Europe with ITER-like configuration capable of regime with the high plasma confinement. It is the Joint European Torus (JET) and the German tokamak ASDEX Upgrade (Institut f\u00fcr Plasmaphysik, Garching, Germany). JET is the biggest experimental device of this type in the world.\nIn 2002, British scientists started alternative research on larger, spherical tokamak MAST. Operation of COMPASS was discontinued due to insufficient resources for operation of both tokamaks, however, the research program foreseen for the latter tokamak was not concluded. Due to its important and not completely realised opportunities - and, in particular, due to its direct relevance to the ITER project - the facility was offered for free by the European Commission and UKAEA to the Institute of Plasma Physics AS CR in Prague in autumn 2004.\nThe Prague institute has been coordinating research in thermonuclear fusion in the Czech Republic in the framework of EURATOM since 1999. Team of physicists from the institute has a long-time experience in this field of research including operation of a small tokamak CASTOR. The European Commission has declared that the institute is fully competent to operate the tokamak COMPASS. \n\n\n== Parameters of the tokamak COMPASS ==\nMovie: COMPASS discharge using fast - visible camera: [4]\n\n\n== References ==\n\n\n== See also ==\nList of fusion experiments\nELM (Edge Localized Mode)\nBall-pen probe\nLangmuir probe\nThomson scattering\nResonant magnetic perturbations\n\n\n== External links ==\nMagnetic fusion in the Czech Republic\nDiagnostic system on COMPASS", 
                "titleUrl": "https://en.wikipedia.org/wiki/COMPASS_tokamak", 
                "title": "COMPASS tokamak"
            }
        ], 
        "phraseCharStart": "1423"
    }, 
    {
        "phraseCharEnd": "1474", 
        "phraseIndex": "T31", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "carbon materials", 
        "wikiSearchResults": [
            {
                "snippet": "Amorphous carbon is free, reactive carbon that does not have any crystalline structure (also called diamond-like carbon). Amorphous carbon materials may be", 
                "pageCategories": "Carbon forms\nWikipedia articles needing rewrite from December 2012", 
                "pageContent": "Amorphous carbon is free, reactive carbon that does not have any crystalline structure (also called diamond-like carbon). Amorphous carbon materials may be stabilized by terminating dangling-\u03c0 bonds with hydrogen. As with other amorphous solids, some short-range order can be observed. Amorphous carbon is often abbreviated to aC for general amorphous carbon, aC:H or HAC for hydrogenated amorphous carbon, or to ta-C for tetrahedral amorphous carbon.\n\n\n== In mineralogy ==\nIn mineralogy, amorphous carbon is the name used for coal, soot, carbide-derived carbon, and other impure forms of carbon that are neither graphite nor diamond. In a crystallographic sense, however, the materials are not truly amorphous but rather polycrystalline materials of graphite or diamond within an amorphous carbon matrix. Commercial carbon also usually contains significant quantities of other elements, which may also form crystalline impurities.\n\n\n== In modern science ==\nWith the development of modern thin film deposition and growth techniques in the latter half of the 20th century, such as chemical vapour deposition, sputter deposition, and cathodic arc deposition, it became possible to fabricate truly amorphous carbon materials.\nTrue amorphous carbon has localized \u03c0 electrons (as opposed to the aromatic \u03c0 bonds in graphite), and its bonds form with lengths and distances that are inconsistent with any other allotrope of carbon. It also contains a high concentration of dangling bonds; these cause deviations in interatomic spacing (as measured using diffraction) of more than 5% as well as noticeable variation in bond angle.\nThe properties of amorphous carbon films vary depending on the parameters used during deposition. The primary method for characterizing amorphous carbon is through the ratio of sp2 to sp3 hybridized bonds present in the material. Graphite consists purely of sp2 hybridized bonds, whereas diamond consists purely of sp3 hybridized bonds. Materials that are high in sp3 hybridized bonds are referred to as tetrahedral amorphous carbon, owing to the tetrahedral shape formed by sp3 hybridized bonds, or as diamond-like carbon (owing to the similarity of many physical properties to those of diamond).\nExperimentally, sp2 to sp3 ratios can be determined by comparing the relative intensities of various spectroscopic peaks (including EELS, XPS, and Raman spectroscopy) to those expected for graphite or diamond. In theoretical works, the sp2 to sp3 ratios are often obtained by counting the number of carbon atoms with three bonded neighbors versus those with four bonded neighbors. (This technique requires deciding on a somewhat arbitrary metric for determining whether neighboring atoms are considered bonded or not, and is therefore merely used as an indication of the relative sp2-sp3 ratio.)\nAlthough the characterization of amorphous carbon materials by the sp2-sp3 ratio may seem to indicate a one-dimensional range of properties between graphite and diamond, this is most definitely not the case. Research is currently ongoing into ways to characterize and expand on the range of properties offered by amorphous carbon materials.\nAll practical forms of hydrogenated carbon (e.g. smoke, chimney soot, mined coal such as bitumen and anthracite) contain large amounts of polycyclic aromatic hydrocarbon tars, and are therefore almost certainly carcinogenic.\n\n\n== See also ==\nGlassy carbon\nDiamond-like carbon\nCarbon black\nSoot\nCarbon\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Amorphous_carbon", 
                "title": "Amorphous carbon"
            }, 
            {
                "snippet": "The Butterfly blades use carbon materials such as Tamasu Carbon (TamCa) 5000, or Uniaxial Light Carbon (ULC), fiber materials such as Arylate (AL) or Zylon", 
                "pageCategories": "1950 establishments in Japan\nArticles containing Japanese-language text\nCompanies based in Tokyo\nCompanies established in 1950", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Tamasu_(corporation)", 
                "title": "Tamasu (corporation)"
            }, 
            {
                "snippet": "Carbide-derived carbon (CDC), also known as tunable nanoporous carbon, is the common term for carbon materials derived from carbide precursors, such as", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from July 2015\nCapacitors\nCarbon forms\nNanomaterials", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Carbide-derived_carbon", 
                "title": "Carbide-derived carbon"
            }, 
            {
                "snippet": "visited the University of Cambridge during 1979 to perform studies on carbon materials. He worked for the Research Development Corporation of Japan from 1982", 
                "pageCategories": "1939 births\nCS1 Norwegian-language sources (no)\nFellows of the Royal Microscopical Society\nForeign Members of the Chinese Academy of Sciences\nJapanese physicists\nLiving people\nMembers of the Norwegian Academy of Science and Letters\nMembers of the United States National Academy of Sciences\nMicroscopists\nNanotechnologists", 
                "pageContent": "Sumio Iijima (\u98ef\u5cf6 \u6f84\u7537 Iijima Sumio, born May 2, 1939) is a Japanese physicist, often cited as the inventor of carbon nanotubes. Although carbon nanotubes had been observed prior to his \"invention\", Iijima's 1991 paper generated unprecedented interest in the carbon nanostructures and has since fueled intense research in the area of nanotechnology. For this and other work Sumio Iijima was awarded, together with Louis Brus, the inaugural Kavli Prize for nanoscience in 2008.\nBorn in Saitama Prefecture in 1939, Iijima graduated with a Bachelor of Engineering degree in 1963 from the University of Electro-Communications, Tokyo. He received a Master's degree in 1965 and completed his Ph.D. in solid-state physics in 1968, both at Tohoku University in Sendai.\nBetween 1970 and 1982 he performed research with crystalline materials and high-resolution electron microscopy at Arizona State University. He visited the University of Cambridge during 1979 to perform studies on carbon materials.\nHe worked for the Research Development Corporation of Japan from 1982 to 1987, studying ultra-fine particles, after which he joined NEC Corporation where he is still employed. He discovered carbon nanotubes in 1991 while working with NEC. He is also a University Professor at Meijo University since 1999. Furthermore, he is the Honorary AIST Fellow of the National Institute of Advanced Industrial Science and Technology, Distinguished Invited University Professor of Nagoya University.\nHe was awarded the Benjamin Franklin Medal in Physics in 2002, \"for the discovery and elucidation of the atomic structure and helical character of multi-wall and single-wall carbon nanotubes, which have had an enormous impact on the rapidly growing condensed matter and materials science field of nanoscale science and electronics.\"\nHe is a foreign associate of National Academy of Sciences, foreign member of the Norwegian Academy of Science and Letters. Also, He is a Member of the Japan Academy.\n\n\n== Research Fields ==\nNano Science, Crystallography, Electron Microscopy, Solid-State Physics, Materials Science\n\n\n== Professional Record ==\n1968 - 1974: Research Associate, Research Institute for Scientific Measurements, Tohoku University, Sendai\n1970 - 1977: Research Associate, Department of Physics, Arizona State University, Tempe, Arizona\n1977 - 1982: Senior Research Associate, Center for Solid State Science, Arizona State University, Tempe, Arizona\n1979: Visiting Senior Scientist, Department of Metallurgy and Materials Science, University of Cambridge, Cambridge\n1982 - 1987: Group Leader, ERATO Program, Research Development Corporation of Japan, Nagoya\n1987 \u2013 Present: Senior Research Fellow, NEC Corporation, Tsukuba (Joined NEC in 1987 as Senior Principal Researcher)\n1998 - 2002: Research Director, JST/ICORP \"Nanotubulites\" Project Tsukuba and Nagoya\n1999 \u2013 Present: University Professor, Meijo University, Nagoya\n2001 \u2013 2015: Director, Nanotube Research Center, National Institute of Advanced Industrial Science and Technology (AIST), Tsukuba\n2005 \u2013 2012: Dean, SKKU Advanced Institute of Nanotechnology (SAINT, http://saint.skku.edu), Sungkyunkwan University, Suwon, Korea.\n2006 \u2013 2009: Project Reader, NEDO \u201cCarbon Nanotube Capacitor Development Project\u201d\n2007 \u2013 Present: Distinguished Invited University Professor of Nagoya University, Nagoya\n2008 \u2013 2012: Distinguished Invited Chair Professor for World Class University (WCU) Program, Sungkyunkwan University, Suwon, Korea.\n2015 \u2013 Present: Honorary AIST Fellow, National Institute of Advanced Industrial Science and Technology (AIST)\n\n\n== Academy ==\n2007: Foreign Associate, The National Academy of Sciences\n2009: Foreign Member, The Norwegian Academy of Science and Letters.\n2010: Member, The Japan Academy\n2011: Foreign Fellow, Chinese Academy of Science\n\n\n== Honors ==\n2000: Fellow, The American Physical Society\n2001: Honorary Fellowship, Royal Microscopical Society\n2002: Honorary Doctor, University of Antwerp\n2002: Honorary Member, The Crystallographic Society of Japan\n2003: Honorary Doctor, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne(EPFL)\n2004: Honorary Member, The Japanese Society of Microscopy\n2005: Honorary Professor, Xi\u2019an Jiaotong University\n2005: Honorary Professor, Peking University\n2007: Fellow, The Japan Society of Applied Physics\n2009: Fellow, The Microscopy Society of America\n2009: Honorary Member, The Chemical Society of Japan\n2009: Honorary Professor, Tsinghua University\n2009: Distinguished Professor, The University of Electro-Communications\n2010: Honorary Professor, Zhejiang University\n2010: Honorary Professor, Southeast University\n2014: Honorary Doctor, Aalto University\n\n\n== Major Awards ==\n1976: Bertram Eugene Warren Diffraction Physics Award, (The American Crystallography Society)\n1985: Nishina Memorial Award, (The Nishina Memorial Foundation)\n1996: Asahi Prize, (The Asahi Shinbun Cultural Foundation)\n2002: Agilent EuroPhysics Prize, (European Physical Society)\n2002: James C. McGroddy Prize for New Materials, (American Physical Society)\n2002: Benjamin Franklin Medal in Physics, (The Franklin Institute)\n2002: Japan Academy Award and Imperial Award, (The Japan Academy)\n2003: Person of Cultural Merit\n2007: Gregori Aminoff Prize in crystallography 2007, (Royal Swedish Academy of Sciences)\n2007: Fujihara Award, (The Fujihara Foundation of Science)\n2007: Balzan Prize for Nanoscience, (The International Balzan Prize Foundation)\n2008: The Kavli Prize Nanoscience 2008 (The Kavli Foundation)\n2008: The Prince of Asturias Award for Technical Scientific Research 2008, (The Prince of Asturias Foundation)\n2009: Order of Culture\nand others\n\n\n== References ==\n\n\n== External links ==\n\"About Myself\" - NEC's page about Dr. Sumio Iijima\n\"Nanotubulites\" - about Dr. Sumio Iijima\nNanotubes: The Materials of the 21st Century - video presentation by Sumio Iijima\nArizona State University story on Kavli Prize", 
                "titleUrl": "https://en.wikipedia.org/wiki/Sumio_Iijima", 
                "title": "Sumio Iijima"
            }, 
            {
                "snippet": "Cambridge. He is a leading specialist in the theory of amorphous carbon and related materials.   Robertson received his Bachelor of Arts and Doctor of Philosophy", 
                "pageCategories": "1950 births\nAll articles with unsourced statements\nAlumni of the University of Cambridge\nArticles with unsourced statements from May 2015\nBritish electrical engineers\nEnglish physicists\nFellow Members of the IEEE\nFellows of the American Physical Society\nFellows of the Royal Society\nLiving people", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/John_Robertson_(physicist)", 
                "title": "John Robertson (physicist)"
            }, 
            {
                "snippet": "increasing nozzle ratio. As a rule, their modern design assumes use of carbon-carbon materials without regenerative cooling. Along with sliding expanders, stationary", 
                "pageCategories": "Articles with Russian-language external links\nNozzles\nRocket propulsion\nSpacecraft propulsion", 
                "pageContent": "Nozzle extension \u2014 nozzle expander of reaction/rocket engine. The application of nozzle extensions improves efficiency of rocket engines in vacuum by increasing nozzle ratio. As a rule, their modern design assumes use of carbon-carbon materials without regenerative cooling. Along with sliding expanders, stationary nozzle extensions have some applications too.\n\n\n== Description ==\n\nAs of 2009, the search for various schemes to achieve higher area ratios for rocket nozzles remains an active field of research and patenting. Generally, modern application of these designs can be divided into engines, which start their work at sea level and finish it at vacuum conditions, and engines, which perform all their operations in a vacuum.\n\n\n=== \"Air-to-vacuum\" engines ===\nFor first stage rocket engines, the engine works with nozzle extension in disposed position during the first minutes of flight and expands it at some predefined level of air pressure. This scheme assumes the outer skirt of the bell is extended while the engine is functioning and its installation to working position happens in the upper layers of the atmosphere. It excludes problems with flow separation at sea level and increases efficiency of the engine in vacuum. For example, application of nozzle extension for liquid rocket engine NK-33 improves the value of specific impulse up to 15-20 sec for near-space conditions. Therefore, this scheme adjusts the system to ambient conditions along the trajectory or, in other words, allows altitude compensation.\n\n\n=== \"Vacuum\" engines ===\nRocket engines of upper stages perform all their operations in space and therefore in a vacuum. In order to achieve maximum efficiency for this class of engines they need high area ratios. This makes the nozzles a very sizable part of the engine, which must be completely enclosed below the nose cone of a rocket. The payload fairing and supporting constructions must endure all stresses and loads during launch and flight. Consequently, the use of an outer expandable skirt in this case allows the size of the upper stage and payload fairing to be minimized, which in turn decreases the total mass of the nose cone. For these reasons, nozzle extensions are used for rocket engines RL-10 and RD-58.\n\n\n== See also ==\nRocket engine nozzle\nDe Laval nozzle\nStepped nozzle\nF-1\nNK-33\n\n\n== References ==\n\n\n== External links ==\n(Russian) Surprises of \"Engines-2000\", News of cosmonautics, April 2000\n(Russian) Patent of NPO Iskra, Patent department\n(Russian) The research of possible options for construction of liquid rocket engine with changeable nozzle ratio, Magazine \"Engine\"\n(Russian) Casing for fire, Magazine \"Engine\"\nVulcain-2 Cryogenic Engine Passes First Test with New Nozzle Extension, European Space Agency", 
                "titleUrl": "https://en.wikipedia.org/wiki/Nozzle_extension", 
                "title": "Nozzle extension"
            }, 
            {
                "snippet": "Materials used to absorb other materials due to their high affinity for doing so. Examples include: In composting, dry (brown, high-carbon) materials", 
                "pageCategories": "All articles lacking sources\nAll stub articles\nArticles lacking sources from December 2009\nDesiccants\nMaterials stubs\nNatural materials\nSynthetic materials", 
                "pageContent": "A sorbent is a material used to absorb or adsorb liquids or gases. Examples include:\nA material similar to molecular sieve material, which acts by adsorption (attracting molecules to its surface). It has a large internal surface area and good thermal conductivity. It is typically supplied in pellets of 1 mm to 2 mm diameter and roughly 5 mm length or as grains of the order 1 mm. Occasionally as beads up to 5 mm diameter. They are typically made from aluminium oxide with a porous structure.\nMaterials used to absorb other materials due to their high affinity for doing so. Examples include:\nIn composting, dry (brown, high-carbon) materials absorb many odoriferous chemicals, and these chemicals help to decompose these sorbents.\nA sponge absorbs many times its own weight in water.\nA polypropylene fiber mat may be used to absorb oil.\nA cellulose fiber product may be used to absorb oil.\nThe granular gel material in a baby diaper will absorb several times its original weight in urine.\nPolymethylsiloxane polyhydrate (PMSPH) is a gelly-like polymeric organosilicon compound. PMSPG is an sorbent designed for binding toxic substances of different nature, pathogens and metabolites in the gastrointestinal tract and their excretion.\nDesiccants absorb water, drying out (desiccating) the surrounding materials.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Sorbent", 
                "title": "Sorbent"
            }, 
            {
                "snippet": "oscillators. Carbon nanotubes and graphene's physical strength allows carbon based materials to meet higher stress demands, when common materials would normally", 
                "pageCategories": "All articles with unsourced statements\nApplied sciences\nArticles with unsourced statements from April 2016\nEmerging technologies\nNanoelectronics", 
                "pageContent": "Nanoelectromechanical systems (NEMS) are a class of devices integrating electrical and mechanical functionality on the nanoscale. NEMS form the logical next miniaturization step from so-called microelectromechanical systems, or MEMS devices. NEMS typically integrate transistor-like nanoelectronics with mechanical actuators, pumps, or motors, and may thereby form physical, biological, and chemical sensors. The name derives from typical device dimensions in the nanometer range, leading to low mass, high mechanical resonance frequencies, potentially large quantum mechanical effects such as zero point motion, and a high surface-to-volume ratio useful for surface-based sensing mechanisms. Uses include accelerometers, or detectors of chemical substances in the air.\n\n\n== Overview ==\nAs noted by Richard Feynman in his famous talk in 1959, \"There's Plenty of Room at the Bottom,\" there are many potential applications of machines at smaller and smaller sizes; by building and controlling devices at smaller scales, all technology benefits. Among the expected benefits include greater efficiencies and reduced size, decreased power consumption and lower costs of production in electromechanical systems.\nIn 2000, the first very-large-scale integration (VLSI) NEMS device was demonstrated by researchers at IBM. Its premise was an array of AFM tips which can heat/sense a deformable substrate in order to function as a memory device. Further devices have been described by Stefan de Haan. In 2007, the International Technical Roadmap for Semiconductors (ITRS) contains NEMS Memory as a new entry for the Emerging Research Devices section.\n\n\n== Atomic force microscopy ==\nA key application of NEMS is atomic force microscope tips. The increased sensitivity achieved by NEMS leads to smaller and more efficient sensors to detect stresses, vibrations, forces at the atomic level, and chemical signals. AFM tips and other detection at the nanoscale rely heavily on NEMS.\n\n\n== Approaches to miniaturization ==\nTwo complementary approaches to fabrication of NEMS can be found. The top-down approach uses the traditional microfabrication methods, i.e. optical and electron beam lithography, to manufacture devices. While being limited by the resolution of these methods, it allows a large degree of control over the resulting structures. Typically, devices are fabricated from metallic thin films or etched semiconductor layers.\nBottom-up approaches, in contrast, use the chemical properties of single molecules to cause single-molecule components to self-organize or self-assemble into some useful conformation, or rely on positional assembly. These approaches utilize the concepts of molecular self-assembly and/or molecular recognition. This allows fabrication of much smaller structures, albeit often at the cost of limited control of the fabrication process.\nA combination of these approaches may also be used, in which nanoscale molecules are integrated into a top-down framework. One such example is the carbon nanotube nanomotor.\n\n\n== Materials ==\n\n\n=== Carbon allotropes ===\nMany of the commonly used materials for NEMS technology have been carbon based, specifically diamond, carbon nanotubes and graphene. This is mainly because of the useful properties of carbon based materials which directly meet the needs of NEMS. The mechanical properties of carbon (such as large Young's modulus) are fundamental to the stability of NEMS while the metallic and semiconductor conductivities of carbon based materials allow them to function as transistors.\nBoth graphene and diamond exhibit high Young's modulus, low density, low friction, exceedingly low mechanical dissipation, and large surface area. The low friction of CNTs, allow practically frictionless bearings and has thus been a huge motivation towards practical applications of CNTs as constitutive elements in NEMS, such as nanomotors, switches, and high-frequency oscillators. Carbon nanotubes and graphene's physical strength allows carbon based materials to meet higher stress demands, when common materials would normally fail and thus further support their use as a major materials in NEMS technological development.\nAlong with the mechanical benefits of carbon based materials, the electrical properties of carbon nanotubes and graphene allow it to be used in many electrical components of NEMS. Nanotransistors have been developed for both carbon nanotubes as well as graphene. Transistors are one of the basic building blocks for all electronic devices, so by effectively developing usable transistors, carbon nanotubes and graphene are both very crucial to NEMS.\n\n\n==== Metallic carbon nanotubes ====\n\nCarbon nanotubes (CNTs) are allotropes of carbon with a cylindrical nanostructure. They can be considered a rolled up graphene. When rolled at specific and discrete (\"chiral\") angles, and the combination of the rolling angle and radius decides whether the nanotube has a bandgap (semiconductoring) or no bandgap (metallic).\nMetallic carbon nanotubes have also been proposed for nanoelectronic interconnects since they can carry high current densities. This is a useful property as wires to transfer current are another basic building block of any electrical system. Carbon nanotubes have specifically found so much use in NEMS that methods have already been discovered to connect suspended carbon nanotubes to other nanostructures. This allows carbon nanotubes to form complicated nanoelectric systems. Because carbon based products can be properly controlled and act as interconnects as well as transistors, they serve as a fundamental material in the electrical components of NEMS.\n\n\n==== Difficulties ====\nDespite all of the useful properties of carbon nanotubes and graphene for NEMS technology, both of these products face several hindrances to their implementation. One of the main problems is carbon\u2019s response to real life environments. Carbon nanotubes exhibit a large change in electronic properties when exposed to oxygen. Similarly, other changes to the electronic and mechanical attributes of carbon based materials must fully be explored before their implementation, especially because of their high surface area which can easily react with surrounding environments. Carbon nanotubes were also found to have varying conductivities, being either metallic or semiconducting depending on their helicity when processed. Because of this, special treatment must be given to the nanotubes during processing to assure that all of the nanotubes have appropriate conductivities. Graphene also has complicated electric conductivity properties compared to traditional semiconductors because it lacks an energy band gap and essentially changes all the rules for how electrons move through a graphene based device. This means that traditional constructions of electronic devices will likely not work and completely new architectures must be designed for these new electronic devices.\n\n\n== Simulations ==\nComputer simulations have long been important counterparts to experimental studies of NEMS devices. Through continuum mechanics and molecular dynamics (MD), important behaviors of NEMS devices can be predicted via computational modeling before engaging in experiments. Additionally, combining continuum and MD techniques enables engineers to efficiently analyze the stability of NEMS devices without resorting to ultra-fine meshes and time-intensive simulations. Simulations have other advantages as well: they do not require the time and expertise associated with fabricating NEMS devices; they can effectively predict the interrelated roles of various electromechanical effects; and parametric studies can be conducted fairly readily as compared with experimental approaches. For example, computational studies have predicted the charge distributions and \u201cpull-in\u201d electromechanical responses of NEMS devices. Using simulations to predict mechanical and electrical behavior of these devices can help optimize NEMS device design parameters.\n\n\n== Future of NEMS ==\nKey hurdles currently preventing the commercial application of many NEMS devices include low-yields and high device quality variability. Before NEMS devices can actually be implemented, reasonable integrations of carbon based products must be created. A recent step in that direction has been demonstrated for diamond, achieving a processing level comparable to that of silicon. The focus is currently shifting from experimental work towards practical applications and device structures that will implement and profit from such novel devices. The next challenge to overcome involves understanding all of the properties of these carbon-based tools, and using the properties to make efficient and durable NEMS with low failure rates.\nCarbon-based materials have served as prime materials for NEMS use, because of their exceptional mechanical and electrical properties.\nThe global market of NEMS is projected to reach $108.88 million by 2022 \n\n\n== Applications ==\nNanoelectromechanical relay\nNanoelectromechanical systems mass spectrometer\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Nanoelectromechanical_systems", 
                "title": "Nanoelectromechanical systems"
            }, 
            {
                "snippet": "first to novel carbon materials such as fullerenes (C60), graphene and carbon nanotubes. After discovering how to mass-produce carbon nanotubes, he and", 
                "pageCategories": "1954 births\nCommons category with page title same as on Wikidata\nLiving people\nMembers of the French Academy of Sciences\nNanotechnologists\nNorwegian expatriates in France\nNorwegian physical chemists\nOberlin College alumni\nPierre and Marie Curie University alumni\nUniversity of Strasbourg faculty", 
                "pageContent": "Thomas Ebbesen (born 30 January 1954 in Oslo) is a physical chemist and professor at the University of Strasbourg in France, known for his pioneering work in nanoscience and received the Kavli Prize in Nanoscience \u201cfor transformative contributions to the field of nano-optics that have broken long-held beliefs about the limitations of the resolution limits of optical microscopy and imaging\u201d, together with Stefan Hell, and Sir John Pendry in 2014.\nThomas Ebbesen received his bachelors from Oberlin College, and a PhD from Pierre and Marie Curie University in Paris in the field of photo-physical chemistry. He then worked at the Notre Dame Radiation Laboratory before joining the NEC Fundamental Research Laboratories in Japan in 1988 where his research shifted first to novel carbon materials such as fullerenes (C60), graphene and carbon nanotubes. After discovering how to mass-produce carbon nanotubes, he and his colleagues measured many of their unique features such as their mechanical and wetting properties. For his pioneering and extensive contribution to the field of carbon nanotubes, he shared the 2001 Agilent Europhysics Prize with Sumio Iijima, Cees Dekker and Paul McEuen.\nWhile working at NEC, Ebbesen discovered a major new optical phenomenon. He found that, contrary to the then accepted theory, it was possible to transmit light extremely efficiently through subwavelength holes milled in opaque metal films under certain conditions. The phenomenon, known as extraordinary optical transmission, involves surface plasmons. It has raised fundamental questions and is finding applications in broad variety of areas from chemistry to opto-electronics. Ebbesen has received several awards for the discovery of the extraordinary optical transmission such as the 2005 France Telecom Prize of the French Academy of Sciences and the 2009 Quantum Electronics and Optics Prize of the European Physical Society.\nHis current research is focused on the physics and chemistry of light-matter interactions at the nanoscale.\nIn 1999, Thomas Ebbesen joined ISIS founded by Jean-Marie Lehn at the University of Strasbourg, which he headed from 2004 to 2012. He is the director of the International Center for Frontier Research in Chemistry. and the University of Strasbourg Institute for Advanced Study. He is a member of the Institut Universitaire de France, the Norwegian Academy of Science and Letters, the French Academy of Science and the Royal Flemish Academy of Belgium for Sciences and the Arts.\nHe is married to the pianist Masako Hayashi-Ebbesen. They have two daughters.\n\n\n== Awards ==\nNEC Research Prize 1992\nRanders Prize 2001\nAgilent Europhysics Prize 2001\nPrix France Telecom 2005\nTomassoni Prize 2009\nScola Physica Romana Medal 2009\nQuantum Electronics and Optics Prize 2009\nDr. Scient. H.C., University of Southern Denmark 2009\nKavli Prize in Nanoscience 2014\nPrix Special of the French Physics Society, 2014\nHonorary Doctorate, Oberlin College, USA 2015\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Thomas_Ebbesen", 
                "title": "Thomas Ebbesen"
            }, 
            {
                "snippet": "natural diamond requires very specific conditions\u2014exposure of carbon-bearing materials to high pressure, ranging approximately between 45 and 60 kilobars", 
                "pageCategories": "Abrasives\nAll Wikipedia articles written in American English\nArticles containing video clips\nCubic minerals\nDiamond\nEconomic geology\nFeatured articles\nGroup IV semiconductors\nImpact event minerals\nLuminescent minerals", 
                "pageContent": "Diamond (pronunciation: /\u02c8da\u026a\u0259m\u0259nd/ or /\u02c8da\u026am\u0259nd/) is a metastable allotrope of carbon, where the carbon atoms are arranged in a variation of the face-centered cubic crystal structure called a diamond lattice. Diamond is less stable than graphite, but the conversion rate from diamond to graphite is negligible at standard conditions. Diamond is renowned as a material with superlative physical qualities, most of which originate from the strong covalent bonding between its atoms. In particular, diamond has the highest hardness and thermal conductivity of any bulk material. Those properties determine the major industrial application of diamond in cutting and polishing tools and the scientific applications in diamond knives and diamond anvil cells.\nBecause of its extremely rigid lattice, it can be contaminated by very few types of impurities, such as boron and nitrogen. Small amounts of defects or impurities (about one per million of lattice atoms) color diamond blue (boron), yellow (nitrogen), brown (lattice defects), green (radiation exposure), purple, pink, orange or red. Diamond also has relatively high optical dispersion (ability to disperse light of different colors).\nMost natural diamonds are formed at high temperature and pressure at depths of 140 to 190 kilometers (87 to 118 mi) in the Earth's mantle. Carbon-containing minerals provide the carbon source, and the growth occurs over periods from 1 billion to 3.3 billion years (25% to 75% of the age of the Earth). Diamonds are brought close to the Earth's surface through deep volcanic eruptions by magma, which cools into igneous rocks known as kimberlites and lamproites. Diamonds can also be produced synthetically in a HPHT method which approximately simulates the conditions in the Earth's mantle. An alternative, and completely different growth technique is chemical vapor deposition (CVD). Several non-diamond materials, which include cubic zirconia and silicon carbide and are often called diamond simulants, resemble diamond in appearance and many properties. Special gemological techniques have been developed to distinguish natural diamonds, synthetic diamonds, and diamond simulants. The word is from the ancient Greek \u1f00\u03b4\u03ac\u03bc\u03b1\u03c2 \u2013 ad\u00e1mas \"unbreakable\".\n\n\n== History ==\n\nThe name diamond is derived from the ancient Greek \u03b1\u03b4\u03ac\u03bc\u03b1\u03c2 (ad\u00e1mas), \"proper\", \"unalterable\", \"unbreakable\", \"untamed\", from \u1f00- (a-), \"un-\" + \u03b4\u03b1\u03bc\u03ac\u03c9 (dam\u00e1\u014d), \"I overpower\", \"I tame\". Diamonds are thought to have been first recognized and mined in India, where significant alluvial deposits of the stone could be found many centuries ago along the rivers Penner, Krishna and Godavari. Diamonds have been known in India for at least 3,000 years but most likely 6,000 years.\nDiamonds have been treasured as gemstones since their use as religious icons in ancient India. Their usage in engraving tools also dates to early human history. The popularity of diamonds has risen since the 19th century because of increased supply, improved cutting and polishing techniques, growth in the world economy, and innovative and successful advertising campaigns.\nIn 1772, Antoine Lavoisier used a lens to concentrate the rays of the sun on a diamond in an atmosphere of oxygen, and showed that the only product of the combustion was carbon dioxide, proving that diamond is composed of carbon. Later in 1797, Smithson Tennant repeated and expanded that experiment. By demonstrating that burning diamond and graphite releases the same amount of gas, he established the chemical equivalence of these substances.\nThe most familiar uses of diamonds today are as gemstones used for adornment, a use which dates back into antiquity, and as industrial abrasives for cutting hard materials. The dispersion of white light into spectral colors is the primary gemological characteristic of gem diamonds. In the 20th century, experts in gemology developed methods of grading diamonds and other gemstones based on the characteristics most important to their value as a gem. Four characteristics, known informally as the four Cs, are now commonly used as the basic descriptors of diamonds: these are carat (its weight), cut (quality of the cut is graded according to proportions, symmetry and polish), color (how close to white or colorless; for fancy diamonds how intense is its hue), and clarity (how free is it from inclusions). A large, flawless diamond is known as a paragon.\n\n\n=== Natural history ===\nThe formation of natural diamond requires very specific conditions\u2014exposure of carbon-bearing materials to high pressure, ranging approximately between 45 and 60 kilobars (4.5 and 6 GPa), but at a comparatively low temperature range between approximately 900 and 1,300 \u00b0C (1,650 and 2,370 \u00b0F). These conditions are met in two places on Earth; in the lithospheric mantle below relatively stable continental plates, and at the site of a meteorite strike.\n\n\n==== Formation in cratons ====\n\nThe conditions for diamond formation to happen in the lithospheric mantle occur at considerable depth corresponding to the requirements of temperature and pressure. These depths are estimated between 140 and 190 kilometers (87 and 118 mi) though occasionally diamonds have crystallized at depths about 300 km (190 mi). The rate at which temperature changes with increasing depth into the Earth varies greatly in different parts of the Earth. In particular, under oceanic plates the temperature rises more quickly with depth, beyond the range required for diamond formation at the depth required. The correct combination of temperature and pressure is only found in the thick, ancient, and stable parts of continental plates where regions of lithosphere known as cratons exist. Long residence in the cratonic lithosphere allows diamond crystals to grow larger.\nThrough studies of carbon isotope ratios (similar to the methodology used in carbon dating, except with the stable isotopes C-12 and C-13), it has been shown that the carbon found in diamonds comes from both inorganic and organic sources. Some diamonds, known as harzburgitic, are formed from inorganic carbon originally found deep in the Earth's mantle. In contrast, eclogitic diamonds contain organic carbon from organic detritus that has been pushed down from the surface of the Earth's crust through subduction (see plate tectonics) before transforming into diamond. These two different source of carbon have measurably different 13C:12C ratios. Diamonds that have come to the Earth's surface are generally quite old, ranging from under 1 billion to 3.3 billion years old. This is 22% to 73% of the age of the Earth.\nDiamonds occur most often as euhedral or rounded octahedra and twinned octahedra known as macles. As diamond's crystal structure has a cubic arrangement of the atoms, they have many facets that belong to a cube, octahedron, rhombicosidodecahedron, tetrakis hexahedron or disdyakis dodecahedron. The crystals can have rounded off and unexpressive edges and can be elongated. Sometimes they are found grown together or form double \"twinned\" crystals at the surfaces of the octahedron. These different shapes and habits of some diamonds result from differing external circumstances. Diamonds (especially those with rounded crystal faces) are commonly found coated in nyf, an opaque gum-like skin.\n\n\n==== Transport from mantle ====\n\nDiamond-bearing rock is carried from the mantle to the Earth's surface by deep-origin volcanic eruptions. The magma for such a volcano must originate at a depth where diamonds can be formed\u2014150 km (93 mi) or more (three times or more the depth of source magma for most volcanoes). This is a relatively rare occurrence. These typically small surface volcanic craters extend downward in formations known as volcanic pipes. The pipes contain material that was transported toward the surface by volcanic action, but was not ejected before the volcanic activity ceased. During eruption these pipes are open to the surface, resulting in open circulation; many xenoliths of surface rock and even wood and fossils are found in volcanic pipes. Diamond-bearing volcanic pipes are closely related to the oldest, coolest regions of continental crust (cratons). This is because cratons are very thick, and their lithospheric mantle extends to great enough depth that diamonds are stable. Not all pipes contain diamonds, and even fewer contain enough diamonds to make mining economically viable.\nThe magma in volcanic pipes is usually one of two characteristic types, which cool into igneous rock known as either kimberlite or lamproite. The magma itself does not contain diamond; instead, it acts as an elevator that carries deep-formed rocks (xenoliths), minerals (xenocrysts), and fluids upward. These rocks are characteristically rich in magnesium-bearing olivine, pyroxene, and amphibole minerals which are often altered to serpentine by heat and fluids during and after eruption. Certain indicator minerals typically occur within diamantiferous kimberlites and are used as mineralogical tracers by prospectors, who follow the indicator trail back to the volcanic pipe which may contain diamonds. These minerals are rich in chromium (Cr) or titanium (Ti), elements which impart bright colors to the minerals. The most common indicator minerals are chromium garnets (usually bright red chromium-pyrope, and occasionally green ugrandite-series garnets), eclogitic garnets, orange titanium-pyrope, red high-chromium spinels, dark chromite, bright green chromium-diopside, glassy green olivine, black picroilmenite, and magnetite. Kimberlite deposits are known as blue ground for the deeper serpentinized part of the deposits, or as yellow ground for the near surface smectite clay and carbonate weathered and oxidized portion.\nOnce diamonds have been transported to the surface by magma in a volcanic pipe, they may erode out and be distributed over a large area. A volcanic pipe containing diamonds is known as a primary source of diamonds. Secondary sources of diamonds include all areas where a significant number of diamonds have been eroded out of their kimberlite or lamproite matrix, and accumulated because of water or wind action. These include alluvial deposits and deposits along existing and ancient shorelines, where loose diamonds tend to accumulate because of their size and density. Diamonds have also rarely been found in deposits left behind by glaciers (notably in Wisconsin and Indiana); in contrast to alluvial deposits, glacial deposits are minor and are therefore not viable commercial sources of diamond.\n\n\n==== Space diamonds ====\n\nNot all diamonds found on Earth originated on Earth. Primitive interstellar meteorites were found to contain carbon possibly in the form of diamond. A type of diamond called carbonado that is found in South America and Africa may have been deposited there via an asteroid impact (not formed from the impact) about 3 billion years ago. These diamonds may have formed in the intrastellar environment, but as of 2008, there was no scientific consensus on how carbonado diamonds originated.\nDiamonds can also form under other naturally occurring high-pressure conditions. Very small diamonds of micrometer and nanometer sizes, known as microdiamonds or nanodiamonds respectively, have been found in meteorite impact craters. Such impact events create shock zones of high pressure and temperature suitable for diamond formation. Impact-type microdiamonds can be used as an indicator of ancient impact craters. Popigai crater in Russia may have the world's largest diamond deposit, estimated at trillions of carats, and formed by an asteroid impact.\nScientific evidence indicates that white dwarf stars have a core of crystallized carbon and oxygen nuclei. The largest of these found in the universe so far, BPM 37093, is located 50 light-years (4.7\u00d71014 km) away in the constellation Centaurus. A news release from the Harvard-Smithsonian Center for Astrophysics described the 2,500-mile (4,000 km)-wide stellar core as a diamond.\n\n\n== Material properties ==\n\nA diamond is a transparent crystal of tetrahedrally bonded carbon atoms in a covalent network lattice (sp3) that crystallizes into the diamond lattice which is a variation of the face centered cubic structure. Diamonds have been adapted for many uses because of the material's exceptional physical characteristics. Most notable are its extreme hardness and thermal conductivity (900\u20137003232000000000000\u26602320 W\u00b7m\u22121\u00b7K\u22121), as well as wide bandgap and high optical dispersion. Above 7003197315000000000\u26601700 \u00b0C (7003197300000000000\u26601973 K / 7003224592777777777\u26603583 \u00b0F) in vacuum or oxygen-free atmosphere, diamond converts to graphite; in air, transformation starts at ~7002973150000000000\u2660700 \u00b0C. Diamond's ignition point is 720 \u2013 7003107315000000000\u2660800 \u00b0C in oxygen and 850 \u2013 7003127315000000000\u26601000 \u00b0C in air. Naturally occurring diamonds have a density ranging from 3.15\u20137003353000000000000\u26603.53 g/cm3, with pure diamond close to 7003352000000000000\u26603.52 g/cm3. The chemical bonds that hold the carbon atoms in diamonds together are weaker than those in graphite. In diamonds, the bonds form an inflexible three-dimensional lattice, whereas in graphite, the atoms are tightly bonded into sheets, which can slide easily over one another, making the overall structure weaker. In a diamond, each carbon atom is surrounded by neighboring four carbon atoms forming a tetrahedral shaped unit.\n\n\n=== Hardness ===\nDiamond is the hardest known natural material on both the Vickers and the Mohs scale. Diamond's hardness has been known since antiquity, and is the source of its name.\nDiamond hardness depends on its purity, crystalline perfection and orientation: hardness is higher for flawless, pure crystals oriented to the <111> direction (along the longest diagonal of the cubic diamond lattice). Therefore, whereas it might be possible to scratch some diamonds with other materials, such as boron nitride, the hardest diamonds can only be scratched by other diamonds and nanocrystalline diamond aggregates.\nThe hardness of diamond contributes to its suitability as a gemstone. Because it can only be scratched by other diamonds, it maintains its polish extremely well. Unlike many other gems, it is well-suited to daily wear because of its resistance to scratching\u2014perhaps contributing to its popularity as the preferred gem in engagement or wedding rings, which are often worn every day.\n\nThe hardest natural diamonds mostly originate from the Copeton and Bingara fields located in the New England area in New South Wales, Australia. These diamonds are generally small, perfect to semiperfect octahedra, and are used to polish other diamonds. Their hardness is associated with the crystal growth form, which is single-stage crystal growth. Most other diamonds show more evidence of multiple growth stages, which produce inclusions, flaws, and defect planes in the crystal lattice, all of which affect their hardness. It is possible to treat regular diamonds under a combination of high pressure and high temperature to produce diamonds that are harder than the diamonds used in hardness gauges.\nSomewhat related to hardness is another mechanical property toughness, which is a material's ability to resist breakage from forceful impact. The toughness of natural diamond has been measured as 7.5\u201310 MPa\u00b7m1/2. This value is good compared to other ceramic materials, but poor compared to most engineering materials such as engineering alloys, which typically exhibit toughnesses over 100 MPa\u00b7m1/2. As with any material, the macroscopic geometry of a diamond contributes to its resistance to breakage. Diamond has a cleavage plane and is therefore more fragile in some orientations than others. Diamond cutters use this attribute to cleave some stones, prior to faceting. \"Impact toughness\" is one of the main indexes to measure the quality of synthetic industrial diamonds.\n\n\n=== Pressure resistance ===\nUsed in so-called diamond anvil experiments to create high-pressure environments, diamonds are able to withstand crushing pressures in excess of 600 gigapascals (6 million atmospheres).\n\n\n=== Electrical conductivity ===\nOther specialized applications also exist or are being developed, including use as semiconductors: some blue diamonds are natural semiconductors, in contrast to most diamonds, which are excellent electrical insulators. The conductivity and blue color originate from boron impurity. Boron substitutes for carbon atoms in the diamond lattice, donating a hole into the valence band.\nSubstantial conductivity is commonly observed in nominally undoped diamond grown by chemical vapor deposition. This conductivity is associated with hydrogen-related species adsorbed at the surface, and it can be removed by annealing or other surface treatments.\n\n\n=== Surface property ===\nDiamonds are naturally lipophilic and hydrophobic, which means the diamonds' surface cannot be wet by water but can be easily wet and stuck by oil. This property can be utilized to extract diamonds using oil when making synthetic diamonds. However, when diamond surfaces are chemically modified with certain ions, they are expected to become so hydrophilic that they can stabilize multiple layers of water ice at human body temperature.\nThe surface of diamonds is partially oxidized. The oxidized surface can be reduced by heat treatment under hydrogen flow. That is to say, this heat treatment partially removes oxygen-containing functional groups. But diamonds (sp3C) are unstable against high temperature (above about 400 \u00b0C (752 \u00b0F) ) under atmospheric pressure. The structure gradually changes into sp2C above this temperature. Thus, diamonds should be reduced under this temperature.\n\n\n=== Chemical stability ===\nDiamonds are not very reactive. Under room temperature diamonds do not react with any chemical reagents including strong acids and bases. A diamond's surface can only be oxidized at temperatures above about 850 \u00b0C (1,560 \u00b0F) in air. Diamond also reacts with fluorine gas above about 700 \u00b0C (1,292 \u00b0F).\n\n\n=== Color ===\n\nDiamond has a wide bandgap of 6981881197067849999\u26605.5 eV corresponding to the deep ultraviolet wavelength of 225 nanometers. This means pure diamond should transmit visible light and appear as a clear colorless crystal. Colors in diamond originate from lattice defects and impurities. The diamond crystal lattice is exceptionally strong and only atoms of nitrogen, boron and hydrogen can be introduced into diamond during the growth at significant concentrations (up to atomic percents). Transition metals nickel and cobalt, which are commonly used for growth of synthetic diamond by high-pressure high-temperature techniques, have been detected in diamond as individual atoms; the maximum concentration is 0.01% for nickel and even less for cobalt. Virtually any element can be introduced to diamond by ion implantation.\nNitrogen is by far the most common impurity found in gem diamonds and is responsible for the yellow and brown color in diamonds. Boron is responsible for the blue color. Color in diamond has two additional sources: irradiation (usually by alpha particles), that causes the color in green diamonds; and plastic deformation of the diamond crystal lattice. Plastic deformation is the cause of color in some brown and perhaps pink and red diamonds. In order of rarity, yellow diamond is followed by brown, colorless, then by blue, green, black, pink, orange, purple, and red. \"Black\", or Carbonado, diamonds are not truly black, but rather contain numerous dark inclusions that give the gems their dark appearance. Colored diamonds contain impurities or structural defects that cause the coloration, while pure or nearly pure diamonds are transparent and colorless. Most diamond impurities replace a carbon atom in the crystal lattice, known as a carbon flaw. The most common impurity, nitrogen, causes a slight to intense yellow coloration depending upon the type and concentration of nitrogen present. The Gemological Institute of America (GIA) classifies low saturation yellow and brown diamonds as diamonds in the normal color range, and applies a grading scale from \"D\" (colorless) to \"Z\" (light yellow). Diamonds of a different color, such as blue, are called fancy colored diamonds, and fall under a different grading scale.\nIn 2008, the Wittelsbach Diamond, a 35.56-carat (7.112 g) blue diamond once belonging to the King of Spain, fetched over US$24 million at a Christie's auction. In May 2009, a 7.03-carat (1.406 g) blue diamond fetched the highest price per carat ever paid for a diamond when it was sold at auction for 10.5 million Swiss francs (6.97 million euro or US$9.5 million at the time). That record was however beaten the same year: a 5-carat (1.0 g) vivid pink diamond was sold for $10.8 million in Hong Kong on December 1, 2009.\n\n\n=== Identification ===\nDiamonds can be identified by their high thermal conductivity. Their high refractive index is also indicative, but other materials have similar refractivity. Diamonds cut glass, but this does not positively identify a diamond because other materials, such as quartz, also lie above glass on the Mohs scale and can also cut it. Diamonds can scratch other diamonds, but this can result in damage to one or both stones. Hardness tests are infrequently used in practical gemology because of their potentially destructive nature. The extreme hardness and high value of diamond means that gems are typically polished slowly using painstaking traditional techniques and greater attention to detail than is the case with most other gemstones; these tend to result in extremely flat, highly polished facets with exceptionally sharp facet edges. Diamonds also possess an extremely high refractive index and fairly high dispersion. Taken together, these factors affect the overall appearance of a polished diamond and most diamantaires still rely upon skilled use of a loupe (magnifying glass) to identify diamonds 'by eye'.\n\n\n== Industry ==\n\nThe diamond industry can be separated into two distinct categories: one dealing with gem-grade diamonds and another for industrial-grade diamonds. Both markets value diamonds differently.\n\n\n=== Gem-grade diamonds ===\n\nA large trade in gem-grade diamonds exists. Although most gem-grade diamonds are sold newly polished, there is a well-established market for resale of polished diamonds (e.g. pawnbroking, auctions, second-hand jewelry stores, diamantaires, bourses, etc.). One hallmark of the trade in gem-quality diamonds is its remarkable concentration: wholesale trade and diamond cutting is limited to just a few locations; in 2003, 92% of the world's diamonds were cut and polished in Surat, India. Other important centers of diamond cutting and trading are the Antwerp diamond district in Belgium, where the International Gemological Institute is based, London, the Diamond District in New York City, the Diamond Exchange District in Tel Aviv, and Amsterdam. One contributory factor is the geological nature of diamond deposits: several large primary kimberlite-pipe mines each account for significant portions of market share (such as the Jwaneng mine in Botswana, which is a single large-pit mine that can produce between 12,500,000 carats (2,500 kg) to 15,000,000 carats (3,000 kg) of diamonds per year). Secondary alluvial diamond deposits, on the other hand, tend to be fragmented amongst many different operators because they can be dispersed over many hundreds of square kilometers (e.g., alluvial deposits in Brazil).\nThe production and distribution of diamonds is largely consolidated in the hands of a few key players, and concentrated in traditional diamond trading centers, the most important being Antwerp, where 80% of all rough diamonds, 50% of all cut diamonds and more than 50% of all rough, cut and industrial diamonds combined are handled. This makes Antwerp a de facto \"world diamond capital\". The city of Antwerp also hosts the Antwerpsche Diamantkring, created in 1929 to become the first and biggest diamond bourse dedicated to rough diamonds. Another important diamond center is New York City, where almost 80% of the world's diamonds are sold, including auction sales.\nThe De Beers company, as the world's largest diamond mining company, holds a dominant position in the industry, and has done so since soon after its founding in 1888 by the British imperialist Cecil Rhodes. De Beers is currently the world's largest operator of diamond production facilities (mines) and distribution channels for gem-quality diamonds. The Diamond Trading Company (DTC) is a subsidiary of De Beers and markets rough diamonds from De Beers-operated mines. De Beers and its subsidiaries own mines that produce some 40% of annual world diamond production. For most of the 20th century over 80% of the world's rough diamonds passed through De Beers, but by 2001\u20132009 the figure had decreased to around 45%, and by 2013 the company's market share had further decreased to around 38% in value terms and even less by volume. De Beers sold off the vast majority of its diamond stockpile in the late 1990s \u2013 early 2000s and the remainder largely represents working stock (diamonds that are being sorted before sale). This was well documented in the press but remains little known to the general public.\nAs a part of reducing its influence, De Beers withdrew from purchasing diamonds on the open market in 1999 and ceased, at the end of 2008, purchasing Russian diamonds mined by the largest Russian diamond company Alrosa. As of January 2011, De Beers states that it only sells diamonds from the following four countries: Botswana, Namibia, South Africa and Canada. Alrosa had to suspend their sales in October 2008 due to the global energy crisis, but the company reported that it had resumed selling rough diamonds on the open market by October 2009. Apart from Alrosa, other important diamond mining companies include BHP Billiton, which is the world's largest mining company; Rio Tinto Group, the owner of Argyle (100%), Diavik (60%), and Murowa (78%) diamond mines; and Petra Diamonds, the owner of several major diamond mines in Africa.\n\nFurther down the supply chain, members of The World Federation of Diamond Bourses (WFDB) act as a medium for wholesale diamond exchange, trading both polished and rough diamonds. The WFDB consists of independent diamond bourses in major cutting centers such as Tel Aviv, Antwerp, Johannesburg and other cities across the USA, Europe and Asia. In 2000, the WFDB and The International Diamond Manufacturers Association established the World Diamond Council to prevent the trading of diamonds used to fund war and inhumane acts. WFDB's additional activities include sponsoring the World Diamond Congress every two years, as well as the establishment of the International Diamond Council (IDC) to oversee diamond grading.\nOnce purchased by Sightholders (which is a trademark term referring to the companies that have a three-year supply contract with DTC), diamonds are cut and polished in preparation for sale as gemstones ('industrial' stones are regarded as a by-product of the gemstone market; they are used for abrasives). The cutting and polishing of rough diamonds is a specialized skill that is concentrated in a limited number of locations worldwide. Traditional diamond cutting centers are Antwerp, Amsterdam, Johannesburg, New York City, and Tel Aviv. Recently, diamond cutting centers have been established in China, India, Thailand, Namibia and Botswana. Cutting centers with lower cost of labor, notably Surat in Gujarat, India, handle a larger number of smaller carat diamonds, while smaller quantities of larger or more valuable diamonds are more likely to be handled in Europe or North America. The recent expansion of this industry in India, employing low cost labor, has allowed smaller diamonds to be prepared as gems in greater quantities than was previously economically feasible.\nDiamonds which have been prepared as gemstones are sold on diamond exchanges called bourses. There are 28 registered diamond bourses in the world. Bourses are the final tightly controlled step in the diamond supply chain; wholesalers and even retailers are able to buy relatively small lots of diamonds at the bourses, after which they are prepared for final sale to the consumer. Diamonds can be sold already set in jewelry, or sold unset (\"loose\"). According to the Rio Tinto Group, in 2002 the diamonds produced and released to the market were valued at US$9 billion as rough diamonds, US$14 billion after being cut and polished, US$28 billion in wholesale diamond jewelry, and US$57 billion in retail sales.\n\n\n==== Cutting ====\n\nMined rough diamonds are converted into gems through a multi-step process called \"cutting\". Diamonds are extremely hard, but also brittle and can be split up by a single blow. Therefore, diamond cutting is traditionally considered as a delicate procedure requiring skills, scientific knowledge, tools and experience. Its final goal is to produce a faceted jewel where the specific angles between the facets would optimize the diamond luster, that is dispersion of white light, whereas the number and area of facets would determine the weight of the final product. The weight reduction upon cutting is significant and can be of the order of 50%. Several possible shapes are considered, but the final decision is often determined not only by scientific, but also practical considerations. For example, the diamond might be intended for display or for wear, in a ring or a necklace, singled or surrounded by other gems of certain color and shape. Some of them may be considered as classical, such as round, pear, marquise, oval, hearts and arrows diamonds, etc. Some of them are special, produced by certain companies, for example, Phoenix, Cushion, Sole Mio diamonds, etc.\nThe most time-consuming part of the cutting is the preliminary analysis of the rough stone. It needs to address a large number of issues, bears much responsibility, and therefore can last years in case of unique diamonds. The following issues are considered:\nThe hardness of diamond and its ability to cleave strongly depend on the crystal orientation. Therefore, the crystallographic structure of the diamond to be cut is analyzed using X-ray diffraction to choose the optimal cutting directions.\nMost diamonds contain visible non-diamond inclusions and crystal flaws. The cutter has to decide which flaws are to be removed by the cutting and which could be kept.\nThe diamond can be split by a single, well calculated blow of a hammer to a pointed tool, which is quick, but risky. Alternatively, it can be cut with a diamond saw, which is a more reliable but tedious procedure.\nAfter initial cutting, the diamond is shaped in numerous stages of polishing. Unlike cutting, which is a responsible but quick operation, polishing removes material by gradual erosion and is extremely time consuming. The associated technique is well developed; it is considered as a routine and can be performed by technicians. After polishing, the diamond is reexamined for possible flaws, either remaining or induced by the process. Those flaws are concealed through various diamond enhancement techniques, such as repolishing, crack filling, or clever arrangement of the stone in the jewelry. Remaining non-diamond inclusions are removed through laser drilling and filling of the voids produced.\n\n\n==== Marketing ====\nMarketing has significantly affected the image of diamond as a valuable commodity.\nN. W. Ayer & Son, the advertising firm retained by De Beers in the mid-20th century, succeeded in reviving the American diamond market. And the firm created new markets in countries where no diamond tradition had existed before. N. W. Ayer's marketing included product placement, advertising focused on the diamond product itself rather than the De Beers brand, and associations with celebrities and royalty. Without advertising the De Beers brand, De Beers was advertising its competitors' diamond products as well, but this was not a concern as De Beers dominated the diamond market throughout the 20th century. De Beers' market share dipped temporarily to 2nd place in the global market below Alrosa in the aftermath of the global economic crisis of 2008, down to less than 29% in terms of carats mined, rather than sold. The campaign lasted for decades but was effectively discontinued by early 2011. De Beers still advertises diamonds, but the advertising now mostly promotes its own brands, or licensed product lines, rather than completely \"generic\" diamond products. The campaign was perhaps best captured by the slogan \"a diamond is forever\". This slogan is now being used by De Beers Diamond Jewelers, a jewelry firm which is a 50%/50% joint venture between the De Beers mining company and LVMH, the luxury goods conglomerate.\nBrown-colored diamonds constituted a significant part of the diamond production, and were predominantly used for industrial purposes. They were seen as worthless for jewelry (not even being assessed on the diamond color scale). After the development of Argyle diamond mine in Australia in 1986, and marketing, brown diamonds have become acceptable gems. The change was mostly due to the numbers: the Argyle mine, with its 35,000,000 carats (7,000 kg) of diamonds per year, makes about one-third of global production of natural diamonds; 80% of Argyle diamonds are brown.\n\n\n=== Industrial-grade diamonds ===\n\nIndustrial diamonds are valued mostly for their hardness and thermal conductivity, making many of the gemological characteristics of diamonds, such as the 4 Cs, irrelevant for most applications. 80% of mined diamonds (equal to about 135,000,000 carats (27,000 kg) annually), are unsuitable for use as gemstones, and used industrially. In addition to mined diamonds, synthetic diamonds found industrial applications almost immediately after their invention in the 1950s; another 570,000,000 carats (114,000 kg) of synthetic diamond is produced annually for industrial use (in 2004; in 2014 it's 4,500,000,000 carats (900,000 kg), 90% of which is produced in China). Approximately 90% of diamond grinding grit is currently of synthetic origin.\nThe boundary between gem-quality diamonds and industrial diamonds is poorly defined and partly depends on market conditions (for example, if demand for polished diamonds is high, some lower-grade stones will be polished into low-quality or small gemstones rather than being sold for industrial use). Within the category of industrial diamonds, there is a sub-category comprising the lowest-quality, mostly opaque stones, which are known as bort.\nIndustrial use of diamonds has historically been associated with their hardness, which makes diamond the ideal material for cutting and grinding tools. As the hardest known naturally occurring material, diamond can be used to polish, cut, or wear away any material, including other diamonds. Common industrial applications of this property include diamond-tipped drill bits and saws, and the use of diamond powder as an abrasive. Less expensive industrial-grade diamonds, known as bort, with more flaws and poorer color than gems, are used for such purposes. Diamond is not suitable for machining ferrous alloys at high speeds, as carbon is soluble in iron at the high temperatures created by high-speed machining, leading to greatly increased wear on diamond tools compared to alternatives.\nSpecialized applications include use in laboratories as containment for high pressure experiments (see diamond anvil cell), high-performance bearings, and limited use in specialized windows. With the continuing advances being made in the production of synthetic diamonds, future applications are becoming feasible. The high thermal conductivity of diamond makes it suitable as a heat sink for integrated circuits in electronics.\n\n\n=== Mining ===\n\nApproximately 130,000,000 carats (26,000 kg) of diamonds are mined annually, with a total value of nearly US$9 billion, and about 100,000 kg (220,000 lb) are synthesized annually.\nRoughly 49% of diamonds originate from Central and Southern Africa, although significant sources of the mineral have been discovered in Canada, India, Russia, Brazil, and Australia. They are mined from kimberlite and lamproite volcanic pipes, which can bring diamond crystals, originating from deep within the Earth where high pressures and temperatures enable them to form, to the surface. The mining and distribution of natural diamonds are subjects of frequent controversy such as concerns over the sale of blood diamonds or conflict diamonds by African paramilitary groups. The diamond supply chain is controlled by a limited number of powerful businesses, and is also highly concentrated in a small number of locations around the world.\nOnly a very small fraction of the diamond ore consists of actual diamonds. The ore is crushed, during which care is required not to destroy larger diamonds, and then sorted by density. Today, diamonds are located in the diamond-rich density fraction with the help of X-ray fluorescence, after which the final sorting steps are done by hand. Before the use of X-rays became commonplace, the separation was done with grease belts; diamonds have a stronger tendency to stick to grease than the other minerals in the ore.\n\nHistorically, diamonds were found only in alluvial deposits in Guntur and Krishna district of the Krishna River delta in Southern India. India led the world in diamond production from the time of their discovery in approximately the 9th century BC to the mid-18th century AD, but the commercial potential of these sources had been exhausted by the late 18th century and at that time India was eclipsed by Brazil where the first non-Indian diamonds were found in 1725. Currently, one of the most prominent Indian mines is located at Panna.\nDiamond extraction from primary deposits (kimberlites and lamproites) started in the 1870s after the discovery of the Diamond Fields in South Africa. Production has increased over time and now an accumulated total of 4,500,000,000 carats (900,000 kg) have been mined since that date. Twenty percent of that amount has been mined in the last five years, and during the last 10 years, nine new mines have started production; four more are waiting to be opened soon. Most of these mines are located in Canada, Zimbabwe, Angola, and one in Russia.\nIn the U.S., diamonds have been found in Arkansas, Colorado, Wyoming, and Montana. In 2004, the discovery of a microscopic diamond in the U.S. led to the January 2008 bulk-sampling of kimberlite pipes in a remote part of Montana. The Crater of Diamonds State Park in Arkansas is open to the public, and is the only mine in the world where members of the public can dig for diamonds.\nToday, most commercially viable diamond deposits are in Russia (mostly in Sakha Republic, for example Mir pipe and Udachnaya pipe), Botswana, Australia (Northern and Western Australia) and the Democratic Republic of the Congo. In 2005, Russia produced almost one-fifth of the global diamond output, according to the British Geological Survey. Australia boasts the richest diamantiferous pipe, with production from the Argyle diamond mine reaching peak levels of 42 metric tons per year in the 1990s. There are also commercial deposits being actively mined in the Northwest Territories of Canada and Brazil. Diamond prospectors continue to search the globe for diamond-bearing kimberlite and lamproite pipes.\n\n\n=== Political issues ===\n\nIn some of the more politically unstable central African and west African countries, revolutionary groups have taken control of diamond mines, using proceeds from diamond sales to finance their operations. Diamonds sold through this process are known as conflict diamonds or blood diamonds. Major diamond trading corporations continue to fund and fuel these conflicts by doing business with armed groups.\nIn response to public concerns that their diamond purchases were contributing to war and human rights abuses in central and western Africa, the United Nations, the diamond industry and diamond-trading nations introduced the Kimberley Process in 2002. The Kimberley Process aims to ensure that conflict diamonds do not become intermixed with the diamonds not controlled by such rebel groups. This is done by requiring diamond-producing countries to provide proof that the money they make from selling the diamonds is not used to fund criminal or revolutionary activities. Although the Kimberley Process has been moderately successful in limiting the number of conflict diamonds entering the market, some still find their way in. According to the International Diamond Manufacturers Association, conflict diamonds constitute 2\u20133% of all diamonds traded. Two major flaws still hinder the effectiveness of the Kimberley Process: (1) the relative ease of smuggling diamonds across African borders, and (2) the violent nature of diamond mining in nations that are not in a technical state of war and whose diamonds are therefore considered \"clean\".\nThe Canadian Government has set up a body known as the Canadian Diamond Code of Conduct to help authenticate Canadian diamonds. This is a stringent tracking system of diamonds and helps protect the \"conflict free\" label of Canadian diamonds.\n\n\n== Synthetics, simulants, and enhancements ==\n\n\n=== Synthetics ===\n\nSynthetic diamonds are diamonds manufactured in a laboratory, as opposed to diamonds mined from the Earth. The gemological and industrial uses of diamond have created a large demand for rough stones. This demand has been satisfied in large part by synthetic diamonds, which have been manufactured by various processes for more than half a century. However, in recent years it has become possible to produce gem-quality synthetic diamonds of significant size. It is possible to make colorless synthetic gemstones that, on a molecular level, are identical to natural stones and so visually similar that only a gemologist with special equipment can tell the difference.\nThe majority of commercially available synthetic diamonds are yellow and are produced by so-called high-pressure high-temperature (HPHT) processes. The yellow color is caused by nitrogen impurities. Other colors may also be reproduced such as blue, green or pink, which are a result of the addition of boron or from irradiation after synthesis.\n\nAnother popular method of growing synthetic diamond is chemical vapor deposition (CVD). The growth occurs under low pressure (below atmospheric pressure). It involves feeding a mixture of gases (typically 1 to 99 methane to hydrogen) into a chamber and splitting them to chemically active radicals in a plasma ignited by microwaves, hot filament, arc discharge, welding torch or laser. This method is mostly used for coatings, but can also produce single crystals several millimeters in size (see picture).\nAs of 2010, nearly all 5,000 million carats (1,000 tonnes) of synthetic diamonds produced per year are for industrial use. Around 50% of the 133 million carats of natural diamonds mined per year end up in industrial use. Mining companies' expenses average $40 to $60 per carat for natural colorless diamonds, while synthetic manufacturers' expenses average $2,500 per carat for synthetic, gem-quality colorless diamonds. However, a purchaser is more likely to encounter a synthetic when looking for a fancy-colored diamond because nearly all synthetic diamonds are fancy-colored, while only 0.01% of natural diamonds are.\n\n\n=== Simulants ===\n\nA diamond simulant is a non-diamond material that is used to simulate the appearance of a diamond, and may be referred to as diamante. Cubic zirconia is the most common. The gemstone moissanite (silicon carbide) can be treated as a diamond simulant, though more costly to produce than cubic zirconia. Both are produced synthetically.\n\n\n=== Enhancements ===\n\nDiamond enhancements are specific treatments performed on natural or synthetic diamonds (usually those already cut and polished into a gem), which are designed to better the gemological characteristics of the stone in one or more ways. These include laser drilling to remove inclusions, application of sealants to fill cracks, treatments to improve a white diamond's color grade, and treatments to give fancy color to a white diamond.\nCoatings are increasingly used to give a diamond simulant such as cubic zirconia a more \"diamond-like\" appearance. One such substance is diamond-like carbon\u2014an amorphous carbonaceous material that has some physical properties similar to those of the diamond. Advertising suggests that such a coating would transfer some of these diamond-like properties to the coated stone, hence enhancing the diamond simulant. Techniques such as Raman spectroscopy should easily identify such a treatment.\n\n\n=== Identification ===\nEarly diamond identification tests included a scratch test relying on the superior hardness of diamond. This test is destructive, as a diamond can scratch another diamond, and is rarely used nowadays. Instead, diamond identification relies on its superior thermal conductivity. Electronic thermal probes are widely used in the gemological centers to separate diamonds from their imitations. These probes consist of a pair of battery-powered thermistors mounted in a fine copper tip. One thermistor functions as a heating device while the other measures the temperature of the copper tip: if the stone being tested is a diamond, it will conduct the tip's thermal energy rapidly enough to produce a measurable temperature drop. This test takes about 2\u20133 seconds.\nWhereas the thermal probe can separate diamonds from most of their simulants, distinguishing between various types of diamond, for example synthetic or natural, irradiated or non-irradiated, etc., requires more advanced, optical techniques. Those techniques are also used for some diamonds simulants, such as silicon carbide, which pass the thermal conductivity test. Optical techniques can distinguish between natural diamonds and synthetic diamonds. They can also identify the vast majority of treated natural diamonds. \"Perfect\" crystals (at the atomic lattice level) have never been found, so both natural and synthetic diamonds always possess characteristic imperfections, arising from the circumstances of their crystal growth, that allow them to be distinguished from each other.\nLaboratories use techniques such as spectroscopy, microscopy and luminescence under shortwave ultraviolet light to determine a diamond's origin. They also use specially made instruments to aid them in the identification process. Two screening instruments are the DiamondSure and the DiamondView, both produced by the DTC and marketed by the GIA.\nSeveral methods for identifying synthetic diamonds can be performed, depending on the method of production and the color of the diamond. CVD diamonds can usually be identified by an orange fluorescence. D-J colored diamonds can be screened through the Swiss Gemmological Institute's Diamond Spotter. Stones in the D-Z color range can be examined through the DiamondSure UV/visible spectrometer, a tool developed by De Beers. Similarly, natural diamonds usually have minor imperfections and flaws, such as inclusions of foreign material, that are not seen in synthetic diamonds.\nScreening devices based on diamond type detection can be used to make a distinction between diamonds that are certainly natural and diamonds that are potentially synthetic. Those potentially synthetic diamonds require more investigation in a specialized lab. Examples of commercial screening devices are D-Screen (WTOCD / HRD Antwerp) and Alpha Diamond Analyzer (Bruker / HRD Antwerp).\n\n\n== Stolen diamonds ==\nOccasionally large thefts of diamonds take place. In February 2013 armed robbers carried out a raid at Brussels Airport and escaped with gems estimated to be worth $50m (\u00a332m; 37m euros). The gang broke through a perimeter fence and raided the cargo hold of a Swiss-bound plane. The gang have since been arrested and large amounts of cash and diamonds recovered.\nThe identification of stolen diamonds presents a set of difficult problems. Rough diamonds will have a distinctive shape depending on whether their source is a mine or from an alluvial environment such as a beach or river - alluvial diamonds have smoother surfaces than those that have been mined. Determining the provenance of cut and polished stones is much more complex.\nThe Kimberley Process was developed to monitor the trade in rough diamonds and prevent their being used to fund violence. Before exporting, rough diamonds are certificated by the government of the country of origin. Some countries, such as Venezuela, are not party to the agreement. The Kimberley Process does not apply to local sales of rough diamonds within a country.\nDiamonds may be etched by laser with marks invisible to the naked eye. Lazare Kaplan, a US-based company, developed this method. However, whatever is marked on a diamond can readily be removed.\n\n\n== See also ==\n\nList of diamonds\nList of largest rough diamonds\n\nList of minerals\nDiamonds on Jupiter and Saturn\nLonsdaleite\n\n\n== References ==\n\n\n== Books ==\nC. Even-Zohar (2007). From Mine to Mistress: Corporate Strategies and Government Policies in the International Diamond Industry (2nd ed.). Mining Journal Press. \nG. Davies (1994). Properties and growth of diamond. INSPEC. ISBN 0-85296-875-2. \nM. O'Donoghue, M (2006). Gems. Elsevier. ISBN 0-7506-5856-8. \nM. O'Donoghue and L. Joyner (2003). Identification of gemstones. Great Britain: Butterworth-Heinemann. ISBN 0-7506-5512-7. \nA. Feldman and L.H. Robins (1991). Applications of Diamond Films and Related Materials. Elsevier. \nJ.E. Field (1979). The Properties of Diamond. London: Academic Press. ISBN 0-12-255350-0. \nJ.E. Field (1992). The Properties of Natural and Synthetic Diamond. London: Academic Press. ISBN 0-12-255352-7. \nW. Hershey (1940). The Book of Diamonds. Hearthside Press New York. ISBN 1-4179-7715-9. \nS. Koizumi, C.E. Nebel and M. Nesladek (2008). Physics and Applications of CVD Diamond. Wiley VCH. ISBN 3-527-40801-0. \nL.S. Pan and D.R. Kani (1995). Diamond: Electronic Properties and Applications. Kluwer Academic Publishers. ISBN 0-7923-9524-7. \nPagel-Theisen, Verena (2001). Diamond Grading ABC: the Manual. Antwerp: Rubin & Son. ISBN 3-9800434-6-0. \nR.L. Radovic, P.M. Walker and P.A. Thrower (1965). Chemistry and physics of carbon: a series of advances. New York: Marcel Dekker. ISBN 0-8247-0987-X. \nM. Tolkowsky (1919). Diamond Design: A Study of the Reflection and Refraction of Light in a Diamond. London: E. & F.N. Spon. \nR.W. Wise (2003). Secrets of the Gem Trade: The Connoisseur's Guide to Precious Gemstones. Brunswick House Press. \nA.M. Zaitsev (2001). Optical Properties of Diamond: A Data Handbook. Springer. ISBN 3-540-66582-X. \n\n\n== External links ==\nProperties of diamond: Ioffe database\n\"A Contribution to the Understanding of Blue Fluorescence on the Appearance of Diamonds\". (2007) Gemological Institute of America (GIA)\nTyson, Peter (November 2000). \"Diamonds in the Sky\". Retrieved March 10, 2005.\nHave You Ever Tried to Sell a Diamond?", 
                "titleUrl": "https://en.wikipedia.org/wiki/Diamond", 
                "title": "Diamond"
            }
        ], 
        "phraseCharStart": "1458"
    }, 
    {
        "phraseCharEnd": "1553", 
        "phraseIndex": "T32", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "tungsten tiles", 
        "wikiSearchResults": [
            {
                "snippet": "rectangular tungsten tiles sits 3.2 meters above the detectors. The detector system contains a forward plane of 128 x 128 Cadmium-Telluride tiles (ISGRI-", 
                "pageCategories": "All articles containing potentially dated statements\nAll articles to be expanded\nArticles containing potentially dated statements from January 2015\nArticles to be expanded from September 2015\nArticles using small message boxes\nCS1 maint: Multiple names: authors list\nEuropean Space Agency satellites\nGamma-ray telescopes\nSpace observatories\nSpacecraft launched in 2002", 
                "pageContent": "In mathematics, an integral assigns numbers to functions in a way that can describe displacement, area, volume, and other concepts that arise by combining infinitesimal data. Integration is one of the two main operations of calculus, with its inverse, differentiation, being the other. Given a function f of a real variable x and an interval [a, b] of the real line, the definite integral\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle \\int _{a}^{b}\\!f(x)\\,dx}\n  \nis defined informally as the signed area of the region in the xy-plane that is bounded by the graph of f, the x-axis and the vertical lines x = a and x = b. The area above the x-axis adds to the total and that below the x-axis subtracts from the total.\nRoughly speaking, the operation of integration is the reverse of differentiation. For this reason, the term integral may also refer to the related notion of the antiderivative, a function F whose derivative is the given function f. In this case, it is called an indefinite integral and is written:\n\n  \n    \n      \n        F\n        (\n        x\n        )\n        =\n        \u222b\n        f\n        (\n        x\n        )\n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle F(x)=\\int f(x)\\,dx.}\n  \nThe integrals discussed in this article are those termed definite integrals. It is the fundamental theorem of calculus that connects differentiation with the definite integral: if f is a continuous real-valued function defined on a closed interval [a, b], then, once an antiderivative F of f is known, the definite integral of f over that interval is given by\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        =\n        F\n        (\n        b\n        )\n        \u2212\n        F\n        (\n        a\n        )\n        .\n      \n    \n    {\\displaystyle \\int _{a}^{b}\\!f(x)\\,dx=F(b)-F(a).}\n  \nThe principles of integration were formulated independently by Isaac Newton and Gottfried Leibniz in the late 17th century, who thought of the integral as an infinite sum of rectangles of infinitesimal width. A rigorous mathematical definition of the integral was given by Bernhard Riemann. It is based on a limiting procedure which approximates the area of a curvilinear region by breaking the region into thin vertical slabs. Beginning in the nineteenth century, more sophisticated notions of integrals began to appear, where the type of the function as well as the domain over which the integration is performed has been generalised. A line integral is defined for functions of two or three variables, and the interval of integration [a, b] is replaced by a certain curve connecting two points on the plane or in the space. In a surface integral, the curve is replaced by a piece of a surface in the three-dimensional space.\n\n\n== History ==\n\n\n=== Pre-calculus integration ===\nThe first documented systematic technique capable of determining integrals is the method of exhaustion of the ancient Greek astronomer Eudoxus (ca. 370 BC), which sought to find areas and volumes by breaking them up into an infinite number of divisions for which the area or volume was known. This method was further developed and employed by Archimedes in the 3rd century BC and used to calculate areas for parabolas and an approximation to the area of a circle.\nA similar method was independently developed in China around the 3rd century AD by Liu Hui, who used it to find the area of the circle. This method was later used in the 5th century by Chinese father-and-son mathematicians Zu Chongzhi and Zu Geng to find the volume of a sphere (Shea 2007; Katz 2004, pp. 125\u2013126).\nThe next significant advances in integral calculus did not begin to appear until the 17th century. At this time the work of Cavalieri with his method of Indivisibles, and work by Fermat, began to lay the foundations of modern calculus, with Cavalieri computing the integrals of xn up to degree n = 9 in Cavalieri's quadrature formula. Further steps were made in the early 17th century by Barrow and Torricelli, who provided the first hints of a connection between integration and differentiation. Barrow provided the first proof of the fundamental theorem of calculus. Wallis generalized Cavalieri's method, computing integrals of x to a general power, including negative powers and fractional powers.\n\n\n=== Newton and Leibniz ===\nThe major advance in integration came in the 17th century with the independent discovery of the fundamental theorem of calculus by Newton and Leibniz. The theorem demonstrates a connection between integration and differentiation. This connection, combined with the comparative ease of differentiation, can be exploited to calculate integrals. In particular, the fundamental theorem of calculus allows one to solve a much broader class of problems. Equal in importance is the comprehensive mathematical framework that both Newton and Leibniz developed. Given the name infinitesimal calculus, it allowed for precise analysis of functions within continuous domains. This framework eventually became modern calculus, whose notation for integrals is drawn directly from the work of Leibniz.\n\n\n=== Formalization ===\nWhile Newton and Leibniz provided a systematic approach to integration, their work lacked a degree of rigour. Bishop Berkeley memorably attacked the vanishing increments used by Newton, calling them \"ghosts of departed quantities\". Calculus acquired a firmer footing with the development of limits. Integration was first rigorously formalized, using limits, by Riemann. Although all bounded piecewise continuous functions are Riemann-integrable on a bounded interval, subsequently more general functions were considered\u2014particularly in the context of Fourier analysis\u2014to which Riemann's definition does not apply, and Lebesgue formulated a different definition of integral, founded in measure theory (a subfield of real analysis). Other definitions of integral, extending Riemann's and Lebesgue's approaches, were proposed. These approaches based on the real number system are the ones most common today, but alternative approaches exist, such as a definition of integral as the standard part of an infinite Riemann sum, based on the hyperreal number system.\n\n\n=== Historical notation ===\nIsaac Newton used a small vertical bar above a variable to indicate integration, or placed the variable inside a box. The vertical bar was easily confused with .x or x\u2032, which Newton used to indicate differentiation, and the box notation was difficult for printers to reproduce, so these notations were not widely adopted.\nThe modern notation for the indefinite integral was introduced by Gottfried Leibniz in 1675 (Burton 1988, p. 359; Leibniz 1899, p. 154). He adapted the integral symbol, \u222b, from the letter \u017f (long s), standing for summa (written as \u017fumma; Latin for \"sum\" or \"total\"). The modern notation for the definite integral, with limits above and below the integral sign, was first used by Joseph Fourier in M\u00e9moires of the French Academy around 1819\u201320, reprinted in his book of 1822 (Cajori 1929, pp. 249\u2013250; Fourier 1822, \u00a7231).\n\n\n== Terminology and notation ==\n\n\n=== Standard ===\nThe integral with respect to x of a real-valued function f(x) of a real variable x on the interval [a, b] is written as\n\n  \n    \n      \n        \n          \n            \u222b\n            \n              a\n            \n            \n              b\n            \n          \n          f\n          (\n          x\n          )\n          \n          d\n          x\n          .\n        \n      \n    \n    {\\displaystyle \\displaystyle \\int _{a}^{b}f(x)\\,dx.}\n  \nThe integral sign \u222b represents integration. The symbol dx, called the differential of the variable x, indicates that the variable of integration is x. The function f(x) to be integrated is called the integrand. The symbol dx is separated from the integrand by a space (as shown). If a function has an integral, it is said to be integrable. The points a and b are called the limits of the integral. An integral where the limits are specified is called a definite integral. The integral is said to be over the interval [a, b].\nWhen the limits are omitted, as in\n\n  \n    \n      \n        \u222b\n        f\n        (\n        x\n        )\n        \n        d\n        x\n        ,\n      \n    \n    {\\displaystyle \\int f(x)\\,dx,}\n  \nthe integral is called an indefinite integral (also known as antiderivative). The fundamental theorem of calculus relates indefinite integrals and definite integrals. There are many extensions of this notation to generalizations of the integral.\n\n\n=== Variants ===\nIn modern Arabic mathematical notation, a reflected integral symbol  is used instead of the symbol \u222b. Some authors use an upright \"d\" to indicate the variable of integration (i.e., dx instead of dx). The symbol dx is not always placed after f(x), as for instance in\n\n  \n    \n      \n        \n          \u222b\n          \n            0\n          \n          \n            1\n          \n        \n        \n          \n            \n              3\n               \n              d\n              x\n            \n            \n              \n                x\n                \n                  2\n                \n              \n              +\n              1\n            \n          \n        \n        .\n      \n    \n    {\\displaystyle \\int \\limits _{0}^{1}{\\frac {3\\ dx}{x^{2}+1}}.}\n  \n\n\n== Interpretations of the integral ==\nIntegrals appear in many practical situations. If a swimming pool is rectangular with a flat bottom, then from its length, width, and depth its volume and its perimeter can be determined using arithmetic. But if it is oval with a rounded bottom, all of these quantities call for integrals. Practical approximations may suffice for such trivial examples, but precision engineering (of any discipline) requires exact and rigorous values for these elements.\n\nTo start off, consider the curve y = f(x) between x = 0 and x = 1 with f(x) = \u221ax (see figure). We ask:\nWhat is the area under the function f, in the interval from 0 to 1?\nand call this (yet unknown) area the (definite) integral of f. The notation for this integral will be\n\n  \n    \n      \n        \n          \u222b\n          \n            0\n          \n          \n            1\n          \n        \n        \n          \n            x\n          \n        \n         \n        d\n        x\n        .\n      \n    \n    {\\displaystyle \\int \\limits _{0}^{1}{\\sqrt {x}}\\ dx.}\n  \nAs a first approximation, look at the unit square given by the sides x = 0 to x = 1 and y = f(0) = 0 and y = f(1) = 1. Its area is exactly 1. As it is, the true value of the integral must be somewhat less than 1. Decreasing the width of the approximation rectangles and increasing the number of rectangles shall give a better result; so cross the interval in five steps, using the approximation points 0, 1/5, 2/5, and so on to 1. Fit a box for each step using the right end height of each curve piece, thus \u221a1/5, \u221a2/5, and so on to \u221a1 = 1. Summing the areas of these rectangles, we get a better approximation for the sought integral, namely\n\n  \n    \n      \n        \n          \n            \n              \n                1\n                5\n              \n            \n          \n          \n            (\n            \n              \n                1\n                5\n              \n            \n            \u2212\n            0\n            )\n          \n          +\n          \n            \n              \n                2\n                5\n              \n            \n          \n          \n            (\n            \n              \n                2\n                5\n              \n            \n            \u2212\n            \n              \n                1\n                5\n              \n            \n            )\n          \n          +\n          \u22ef\n          +\n          \n            \n              \n                5\n                5\n              \n            \n          \n          \n            (\n            \n              \n                5\n                5\n              \n            \n            \u2212\n            \n              \n                4\n                5\n              \n            \n            )\n          \n          \u2248\n          0.7497.\n        \n      \n    \n    {\\displaystyle \\textstyle {\\sqrt {\\frac {1}{5}}}\\left({\\frac {1}{5}}-0\\right)+{\\sqrt {\\frac {2}{5}}}\\left({\\frac {2}{5}}-{\\frac {1}{5}}\\right)+\\cdots +{\\sqrt {\\frac {5}{5}}}\\left({\\frac {5}{5}}-{\\frac {4}{5}}\\right)\\approx 0.7497.}\n  \nWe are taking a sum of finitely many function values of f, multiplied with the differences of two subsequent approximation points. We can easily see that the approximation is still too large. Using more steps produces a closer approximation, but will never be exact: replacing the 5 subintervals by twelve in the same way, but with the left end height of each piece, we will get an approximate value for the area of 0.6203, which is too small. The key idea is the transition from adding finitely many differences of approximation points multiplied by their respective function values to using infinitely many fine, or infinitesimal steps.\nThe notation\n\n  \n    \n      \n        \u222b\n        f\n        (\n        x\n        )\n         \n        d\n        x\n      \n    \n    {\\displaystyle \\int f(x)\\ dx}\n  \nconceives the integral as a weighted sum, denoted by the elongated s, of function values, f(x), multiplied by infinitesimal step widths, the so-called differentials, denoted by dx. The multiplication sign is usually omitted.\nHistorically, after the failure of early efforts to rigorously interpret infinitesimals, Riemann formally defined integrals as a limit of weighted sums, so that the dx suggested the limit of a difference (namely, the interval width). Shortcomings of Riemann's dependence on intervals and continuity motivated newer definitions, especially the Lebesgue integral, which is founded on an ability to extend the idea of \"measure\" in much more flexible ways. Thus the notation\n\n  \n    \n      \n        \n          \u222b\n          \n            A\n          \n        \n        f\n        (\n        x\n        )\n         \n        d\n        \u03bc\n      \n    \n    {\\displaystyle \\int _{A}f(x)\\ d\\mu }\n  \nrefers to a weighted sum in which the function values are partitioned, with \u03bc measuring the weight to be assigned to each value. Here A denotes the region of integration.\n\n\n== Formal definitions ==\n\nThere are many ways of formally defining an integral, not all of which are equivalent. The differences exist mostly to deal with differing special cases which may not be integrable under other definitions, but also occasionally for pedagogical reasons. The most commonly used definitions of integral are Riemann integrals and Lebesgue integrals.\n\n\n=== Riemann integral ===\n\nThe Riemann integral is defined in terms of Riemann sums of functions with respect to tagged partitions of an interval. Let [a, b] be a closed interval of the real line; then a tagged partition of [a, b] is a finite sequence\n\n  \n    \n      \n        a\n        =\n        \n          x\n          \n            0\n          \n        \n        \u2264\n        \n          t\n          \n            1\n          \n        \n        \u2264\n        \n          x\n          \n            1\n          \n        \n        \u2264\n        \n          t\n          \n            2\n          \n        \n        \u2264\n        \n          x\n          \n            2\n          \n        \n        \u2264\n        \u22ef\n        \u2264\n        \n          x\n          \n            n\n            \u2212\n            1\n          \n        \n        \u2264\n        \n          t\n          \n            n\n          \n        \n        \u2264\n        \n          x\n          \n            n\n          \n        \n        =\n        b\n        .\n        \n        \n      \n    \n    {\\displaystyle a=x_{0}\\leq t_{1}\\leq x_{1}\\leq t_{2}\\leq x_{2}\\leq \\cdots \\leq x_{n-1}\\leq t_{n}\\leq x_{n}=b.\\,\\!}\n  \nThis partitions the interval [a, b] into n sub-intervals [xi\u22121, xi] indexed by i, each of which is \"tagged\" with a distinguished point ti \u2208 [xi\u22121, xi]. A Riemann sum of a function f with respect to such a tagged partition is defined as\n\n  \n    \n      \n        \n          \u2211\n          \n            i\n            =\n            1\n          \n          \n            n\n          \n        \n        f\n        (\n        \n          t\n          \n            i\n          \n        \n        )\n        \n          \u0394\n          \n            i\n          \n        \n        ;\n      \n    \n    {\\displaystyle \\sum _{i=1}^{n}f(t_{i})\\Delta _{i};}\n  \nthus each term of the sum is the area of a rectangle with height equal to the function value at the distinguished point of the given sub-interval, and width the same as the sub-interval width. Let \u0394i = xi\u2212xi\u22121 be the width of sub-interval i; then the mesh of such a tagged partition is the width of the largest sub-interval formed by the partition, maxi=1...n \u0394i. The Riemann integral of a function f over the interval [a, b] is equal to S if:\nFor all \u03b5 > 0 there exists \u03b4 > 0 such that, for any tagged partition [a, b] with mesh less than \u03b4, we have\n\n  \n    \n      \n        \n          |\n          S\n          \u2212\n          \n            \u2211\n            \n              i\n              =\n              1\n            \n            \n              n\n            \n          \n          f\n          (\n          \n            t\n            \n              i\n            \n          \n          )\n          \n            \u0394\n            \n              i\n            \n          \n          |\n        \n        <\n        \u03b5\n        .\n      \n    \n    {\\displaystyle \\left|S-\\sum _{i=1}^{n}f(t_{i})\\Delta _{i}\\right|<\\varepsilon .}\n  \n\nWhen the chosen tags give the maximum (respectively, minimum) value of each interval, the Riemann sum becomes an upper (respectively, lower) Darboux sum, suggesting the close connection between the Riemann integral and the Darboux integral.\n\n\n=== Lebesgue integral ===\n\nIt is often of interest, both in theory and applications, to be able to pass to the limit under the integral. For instance, a sequence of functions can frequently be constructed that approximate, in a suitable sense, the solution to a problem. Then the integral of the solution function should be the limit of the integrals of the approximations. However, many functions that can be obtained as limits are not Riemann-integrable, and so such limit theorems do not hold with the Riemann integral. Therefore, it is of great importance to have a definition of the integral that allows a wider class of functions to be integrated (Rudin 1987).\nSuch an integral is the Lebesgue integral, that exploits the following fact to enlarge the class of integrable functions: if the values of a function are rearranged over the domain, the integral of a function should remain the same. Thus Henri Lebesgue introduced the integral bearing his name, explaining this integral thus in a letter to Paul Montel:\n\nI have to pay a certain sum, which I have collected in my pocket. I take the bills and coins out of my pocket and give them to the creditor in the order I find them until I have reached the total sum. This is the Riemann integral. But I can proceed differently. After I have taken all the money out of my pocket I order the bills and coins according to identical values and then I pay the several heaps one after the other to the creditor. This is my integral.\n\nAs Folland (1984, p. 56) puts it, \"To compute the Riemann integral of f, one partitions the domain [a, b] into subintervals\", while in the Lebesgue integral, \"one is in effect partitioning the range of f\". The definition of the Lebesgue integral thus begins with a measure, \u03bc. In the simplest case, the Lebesgue measure \u03bc(A) of an interval A = [a, b] is its width, b \u2212 a, so that the Lebesgue integral agrees with the (proper) Riemann integral when both exist. In more complicated cases, the sets being measured can be highly fragmented, with no continuity and no resemblance to intervals.\nUsing the \"partitioning the range of f\" philosophy, the integral of a non-negative function f : R \u2192 R should be the sum over t of the areas between a thin horizontal strip between y = t and y = t + dt. This area is just \u03bc{ x : f(x) > t}\u2009dt. Let f\u2217(t) = \u03bc{ x : f(x) > t}. The Lebesgue integral of f is then defined by (Lieb & Loss 2001)\n\n  \n    \n      \n        \u222b\n        f\n        =\n        \n          \u222b\n          \n            0\n          \n          \n            \u221e\n          \n        \n        \n          f\n          \n            \u2217\n          \n        \n        (\n        t\n        )\n        \n        d\n        t\n      \n    \n    {\\displaystyle \\int f=\\int _{0}^{\\infty }f^{*}(t)\\,dt}\n  \nwhere the integral on the right is an ordinary improper Riemann integral (f\u2217 is a strictly decreasing positive function, and therefore has a well-defined improper Riemann integral). For a suitable class of functions (the measurable functions) this defines the Lebesgue integral.\nA general measurable function f is Lebesgue-integrable if the area between the graph of f and the x-axis is finite:\n\n  \n    \n      \n        \n          \u222b\n          \n            E\n          \n        \n        \n          |\n        \n        f\n        \n          |\n        \n        \n        d\n        \u03bc\n        <\n        +\n        \u221e\n        .\n      \n    \n    {\\displaystyle \\int _{E}|f|\\,d\\mu <+\\infty .}\n  \nIn that case, the integral is, as in the Riemannian case, the difference between the area above the x-axis and the area below the x-axis:\n\n  \n    \n      \n        \n          \u222b\n          \n            E\n          \n        \n        f\n        \n        d\n        \u03bc\n        =\n        \n          \u222b\n          \n            E\n          \n        \n        \n          f\n          \n            +\n          \n        \n        \n        d\n        \u03bc\n        \u2212\n        \n          \u222b\n          \n            E\n          \n        \n        \n          f\n          \n            \u2212\n          \n        \n        \n        d\n        \u03bc\n      \n    \n    {\\displaystyle \\int _{E}f\\,d\\mu =\\int _{E}f^{+}\\,d\\mu -\\int _{E}f^{-}\\,d\\mu }\n  \nwhere\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  f\n                  \n                    +\n                  \n                \n                (\n                x\n                )\n                =\n                max\n                (\n                {\n                f\n                (\n                x\n                )\n                ,\n                0\n                }\n                )\n              \n              \n                \n                =\n                \n                  \n                    {\n                    \n                      \n                        \n                          f\n                          (\n                          x\n                          )\n                          ,\n                        \n                        \n                          \n                            if \n                          \n                          f\n                          (\n                          x\n                          )\n                          >\n                          0\n                          ,\n                        \n                      \n                      \n                        \n                          0\n                          ,\n                        \n                        \n                          \n                            otherwise,\n                          \n                        \n                      \n                    \n                    \n                  \n                \n              \n            \n            \n              \n                \n                  f\n                  \n                    \u2212\n                  \n                \n                (\n                x\n                )\n                =\n                max\n                (\n                {\n                \u2212\n                f\n                (\n                x\n                )\n                ,\n                0\n                }\n                )\n              \n              \n                \n                =\n                \n                  \n                    {\n                    \n                      \n                        \n                          \u2212\n                          f\n                          (\n                          x\n                          )\n                          ,\n                        \n                        \n                          \n                            if \n                          \n                          f\n                          (\n                          x\n                          )\n                          <\n                          0\n                          ,\n                        \n                      \n                      \n                        \n                          0\n                          ,\n                        \n                        \n                          \n                            otherwise.\n                          \n                        \n                      \n                    \n                    \n                  \n                \n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}f^{+}(x)=\\max(\\{f(x),0\\})&={\\begin{cases}f(x),&{\\text{if }}f(x)>0,\\\\0,&{\\text{otherwise,}}\\end{cases}}\\\\[4pt]f^{-}(x)=\\max(\\{-f(x),0\\})&={\\begin{cases}-f(x),&{\\text{if }}f(x)<0,\\\\0,&{\\text{otherwise.}}\\end{cases}}\\end{aligned}}}\n  \n\n\n=== Other integrals ===\nAlthough the Riemann and Lebesgue integrals are the most widely used definitions of the integral, a number of others exist, including:\nThe Darboux integral, which is constructed using Darboux sums and is equivalent to a Riemann integral, meaning that a function is Darboux-integrable if and only if it is Riemann-integrable. Darboux integrals have the advantage of being simpler to define than Riemann integrals.\nThe Riemann\u2013Stieltjes integral, an extension of the Riemann integral.\nThe Lebesgue\u2013Stieltjes integral, further developed by Johann Radon, which generalizes the Riemann\u2013Stieltjes and Lebesgue integrals.\nThe Daniell integral, which subsumes the Lebesgue integral and Lebesgue\u2013Stieltjes integral without the dependence on measures.\nThe Haar integral, used for integration on locally compact topological groups, introduced by Alfr\u00e9d Haar in 1933.\nThe Henstock\u2013Kurzweil integral, variously defined by Arnaud Denjoy, Oskar Perron, and (most elegantly, as the gauge integral) Jaroslav Kurzweil, and developed by Ralph Henstock.\nThe It\u014d integral and Stratonovich integral, which define integration with respect to semimartingales such as Brownian motion.\nThe Young integral, which is a kind of Riemann\u2013Stieltjes integral with respect to certain functions of unbounded variation.\nThe rough path integral, which is defined for functions equipped with some additional \"rough path\" structure and generalizes stochastic integration against both semimartingales and processes such as the fractional Brownian motion.\n\n\n== Properties ==\n\n\n=== Linearity ===\nThe collection of Riemann-integrable functions on a closed interval [a, b] forms a vector space under the operations of pointwise addition and multiplication by a scalar, and the operation of integration\n\n  \n    \n      \n        f\n        \u21a6\n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle f\\mapsto \\int _{a}^{b}f(x)\\;dx}\n  \nis a linear functional on this vector space. Thus, firstly, the collection of integrable functions is closed under taking linear combinations; and, secondly, the integral of a linear combination is the linear combination of the integrals,\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        (\n        \u03b1\n        f\n        +\n        \u03b2\n        g\n        )\n        (\n        x\n        )\n        \n        d\n        x\n        =\n        \u03b1\n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        +\n        \u03b2\n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        g\n        (\n        x\n        )\n        \n        d\n        x\n        .\n        \n      \n    \n    {\\displaystyle \\int _{a}^{b}(\\alpha f+\\beta g)(x)\\,dx=\\alpha \\int _{a}^{b}f(x)\\,dx+\\beta \\int _{a}^{b}g(x)\\,dx.\\,}\n  \nSimilarly, the set of real-valued Lebesgue-integrable functions on a given measure space E with measure \u03bc is closed under taking linear combinations and hence form a vector space, and the Lebesgue integral\n\n  \n    \n      \n        f\n        \u21a6\n        \n          \u222b\n          \n            E\n          \n        \n        f\n        \n        d\n        \u03bc\n      \n    \n    {\\displaystyle f\\mapsto \\int _{E}f\\,d\\mu }\n  \nis a linear functional on this vector space, so that\n\n  \n    \n      \n        \n          \u222b\n          \n            E\n          \n        \n        (\n        \u03b1\n        f\n        +\n        \u03b2\n        g\n        )\n        \n        d\n        \u03bc\n        =\n        \u03b1\n        \n          \u222b\n          \n            E\n          \n        \n        f\n        \n        d\n        \u03bc\n        +\n        \u03b2\n        \n          \u222b\n          \n            E\n          \n        \n        g\n        \n        d\n        \u03bc\n        .\n      \n    \n    {\\displaystyle \\int _{E}(\\alpha f+\\beta g)\\,d\\mu =\\alpha \\int _{E}f\\,d\\mu +\\beta \\int _{E}g\\,d\\mu .}\n  \nMore generally, consider the vector space of all measurable functions on a measure space (E,\u03bc), taking values in a locally compact complete topological vector space V over a locally compact topological field K, f : E \u2192 V. Then one may define an abstract integration map assigning to each function f an element of V or the symbol \u221e,\n\n  \n    \n      \n        f\n        \u21a6\n        \n          \u222b\n          \n            E\n          \n        \n        f\n        \n        d\n        \u03bc\n        ,\n        \n      \n    \n    {\\displaystyle f\\mapsto \\int _{E}f\\,d\\mu ,\\,}\n  \nthat is compatible with linear combinations. In this situation the linearity holds for the subspace of functions whose integral is an element of V (i.e. \"finite\"). The most important special cases arise when K is R, C, or a finite extension of the field Qp of p-adic numbers, and V is a finite-dimensional vector space over K, and when K = C and V is a complex Hilbert space.\nLinearity, together with some natural continuity properties and normalisation for a certain class of \"simple\" functions, may be used to give an alternative definition of the integral. This is the approach of Daniell for the case of real-valued functions on a set X, generalized by Nicolas Bourbaki to functions with values in a locally compact topological vector space. See (Hildebrandt 1953) for an axiomatic characterisation of the integral.\n\n\n=== Inequalities ===\nA number of general inequalities hold for Riemann-integrable functions defined on a closed and bounded interval [a, b] and can be generalized to other notions of integral (Lebesgue and Daniell).\nUpper and lower bounds. An integrable function f on [a, b], is necessarily bounded on that interval. Thus there are real numbers m and M so that m \u2264 f\u2009(x) \u2264 M for all x in [a, b]. Since the lower and upper sums of f over [a, b] are therefore bounded by, respectively, m(b \u2212 a) and M(b \u2212 a), it follows that\n\n  \n    \n      \n        m\n        (\n        b\n        \u2212\n        a\n        )\n        \u2264\n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        \u2264\n        M\n        (\n        b\n        \u2212\n        a\n        )\n        .\n      \n    \n    {\\displaystyle m(b-a)\\leq \\int _{a}^{b}f(x)\\,dx\\leq M(b-a).}\n  \n\nInequalities between functions. If f(x) \u2264 g(x) for each x in [a, b] then each of the upper and lower sums of f is bounded above by the upper and lower sums, respectively, of g. Thus\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        \u2264\n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        g\n        (\n        x\n        )\n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle \\int _{a}^{b}f(x)\\,dx\\leq \\int _{a}^{b}g(x)\\,dx.}\n  \n\nThis is a generalization of the above inequalities, as M(b \u2212 a) is the integral of the constant function with value M over [a, b].\nIn addition, if the inequality between functions is strict, then the inequality between integrals is also strict. That is, if f(x) < g(x) for each x in [a, b], then\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        <\n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        g\n        (\n        x\n        )\n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle \\int _{a}^{b}f(x)\\,dx<\\int _{a}^{b}g(x)\\,dx.}\n  \n\nSubintervals. If [c, d] is a subinterval of [a, b] and f(x) is non-negative for all x, then\n\n  \n    \n      \n        \n          \u222b\n          \n            c\n          \n          \n            d\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        \u2264\n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle \\int _{c}^{d}f(x)\\,dx\\leq \\int _{a}^{b}f(x)\\,dx.}\n  \n\nProducts and absolute values of functions. If f and g are two functions then we may consider their pointwise products and powers, and absolute values:\n\n  \n    \n      \n        (\n        f\n        g\n        )\n        (\n        x\n        )\n        =\n        f\n        (\n        x\n        )\n        g\n        (\n        x\n        )\n        ,\n        \n        \n          f\n          \n            2\n          \n        \n        (\n        x\n        )\n        =\n        (\n        f\n        (\n        x\n        )\n        \n          )\n          \n            2\n          \n        \n        ,\n        \n        \n          |\n        \n        f\n        \n          |\n        \n        (\n        x\n        )\n        =\n        \n          |\n        \n        f\n        (\n        x\n        )\n        \n          |\n        \n        .\n        \n      \n    \n    {\\displaystyle (fg)(x)=f(x)g(x),\\;f^{2}(x)=(f(x))^{2},\\;|f|(x)=|f(x)|.\\,}\n  \n\nIf f is Riemann-integrable on [a, b] then the same is true for |\u200af\u200a|, and\n\n  \n    \n      \n        \n          |\n          \n            \u222b\n            \n              a\n            \n            \n              b\n            \n          \n          f\n          (\n          x\n          )\n          \n          d\n          x\n          |\n        \n        \u2264\n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        \n          |\n        \n        f\n        (\n        x\n        )\n        \n          |\n        \n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle \\left|\\int _{a}^{b}f(x)\\,dx\\right|\\leq \\int _{a}^{b}|f(x)|\\,dx.}\n  \n\nMoreover, if f and g are both Riemann-integrable then fg is also Riemann-integrable, and\n\n  \n    \n      \n        \n          \n            (\n            \n              \u222b\n              \n                a\n              \n              \n                b\n              \n            \n            (\n            f\n            g\n            )\n            (\n            x\n            )\n            \n            d\n            x\n            )\n          \n          \n            2\n          \n        \n        \u2264\n        \n          (\n          \n            \u222b\n            \n              a\n            \n            \n              b\n            \n          \n          f\n          (\n          x\n          \n            )\n            \n              2\n            \n          \n          \n          d\n          x\n          )\n        \n        \n          (\n          \n            \u222b\n            \n              a\n            \n            \n              b\n            \n          \n          g\n          (\n          x\n          \n            )\n            \n              2\n            \n          \n          \n          d\n          x\n          )\n        \n        .\n      \n    \n    {\\displaystyle \\left(\\int _{a}^{b}(fg)(x)\\,dx\\right)^{2}\\leq \\left(\\int _{a}^{b}f(x)^{2}\\,dx\\right)\\left(\\int _{a}^{b}g(x)^{2}\\,dx\\right).}\n  \n\nThis inequality, known as the Cauchy\u2013Schwarz inequality, plays a prominent role in Hilbert space theory, where the left hand side is interpreted as the inner product of two square-integrable functions f and g on the interval [a, b].\nH\u00f6lder's inequality. Suppose that p and q are two real numbers, 1 \u2264 p, q \u2264 \u221e with 1/p + 1/q = 1, and f and g are two Riemann-integrable functions. Then the functions |\u200af\u200a|p and |\u200ag\u200a|q are also integrable and the following H\u00f6lder's inequality holds:\n\n  \n    \n      \n        \n          |\n          \u222b\n          f\n          (\n          x\n          )\n          g\n          (\n          x\n          )\n          \n          d\n          x\n          |\n        \n        \u2264\n        \n          \n            (\n            \u222b\n            \n              \n                |\n                f\n                (\n                x\n                )\n                |\n              \n              \n                p\n              \n            \n            \n            d\n            x\n            )\n          \n          \n            1\n            \n              /\n            \n            p\n          \n        \n        \n          \n            (\n            \u222b\n            \n              \n                |\n                g\n                (\n                x\n                )\n                |\n              \n              \n                q\n              \n            \n            \n            d\n            x\n            )\n          \n          \n            1\n            \n              /\n            \n            q\n          \n        \n        .\n      \n    \n    {\\displaystyle \\left|\\int f(x)g(x)\\,dx\\right|\\leq \\left(\\int \\left|f(x)\\right|^{p}\\,dx\\right)^{1/p}\\left(\\int \\left|g(x)\\right|^{q}\\,dx\\right)^{1/q}.}\n  \nFor p = q = 2, H\u00f6lder's inequality becomes the Cauchy\u2013Schwarz inequality.\nMinkowski inequality. Suppose that p \u2265 1 is a real number and f and g are Riemann-integrable functions. Then |\u200a f \u200a|p, |\u200a g \u200a|p and |\u200a f + g \u200a|p are also Riemann-integrable and the following Minkowski inequality holds:\n\n  \n    \n      \n        \n          \n            (\n            \u222b\n            \n              \n                |\n                f\n                (\n                x\n                )\n                +\n                g\n                (\n                x\n                )\n                |\n              \n              \n                p\n              \n            \n            \n            d\n            x\n            )\n          \n          \n            1\n            \n              /\n            \n            p\n          \n        \n        \u2264\n        \n          \n            (\n            \u222b\n            \n              \n                |\n                f\n                (\n                x\n                )\n                |\n              \n              \n                p\n              \n            \n            \n            d\n            x\n            )\n          \n          \n            1\n            \n              /\n            \n            p\n          \n        \n        +\n        \n          \n            (\n            \u222b\n            \n              \n                |\n                g\n                (\n                x\n                )\n                |\n              \n              \n                p\n              \n            \n            \n            d\n            x\n            )\n          \n          \n            1\n            \n              /\n            \n            p\n          \n        \n        .\n      \n    \n    {\\displaystyle \\left(\\int \\left|f(x)+g(x)\\right|^{p}\\,dx\\right)^{1/p}\\leq \\left(\\int \\left|f(x)\\right|^{p}\\,dx\\right)^{1/p}+\\left(\\int \\left|g(x)\\right|^{p}\\,dx\\right)^{1/p}.}\n  \nAn analogue of this inequality for Lebesgue integral is used in construction of Lp spaces.\n\n\n=== Conventions ===\nIn this section f is a real-valued Riemann-integrable function. The integral\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle \\int _{a}^{b}f(x)\\,dx}\n  \nover an interval [a, b] is defined if a < b. This means that the upper and lower sums of the function f are evaluated on a partition a = x0 \u2264 x1 \u2264 . . . \u2264 xn = b whose values xi are increasing. Geometrically, this signifies that integration takes place \"left to right\", evaluating f within intervals [x\u2009i\u2009, x\u2009i\u2009+1] where an interval with a higher index lies to the right of one with a lower index. The values a and b, the end-points of the interval, are called the limits of integration of f. Integrals can also be defined if a > b:\nReversing limits of integration. If a > b then define\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        =\n        \u2212\n        \n          \u222b\n          \n            b\n          \n          \n            a\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle \\int _{a}^{b}f(x)\\,dx=-\\int _{b}^{a}f(x)\\,dx.}\n  \n\nThis, with a = b, implies:\nIntegrals over intervals of length zero. If a is a real number then\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            a\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        =\n        0.\n      \n    \n    {\\displaystyle \\int _{a}^{a}f(x)\\,dx=0.}\n  \n\nThe first convention is necessary in consideration of taking integrals over subintervals of [a, b]; the second says that an integral taken over a degenerate interval, or a point, should be zero. One reason for the first convention is that the integrability of f on an interval [a, b] implies that f is integrable on any subinterval [c, d], but in particular integrals have the property that:\nAdditivity of integration on intervals. If c is any element of [a, b], then\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        =\n        \n          \u222b\n          \n            a\n          \n          \n            c\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        +\n        \n          \u222b\n          \n            c\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle \\int _{a}^{b}f(x)\\,dx=\\int _{a}^{c}f(x)\\,dx+\\int _{c}^{b}f(x)\\,dx.}\n  \n\nWith the first convention, the resulting relation\n\n  \n    \n      \n        \n          \n            \n              \n                \n                  \u222b\n                  \n                    a\n                  \n                  \n                    c\n                  \n                \n                f\n                (\n                x\n                )\n                \n                d\n                x\n              \n              \n                \n                \n\n                \n                =\n                \n                  \u222b\n                  \n                    a\n                  \n                  \n                    b\n                  \n                \n                f\n                (\n                x\n                )\n                \n                d\n                x\n                \u2212\n                \n                  \u222b\n                  \n                    c\n                  \n                  \n                    b\n                  \n                \n                f\n                (\n                x\n                )\n                \n                d\n                x\n              \n            \n            \n              \n              \n                \n                \n\n                \n                =\n                \n                  \u222b\n                  \n                    a\n                  \n                  \n                    b\n                  \n                \n                f\n                (\n                x\n                )\n                \n                d\n                x\n                +\n                \n                  \u222b\n                  \n                    b\n                  \n                  \n                    c\n                  \n                \n                f\n                (\n                x\n                )\n                \n                d\n                x\n              \n            \n          \n        \n      \n    \n    {\\displaystyle {\\begin{aligned}\\int _{a}^{c}f(x)\\,dx&{}=\\int _{a}^{b}f(x)\\,dx-\\int _{c}^{b}f(x)\\,dx\\\\&{}=\\int _{a}^{b}f(x)\\,dx+\\int _{b}^{c}f(x)\\,dx\\end{aligned}}}\n  \nis then well-defined for any cyclic permutation of a, b, and c.\n\n\n== Fundamental theorem of calculus ==\n\nThe fundamental theorem of calculus is the statement that differentiation and integration are inverse operations: if a continuous function is first integrated and then differentiated, the original function is retrieved. An important consequence, sometimes called the second fundamental theorem of calculus, allows one to compute integrals by using an antiderivative of the function to be integrated.\n\n\n=== Statements of theorems ===\n\n\n==== Fundamental theorem of calculus ====\nLet f be a continuous real-valued function defined on a closed interval [a, b]. Let F be the function defined, for all x in [a, b], by\n\n  \n    \n      \n        F\n        (\n        x\n        )\n        =\n        \n          \u222b\n          \n            a\n          \n          \n            x\n          \n        \n        f\n        (\n        t\n        )\n        \n        d\n        t\n        .\n      \n    \n    {\\displaystyle F(x)=\\int _{a}^{x}f(t)\\,dt.}\n  \nThen, F is continuous on [a, b], differentiable on the open interval (a, b), and\n\n  \n    \n      \n        \n          F\n          \u2032\n        \n        (\n        x\n        )\n        =\n        f\n        (\n        x\n        )\n      \n    \n    {\\displaystyle F'(x)=f(x)}\n  \nfor all x in (a, b).\n\n\n==== Second fundamental theorem of calculus ====\nLet f be a real-valued function defined on a closed interval [a, b] that admits an antiderivative F on [a, b]. That is, f and F are functions such that for all x in [a, b],\n\n  \n    \n      \n        f\n        (\n        x\n        )\n        =\n        \n          F\n          \u2032\n        \n        (\n        x\n        )\n        .\n      \n    \n    {\\displaystyle f(x)=F'(x).}\n  \nIf f is integrable on [a, b] then\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        =\n        F\n        (\n        b\n        )\n        \u2212\n        F\n        (\n        a\n        )\n        .\n      \n    \n    {\\displaystyle \\int _{a}^{b}f(x)\\,dx=F(b)-F(a).}\n  \n\n\n=== Calculating integrals ===\nThe second fundamental theorem allows many integrals to be calculated explicitly. For example, to calculate the integral\n\n  \n    \n      \n        \n          \u222b\n          \n            0\n          \n          \n            1\n          \n        \n        \n          x\n          \n            1\n            \n              /\n            \n            2\n          \n        \n        \n        d\n        x\n        ,\n      \n    \n    {\\displaystyle \\int _{0}^{1}x^{1/2}\\,dx,}\n  \nof the square root function f(x) = x1/2 between 0 and 1, it is sufficient to find an antiderivative, that is, a function F(x) whose derivative equals f(x):\n\n  \n    \n      \n        \n          F\n          \u2032\n        \n        (\n        x\n        )\n        =\n        f\n        (\n        x\n        )\n        .\n      \n    \n    {\\displaystyle F'(x)=f(x).}\n  \nOne such function is F(x) = 2/3x3/2. Then the value of the integral in question is\n\n  \n    \n      \n        \n          \u222b\n          \n            0\n          \n          \n            1\n          \n        \n        \n          x\n          \n            1\n            \n              /\n            \n            2\n          \n        \n        \n        d\n        x\n        =\n        F\n        (\n        1\n        )\n        \u2212\n        F\n        (\n        0\n        )\n        =\n        \n          \n            2\n            3\n          \n        \n        (\n        1\n        \n          )\n          \n            3\n            \n              /\n            \n            2\n          \n        \n        \u2212\n        \n          \n            2\n            3\n          \n        \n        (\n        0\n        \n          )\n          \n            3\n            \n              /\n            \n            2\n          \n        \n        =\n        \n          \n            2\n            3\n          \n        \n        .\n      \n    \n    {\\displaystyle \\int _{0}^{1}x^{1/2}\\,dx=F(1)-F(0)={\\frac {2}{3}}(1)^{3/2}-{\\frac {2}{3}}(0)^{3/2}={\\frac {2}{3}}.}\n  \nThis is a case of a general rule, that for f(x) = xq, with q \u2260 \u22121, the related function, the so-called antiderivative is F(x) = xq + 1/(q + 1). Tables of this and similar antiderivatives can be used to calculate integrals explicitly, in much the same way that tables of derivatives can be used.\n\n\n== Extensions ==\n\n\n=== Improper integrals ===\n\nA \"proper\" Riemann integral assumes the integrand is defined and finite on a closed and bounded interval, bracketed by the limits of integration. An improper integral occurs when one or more of these conditions is not satisfied. In some cases such integrals may be defined by considering the limit of a sequence of proper Riemann integrals on progressively larger intervals.\nIf the interval is unbounded, for instance at its upper end, then the improper integral is the limit as that endpoint goes to infinity.\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            \u221e\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        =\n        \n          lim\n          \n            b\n            \u2192\n            \u221e\n          \n        \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle \\int _{a}^{\\infty }f(x)\\,dx=\\lim _{b\\to \\infty }\\int _{a}^{b}f(x)\\,dx}\n  \nIf the integrand is only defined or finite on a half-open interval, for instance (a, b], then again a limit may provide a finite result.\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        =\n        \n          lim\n          \n            \u03f5\n            \u2192\n            0\n          \n        \n        \n          \u222b\n          \n            a\n            +\n            \u03f5\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n      \n    \n    {\\displaystyle \\int _{a}^{b}f(x)\\,dx=\\lim _{\\epsilon \\to 0}\\int _{a+\\epsilon }^{b}f(x)\\,dx}\n  \nThat is, the improper integral is the limit of proper integrals as one endpoint of the interval of integration approaches either a specified real number, or \u221e, or \u2212\u221e. In more complicated cases, limits are required at both endpoints, or at interior points.\n\n\n=== Multiple integration ===\n\nJust as the definite integral of a positive function of one variable represents the area of the region between the graph of the function and the x-axis, the double integral of a positive function of two variables represents the volume of the region between the surface defined by the function and the plane which contains its domain. For example, a function in two dimensions depends on two real variables, x and y, and the integral of a function f over the rectangle R given as the Cartesian product of two intervals \n  \n    \n      \n        R\n        =\n        [\n        a\n        ,\n        b\n        ]\n        \u00d7\n        [\n        c\n        ,\n        d\n        ]\n      \n    \n    {\\displaystyle R=[a,b]\\times [c,d]}\n   can be written\n\n  \n    \n      \n        \n          \u222b\n          \n            R\n          \n        \n        f\n        (\n        x\n        ,\n        y\n        )\n        \n        d\n        A\n      \n    \n    {\\displaystyle \\int _{R}f(x,y)\\,dA}\n  \nwhere the differential dA indicates that integration is taken with respect to area. This double integral can be defined using Riemann sums, and represents the (signed) volume under the graph of z = f(x,y) over the domain R. Under suitable conditions (e.g., if f is continuous), then Fubini's theorem guarantees that this integral can be expressed as an equivalent iterated integral\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        \n          [\n          \n            \u222b\n            \n              c\n            \n            \n              d\n            \n          \n          f\n          (\n          x\n          ,\n          y\n          )\n          \n          d\n          y\n          ]\n        \n        \n        d\n        x\n        .\n      \n    \n    {\\displaystyle \\int _{a}^{b}\\left[\\int _{c}^{d}f(x,y)\\,dy\\right]\\,dx.}\n  \nThis reduces the problem of computing a double integral to computing one-dimensional integrals. Because of this, another notation for the integral over R uses a double integral sign:\n\n  \n    \n      \n        \n          \u222c\n          \n            R\n          \n        \n        f\n        (\n        x\n        ,\n        y\n        )\n        d\n        A\n        .\n      \n    \n    {\\displaystyle \\iint _{R}f(x,y)dA.}\n  \nIntegration over more general domains is possible. The integral of a function f, with respect to volume, over a subset D of \u211dn is denoted by notation such as\n\n  \n    \n      \n        \n          \u222b\n          \n            D\n          \n        \n        f\n        (\n        \n          x\n        \n        )\n        \n          d\n          \n            n\n          \n        \n        \n          x\n        \n        ,\n        \n        \n          \u222b\n          \n            D\n          \n        \n        f\n        \n        d\n        V\n      \n    \n    {\\displaystyle \\int _{D}f(\\mathbf {x} )d^{n}\\mathbf {x} ,\\quad \\int _{D}f\\,dV}\n  \nor similar. See volume integral.\n\n\n=== Line integrals ===\n\nThe concept of an integral can be extended to more general domains of integration, such as curved lines and surfaces. Such integrals are known as line integrals and surface integrals respectively. These have important applications in physics, as when dealing with vector fields.\nA line integral (sometimes called a path integral) is an integral where the function to be integrated is evaluated along a curve. Various different line integrals are in use. In the case of a closed curve it is also called a contour integral.\nThe function to be integrated may be a scalar field or a vector field. The value of the line integral is the sum of values of the field at all points on the curve, weighted by some scalar function on the curve (commonly arc length or, for a vector field, the scalar product of the vector field with a differential vector in the curve). This weighting distinguishes the line integral from simpler integrals defined on intervals. Many simple formulas in physics have natural continuous analogs in terms of line integrals; for example, the fact that work is equal to force, F, multiplied by displacement, s, may be expressed (in terms of vector quantities) as:\n\n  \n    \n      \n        W\n        =\n        \n          F\n        \n        \u22c5\n        \n          s\n        \n        .\n      \n    \n    {\\displaystyle W=\\mathbf {F} \\cdot \\mathbf {s} .}\n  \nFor an object moving along a path C in a vector field F such as an electric field or gravitational field, the total work done by the field on the object is obtained by summing up the differential work done in moving from s to s + ds. This gives the line integral\n\n  \n    \n      \n        W\n        =\n        \n          \u222b\n          \n            C\n          \n        \n        \n          F\n        \n        \u22c5\n        d\n        \n          s\n        \n        .\n      \n    \n    {\\displaystyle W=\\int _{C}\\mathbf {F} \\cdot d\\mathbf {s} .}\n  \n\n\n=== Surface integrals ===\n\nA surface integral is a definite integral taken over a surface (which may be a curved set in space); it can be thought of as the double integral analog of the line integral. The function to be integrated may be a scalar field or a vector field. The value of the surface integral is the sum of the field at all points on the surface. This can be achieved by splitting the surface into surface elements, which provide the partitioning for Riemann sums.\nFor an example of applications of surface integrals, consider a vector field v on a surface S; that is, for each point x in S, v(x) is a vector. Imagine that we have a fluid flowing through S, such that v(x) determines the velocity of the fluid at x. The flux is defined as the quantity of fluid flowing through S in unit amount of time. To find the flux, we need to take the dot product of v with the unit surface normal to S at each point, which will give us a scalar field, which we integrate over the surface:\n\n  \n    \n      \n        \n          \u222b\n          \n            S\n          \n        \n        \n          \n            v\n          \n        \n        \u22c5\n        \n        d\n        \n          \n            S\n          \n        \n        .\n      \n    \n    {\\displaystyle \\int _{S}{\\mathbf {v} }\\cdot \\,d{\\mathbf {S} }.}\n  \nThe fluid flux in this example may be from a physical fluid such as water or air, or from electrical or magnetic flux. Thus surface integrals have applications in physics, particularly with the classical theory of electromagnetism.\n\n\n=== Contour integrals ===\nIn complex analysis, the integrand is a complex-valued function of a complex variable z instead of a real function of a real variable x. When a complex function is integrated along a curve \n  \n    \n      \n        \u03b3\n      \n    \n    {\\displaystyle \\gamma }\n   in the complex plane, the integral is denoted as follows\n\n  \n    \n      \n        \n          \u222b\n          \n            \u03b3\n          \n        \n        f\n        (\n        z\n        )\n        \n        d\n        z\n        .\n      \n    \n    {\\displaystyle \\int _{\\gamma }f(z)\\,dz.}\n  \nThis is known as a contour integral.\n\n\n=== Integrals of differential forms ===\n\nA differential form is a mathematical concept in the fields of multivariable calculus, differential topology, and tensors. Differential forms are organized by degree. For example, a one-form is a weighted sum of the differentials of the coordinates, such as:\n\n  \n    \n      \n        E\n        (\n        x\n        ,\n        y\n        ,\n        z\n        )\n        \n        d\n        x\n        +\n        F\n        (\n        x\n        ,\n        y\n        ,\n        z\n        )\n        \n        d\n        y\n        +\n        G\n        (\n        x\n        ,\n        y\n        ,\n        z\n        )\n        \n        d\n        z\n      \n    \n    {\\displaystyle E(x,y,z)\\,dx+F(x,y,z)\\,dy+G(x,y,z)\\,dz}\n  \nwhere E, F, G are functions in three dimensions. A differential one-form can be integrated over an oriented path, and the resulting integral is just another way of writing a line integral. Here the basic differentials dx, dy, dz measure infinitesimal oriented lengths parallel to the three coordinate axes.\nA differential two-form is a sum of the form\n\n  \n    \n      \n        G\n        (\n        x\n        ,\n        y\n        ,\n        z\n        )\n        \n        d\n        x\n        \u2227\n        d\n        y\n        +\n        E\n        (\n        x\n        ,\n        y\n        ,\n        z\n        )\n        \n        d\n        y\n        \u2227\n        d\n        z\n        +\n        F\n        (\n        x\n        ,\n        y\n        ,\n        z\n        )\n        \n        d\n        z\n        \u2227\n        d\n        x\n        .\n      \n    \n    {\\displaystyle G(x,y,z)\\,dx\\wedge dy+E(x,y,z)\\,dy\\wedge dz+F(x,y,z)\\,dz\\wedge dx.}\n  \nHere the basic two-forms \n  \n    \n      \n        d\n        x\n        \u2227\n        d\n        y\n        ,\n        d\n        z\n        \u2227\n        d\n        x\n        ,\n        d\n        y\n        \u2227\n        d\n        z\n      \n    \n    {\\displaystyle dx\\wedge dy,dz\\wedge dx,dy\\wedge dz}\n   measure oriented areas parallel to the coordinate two-planes. The symbol \n  \n    \n      \n        \u2227\n      \n    \n    {\\displaystyle \\wedge }\n   denotes the wedge product, which is similar to the cross product in the sense that the wedge product of two forms representing oriented lengths represents an oriented area. A two-form can be integrated over an oriented surface, and the resulting integral is equivalent to the surface integral giving the flux of \n  \n    \n      \n        E\n        \n          i\n        \n        +\n        F\n        \n          j\n        \n        +\n        G\n        \n          k\n        \n        .\n      \n    \n    {\\displaystyle E\\mathbf {i} +F\\mathbf {j} +G\\mathbf {k} .}\n  \nUnlike the cross product, and the three-dimensional vector calculus, the wedge product and the calculus of differential forms makes sense in arbitrary dimension and on more general manifolds (curves, surfaces, and their higher-dimensional analogs). The exterior derivative plays the role of the gradient and curl of vector calculus, and Stokes' theorem simultaneously generalizes the three theorems of vector calculus: the divergence theorem, Green's theorem, and the Kelvin\u2013Stokes theorem.\n\n\n=== Summations ===\nThe discrete equivalent of integration is summation. Summations and integrals can be put on the same foundations using the theory of Lebesgue integrals or time scale calculus.\n\n\n== Computation ==\n\n\n=== Analytical ===\nThe most basic technique for computing definite integrals of one real variable is based on the fundamental theorem of calculus. Let f(x) be the function of x to be integrated over a given interval [a, b]. Then, find an antiderivative of f; that is, a function F such that F\u2032 = f on the interval. Provided the integrand and integral have no singularities on the path of integration, by the fundamental theorem of calculus,\n\n  \n    \n      \n        \n          \u222b\n          \n            a\n          \n          \n            b\n          \n        \n        f\n        (\n        x\n        )\n        \n        d\n        x\n        =\n        F\n        (\n        b\n        )\n        \u2212\n        F\n        (\n        a\n        )\n        .\n      \n    \n    {\\displaystyle \\int _{a}^{b}f(x)\\,dx=F(b)-F(a).}\n  \nThe integral is not actually the antiderivative, but the fundamental theorem provides a way to use antiderivatives to evaluate definite integrals.\nThe most difficult step is usually to find the antiderivative of f. It is rarely possible to glance at a function and write down its antiderivative. More often, it is necessary to use one of the many techniques that have been developed to evaluate integrals. Most of these techniques rewrite one integral as a different one which is hopefully more tractable. Techniques include:\nIntegration by substitution\nIntegration by parts\nInverse function integration\nChanging the order of integration\nIntegration by trigonometric substitution\nTangent half-angle substitution\nIntegration by partial fractions\nIntegration by reduction formulae\nIntegration using parametric derivatives\nIntegration using Euler's formula\nEuler substitution\nDifferentiation under the integral sign\nContour integration\nAlternative methods exist to compute more complex integrals. Many nonelementary integrals can be expanded in a Taylor series and integrated term by term. Occasionally, the resulting infinite series can be summed analytically. The method of convolution using Meijer G-functions can also be used, assuming that the integrand can be written as a product of Meijer G-functions. There are also many less common ways of calculating definite integrals; for instance, Parseval's identity can be used to transform an integral over a rectangular region into an infinite sum. Occasionally, an integral can be evaluated by a trick; for an example of this, see Gaussian integral.\nComputations of volumes of solids of revolution can usually be done with disk integration or shell integration.\nSpecific results which have been worked out by various techniques are collected in the list of integrals.\n\n\n=== Symbolic ===\n\nMany problems in mathematics, physics, and engineering involve integration where an explicit formula for the integral is desired. Extensive tables of integrals have been compiled and published over the years for this purpose. With the spread of computers, many professionals, educators, and students have turned to computer algebra systems that are specifically designed to perform difficult or tedious tasks, including integration. Symbolic integration has been one of the motivations for the development of the first such systems, like Macsyma.\nA major mathematical difficulty in symbolic integration is that in many cases, a closed formula for the antiderivative of a rather simple-looking function does not exist. For instance, it is known that the antiderivatives of the functions exp(x2), xx and (sin x)/x cannot be expressed in the closed form involving only rational and exponential functions, logarithm, trigonometric and inverse trigonometric functions, and the operations of multiplication and composition; in other words, none of the three given functions is integrable in elementary functions, which are the functions which may be built from rational functions, roots of a polynomial, logarithm, and exponential functions. The Risch algorithm provides a general criterion to determine whether the antiderivative of an elementary function is elementary, and, if it is, to compute it. Unfortunately, it turns out that functions with closed expressions of antiderivatives are the exception rather than the rule. Consequently, computerized algebra systems have no hope of being able to find an antiderivative for a randomly constructed elementary function. On the positive side, if the 'building blocks' for antiderivatives are fixed in advance, it may be still be possible to decide whether the antiderivative of a given function can be expressed using these blocks and operations of multiplication and composition, and to find the symbolic answer whenever it exists. The Risch algorithm, implemented in Mathematica and other computer algebra systems, does just that for functions and antiderivatives built from rational functions, radicals, logarithm, and exponential functions.\nSome special integrands occur often enough to warrant special study. In particular, it may be useful to have, in the set of antiderivatives, the special functions of physics (like the Legendre functions, the hypergeometric function, the Gamma function, the Incomplete Gamma function and so on \u2014 see Symbolic integration for more details). Extending the Risch's algorithm to include such functions is possible but challenging and has been an active research subject.\nMore recently a new approach has emerged, using D-finite functions, which are the solutions of linear differential equations with polynomial coefficients. Most of the elementary and special functions are D-finite, and the integral of a D-finite function is also a D-finite function. This provides an algorithm to express the antiderivative of a D-finite function as the solution of a differential equation.\nThis theory also allows one to compute the definite integral of a D-function as the sum of a series given by the first coefficients, and provides an algorithm to compute any coefficient.\n\n\n=== Numerical ===\n\nSome integrals found in real applications can be computed by closed-form antiderivatives. Others are not so accommodating. Some antiderivatives do not have closed forms, some closed forms require special functions which themselves are a challenge to compute, and others are so complex that finding the exact answer is too slow. This motivates the study and application of numerical approximations of integrals. This subject, called numerical integration or numerical quadrature, arose early in the study of integration for the purpose of making hand calculations. The development of general-purpose computers made numerical integration more practical and drove a desire for improvements. The goals of numerical integration are accuracy, reliability, efficiency, and generality, and sophisticated modern methods can vastly outperform a naive method by all four measures (Dahlquist & Bj\u00f6rck 2008; Kahaner, Moler & Nash 1989; Stoer & Bulirsch 2002).\nConsider, for example, the integral\n\n  \n    \n      \n        \n          \u222b\n          \n            \u2212\n            2\n          \n          \n            2\n          \n        \n        \n          \n            \n              1\n              5\n            \n          \n        \n        \n          (\n          \n            \n              \n                1\n                100\n              \n            \n          \n          (\n          322\n          +\n          3\n          x\n          (\n          98\n          +\n          x\n          (\n          37\n          +\n          x\n          )\n          )\n          )\n          \u2212\n          24\n          \n            \n              x\n              \n                1\n                +\n                \n                  x\n                  \n                    2\n                  \n                \n              \n            \n          \n          )\n        \n        d\n        x\n      \n    \n    {\\displaystyle \\int _{-2}^{2}{\\tfrac {1}{5}}\\left({\\tfrac {1}{100}}(322+3x(98+x(37+x)))-24{\\frac {x}{1+x^{2}}}\\right)dx}\n  \nwhich has the exact answer 94/25 = 3.76. (In ordinary practice the answer is not known in advance, so an important task \u2014 not explored here \u2014 is to decide when an approximation is good enough.) A \u201ccalculus book\u201d approach divides the integration range into, say, 16 equal pieces, and computes function values.\n\nUsing the left end of each piece, the rectangle method sums 16 function values and multiplies by the step width, h, here 0.25, to get an approximate value of 3.94325 for the integral. The accuracy is not impressive, but calculus formally uses pieces of infinitesimal width, so initially this may seem little cause for concern. Indeed, repeatedly doubling the number of steps eventually produces an approximation of 3.76001. However, 218 pieces are required, a great computational expense for such little accuracy; and a reach for greater accuracy can force steps so small that arithmetic precision becomes an obstacle.\nA better approach replaces the rectangles used in a Riemann sum with trapezoids. The trapezoid rule is almost as easy to calculate; it sums all 17 function values, but weights the first and last by one half, and again multiplies by the step width. This immediately improves the approximation to 3.76925, which is noticeably more accurate. Furthermore, only 210 pieces are needed to achieve 3.76000, substantially less computation than the rectangle method for comparable accuracy. The idea behind the trapezoid rule, that more accurate approximations to the function yield better approximations to the integral, can be carried further. Simpson's rule approximates the integrand by a piecewise quadratic function. Riemann sums, the trapezoid rule, and Simpson's rule are examples of a family of quadrature rules called Newton\u2013Cotes formulas. The degree n Newton\u2013Cotes quadrature rule approximates the polynomial on each subinterval by a degree n polynomial. This polynomial is chosen to interpolate the values of the function on the interval. Higher degree Newton-Cotes approximations can be more accurate, but they require more function evaluations (already Simpson's rule requires twice the function evaluations of the trapezoid rule), and they can suffer from numerical inaccuracy due to Runge's phenomenon. One solution to this problem is Clenshaw\u2013Curtis quadrature. In Clenshaw\u2013Curtis quadrature, the integrand is approximated by expanding it in terms of Chebyshev polynomials. This produces an approximation whose values never deviate far from those of the original function.\nRomberg's method builds on the trapezoid method to great effect. First, the step lengths are halved incrementally, giving trapezoid approximations denoted by T(h0), T(h1), and so on, where hk+1 is half of hk. For each new step size, only half the new function values need to be computed; the others carry over from the previous size (as shown in the table above). But the really powerful idea is to interpolate a polynomial through the approximations, and extrapolate to T(0). With this method a numerically exact answer here requires only four pieces (five function values). The Lagrange polynomial interpolating {hk,T(hk)}k = 0...2 = {(4.00,6.128), (2.00,4.352), (1.00,3.908)} is 3.76 + 0.148h2, producing the extrapolated value 3.76 at h = 0.\nGaussian quadrature often requires noticeably less work for superior accuracy. In this example, it can compute the function values at just two x positions, \u00b12 \u2044 \u221a3, then double each value and sum to get the numerically exact answer. The explanation for this dramatic success lies in the choice of points. Unlike Newton\u2013Cotes rules, which interpolate the integrand at evenly spaced points, Gaussian quadrature evaluates the function at the roots of a set of orthogonal polynomials. An n-point Gaussian method is exact for polynomials of degree up to 2n \u2212 1. The function in this example is a degree 3 polynomial, plus a term that cancels because the chosen endpoints are symmetric around zero. (Cancellation also benefits the Romberg method.)\nIn practice, each method must use extra evaluations to ensure an error bound on an unknown function; this tends to offset some of the advantage of the pure Gaussian method, and motivates the popular Gauss\u2013Kronrod quadrature formulae. More broadly, adaptive quadrature partitions a range into pieces based on function properties, so that data points are concentrated where they are needed most.\nThe computation of higher-dimensional integrals (for example, volume calculations) makes important use of such alternatives as Monte Carlo integration.\nA calculus text is no substitute for numerical analysis, but the reverse is also true. Even the best adaptive numerical code sometimes requires a user to help with the more demanding integrals. For example, improper integrals may require a change of variable or methods that can avoid infinite function values, and known properties like symmetry and periodicity may provide critical leverage. For example, the integral \n  \n    \n      \n        \n          \u222b\n          \n            0\n          \n          \n            1\n          \n        \n        \n          x\n          \n            \u2212\n            1\n            \n              /\n            \n            2\n          \n        \n        \n          e\n          \n            \u2212\n            x\n          \n        \n        \n        d\n        x\n      \n    \n    {\\displaystyle \\int _{0}^{1}x^{-1/2}e^{-x}\\,dx}\n   is difficult to evaluate numerically because it is infinite at x = 0. However, the substitution u = \u221ax transforms the integral into \n  \n    \n      \n        \n          \u222b\n          \n            0\n          \n          \n            1\n          \n        \n        \n          e\n          \n            \u2212\n            \n              u\n              \n                2\n              \n            \n          \n        \n        \n        d\n        u\n      \n    \n    {\\displaystyle \\int _{0}^{1}e^{-u^{2}}\\,du}\n  , which has no singularities at all.\n\n\n=== Mechanical ===\nThe area of an arbitrary two-dimensional shape can be determined using a measuring instrument called planimeter. The volume of irregular objects can be measured with precision by the fluid displaced as the object is submerged.\n\n\n=== Geometrical ===\n\nArea can be found via geometrical compass-and-straightedge constructions of an equivalent square, e.g., squaring the circle.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\nHazewinkel, Michiel, ed. (2001), \"Integral\", Encyclopedia of Mathematics, Springer, ISBN 978-1-55608-010-4 \nOnline Integral Calculator, Wolfram Alpha.\nIntroduction to definite integrals by Khan Academy\n\n\n=== Online books ===\nKeisler, H. Jerome, Elementary Calculus: An Approach Using Infinitesimals, University of Wisconsin\nStroyan, K.D., A Brief Introduction to Infinitesimal Calculus, University of Iowa\nMauch, Sean, Sean's Applied Math Book, CIT, an online textbook that includes a complete introduction to calculus\nCrowell, Benjamin, Calculus, Fullerton College, an online textbook\nGarrett, Paul, Notes on First-Year Calculus\nHussain, Faraz, Understanding Calculus, an online textbook\nJohnson, William Woolsey (1909) Elementary Treatise on Integral Calculus, link from HathiTrust.\nKowalk, W.P., Integration Theory, University of Oldenburg. A new concept to an old problem. Online textbook\nSloughter, Dan, Difference Equations to Differential Equations, an introduction to calculus\nNumerical Methods of Integration at Holistic Numerical Methods Institute\nP.S. Wang, Evaluation of Definite Integrals by Symbolic Manipulation (1972) \u2014 a cookbook of definite integral techniques", 
                "titleUrl": "https://en.wikipedia.org/wiki/INTEGRAL", 
                "title": "INTEGRAL"
            }, 
            {
                "snippet": "allowing the tile to be snapped.       The harder grades of ceramic tiles like fully vitrified porcelain tiles, stone tiles, and some clay tiles with textured", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from October 2015\nCutting tools\nGrinding machines\nHand-held power tools\nMechanical hand tools\nMosaic", 
                "pageContent": "Ceramic tile cutters are used to cut tiles to a required size or shape. They come in a number of different forms, from basic manual devices to complex attachments for power tools.\n\n\n== Hand tools ==\nMass-produced ceramic tiles of medium to soft grades are cut easily with hand tools.\n\n\n=== Beam score cutters, cutter boards ===\n\nThe ceramic tile cutter works by first scratching a straight line across the surface of the tile with a hardened metal wheel and then applying pressure directly below the line and on each side of the line on top. Snapping pressure varies widely, some mass-produced models exerting over 750 kg.\nThe cutting wheel and breaking jig are combined in a carriage that travels along one or two beams to keep the carriage angled correctly and the cut straight. The beam(s) may be height adjustable to handle different thicknesses of tiles.\nThe base of the tool may have adjustable fences for angled cuts and square cuts and fence stops for multiple cuts of exactly the same size.\nThe scoring wheel is easily replaceable.\n\n\n==== History ====\nThe tool invented in 1951 by the Boada brothers, originating in the town of Rubi (Barcelona) in Spain. From the beginning it was popularly known by the users with the nickname \"the RUBI\", for the birthplace of their inventors and later the Registered Trade Mark.\nThe first tile cutter was designed to facilitate the work and solve the problems that masons had when cutting hydraulic mosaic or encaustic cement tiles (a type of decorative tile with pigmented cement, highly used in 50s, due to the high strength needed because of the high hardness and thickness of these tiles).\nOver the time the tool evolved, incorporating elements that made it more accurate and productive. The first cutter had an iron point to scratch the tiles. It was later replaced by the current tungsten carbide scratching wheel.\nAnother built-in device introduced in 1960 was the snapping element. It allowed users to snap the tiles easily and not with the bench, the cutter handle or hitting the tile with a knee as it was done before. This was a revolution in the cutting process of the ceramic world.\n\n\n=== Tile nippers ===\nTile nippers are similar to small pairs of pincers, with part of the width of the tool removed so that they can be fit into small holes. They can be used to break off small edges of tiles that have been scored or nibble out small chips enlarging holes etc.\n\n\n=== Glass cutter ===\n\nA simple hand held glass cutter is capable of scoring smooth surface glazes allowing the tile to be snapped.\n\n\n== Power tools ==\n\nThe harder grades of ceramic tiles like fully vitrified porcelain tiles, stone tiles, and some clay tiles with textured surfaces have to be cut with a diamond blade. The diamond blades are mounted in:-\n\n\n=== Angle grinders ===\nA angle grinder can be used for short, sometimes curved cuts. It can also be used for \"L\" shaped cuts and for making holes. It can be used dry and, more rarely, wet.\n\n\n=== Tile saws ===\n\nDedicated tile saws are designed to be used with water as a coolant for the diamond blade.\nThey are available in different sizes.\nAdjustable fences for angled cuts and square cuts.\nFence stops for multiple cuts of exactly the same size.\n\n\n== Gallery ==\n\n\n== References ==\n\n\n== See also ==\nHand tool\nPower tool\nDiamond tool\nEncaustic tile\nPorcelain tile\nDimension stone\nGlass tiles\nQuarry tile", 
                "titleUrl": "https://en.wikipedia.org/wiki/Ceramic_tile_cutter", 
                "title": "Ceramic tile cutter"
            }, 
            {
                "snippet": "70% higher than that of lead, and slightly lower than that of gold or tungsten. It occurs naturally in low concentrations of a few parts per million in", 
                "pageCategories": "Actinides\nAll Wikipedia articles in need of updating\nCS1 Turkish-language sources (tr)\nCS1 maint: Multiple names: authors list\nChemical elements\nCommons category with local link same as on Wikidata\nFeatured articles\nManhattan Project\nNuclear fuels\nNuclear materials", 
                "pageContent": "Uranium is a chemical element with symbol U and atomic number 92. It is a silvery-white metal in the actinide series of the periodic table. A uranium atom has 92 protons and 92 electrons, of which 6 are valence electrons. Uranium is weakly radioactive because all its isotopes are unstable (with half-lives of the six naturally known isotopes, uranium-233 to uranium-238, varying between 69 years and 4.5 billion years). The most common isotopes in natural uranium are uranium-238 (which has 146 neutrons and accounts for over 99%) and uranium-235 (which has 143 neutrons). Uranium has the highest atomic weight of the primordially occurring elements. Its density is about 70% higher than that of lead, and slightly lower than that of gold or tungsten. It occurs naturally in low concentrations of a few parts per million in soil, rock and water, and is commercially extracted from uranium-bearing minerals such as uraninite.\nIn nature, uranium is found as uranium-238 (99.2739\u201399.2752%), uranium-235 (0.7198\u20130.7202%), and a very small amount of uranium-234 (0.0050\u20130.0059%). Uranium decays slowly by emitting an alpha particle. The half-life of uranium-238 is about 4.47 billion years and that of uranium-235 is 704 million years, making them useful in dating the age of the Earth.\nMany contemporary uses of uranium exploit its unique nuclear properties. Uranium-235 has the distinction of being the only naturally occurring fissile isotope. Uranium-238 is fissionable by fast neutrons, and is fertile, meaning it can be transmuted to fissile plutonium-239 in a nuclear reactor. Another fissile isotope, uranium-233, can be produced from natural thorium and is also important in nuclear technology. Uranium-238 has a small probability for spontaneous fission or even induced fission with fast neutrons; uranium-235 and to a lesser degree uranium-233 have a much higher fission cross-section for slow neutrons. In sufficient concentration, these isotopes maintain a sustained nuclear chain reaction. This generates the heat in nuclear power reactors, and produces the fissile material for nuclear weapons. Depleted uranium (238U) is used in kinetic energy penetrators and armor plating. Uranium is used as a colorant in uranium glass, producing lemon yellow to green colors. Uranium glass fluoresces green in ultraviolet light. It was also used for tinting and shading in early photography.\nThe 1789 discovery of uranium in the mineral pitchblende is credited to Martin Heinrich Klaproth, who named the new element after the planet Uranus. Eug\u00e8ne-Melchior P\u00e9ligot was the first person to isolate the metal and its radioactive properties were discovered in 1896 by Henri Becquerel. Research by Otto Hahn, Lise Meitner, Enrico Fermi and others, such as J. Robert Oppenheimer starting in 1934 led to its use as a fuel in the nuclear power industry and in Little Boy, the first nuclear weapon used in war. An ensuing arms race during the Cold War between the United States and the Soviet Union produced tens of thousands of nuclear weapons that used uranium metal and uranium-derived plutonium-239. The security of those weapons and their fissile material following the breakup of the Soviet Union in 1991 is an ongoing concern for public health and safety. See Nuclear proliferation.\n\n\n== Characteristics ==\n\nWhen refined, uranium is a silvery white, weakly radioactive metal. It has a Mohs hardness of 6, sufficient to scratch glass and approximately equal to that of titanium, rhodium, manganese and niobium. It is malleable, ductile, slightly paramagnetic, strongly electropositive and a poor electrical conductor. Uranium metal has a very high density of 19.1 g/cm3, denser than lead (11.3 g/cm3), but slightly less dense than tungsten and gold (19.3 g/cm3).\nUranium metal reacts with almost all non-metal elements (with an exception of the noble gases) and their compounds, with reactivity increasing with temperature. Hydrochloric and nitric acids dissolve uranium, but non-oxidizing acids other than hydrochloric acid attack the element very slowly. When finely divided, it can react with cold water; in air, uranium metal becomes coated with a dark layer of uranium oxide. Uranium in ores is extracted chemically and converted into uranium dioxide or other chemical forms usable in industry.\nUranium-235 was the first isotope that was found to be fissile. Other naturally occurring isotopes are fissionable, but not fissile. On bombardment with slow neutrons, its uranium-235 isotope will most of the time divide into two smaller nuclei, releasing nuclear binding energy and more neutrons. If too many of these neutrons are absorbed by other uranium-235 nuclei, a nuclear chain reaction occurs that results in a burst of heat or (in special circumstances) an explosion. In a nuclear reactor, such a chain reaction is slowed and controlled by a neutron poison, absorbing some of the free neutrons. Such neutron absorbent materials are often part of reactor control rods (see nuclear reactor physics for a description of this process of reactor control).\nAs little as 15 lb (7 kg) of uranium-235 can be used to make an atomic bomb. The first nuclear bomb used in war, Little Boy, relied on uranium fission, but the very first nuclear explosive (the Gadget used at Trinity) and the bomb that destroyed Nagasaki (Fat Man) were both plutonium bombs.\nUranium metal has three allotropic forms:\n\u03b1 (orthorhombic) stable up to 668 \u00b0C. Orthorhombic, space group No. 63, Cmcm, lattice parameters a = 285.4 pm, b = 587 pm, c = 495.5 pm.\n\u03b2 (tetragonal) stable from 668 \u00b0C to 775 \u00b0C. Tetragonal, space group P42/mnm, P42nm, or P4n2, lattice parameters a = 565.6 pm, b = c = 1075.9 pm.\n\u03b3 (body-centered cubic) from 775 \u00b0C to melting point\u2014this is the most malleable and ductile state. Body-centered cubic, lattice parameter a = 352.4 pm.\n\n\n== Applications ==\n\n\n=== Military ===\n\nThe major application of uranium in the military sector is in high-density penetrators. This ammunition consists of depleted uranium (DU) alloyed with 1\u20132% other elements, such as titanium or molybdenum. At high impact speed, the density, hardness, and pyrophoricity of the projectile enable the destruction of heavily armored targets. Tank armor and other removable vehicle armor can also be hardened with depleted uranium plates. The use of depleted uranium became politically and environmentally contentious after the use of such munitions by the US, UK and other countries during wars in the Persian Gulf and the Balkans raised questions concerning uranium compounds left in the soil (see Gulf War Syndrome).\nDepleted uranium is also used as a shielding material in some containers used to store and transport radioactive materials. While the metal itself is radioactive, its high density makes it more effective than lead in halting radiation from strong sources such as radium. Other uses of depleted uranium include counterweights for aircraft control surfaces, as ballast for missile re-entry vehicles and as a shielding material. Due to its high density, this material is found in inertial guidance systems and in gyroscopic compasses. Depleted uranium is preferred over similarly dense metals due to its ability to be easily machined and cast as well as its relatively low cost. The main risk of exposure to depleted uranium is chemical poisoning by uranium oxide rather than radioactivity (uranium being only a weak alpha emitter).\nDuring the later stages of World War II, the entire Cold War, and to a lesser extent afterwards, uranium-235 has been used as the fissile explosive material to produce nuclear weapons. Initially, two major types of fission bombs were built: a relatively simple device that uses uranium-235 and a more complicated mechanism that uses plutonium-239 derived from uranium-238. Later, a much more complicated and far more powerful type of fission/fusion bomb (thermonuclear weapon) was built, that uses a plutonium-based device to cause a mixture of tritium and deuterium to undergo nuclear fusion. Such bombs are jacketed in a non-fissile (unenriched) uranium case, and they derive more than half their power from the fission of this material by fast neutrons from the nuclear fusion process.\n\n\n=== Civilian ===\n\nThe main use of uranium in the civilian sector is to fuel nuclear power plants. One kilogram of uranium-235 can theoretically produce about 20 terajoules of energy (2\u00d71013 joules), assuming complete fission; as much energy as 1500 tonnes of coal.\nCommercial nuclear power plants use fuel that is typically enriched to around 3% uranium-235. The CANDU and Magnox designs are the only commercial reactors capable of using unenriched uranium fuel. Fuel used for United States Navy reactors is typically highly enriched in uranium-235 (the exact values are classified). In a breeder reactor, uranium-238 can also be converted into plutonium through the following reaction:\n238\n92U + n \u2192 239\n92U + \u03b3 \u03b2\u2212\u2192  239\n93Np \u03b2\u2212\u2192  239\n94Pu\n\nBefore (and, occasionally, after) the discovery of radioactivity, uranium was primarily used in small amounts for yellow glass and pottery glazes, such as uranium glass and in Fiestaware.\nThe discovery and isolation of radium in uranium ore (pitchblende) by Marie Curie sparked the development of uranium mining to extract the radium, which was used to make glow-in-the-dark paints for clock and aircraft dials. This left a prodigious quantity of uranium as a waste product, since it takes three tonnes of uranium to extract one gram of radium. This waste product was diverted to the glazing industry, making uranium glazes very inexpensive and abundant. Besides the pottery glazes, uranium tile glazes accounted for the bulk of the use, including common bathroom and kitchen tiles which can be produced in green, yellow, mauve, black, blue, red and other colors.\n\nUranium was also used in photographic chemicals (especially uranium nitrate as a toner), in lamp filaments for stage lighting bulbs, to improve the appearance of dentures, and in the leather and wood industries for stains and dyes. Uranium salts are mordants of silk or wool. Uranyl acetate and uranyl formate are used as electron-dense \"stains\" in transmission electron microscopy, to increase the contrast of biological specimens in ultrathin sections and in negative staining of viruses, isolated cell organelles and macromolecules.\nThe discovery of the radioactivity of uranium ushered in additional scientific and practical uses of the element. The long half-life of the isotope uranium-238 (4.51\u00d7109 years) makes it well-suited for use in estimating the age of the earliest igneous rocks and for other types of radiometric dating, including uranium-thorium dating, uranium-lead dating and uranium-uranium dating. Uranium metal is used for X-ray targets in the making of high-energy X-rays.\n\n\n== History ==\n\n\n=== Pre-discovery use ===\n\nThe use of uranium in its natural oxide form dates back to at least the year 79 CE, when it was used to add a yellow color to ceramic glazes. Yellow glass with 1% uranium oxide was found in a Roman villa on Cape Posillipo in the Bay of Naples, Italy, by R. T. Gunther of the University of Oxford in 1912. Starting in the late Middle Ages, pitchblende was extracted from the Habsburg silver mines in Joachimsthal, Bohemia (now J\u00e1chymov in the Czech Republic), and was used as a coloring agent in the local glassmaking industry. In the early 19th century, the world's only known sources of uranium ore were these mines.\n\n\n=== Discovery ===\n\nThe discovery of the element is credited to the German chemist Martin Heinrich Klaproth. While he was working in his experimental laboratory in Berlin in 1789, Klaproth was able to precipitate a yellow compound (likely sodium diuranate) by dissolving pitchblende in nitric acid and neutralizing the solution with sodium hydroxide. Klaproth assumed the yellow substance was the oxide of a yet-undiscovered element and heated it with charcoal to obtain a black powder, which he thought was the newly discovered metal itself (in fact, that powder was an oxide of uranium). He named the newly discovered element after the planet Uranus, (named after the primordial Greek god of the sky), which had been discovered eight years earlier by William Herschel.\nIn 1841, Eug\u00e8ne-Melchior P\u00e9ligot, Professor of Analytical Chemistry at the Conservatoire National des Arts et M\u00e9tiers (Central School of Arts and Manufactures) in Paris, isolated the first sample of uranium metal by heating uranium tetrachloride with potassium.\nHenri Becquerel discovered radioactivity by using uranium in 1896. Becquerel made the discovery in Paris by leaving a sample of a uranium salt, K2UO2(SO4)2 (potassium uranyl sulfate), on top of an unexposed photographic plate in a drawer and noting that the plate had become \"fogged\". He determined that a form of invisible light or rays emitted by uranium had exposed the plate.\n\n\n=== Fission research ===\n\nA team led by Enrico Fermi in 1934 observed that bombarding uranium with neutrons produces the emission of beta rays (electrons or positrons from the elements produced; see beta particle). The fission products were at first mistaken for new elements of atomic numbers 93 and 94, which the Dean of the Faculty of Rome, Orso Mario Corbino, christened ausonium and hesperium, respectively. The experiments leading to the discovery of uranium's ability to fission (break apart) into lighter elements and release binding energy were conducted by Otto Hahn and Fritz Strassmann in Hahn's laboratory in Berlin. Lise Meitner and her nephew, the physicist Otto Robert Frisch, published the physical explanation in February 1939 and named the process \"nuclear fission\". Soon after, Fermi hypothesized that the fission of uranium might release enough neutrons to sustain a fission reaction. Confirmation of this hypothesis came in 1939, and later work found that on average about 2.5 neutrons are released by each fission of the rare uranium isotope uranium-235. Further work found that the far more common uranium-238 isotope can be transmuted into plutonium, which, like uranium-235, is also fissile by thermal neutrons. These discoveries led numerous countries to begin working on the development of nuclear weapons and nuclear power.\nOn 2 December 1942, as part of the Manhattan Project, another team led by Enrico Fermi was able to initiate the first artificial self-sustained nuclear chain reaction, Chicago Pile-1. Working in a lab below the stands of Stagg Field at the University of Chicago, the team created the conditions needed for such a reaction by piling together 400 short tons (360 metric tons) of graphite, 58 short tons (53 metric tons) of uranium oxide, and six short tons (5.5 metric tons) of uranium metal, a majority of which was supplied by Westinghouse Lamp Plant in a makeshift production process.\n\n\n=== Nuclear weaponry ===\n\nTwo major types of atomic bombs were developed by the United States during World War II: a uranium-based device (codenamed \"Little Boy\") whose fissile material was highly enriched uranium, and a plutonium-based device (see Trinity test and \"Fat Man\") whose plutonium was derived from uranium-238. The uranium-based Little Boy device became the first nuclear weapon used in war when it was detonated over the Japanese city of Hiroshima on 6 August 1945. Exploding with a yield equivalent to 12,500 tonnes of TNT, the blast and thermal wave of the bomb destroyed nearly 50,000 buildings and killed approximately 75,000 people (see Atomic bombings of Hiroshima and Nagasaki). Initially it was believed that uranium was relatively rare, and that nuclear proliferation could be avoided by simply buying up all known uranium stocks, but within a decade large deposits of it were discovered in many places around the world.\n\n\n=== Reactors ===\n\nThe X-10 Graphite Reactor at Oak Ridge National Laboratory (ORNL) in Oak Ridge, Tennessee, formerly known as the Clinton Pile and X-10 Pile, was the world's second artificial nuclear reactor (after Enrico Fermi's Chicago Pile) and was the first reactor designed and built for continuous operation. Argonne National Laboratory's Experimental Breeder Reactor I, located at the Atomic Energy Commission's National Reactor Testing Station near Arco, Idaho, became the first nuclear reactor to create electricity on 20 December 1951. Initially, four 150-watt light bulbs were lit by the reactor, but improvements eventually enabled it to power the whole facility (later, the town of Arco became the first in the world to have all its electricity come from nuclear power generated by BORAX-III, another reactor designed and operated by Argonne National Laboratory). The world's first commercial scale nuclear power station, Obninsk in the Soviet Union, began generation with its reactor AM-1 on 27 June 1954. Other early nuclear power plants were Calder Hall in England, which began generation on 17 October 1956, and the Shippingport Atomic Power Station in Pennsylvania, which began on 26 May 1958. Nuclear power was used for the first time for propulsion by a submarine, the USS Nautilus, in 1954.\n\n\n=== Prehistoric naturally occurring fission ===\n\nIn 1972, the French physicist Francis Perrin discovered fifteen ancient and no longer active natural nuclear fission reactors in three separate ore deposits at the Oklo mine in Gabon, West Africa, collectively known as the Oklo Fossil Reactors. The ore deposit is 1.7 billion years old; then, uranium-235 constituted about 3% of the total uranium on Earth. This is high enough to permit a sustained nuclear fission chain reaction to occur, provided other supporting conditions exist. The capacity of the surrounding sediment to contain the nuclear waste products has been cited by the U.S. federal government as supporting evidence for the feasibility to store spent nuclear fuel at the Yucca Mountain nuclear waste repository.\n\n\n=== Contamination and the Cold War legacy ===\n\nAbove-ground nuclear tests by the Soviet Union and the United States in the 1950s and early 1960s and by France into the 1970s and 1980s spread a significant amount of fallout from uranium daughter isotopes around the world. Additional fallout and pollution occurred from several nuclear accidents.\nUranium miners have a higher incidence of cancer. An excess risk of lung cancer among Navajo uranium miners, for example, has been documented and linked to their occupation. The Radiation Exposure Compensation Act, a 1990 law in the USA, required $100,000 in \"compassion payments\" to uranium miners diagnosed with cancer or other respiratory ailments.\nDuring the Cold War between the Soviet Union and the United States, huge stockpiles of uranium were amassed and tens of thousands of nuclear weapons were created using enriched uranium and plutonium made from uranium. Since the break-up of the Soviet Union in 1991, an estimated 600 short tons (540 metric tons) of highly enriched weapons grade uranium (enough to make 40,000 nuclear warheads) have been stored in often inadequately guarded facilities in the Russian Federation and several other former Soviet states. Police in Asia, Europe, and South America on at least 16 occasions from 1993 to 2005 have intercepted shipments of smuggled bomb-grade uranium or plutonium, most of which was from ex-Soviet sources. From 1993 to 2005 the Material Protection, Control, and Accounting Program, operated by the federal government of the United States, spent approximately US $550 million to help safeguard uranium and plutonium stockpiles in Russia. This money was used for improvements and security enhancements at research and storage facilities. Scientific American reported in February 2006 that in some of the facilities security consisted of chain link fences which were in severe states of disrepair. According to an interview from the article, one facility had been storing samples of enriched (weapons grade) uranium in a broom closet before the improvement project; another had been keeping track of its stock of nuclear warheads using index cards kept in a shoe box.\n\n\n== Occurrence ==\n\n\n=== Biotic and abiotic ===\n\nUranium is a naturally occurring element that can be found in low levels within all rock, soil, and water. Uranium is the 51st element in order of abundance in the Earth's crust. Uranium is also the highest-numbered element to be found naturally in significant quantities on Earth and is almost always found combined with other elements. Along with all elements having atomic weights higher than that of iron, it is only naturally formed in supernovae. The decay of uranium, thorium, and potassium-40 in the Earth's mantle is thought to be the main source of heat that keeps the outer core liquid and drives mantle convection, which in turn drives plate tectonics.\nUranium's average concentration in the Earth's crust is (depending on the reference) 2 to 4 parts per million, or about 40 times as abundant as silver. The Earth's crust from the surface to 25 km (15 mi) down is calculated to contain 1017 kg (2\u00d71017 lb) of uranium while the oceans may contain 1013 kg (2\u00d71013 lb). The concentration of uranium in soil ranges from 0.7 to 11 parts per million (up to 15 parts per million in farmland soil due to use of phosphate fertilizers), and its concentration in sea water is 3 parts per billion.\nUranium is more plentiful than antimony, tin, cadmium, mercury, or silver, and it is about as abundant as arsenic or molybdenum. Uranium is found in hundreds of minerals, including uraninite (the most common uranium ore), carnotite, autunite, uranophane, torbernite, and coffinite. Significant concentrations of uranium occur in some substances such as phosphate rock deposits, and minerals such as lignite, and monazite sands in uranium-rich ores (it is recovered commercially from sources with as little as 0.1% uranium).\n\nSome bacteria, such as Shewanella putrefaciens, Geobacter metallireducens and some strains of Burkholderia fungorum, use uranium for their growth and convert U(VI) to U(IV).\nSome organisms, such as the lichen Trapelia involuta or microorganisms such as the bacterium Citrobacter, can absorb concentrations of uranium that are up to 300 times the level of their environment. Citrobacter species absorb uranyl ions when given glycerol phosphate (or other similar organic phosphates). After one day, one gram of bacteria can encrust themselves with nine grams of uranyl phosphate crystals; this creates the possibility that these organisms could be used in bioremediation to decontaminate uranium-polluted water. The proteobacterium Geobacter has also been shown to bioremediate uranium in ground water. The mycorrhizal fungus Glomus intraradices increases uranium content in the roots of its symbiotic plant.\nIn nature, uranium(VI) forms highly soluble carbonate complexes at alkaline pH. This leads to an increase in mobility and availability of uranium to groundwater and soil from nuclear wastes which leads to health hazards. However, it is difficult to precipitate uranium as phosphate in the presence of excess carbonate at alkaline pH. A Sphingomonas sp. strain BSAR-1 has been found to express a high activity alkaline phosphatase (PhoK) that has been applied for bioprecipitation of uranium as uranyl phosphate species from alkaline solutions. The precipitation ability was enhanced by overexpressing PhoK protein in E. coli.\nPlants absorb some uranium from soil. Dry weight concentrations of uranium in plants range from 5 to 60 parts per billion, and ash from burnt wood can have concentrations up to 4 parts per million. Dry weight concentrations of uranium in food plants are typically lower with one to two micrograms per day ingested through the food people eat.\n\n\n=== Production and mining ===\n\nWorldwide production of U3O8 (yellowcake) in 2013 amounted to 70,015 tonnes, of which 22,451 t (32%) was mined in Kazakhstan. Other important uranium mining countries are Canada (9,331 t), Australia (6,350 t), Niger (4,518 t), Namibia (4,323 t) and Russia (3,135 t).\nUranium ore is mined in several ways: by open pit, underground, in-situ leaching, and borehole mining (see uranium mining). Low-grade uranium ore mined typically contains 0.01 to 0.25% uranium oxides. Extensive measures must be employed to extract the metal from its ore. High-grade ores found in Athabasca Basin deposits in Saskatchewan, Canada can contain up to 23% uranium oxides on average. Uranium ore is crushed and rendered into a fine powder and then leached with either an acid or alkali. The leachate is subjected to one of several sequences of precipitation, solvent extraction, and ion exchange. The resulting mixture, called yellowcake, contains at least 75% uranium oxides U3O8. Yellowcake is then calcined to remove impurities from the milling process before refining and conversion.\nCommercial-grade uranium can be produced through the reduction of uranium halides with alkali or alkaline earth metals. Uranium metal can also be prepared through electrolysis of KUF\n5 or UF\n4, dissolved in molten calcium chloride (CaCl\n2) and sodium chloride (NaCl) solution. Very pure uranium is produced through the thermal decomposition of uranium halides on a hot filament.\n\n\n=== Resources and reserves ===\nIt is estimated that 5.5 million tonnes of uranium exists in ore reserves that are economically viable at US$59 per lb of uranium, while 35 million tonnes are classed as mineral resources (reasonable prospects for eventual economic extraction). Prices went from about $10/lb in May 2003 to $138/lb in July 2007. This has caused a big increase in spending on exploration, with US$200 million being spent worldwide in 2005, a 54% increase on the previous year. This trend continued through 2006, when expenditure on exploration rocketed to over $774 million, an increase of over 250% compared to 2004. The OECD Nuclear Energy Agency said exploration figures for 2007 would likely match those for 2006.\nAustralia has 31% of the world's known uranium ore reserves and the world's largest single uranium deposit, located at the Olympic Dam Mine in South Australia. There is a significant reserve of uranium in Bakouma a sub-prefecture in the prefecture of Mbomou in Central African Republic.\nSome nuclear fuel comes from nuclear weapons being dismantled, such as from the Megatons to Megawatts Program.\nAn additional 4.6 billion tonnes of uranium are estimated to be in sea water (Japanese scientists in the 1980s showed that extraction of uranium from sea water using ion exchangers was technically feasible). There have been experiments to extract uranium from sea water, but the yield has been low due to the carbonate present in the water. In 2012, ORNL researchers announced the successful development of a new absorbent material dubbed HiCap which performs surface retention of solid or gas molecules, atoms or ions and also effectively removes toxic metals from water, according to results verified by researchers at Pacific Northwest National Laboratory.\n\n\n=== Supplies ===\n\nIn 2005, seventeen countries produced concentrated uranium oxides, with Canada (27.9% of world production) and Australia (22.8%) being the largest producers and Kazakhstan (10.5%), Russia (8.0%), Namibia (7.5%), Niger (7.4%), Uzbekistan (5.5%), the United States (2.5%), Argentina (2.1%), Ukraine (1.9%) and China (1.7%) also producing significant amounts. Kazakhstan continues to increase production and may have become the world's largest producer of uranium by 2009 with an expected production of 12,826 tonnes, compared to Canada with 11,100 t and Australia with 9,430 t. In the late 1960s, UN geologists also discovered major uranium deposits and other rare mineral reserves in Somalia. The find was the largest of its kind, with industry experts estimating the deposits at over 25% of the world's then known uranium reserves of 800,000 tons.\nThe ultimate available supply is believed to be sufficient for at least the next 85 years, although some studies indicate underinvestment in the late twentieth century may produce supply problems in the 21st century. Uranium deposits seem to be log-normal distributed. There is a 300-fold increase in the amount of uranium recoverable for each tenfold decrease in ore grade. In other words, there is little high grade ore and proportionately much more low grade ore available.\n\n\n== Compounds ==\n\n\n=== Oxidation states and oxides ===\n\n\n==== Oxides ====\n\nCalcined uranium yellowcake, as produced in many large mills, contains a distribution of uranium oxidation species in various forms ranging from most oxidized to least oxidized. Particles with short residence times in a calciner will generally be less oxidized than those with long retention times or particles recovered in the stack scrubber. Uranium content is usually referenced to U\n3O\n8, which dates to the days of the Manhattan project when U\n3O\n8 was used as an analytical chemistry reporting standard.\nPhase relationships in the uranium-oxygen system are complex. The most important oxidation states of uranium are uranium(IV) and uranium(VI), and their two corresponding oxides are, respectively, uranium dioxide (UO\n2) and uranium trioxide (UO\n3). Other uranium oxides such as uranium monoxide (UO), diuranium pentoxide (U\n2O\n5), and uranium peroxide (UO\n4\u00b72H\n2O) also exist.\nThe most common forms of uranium oxide are triuranium octoxide (U\n3O\n8) and UO\n2. Both oxide forms are solids that have low solubility in water and are relatively stable over a wide range of environmental conditions. Triuranium octoxide is (depending on conditions) the most stable compound of uranium and is the form most commonly found in nature. Uranium dioxide is the form in which uranium is most commonly used as a nuclear reactor fuel. At ambient temperatures, UO\n2 will gradually convert to U\n3O\n8. Because of their stability, uranium oxides are generally considered the preferred chemical form for storage or disposal.\n\n\n==== Aqueous chemistry ====\n\nSalts of many oxidation states of uranium are water-soluble and may be studied in aqueous solutions. The most common ionic forms are U3+ (brown-red), U4+ (green), UO+\n2 (unstable), and UO2+\n2 (yellow), for U(III), U(IV), U(V), and U(VI), respectively. A few solid and semi-metallic compounds such as UO and US exist for the formal oxidation state uranium(II), but no simple ions are known to exist in solution for that state. Ions of U3+ liberate hydrogen from water and are therefore considered to be highly unstable. The UO2+\n2 ion represents the uranium(VI) state and is known to form compounds such as uranyl carbonate, uranyl chloride and uranyl sulfate. UO2+\n2 also forms complexes with various organic chelating agents, the most commonly encountered of which is uranyl acetate.\nUnlike the uranyl salts of uranium and polyatomic ion uranium-oxide cationic forms, the uranates, salts containing a polyatomic uranium-oxide anion, are generally not water-soluble.\n\n\n==== Carbonates ====\nThe interactions of carbonate anions with uranium(VI) cause the Pourbaix diagram to change greatly when the medium is changed from water to a carbonate containing solution. While the vast majority of carbonates are insoluble in water (students are often taught that all carbonates other than those of alkali metals are insoluble in water), uranium carbonates are often soluble in water. This is because a U(VI) cation is able to bind two terminal oxides and three or more carbonates to form anionic complexes.\n\n\n==== Effects of pH ====\nThe uranium fraction diagrams in the presence of carbonate illustrate this further: when the pH of a uranium(VI) solution increases, the uranium is converted to a hydrated uranium oxide hydroxide and at high pHs it becomes an anionic hydroxide complex.\nWhen carbonate is added, uranium is converted to a series of carbonate complexes if the pH is increased. One effect of these reactions is increased solubility of uranium in the pH range 6 to 8, a fact that has a direct bearing on the long term stability of spent uranium dioxide nuclear fuels.\n\n\n=== Hydrides, carbides and nitrides ===\nUranium metal heated to 250 to 300 \u00b0C (482 to 572 \u00b0F) reacts with hydrogen to form uranium hydride. Even higher temperatures will reversibly remove the hydrogen. This property makes uranium hydrides convenient starting materials to create reactive uranium powder along with various uranium carbide, nitride, and halide compounds. Two crystal modifications of uranium hydride exist: an \u03b1 form that is obtained at low temperatures and a \u03b2 form that is created when the formation temperature is above 250 \u00b0C.\nUranium carbides and uranium nitrides are both relatively inert semimetallic compounds that are minimally soluble in acids, react with water, and can ignite in air to form U\n3O\n8. Carbides of uranium include uranium monocarbide (UC), uranium dicarbide (UC\n2), and diuranium tricarbide (U\n2C\n3). Both UC and UC\n2 are formed by adding carbon to molten uranium or by exposing the metal to carbon monoxide at high temperatures. Stable below 1800 \u00b0C, U\n2C\n3 is prepared by subjecting a heated mixture of UC and UC\n2 to mechanical stress. Uranium nitrides obtained by direct exposure of the metal to nitrogen include uranium mononitride (UN), uranium dinitride (UN\n2), and diuranium trinitride (U\n2N\n3).\n\n\n=== Halides ===\n\nAll uranium fluorides are created using uranium tetrafluoride (UF\n4); UF\n4 itself is prepared by hydrofluorination of uranium dioxide. Reduction of UF\n4 with hydrogen at 1000 \u00b0C produces uranium trifluoride (UF\n3). Under the right conditions of temperature and pressure, the reaction of solid UF\n4 with gaseous uranium hexafluoride (UF\n6) can form the intermediate fluorides of U\n2F\n9, U\n4F\n17, and UF\n5.\nAt room temperatures, UF\n6 has a high vapor pressure, making it useful in the gaseous diffusion process to separate the rare uranium-235 from the common uranium-238 isotope. This compound can be prepared from uranium dioxide and uranium hydride by the following process:\nUO\n2 + 4 HF \u2192 UF\n4 + 2 H\n2O (500 \u00b0C, endothermic)\nUF\n4 + F\n2 \u2192 UF\n6 (350 \u00b0C, endothermic)\nThe resulting UF\n6, a white solid, is highly reactive (by fluorination), easily sublimes (emitting a vapor that behaves as a nearly ideal gas), and is the most volatile compound of uranium known to exist.\nOne method of preparing uranium tetrachloride (UCl\n4) is to directly combine chlorine with either uranium metal or uranium hydride. The reduction of UCl\n4 by hydrogen produces uranium trichloride (UCl\n3) while the higher chlorides of uranium are prepared by reaction with additional chlorine. All uranium chlorides react with water and air.\nBromides and iodides of uranium are formed by direct reaction of, respectively, bromine and iodine with uranium or by adding UH\n3 to those element's acids. Known examples include: UBr\n3, UBr\n4, UI\n3, and UI\n4. Uranium oxyhalides are water-soluble and include UO\n2F\n2, UOCl\n2, UO\n2Cl\n2, and UO\n2Br\n2. Stability of the oxyhalides decrease as the atomic weight of the component halide increases.\n\n\n== Isotopes ==\n\n\n=== Natural concentrations ===\n\nNatural uranium consists of three major isotopes: uranium-238 (99.28% natural abundance), uranium-235 (0.71%), and uranium-234 (0.0054%). All three are radioactive, emitting alpha particles, with the exception that all three of these isotopes have small probabilities of undergoing spontaneous fission, rather than alpha emission. There are also five other trace isotopes: uranium-239, which is formed when 238U undergoes spontaneous fission, releasing neutrons that are captured by another 238U atom; uranium-237, which is formed when 238U captures a neutron but emits two more, which then decays to neptunium-237; and finally, uranium-233, which is formed in the decay chain of that neptunium-237. It is also expected that thorium-232 should be able to undergo double beta decay, which would produce uranium-232, but this has not yet been observed experimentally.\nUranium-238 is the most stable isotope of uranium, with a half-life of about 4.468\u00d7109 years, roughly the age of the Earth. Uranium-235 has a half-life of about 7.13\u00d7108 years, and uranium-234 has a half-life of about 2.48\u00d7105 years. For natural uranium, about 49% of its alpha rays are emitted by each of 238U atom, and also 49% by 234U (since the latter is formed from the former) and about 2.0% of them by the 235U. When the Earth was young, probably about one-fifth of its uranium was uranium-235, but the percentage of 234U was probably much lower than this.\nUranium-238 is usually an \u03b1 emitter (occasionally, it undergoes spontaneous fission), decaying through the \"Uranium Series\" of nuclear decay, which has 18 members, into lead-206, by a variety of different decay paths.\nThe decay series of 235U, which is called the actinium series, has 15 members and eventually decays into lead-207. The constant rates of decay in these decay series makes the comparison of the ratios of parent to daughter elements useful in radiometric dating.\nUranium-234, which is a member of the \"Uranium Series\", decays to lead-206 through a series of relatively short-lived isotopes.\nUranium-233 is made from thorium-232 by neutron bombardment, usually in a nuclear reactor, and 233U is also fissile. Its decay series ends at bismuth-209 and thallium-205.\nUranium-235 is important for both nuclear reactors and nuclear weapons, because it is the only uranium isotope existing in nature on Earth in any significant amount that is fissile. This means that it can be split into two or three fragments (fission products) by thermal neutrons.\nUranium-238 is not fissile, but is a fertile isotope, because after neutron activation it can produce plutonium-239, another fissile isotope. Indeed, the 238U nucleus can absorb one neutron to produce the radioactive isotope uranium-239. 239U decays by beta emission to neptunium-239, also a beta-emitter, that decays in its turn, within a few days into plutonium-239. 239Pu was used as fissile material in the first atomic bomb detonated in the \"Trinity test\" on 15 July 1945 in New Mexico.\n\n\n=== Enrichment ===\n\nIn nature, uranium is found as uranium-238 (99.2742%) and uranium-235 (0.7204%). Isotope separation concentrates (enriches) the fissionable uranium-235 for nuclear weapons and most nuclear power plants, except for gas cooled reactors and pressurised heavy water reactors. Most neutrons released by a fissioning atom of uranium-235 must impact other uranium-235 atoms to sustain the nuclear chain reaction. The concentration and amount of uranium-235 needed to achieve this is called a 'critical mass'.\nTo be considered 'enriched', the uranium-235 fraction should be between 3% and 5%. This process produces huge quantities of uranium that is depleted of uranium-235 and with a correspondingly increased fraction of uranium-238, called depleted uranium or 'DU'. To be considered 'depleted', the uranium-235 isotope concentration should be no more than 0.3%. The price of uranium has risen since 2001, so enrichment tailings containing more than 0.35% uranium-235 are being considered for re-enrichment, driving the price of depleted uranium hexafluoride above $130 per kilogram in July 2007 from $5 in 2001.\nThe gas centrifuge process, where gaseous uranium hexafluoride (UF\n6) is separated by the difference in molecular weight between 235UF6 and 238UF6 using high-speed centrifuges, is the cheapest and leading enrichment process. The gaseous diffusion process had been the leading method for enrichment and was used in the Manhattan Project. In this process, uranium hexafluoride is repeatedly diffused through a silver-zinc membrane, and the different isotopes of uranium are separated by diffusion rate (since uranium 238 is heavier it diffuses slightly slower than uranium-235). The molecular laser isotope separation method employs a laser beam of precise energy to sever the bond between uranium-235 and fluorine. This leaves uranium-238 bonded to fluorine and allows uranium-235 metal to precipitate from the solution. An alternative laser method of enrichment is known as atomic vapor laser isotope separation (AVLIS) and employs visible tunable lasers such as dye lasers. Another method used is liquid thermal diffusion.\n\n\n== Human exposure ==\nA person can be exposed to uranium (or its radioactive daughters, such as radon) by inhaling dust in air or by ingesting contaminated water and food. The amount of uranium in air is usually very small; however, people who work in factories that process phosphate fertilizers, live near government facilities that made or tested nuclear weapons, live or work near a modern battlefield where depleted uranium weapons have been used, or live or work near a coal-fired power plant, facilities that mine or process uranium ore, or enrich uranium for reactor fuel, may have increased exposure to uranium. Houses or structures that are over uranium deposits (either natural or man-made slag deposits) may have an increased incidence of exposure to radon gas. The Occupational Safety and Health Administration (OSHA) has set the permissible exposure limit for uranium exposure in the workplace as 0.25 mg/m3 over an 8-hour workday. The National Institute for Occupational Safety and Health (NIOSH) has set a recommended exposure limit (REL) of 0.2 mg/m3 over an 8-hour workday and a short-term limit of 0.6 mg/m3. At levels of 10 mg/m3, uranium is immediately dangerous to life and health.\nMost ingested uranium is excreted during digestion. Only 0.5% is absorbed when insoluble forms of uranium, such as its oxide, are ingested, whereas absorption of the more soluble uranyl ion can be up to 5%. However, soluble uranium compounds tend to quickly pass through the body, whereas insoluble uranium compounds, especially when inhaled by way of dust into the lungs, pose a more serious exposure hazard. After entering the bloodstream, the absorbed uranium tends to bioaccumulate and stay for many years in bone tissue because of uranium's affinity for phosphates. Uranium is not absorbed through the skin, and alpha particles released by uranium cannot penetrate the skin.\nIncorporated uranium becomes uranyl ions, which accumulate in bone, liver, kidney, and reproductive tissues. Uranium can be decontaminated from steel surfaces and aquifers.\n\n\n=== Effects and precautions ===\nNormal functioning of the kidney, brain, liver, heart, and other systems can be affected by uranium exposure, because, besides being weakly radioactive, uranium is a toxic metal. Uranium is also a reproductive toxicant. Radiological effects are generally local because alpha radiation, the primary form of 238U decay, has a very short range, and will not penetrate skin. Uranyl (UO2+\n2) ions, such as from uranium trioxide or uranyl nitrate and other hexavalent uranium compounds, have been shown to cause birth defects and immune system damage in laboratory animals. While the CDC has published one study that no human cancer has been seen as a result of exposure to natural or depleted uranium, exposure to uranium and its decay products, especially radon, are widely known and significant health threats. Exposure to strontium-90, iodine-131, and other fission products is unrelated to uranium exposure, but may result from medical procedures or exposure to spent reactor fuel or fallout from nuclear weapons. Although accidental inhalation exposure to a high concentration of uranium hexafluoride has resulted in human fatalities, those deaths were associated with the generation of highly toxic hydrofluoric acid and uranyl fluoride rather than with uranium itself. Finely divided uranium metal presents a fire hazard because uranium is pyrophoric; small grains will ignite spontaneously in air at room temperature.\nUranium metal is commonly handled with gloves as a sufficient precaution. Uranium concentrate is handled and contained so as to ensure that people do not inhale or ingest it.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\nEmsley, John (2001). \"Uranium\". Nature's Building Blocks: An A to Z Guide to the Elements. Oxford: Oxford University Press. pp. 476\u2013482. ISBN 0-19-850340-7. \nSeaborg, Glenn T. (1968). \"Uranium\". The Encyclopedia of the Chemical Elements. Skokie, Illinois: Reinhold Book Corporation. pp. 773\u2013786. LCCCN 68-29938. \n\n\n== External links ==\nU.S. EPA: Radiation Information for Uranium\n\"What is Uranium?\" from World Nuclear Association\nNuclear fuel data and analysis from the U.S. Energy Information Administration\nCurrent market price of uranium\nWorld Uranium deposit maps\nAnnotated bibliography for uranium from the Alsos Digital Library\nNLM Hazardous Substances Databank\u2014Uranium, Radioactive\nCDC - NIOSH Pocket Guide to Chemical Hazards\nMining Uranium at Namibia's Langer Heinrich Mine\nWorld Nuclear News\nATSDR Case Studies in Environmental Medicine: Uranium Toxicity U.S. Department of Health and Human Services\nUranium at The Periodic Table of Videos (University of Nottingham)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Uranium", 
                "title": "Uranium"
            }, 
            {
                "snippet": "between tiles or mosaics, and to secure tile to its base. Although ungrouted mosaics do exist, most have grout between the tesserae. Tiling grout is", 
                "pageCategories": "All articles lacking in-text citations\nAll articles needing additional references\nAll articles with unsourced statements\nArticles lacking in-text citations from August 2012\nArticles needing additional references from December 2010\nArticles with unsourced statements from November 2015\nBuilding materials\nCement\nCommons category with local link same as on Wikidata\nConcrete", 
                "pageContent": "Grout is a particularly fluid form of concrete used to fill gaps. It is used in construction to embed rebars in masonry walls, connect sections of pre-cast concrete, fill voids, and seal joints such as those between tiles. Grout is generally a mixture of water, cement, sand, often color tint, and sometimes fine gravel (if it is being used to fill large spaces such as the cores of concrete blocks). Unlike other structural pastes such as plaster or joint compound, correctly-mixed and -applied grout forms a waterproof seal.\nAlthough both are applied as a thick emulsion and harden over time, grout is distinguished from its close relative mortar by its viscosity; grout is thin so it flows readily into gaps, while mortar is thick enough to support not only its own weight, but also that of masonry placed on top of it.\n\n\n== Varieties ==\nGrout varieties include tiling grout, flooring grout, resin grout, non-shrink grout, structural grout and thixotropic grout.\nTiling grout is often used to fill the spaces between tiles or mosaics, and to secure tile to its base. Although ungrouted mosaics do exist, most have grout between the tesserae. Tiling grout is also cement-based, and comes in sanded as well as unsanded varieties. The sanded variety contains finely ground silica sand; unsanded is finer and produces a non-gritty final surface. They are often enhanced with polymers and/ or latex.\nStructural grout is often used in reinforced masonry to fill voids in masonry housing reinforcing steel, securing the steel in place and bonding it to the masonry. Non-shrink grout is used beneath metal bearing plates to ensure a consistent bearing surface between the plate and its substrate.\nPortland cement is the most common cementing agent in grout, but urethane- and epoxy-based formulas are also popular.\nPortland cement-based grouts come in different varieties depending on the particle size of the ground clinker used to make the cement, with a standard size of around 15 microns, microfine at around 6\u201310 microns, and ultrafine below 5 microns. Finer particle sizes let the grout penetrate more deeply into a fissure. Because these grouts depend on the presence of sand for their basic strength, they are often somewhat gritty when finally cured and hardened.\nTools associated with groutwork include:\ngrout saw or grout scraper a manual tool for removal of old and discolored grout. The blade is usually composed of tungsten carbide.\ngrout float a trowel-like tool for smoothing the surface of a grout line, typically made of rubber or soft plastic.\ngrout sealer a water-based or solvent-based sealant applied over dried grout that resists water, oil, and acid-based contaminants.\nDremel grout attachment an attachment guide used in a die grinder for faster removal of old grout than a standard grout saw.\npointing Trowel Used for applying grout in flagstone, and other stone works.\n\n\n== See also ==\nMortar\nMortar joint\nCaulk\nThinset\nGlue\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Grout", 
                "title": "Grout"
            }, 
            {
                "snippet": "tiles. A liquid lithium layer on graphite tiles. A liquid lithium layer on top of a boron layer on graphite tiles. A liquid lithium layer on tungsten-based", 
                "pageCategories": "Fusion power\nMaterials science", 
                "pageContent": "In nuclear fusion power research, the plasma-facing material (or materials) (PFM) is any material used to construct the plasma-facing components (PFC), those components exposed to the plasma within which nuclear fusion occurs, and particularly the material used for the lining or first wall of the reactor vessel.\nFusion reactor designs must consider three overall steps for energy generation:\nGenerating heat through fusion,\nCapturing heat in the first wall,\nTransferring heat at a faster rate than capturing heat.\nCurrently, fusion reactor research focuses on improving efficiency and reliability in heat generation, capture, and rate of transfer. Generating electricity from heat is beyond the scope of current research due to existing efficient heat-transfer cycles, such as heating water to operate steam turbines that drive electrical generators.\nCurrent fusion reactors are fueled by deuterium-tritium (D-T) fusion reactions, which produce high-energy neutrons that can damage the first wall. Tritium is not a commonly available isotope due to its short half-life but can be bred by the nuclear reaction of lithium (Li) isotopes with high-energy neutrons that collide with the first wall.\n\n\n== Requirements ==\nMost magnetic confinement fusion devices (MCFD) consist of several key components in their technical designs, including:\nMagnet system: confines the deuterium-tritium fuel in the form of plasma and in the shape of a torus.\nVacuum vessel: contains the core fusion plasma and maintains fusion conditions.\nFirst wall: positioned between the plasma and magnets in order to protect outer vessel components from radiation damage.\nCooling system: removes heat from the confinement and transfers heat from the first wall.\nThe core fusion plasma must not actually touch the first wall. ITER and many other current and projected fusion experiments, particularly those of the tokamak and stellarator designs, use intense magnetic fields in an attempt to achieve this, although plasma instability problems remain. Even with stable plasma confinement, however, the first wall material would be exposed to a neutron flux higher than in any current nuclear power reactor, which leads to two key problems in selecting the material:\nIt must withstand this neutron flux for a sufficient period of time to be economically viable.\nIt must not become sufficiently radioactive so as to produce unacceptable amounts of nuclear waste when lining replacement or plant decommissioning eventually occurs.\nThe lining material must also:\nAllow the passage of a large heat flux.\nBe compatible with intense and fluctuating magnetic fields.\nMinimize contamination of the plasma.\nBe produced and replaced at a reasonable cost.\nSome critical plasma-facing components, such as and in particular the divertor, are typically protected by a different material than that used for the major area of the first wall.\n\n\n== Proposed materials ==\nMaterials currently in use or under consideration include:\nBoron carbide.\nGraphite.\nCarbon fibre composite (CFC).\nBeryllium.\nTungsten.\nMolybdenum.\nLithium.\nMulti-layer tiles of several of these materials are also being considered and used, for example:\nA thin molybdenum layer on graphite tiles.\nA thin tungsten layer on graphite tiles.\nA tungsten layer on top of a molybdenum layer on graphite tiles.\nA boron carbide layer on top of CFC tiles.\nA liquid lithium layer on graphite tiles.\nA liquid lithium layer on top of a boron layer on graphite tiles.\nA liquid lithium layer on tungsten-based solid PFC surfaces or divertors.\nGraphite was used for the first wall material of the Joint European Torus (JET) at its startup (1983), in Tokamak \u00e0 configuration variable (1992) and in National Spherical Torus Experiment (NSTX, first plasma 1999).\nBeryllium was used to reline JET in 2009 in anticipation of its proposed use in ITER.\nTungsten is used for the divertor in JET, and will be used for the divertor in ITER. It is also used for the first wall in ASDEX Upgrade. Graphite tiles plasma sprayed with tungsten were used for the ASDEX Upgrade divertor.\nMolybdenum is used for the first wall material in Alcator C-Mod (1991).\nLiquid lithium (LL) used to coat the PFC of the Tokamak Fusion Test Reactor (TFTR, 1996).\n\n\n== Considerations ==\nDevelopment of satisfactory plasma-facing materials is one of the key problems still to be solved by current programs.\nPlasma-facing materials can be measured for performance in terms of:\nPower production for a given reactor size.\nCost to generate electricity.\nSelf-sufficiency of tritium production.\nAvailability of materials.\nDesign and fabrication of the PFC.\nSafety in waste disposal and in maintenance.\nThe International Fusion Materials Irradiation Facility (IFMIF) will particularly address this. Materials developed using IFMIF will be used in DEMO, the proposed successor to ITER.\nFrench Nobel laureate in physics, Pierre-Gilles de Gennes said of nuclear fusion, \"We say that we will put the sun into a box. The idea is pretty. The problem is, we don't know how to make the box.\"\n\n\n== Recent developments ==\nSolid plasma-facing materials are known to be susceptible to damage under large heat loads and high neutron flux. If damaged, these solids can contaminate the plasma and decrease plasma confinement stability. In addition, radiation can leak through defections in the solids and contaminate outer vessel components.\nLiquid metal plasma-facing components that enclose the plasma have been proposed to address challenges in the PFC. In particular, liquid lithium (LL) has been confirmed to have various properties that are attractive for fusion reactor performance.\n\n\n== Lithium ==\nLithium (Li) is an alkali metal with a low Z (atomic number). Li has a low first ionization energy of ~ 6 eV and is highly chemically reactive with ion species found in the plasma of fusion reactor cores. In particular, Li readily forms stable lithium compounds with hydrogen isotopes, oxygen, carbon, and other impurities found in D-T plasma.\nThe fusion reaction of D-T produces charged and neutral particles in the plasma. The charged particles remain magnetically confined to as a plasma. The neutral particles are not magnetically confined and will move toward the boundary between the hotter plasma and the colder PFC. Upon reaching the first wall, both neutral particles and charged particles that escaped the plasma become cold neutral particles in gaseous form. An outer edge of cold neutral gas is then \"recycled\", or mixed, with the hotter plasma. A temperature gradient between the cold neutral gas and the hot plasma is believed to be the principal cause of anomalous electron and ion transport from the magnetically confined plasma. As recycling decreases, the temperature gradient decreases and plasma confinement stability increases. With better conditions for fusion in the plasma, the reactor performance increases.\nInitial use of lithium in 1990s was motivated by a need for a low-recycling PFC. In 1996, ~ 0.02 grams of lithium coating was added to the PFC of TFTR, resulting in the fusion power output and the fusion plasma confinement to improve by a factor of two. On the first wall, lithium reacted with neutral particles to produce stable lithium compounds, resulting in low-recycling of cold neutral gas. In addition, lithium contamination in the plasma tended to be well below 1%.\nSince 1996, these results have been confirmed by a large number of magnetic confinement fusion devices (MCFD) that have also used lithium in their PFC, for example:\nTFTR (US), CDX-U (2005)/LTX(2010) (US), CPD (Japan), HT-7 (China), EAST (China), FTU (Italy).\nNSTX (US), T-10 (Russia), T-11M (Russia), TJ-II (Spain), RFX (Italy).\nThe primary energy generation in fusion reactor designs is from the absorption of high-energy neutrons. Results from these MCFD highlight additional benefits of liquid lithium coatings for reliable energy generation, including:\nAbsorb high-energy, or fast-moving, neutrons. About 80% of the energy produced in a fusion reaction of D-T is in the kinetic energy of the newly produced neutron.\nConvert kinetic energies of absorbed neutrons into heat on the first wall. The heat that is produced on the first wall can then be removed by coolants in ancillary systems that generate electricity.\nSelf-sufficient breeding of tritium by nuclear reaction with absorbed neutrons. Neutrons of varying kinetic energies will drive tritium-breeding reactions.\nLiquid lithium\nNewer developments in liquid lithium are currently being tested, for example:\nCoatings made of increasingly complex liquid lithium compounds.\nMulti-layered coatings of LL, B, F, and other low-Z metals.\nHigher density coatings of LL for use on PFC designed for greater heat loads and neutron flux.\n\n\n== See also ==\nInternational Fusion Materials Irradiation Facility#Background information.\nLithium Tokamak Experiment.\n\n\n== References ==\n\n\n== External links ==\nhttp://www.ipp.mpg.de/ippcms/eng/for/projekte/pfmc/index.html Max Planck Institute project page on PFM\nhttp://www.ipp.mpg.de/ippcms/eng/for/veranstaltungen/konferenzen/archiv/2011/03pfmc-13/index.html 13th International Workshop on Plasma-Facing Materials and Components for Fusion Applications / 1st International Conference on Fusion Energy Materials Science\n\"Development of W coatings for fusion applications\". Fusion Engineering and Design. 86: 1677\u20131680. doi:10.1016/j.fusengdes.2011.04.031. Retrieved 10 September 2012. Abstract: The paper gives a short overview on tungsten (W) coatings deposited by various methods on carbon materials (carbon fibre composite \u2013 CFC and fine grain graphite \u2013 FGG). Vacuum Plasma Spray (VPS), Chemical Vapor Deposition (CVD) and Physical Vapor Deposition (PVD)... A particular attention is paid to the Combined Magnetron Sputtering and Ion Implantation (CMSII) technique, which was developed during the last 4 years from laboratory to industrial scale and it is successfully applied for W coating (10\u201315 \u03bcm and 20\u201325 \u03bcm) of more than 2500 tiles for the ITER-like Wall project at JET and ASDEX Upgrade.... Experimentally, W/Mo coatings with a thickness up to 50 \u03bcm were produced and successfully tested in the GLADIS ion beam facility up to 23 MW/m2. Keywords: Tungsten coating; Carbon fibre composite (CFC); ITER-like wall; Magnetron sputtering; Ion implantation", 
                "titleUrl": "https://en.wikipedia.org/wiki/Plasma-facing_material", 
                "title": "Plasma-facing material"
            }, 
            {
                "snippet": "pattern terracotta tiles. In 1960, as the stables were being converted to a garage, the roof tiles were replaced with a concrete tiles. Three timber doors", 
                "pageCategories": "1903 establishments in Australia\nAll articles containing potentially dated statements\nArticles containing potentially dated statements from 2010\nCommons category with page title same as on Wikidata\nCoordinates on Wikidata\nGood articles\nLighthouse museums\nLighthouses completed in 1903\nLighthouses in New South Wales\nMuseums in New South Wales", 
                "pageContent": "Norah Head Light is an active lighthouse located at Norah Head, a headland on the Central Coast, New South Wales, Australia, close to Toukley. It is the last lighthouse of the James Barnet style to be built, and the last staffed lighthouse constructed in New South Wales.\nOfficially displayed for the first time in 1903, the original vapourized kerosene burner was upgraded in 1923, electrified in 1961 and automated and demanned in 1994, after more than 90 years of being staffed. It celebrated its centenary in 2003.\nThe concrete block tower is 27.5 metres (90 ft) high, topped by a bluestone gallery. On top of the gallery is the original Chance Bros. lantern. This lantern holds the original housing of the Chance Bros. 1st order bivalve dioptric Fresnel lens. Other important structures include the chief lightkeeper's cottage and assistant keeper's duplex, and a flag house.\n\n\n== History ==\n\nCalls for construction of a lighthouse at Norah Head (then \"Bungaree Noragh Point\") were made as early as 1861 due to many wrecks occurring in the area. A notable supporter in the end of the 19th century was local landholder Edward Hargraves from Noraville. However, these efforts were fruitless for many years. The first formal recommendation to construct the lighthouse was made by the Newcastle Marine Board, just prior to its abolition, in 1897.\nThe lighthouse was designed in a style similar to the style of James Barnet, by his successor Charles Assinder Harding, who also designed Cape Byron Light and Point Perpendicular Light. It is the last to be designed in this style.\nConstruction commenced in 1901, undertaken by day labour. Materials were brought by boat and unloaded on a wharf constructed at Cabbage Tree Harbour for that purpose. It was officially lit on 15 November 1903, two years after Cape Byron Light. The first keepers were N. H. Williams as chief keeper, with N. Hanson and S. Kells as assistant keepers.\nThe cost of the tower and cottages was nearly \u00a324,000, \u00a319,000 for the construction of the tower and \u00a35,000 for the optical apparatus, a Chance Bros. 1st order bivalve dioptric Fresnel lens with 700 prisms\nThe original light source was a vapourized kerosene burner and mantle generating a light intensity of 438,000 cd, visible for 18 nautical miles (33 km; 21 mi). The original mechanism was a grandfather clock-type mechanism with the counterweights going down a 100 feet (30 m) central column. The weights went down gradually as the light turned and had to be wound every half an hour. The light revolved every 10 seconds, and was floating in a mercury bath of more than 15,000 pounds (6,800 kg) to lessen the friction. The high speed of rotation made operating the light while it was active very difficult.\nOn 13 April 1923 the light source was upgraded to a Ford-Schmidt kerosene burner with an intensity of 700,000 candlepower. It was changed to revolve every thirty seconds in 1928, to east the operation. The stables originally constructed at the complex were converted to a garage in 1960.\nOn 28 March 1961 the light was electrified, with mains electricity as the power source, and an intensity of 1,000,000 cd. The drive was replaced with a 0.3 amp electric motor. At the same time the staff was cut from three lightkeepers to two.\nThe light was automated and demanned in 1994. It was one of the last stations in Australia to demanned, after over 90 years of being staffed.\nThe current light source is a 1000 Watt 120 Volt tungsten-halogen lamp, which flashes white every 15 seconds(Fl.W. 15s) and can be seen for 26 nautical miles (48 km; 30 mi). It also shows a fixed red (F.R.) and green (F.G.) lights for coastal shipping.\nThe lighthouse celebrated its centenary on 15 November 2003, and the lighthouse appeared on the cover of the Wyong Shire Council annual report for 2003.\n\n\n== Structures ==\n\nThe lighthouse is a 27.5 metres (90 ft) tower, made from concrete blocks. The concrete blocks were made on the ground using a local aggregate, lifted and cemented into position and finally cement rendered inside and out with deep ashlar coursing, and painted white. This technique was used in the period to reduce the cost of construction.\nOn the inside of the tower there are 96 steps leading to the gallery in 4 stages, the first 3 of the same grade and the last stage a bit steeper and narrower. The staircase is made of concrete with slate treads and cast iron and brass balustrade.\nOn top of the tower is a bluestone gallery and balcony with gunmetal railings. The gallery has a painted cast iron floor grate with a cast iron stair leading to the outdoor gallery. The lantern room is atop this gallery, made of metal and glass, encircled and protected by perspex and aluminium panels. It has a decorative iron catwalk encircling the glass to allow for cleaning. The lantern house is the original 3,700 millimetres (150 in) Chance Bros. cast iron and copper lantern house, one of about 21 known to exist in Australia.\n\nOn the ground floor there is an entrance door made of cedar set with sidelights and fanlight, with an etching on the door glass saying Olim Periculum Nunc Salus, Latin for \"Once Perilous, Now Safe\". Above the door is the writing \"\u2022A1903D\u2022\", stating the year of official lighting. There is also a \"ghost door\" on the outside which was planned but never completed.\nThe first floor comprises an entry hallway and two rooms. The hallways is set with tessellated tiles and still has the original desk for the visitor's book. Of the two rooms, one was the report room used for administrative work, record keeping and logbooks. It is currently used as a radio room. The second room housed a spare mantle holder. It currently houses the electrical controls, including the sensors responsible for starting the light. It also houses the backup batteries, backup diesel generator and fuel tank, as well as a small workshop.\nThe accommodations in the complex consist of a lightkeepers cottage with garden and Assistant Keeper's duplex, both constructed from concrete blocks, unpainted from construction, and originally having \"Marseilles pattern\" terracotta tile roofs. The keeper's cottage includes an open verandah on three sides, with cast iron posts and curved timber beams. The hipped roof is still the original terracotta tiles and one chimney remains. As for the duplex, circa 1970 the roof was replaced with concrete tiles and the chimneys have been demolished.\nAlso constructed were a small fuel store, workshop, paint store and earth closet near the keeper's cottage, and two fuel stores with earth closet and sink for the assistant cottages. All were constructed in the same form, from unpainted concrete blocked and the same terracotta tile roofs. All still remain in the complex, pretty much intact.\nAs of 2010, one of the cottages is occupied by a resident keeper and another two are available for overnight accommodations.\n\nAnother structure at the complex is a small signal house, which was constructed as a flag house for the timber flagstaff, both constructed with the original structures in 1903. The flag house was constructed to match the lighthouse, from precast concrete blocks, rendered walls, with the same plinth and deep ashlar coursing. The roof is made of concrete in a shallow hipped form, in contrast to the concrete dome proposed in the original drawings. The flag locker now houses maritime signal flags. The timber flagstaff was removed at an unknown date, and what remains of it are a concrete and steel base, a concrete apron, and four concrete and iron anchor points.\nA stable was also constructed at the premises from the same concrete blocks with Marseilles pattern terracotta tiles. In 1960, as the stables were being converted to a garage, the roof tiles were replaced with a concrete tiles. Three timber doors to the former stable, tack and carriage rooms remain.\nAlso at the complex are underground fresh water tanks and sealed off condensation water tank beneath the tower.\nAbout 100 metres (330 ft) up the hill there used to be a weather station, a mother station for Montague Island in the south and South Solitary Island in the north. Reports used to be collected and sent to the Weather Bureau in Sydney. This is all done now electronically.\n\n\n== Site operation ==\nThe light is operated by Roads and Maritime Services (formerly NSW Maritime), while the lighthouse reserve is managed by the New South Wales Department of Landssince 1997. The Norah Head Reserve Lighthouse Trust is a government appointed trust \"Dedicated to the preservation, conservation and management of the Norah Head Lighthouse Reserve.\"\n\n\n== Visiting ==\nThe lighthouse is on a narrow strip of land that separates the sea from Tuggerah Lake. The site is accessible and the lighthouse is open for guided tours on weekends and for group bookings during the week. Two cottages are available for overnight staying, housing eight people each. It is also available for weddings.\n\n\n== See also ==\n\nList of lighthouses and lightvessels in Australia\n\n\n== Notes ==\n\n\n== References ==\n\n\n== External links ==\nNorah's Head Lighthouse - official site\nGrant and Tracey's Lighthouse Pages - Norah Head\nWyong Shire Council Norah Head Lighthouse Centenary 2003", 
                "titleUrl": "https://en.wikipedia.org/wiki/Norah_Head_Light", 
                "title": "Norah Head Light"
            }, 
            {
                "snippet": "energy, and in a too wide cone, to the ceramic tile, damaging it even further. Metals used include a tungsten alloy for the Challenger 2 or, in the case of", 
                "pageCategories": "All articles that may contain original research\nAll articles with unsourced statements\nArticles that may contain original research from September 2007\nArticles with unsourced statements from May 2009\nArticles with unsourced statements from November 2009\nArticles with unsourced statements from November 2010\nBritish inventions\nComposite materials\nScience and technology in the United Kingdom\nVehicle armour", 
                "pageContent": "Chobham armour is the informal name of a composite armour developed in the 1960s at the British tank research centre on Chobham Common, Surrey. The name has since become the common generic term for composite ceramic vehicle armour. Other names informally given to Chobham Armour include \"Burlington\" and \"Dorchester.\"\nAlthough the construction details of the Chobham armour remain a secret, it has been described as being composed of ceramic tiles encased within a metal framework and bonded to a backing plate and several elastic layers. Due to the extreme hardness of the ceramics used, they offer superior resistance against shaped charges such as high explosive anti-tank (HEAT) rounds and they shatter kinetic energy penetrators.\nThe armour was first tested in the context of the development of a British prototype vehicle, the FV4211, and first applied on the preseries of the American M1. Only the M1 Abrams, Challenger 1, and Challenger 2 tanks have been disclosed as being thus armoured. The casting of the ceramic produces large blocks, giving these tanks, and especially their turrets, a distinctive angled appearance.\n\n\n== Protective qualities ==\nDue to the extreme hardness of the ceramics used, they offer superior resistance against a shaped charge jet and they shatter kinetic energy penetrators (KE-penetrators). The (pulverised) ceramic also strongly abrades any penetrator. Against lighter projectiles the hardness of the tiles causes a \"shatter gap\" effect: a higher velocity will, within a certain velocity range (the \"gap\"), not lead to a deeper penetration but destroy the projectile itself instead. Because the ceramic is so brittle the entrance channel of a shaped charge jet is not smooth\u2014as it would be when penetrating a metal\u2014but ragged, causing extreme asymmetric pressures which disturb the geometry of the jet, on which its penetrative capabilities are critically dependent as its mass is relatively low. This initiates a vicious circle as the disturbed jet causes still greater irregularities in the ceramic, until in the end it is defeated. The newer composites, though tougher, optimise this effect as tiles made with them have a layered internal structure conducive to it, causing \"crack deflection\". This mechanism\u2014using the jet's own energy against it\u2014has caused the effects of Chobham to be compared to those of reactive armour. This should not be confused with the effect used in many laminate armours of any kind: that of sandwiching an inert but soft elastic material such as rubber, between two of the armour plates. The impact of either a shaped charge jet or long-rod penetrator after the first layer has been perforated and while the rubber layer is being penetrated will cause the rubber to deform and expand, so deforming both the back and front plates. Both attack methods will suffer from obstruction to their expected paths, so experiencing a greater thickness of armour than there is nominally, thus lowering penetration. Also for rod penetrations, the transverse force experienced due to the deformation may cause the rod to shatter, bend, or just change its path, again lowering penetration.\nTo date, few Chobham armour-protected tanks have been defeated by enemy fire in combat; the relevance of individual cases of lost tanks for determining the protective qualities of Chobham armour is difficult to ascertain as the extent to which such tanks are protected by ceramic modules is undisclosed.\nDuring the second Iraq war in 2003, a Challenger 2 tank became stuck in a ditch while fighting in Basra against Iraqi forces. The crew remained safe inside for many hours, the composite Chobham 2 armour protecting them from enemy fire, including rocket propelled grenades.\n\n\n== Structure ==\nCeramic tiles have a \"multiple hit capability\" problem in that they cannot sustain successive impacts without quickly losing much of their protective value. To minimise the effects of this the tiles are made as small as possible, but the matrix elements have a minimal practical thickness of about one inch (25 mm), and the ratio of coverage provided by tiles would become unfavourable, placing a practical limit at a diameter of about four inches (ten centimetres). The small hexagonal or square ceramic tiles are encased within the matrix either by isostatically pressing them into the heated matrix, or by gluing them with an epoxy resin. Since the early nineties it has been known that holding the tiles under constant compression by their matrix greatly improves their resistance to kinetic penetrators, which is difficult to achieve when using glues.\nThe matrix has to be backed by a plate, both to reinforce the ceramic tiles from behind and to prevent deformation of the metal matrix by a kinetic impact. Typically the backing plate has half of the mass of the composite matrix. The assemblage is again attached to elastic layers. These absorb impacts somewhat, but their main function is to prolong the service life of the composite matrix by protecting it against vibrations. Several assemblages can be stacked, depending on the available space; this way the armour can be made of a modular nature, adaptable to the tactical situation. The thickness of a typical assemblage is today about five to six centimetres. Earlier assemblages, so-called DOP (Depth Of Penetration) -matrices, were thicker. The relative interface defeat component of the protective value of a ceramic is much larger than for steel armour. Using a number of thinner matrices again enlarges that component for the entire armour package, an effect analogous to the use of alternate layers of high hardness and softer steel, which is typical for the glacis of modern Soviet tanks.\nCeramic tiles draw little or no advantage from sloped armour as they lack sufficient toughness to significantly deflect heavy penetrators. Indeed, because a single glancing shot could crack many tiles, the placement of the matrix is chosen so as to optimise the chance of a perpendicular hit, a reversal of the previous desired design feature for conventional armour. Ceramic armour normally even offers better protection for a given areal density when placed perpendicularly than when placed obliquely, because the cracking propagates along the surface normal of the plate. Instead of rounded forms, the turrets of tanks using Chobham armour typically have a slab-sided appearance.\nThe backing plate reflects the impact energy back to the ceramic tile in a wider cone. This dissipates the energy, limiting the cracking of the ceramic, but also means a more extended area is damaged. Spalling caused by the reflected energy can be partially prevented by a malleable thin graphite layer on the face of the ceramic absorbing the energy without making it strongly rebound again as a metal face plate would.\nTiles under compression suffer far less from impacts; in their case it can be advantageous to have a metal face plate bringing the tile also under perpendicular compression. The confined ceramic tile then reinforces the metal face plate, a reversal of the normal situation.\nA gradual technological development has taken place in ceramic armour: ceramic tiles, in themselves vulnerable to low energy impacts, were first reinforced by glueing them to a backplate; in the nineties their resistance was increased by bringing them under compression on two axes; in the final phase a third compression axis was added to optimise impact resistance. To confine the ceramic core several advanced techniques are used, supplementing the traditional machining and welding, including sintering the suspension material around the core; squeeze casting of molten metal around the core and spraying the molten metal onto the ceramic tile.\nThe whole is placed within the shell formed by the outer and inner wall of the tank turret or hull, the inner wall being the thicker.\n\n\n=== Material ===\nOver the years newer and tougher composites have been developed, giving about five times the protection value of the original pure ceramics, the best of which were again about five times as effective as a steel plate of equal weight. These are often a mixture of several ceramic materials, or metal matrix composites which combine ceramic compounds within a metal matrix. The latest developments involve the use of carbon nanotubes to improve toughness even further. Commercially produced or researched ceramics for such type of armour include boron carbide, silicon carbide, aluminium oxide (sapphire or \"alumina\"), aluminium nitride, titanium boride and Syndite, a synthetic diamond composite. Of these boron carbide is the hardest and lightest, but also the most expensive and brittle. Boron carbide composites are today favoured for ceramic plates protecting against smaller projectiles, such as used in body armour and armoured helicopters; this was in fact in the early sixties the first general application of ceramic armour. Silicon carbide, better suited to protect against larger projectiles, was at that time only used in some prototype land vehicles, such as the MBT-70. The ceramics can be created by cold pressing or hot pressing. A high density is striven for in that air bubbles should be almost absent.\nA matrix using a titanium alloy is extremely expensive to manufacture but the metal is favoured for its lightness, strength and resistance to corrosion, which is a constant problem. The Rank company claims to have invented an alumina matrix for the insertion of boron carbide or silicon carbide tiles.\nThe backing plate can be made from steel, but, as its main function is to improve the stability and stiffness of the assemblage, aluminium is more weight-efficient in light AFVs only to be protected against light anti-tank weapons. A deformable composite backing plate can combine the function of a metal backing plate and an elastic layer.\n\n\n=== Heavy metal modules ===\nThe armour configuration of the first western tanks using Chobham armour was optimised to defeat shaped charges as guided missiles were seen as the greatest threat. In the eighties however they began to face improved Soviet kinetic energy penetrator rounds of various sorts, which the ceramic layer was not particularly effective against: the original ceramics had a resistance against penetrators of about a third compared to that against HEAT rounds, for the newest composites it is about one-tenth. For this reason many modern designs include additional layers of heavy metals to add more density to the overall armour package.\nThe introduction of more effective ceramic composite materials allows for a larger width of these metal layers within the armour shell: given a certain protection level provided by the composite matrix, it can be thinner. They typically form an inner layer placed below the much more expensive matrix, to prevent extensive damage to it should the metal layer strongly deform but not defeat a penetrator. They can also be used as the backing plate for the matrix itself, but this compromises the modularity and thus tactical adaptability of the armour system: ceramic and metal modules can then no longer be replaced independently. Furthermore, due to their extreme hardness, they deform insufficiently and would reflect too much of the impact energy, and in a too wide cone, to the ceramic tile, damaging it even further. Metals used include a tungsten alloy for the Challenger 2 or, in the case of the M1A1HA (Heavy Armor) and later American tank variants, a depleted uranium alloy. Some companies offer titanium carbide modules.\nThese metal modules (typically employing perpendicular rods) have many perforations or expansion spaces reducing the weight up to about a third while keeping the protective qualities fairly constant. The depleted uranium alloy of the M1 has been described as \"arranged in a type of armour matrix\" and a single module as a \"stainless-steel shell surrounding a layer (probably an inch or two thick) of depleted uranium, woven into a wire-mesh blanket\".\nSuch modules are also used by tanks not equipped with Chobham armour. The combination of a composite matrix and heavy metal modules is sometimes informally referred to as \"second generation Chobham\".\n\n\n== Development and application ==\n\nThe concept of ceramic armour goes back to 1918, when Major Neville Monroe Hopkins discovered that a plate of ballistic steel was much more resistant to penetration if covered with a thin (1\u20132 millimetres) layer of enamel.\nSince the early sixties there were, in the US, extensive research programmes ongoing aimed at investigating the prospects of employing composite ceramic materials as vehicle armour. This research mainly focused on the use of an aluminium metal matrix composite reinforced by silicon carbide whiskers, to be produced in the form of large sheets. The reinforced light metal sheets were to be sandwiched between steel layers. This arrangement had the advantage of having a good multiple-hit capability and of being able to be curved, allowing the main armour to benefit from a sloped armour effect. However, this composite with a high metal content was primarily intended to increase the protection against KE-penetrators for a given armour weight; its performance against shaped charge attack was mediocre and would have to be improved by means of a laminate spaced armour effect, as researched by the Germans within the joint MBT-70 project.\nAn alternative technology developed in the US was based on the use of glass modules to be inserted into the main armour; although this arrangement offered a better shaped charge protection, its multiple hit capability was poor. A similar system using glass inserts in the main steel armour was from the late fifties researched for the Soviet Obiekt 430 prototype of the T-64; this was later developed into the \"Combination-K\" type, having a ceramic compound mixed with the silicon oxide inserts, which offered about 50% better protection against both shaped charge and KE-penetrator threats, compared with a steel armour of the same weight. It was, later in several improved forms, incorporated into all subsequent Soviet main battle tank designs. After an initial period of speculation in the West as to its true nature, the characteristics of this type were disclosed when the dissolution of the Soviet Union in 1991 and the introduction of a market system forced the Russian industries to find new customers by highlighting its good qualities; it is today rarely referred to as Chobham armour.\n\nIn the United Kingdom another line of ceramic armour development had been started in the early 1960s, meant to improve the existing cast turret configuration of the Chieftain that already offered excellent heavy penetrator protection; the research by a team headed by Gilbert Harvey of the Fighting Vehicles Research and Development Establishment (FVRDE), therefore was strongly oriented at optimising the ceramic composite system for defeating shaped charge attack. The British system consisted of a honeycomb matrix with ceramic tiles backed by ballistic nylon, placed on top of the cast main armour. In July 1973 an American delegation, in search of a new armour type for the XM815 tank prototype, now that the MBT-70 project had failed, visited Chobham Common to be informed about the British system, the development of which had then cost about \u20a46,000,000; earlier information had already been divulged to the US in 1965 and 1968. It was very impressed by the excellent shaped-charge protection combined with the penetrator impact damage limitation, inherent to the principle of using tiles. The Ballistic Research Laboratory at the Aberdeen Proving Ground that year initiated the development of a version, named Burlington, adapted to the specific American situation, characterised by a much higher projected tank production run and the use of a thinner rolled steel main armour. The increased threat posed by a new generation of Soviet guided missiles armed with a shaped charge warhead\u2014as demonstrated in the Yom Kippur War of October 1973, when even older-generation missiles caused considerable tank losses on the Israeli side\u2014made Burlington the preferred choice for the armour configuration of the XM1 (the renamed XM815) prototype.\nHowever, on 11 December 1974 a Memorandum of Understanding was signed between the Federal Republic of Germany and the US about the common future production of a main battle tank; this made any application of Chobham armour dependent on the eventual choice for a tank type. Earlier in 1974 the Americans had asked the Germans to redesign the existing Leopard 2 prototypes, considered by them too lightly armoured, and had suggested adoption of Burlington for this purpose, of which type the Germans had already been informed in March 1970; the Germans however in response in 1974 initiated a new armour development programme of their own. Having already designed a system that in their opinion offered satisfactory protection against shaped charges, consisting of multiple-laminate spaced armour with the spaces filled with ceramic polystyrene foam as fitted to the Leopard 1A3, they put a clear emphasis on improving KE-penetrator protection, reworking the system into a perforated metal module armour. A version with added Burlington was considered, including ceramic inserts in the various spaces, but rejected as it would push vehicle weight well over sixty metric tonnes, a weight then seen as prohibitive by both armies. The US Army in the summer of 1974 faced the choice between the German system and their own Burlington, a decision made more difficult by the fact that Burlington offered, compared with steel armour, no weight advantage against KE-penetrators: the total armour system would have a RHA equivalence against them of about 350 mm (compared to about 700 mm against shaped charges). No consensus developing, General Creighton Abrams himself decided the issue in favour of Burlington. Eventually each army procured its own national tank design, the project of a common tank failing in 1976. In February 1978 the first tanks protected by Burlington left the factory when the first of eleven pilot M1 tanks were delivered by Chrysler Corporation to the US Army.\nBeside these state projects, private enterprise in the US during the seventies also developed ceramic armour types, like the Noroc armour made by the Protective Products Division of the Norton Company, consisting of boron carbide sheets backed by resin-bonded glass cloth.\n\nIn the United Kingdom application of Chobham armour was delayed by the failure of several advanced tank projects: first that of a joint German-British main battle tank; then the purely British MBT-80 programme. A first directive to prepare Chobham armour technology for application in 1975 was already given in 1969. It was determined by a study of a possible Chobham-armour protected MICV that a completely new design using only Chobham armour for the most vulnerable front and side sectors (thus without an underlying steel main armour) could be 10% lighter for the same level of protection against KE-ammunition, but to limit costs it was decided to base the first design on the conventional Chieftain. The prototype, FV 4211 or the \"Aluminium Chieftain\", was fitted with a welded aluminium add-on armour, in essence a box on the front hull and front and side turret to contain the ceramic modules, of which box the fifty millimetre thick inner wall due to its relative softness could serve as their backing plate. The extra weight of the aluminium was limited to less than two tonnes and it was shown that it was not overly susceptible to cracking, as first feared. Ten test vehicles were ordered but only the original one had been built when the project was cancelled in favour of the more advanced programmes. However, the Iranian government ordered 1,225 vehicles of an upgraded Chieftain type, the Shir-2 (FV 4030/3), using the same technology of adding Chobham armour to the main cast armour, bringing total weight to 62 metric tonnes. When this order was cancelled in February 1979 because of the Iranian Revolution, the British government, under pressure to modernise its tank fleet to maintain a qualitative superiority relative to the Soviet tank forces, decided to use the sudden surplus production capacity to procure a number of vehicles very close in design to the Shir-2, called the Challenger 1. On 12 April 1983 the first British tank protected by Chobham armour was delivered to the Royal Hussars.\nIn France from 1966 GIAT Industries performed experiments aimed at developing a light vehicle ceramic armour, in 1970 resulting in the CERALU-system consisting of aluminium-backed alumina weldable to the vehicle, offering a 50% increase in weight-efficiency against ballistic threats compared to steel plate. An improved version was later applied in helicopter seats.\nThe latest version of Chobham armour is used on the Challenger 2 (called Dorchester armour), and (though the composition most probably differs) the M1 Abrams series of tanks, which according to official sources is currently protected by silicon carbide tiles. Given the publicly stated protection level for the earliest M1: 350 mm steel equivalence against KE-penetrators (APFSDS), it seems to have been equipped with alumina tiles.\nThough it is often claimed to be otherwise, the original production model of the Leopard 2 did not use Chobham armour, but a combined spaced armour and perforated armour configuration, cheaper in terms of procurement, maintenance and replacement than a ceramic armour system. For many modern tanks, such the Italian Ariete, it is yet unknown which type is used. There was a general trend in the eighties away from ceramic armour towards perforated armour, but even many tanks from the seventies like the Leopard 1A3 and A4, the French AMX 32 and AMX 40 prototypes used the latter system; the Leclerc has an improved version.\n\n\n== Aerospace applications ==\nThe first ceramic plates found application in the aerospace sector: in 1965, the helicopter UH-1 Huey was modified with HFC (Hard-Faced-Composite) around the seats of pilot and copilot, protecting them against small arms fire. The plates were in boron carbide, which, though exceedingly costly, due to its superior lightness has remained the material of choice for aerospace applications. An example among many, the modern V-22 Osprey is protected similarly.\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Further reading ==\nJeffrey J. Swab (Editor), Dongming Zhu (General Editor), Waltraud M. Kriven (General Editor); Advances in Ceramic Armor: A Collection of Papers Presented at the 29th International Conference on Advanced Ceramics and Composites, January 23\u201328, 2005, Cocoa Beach, Florida, Ceramic Engineering and Science Proceedings, Volume 26, Number 7; ISBN 1-57498-237-0\n\n\n== External links ==\n[1]\nArticle on DSTL/QinetiQ Chertsey and Longcross Test Track (Chobham Tank Research Centre)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Chobham_armour", 
                "title": "Chobham armour"
            }, 
            {
                "snippet": "productions. The crew made several ceilings from bleached muslin tiles. Stern lit the tiles from above to produce a soft, warm light that was intended to", 
                "pageCategories": "2000s drama films\n2000s thriller films\n2008 films\nAll articles with dead external links\nAmerican drama films\nAmerican films\nAmerican thriller drama films\nArticles with dead external links from September 2016\nCS1 Dutch-language sources (nl)\nCS1 French-language sources (fr)", 
                "pageContent": null, 
                "titleUrl": "https://en.wikipedia.org/wiki/Changeling_(film)", 
                "title": "Changeling (film)"
            }, 
            {
                "snippet": "analogues of the third series transition elements, hafnium, tantalum and tungsten. The existence of a second inner transition series, in the form of the", 
                "pageCategories": "1869 works\nCS1 French-language sources (fr)\nCS1 German-language sources (de)\nCS1 Russian-language sources (ru)\nCS1 uses Russian-language script (ru)\nClassification systems\nDmitri Mendeleev\nFeatured articles\nPeriodic table\nRussian inventions", 
                "pageContent": "The periodic table is a tabular arrangement of the chemical elements, ordered by their atomic number (number of protons), electron configurations, and recurring chemical properties. This ordering shows periodic trends, such as elements with similar behaviour in the same column. It also shows four rectangular blocks with some approximately similar chemical properties. In general, within one row (period) the elements are metals on the left, and non-metals on the right.\nThe rows of the table are called periods; the columns are called groups. Six groups (columns) have names as well as numbers: for example, group 17 elements are the halogens; and group 18, the noble gases. The periodic table can be used to derive relationships between the properties of the elements, and predict the properties of new elements yet to be discovered or synthesized. The periodic table provides a useful framework for analyzing chemical behaviour, and is widely used in chemistry and other sciences.\nDmitri Mendeleev published in 1869 the first widely recognized periodic table. He developed his table to illustrate periodic trends in the properties of the then-known elements. Mendeleev also predicted some properties of then-unknown elements that would be expected to fill gaps in this table. Most of his predictions were proved correct when the elements in question were subsequently discovered. Mendeleev's periodic table has since been expanded and refined with the discovery or synthesis of further new elements and the development of new theoretical models to explain chemical behaviour.\nAll elements from atomic numbers 1 (hydrogen) to 118 (ununoctium) have been discovered or synthesized, with the most recent additions (elements 113, 115, 117, and 118) being confirmed by the IUPAC on December 30, 2015. The first 94 elements exist naturally, although some are found only in trace amounts and were synthesized in laboratories before being found in nature. Elements with atomic numbers from 95 to 118 have only been synthesized in laboratories or nuclear reactors. Synthesis of elements having higher atomic numbers is being pursued. Numerous synthetic radionuclides of naturally occurring elements have also been produced in laboratories.\n\n\n== Overview ==\n\nEach chemical element has a unique atomic number (Z) representing the number of protons in its nucleus. Most elements have differing numbers of neutrons among different atoms, with these variants being referred to as isotopes. For example, carbon has three naturally occurring isotopes: all of its atoms have six protons and most have six neutrons as well, but about one per cent have seven neutrons, and a very small fraction have eight neutrons. Isotopes are never separated in the periodic table; they are always grouped together under a single element. Elements with no stable isotopes have the atomic masses of their most stable isotopes, where such masses are shown, listed in parentheses.\nIn the standard periodic table, the elements are listed in order of increasing atomic number (the number of protons in the nucleus of an atom). A new row (period) is started when a new electron shell has its first electron. Columns (groups) are determined by the electron configuration of the atom; elements with the same number of electrons in a particular subshell fall into the same columns (e.g. oxygen and selenium are in the same column because they both have four electrons in the outermost p-subshell). Elements with similar chemical properties generally fall into the same group in the periodic table, although in the f-block, and to some respect in the d-block, the elements in the same period tend to have similar properties, as well. Thus, it is relatively easy to predict the chemical properties of an element if one knows the properties of the elements around it.\nAs of 2016, the periodic table has 118 confirmed elements, from element 1 (hydrogen) to 118 (ununoctium). Elements 113, 115, 117 and 118 were officially confirmed by the International Union of Pure and Applied Chemistry (IUPAC) in December 2015. Their proposed names, nihonium (Nh), moscovium (Mc), tennessine (Ts) and oganesson (Og) respectively, were announced by the IUPAC in June 2016. These names will not be formally approved until after the five-month public comment period ends in November 2016. Until then, they are formally identified by their atomic number (e.g., \"element 113\"), or by their provisional systematic name (\"ununtrium\", symbol \"Uut\").\nThe first 94 elements occur naturally; the remaining 24, americium to ununoctium (95\u2013118) occur only when synthesized in laboratories. Of the 94 naturally occurring elements, 83 are primordial and 11 occur only in decay chains of primordial elements. No element heavier than einsteinium (element 99) has ever been observed in macroscopic quantities in its pure form, nor has astatine (element 85); francium (element 87) has been only photographed in the form of light emitted from microscopic quantities (300,000 atoms).\n\n\n== Grouping methods ==\n\n\n=== Groups ===\n\nA group or family is a vertical column in the periodic table. Groups usually have more significant periodic trends than periods and blocks, explained below. Modern quantum mechanical theories of atomic structure explain group trends by proposing that elements within the same group generally have the same electron configurations in their valence shell. Consequently, elements in the same group tend to have a shared chemistry and exhibit a clear trend in properties with increasing atomic number. However, in some parts of the periodic table, such as the d-block and the f-block, horizontal similarities can be as important as, or more pronounced than, vertical similarities.\nUnder an international naming convention, the groups are numbered numerically from 1 to 18 from the leftmost column (the alkali metals) to the rightmost column (the noble gases). Previously, they were known by roman numerals. In America, the roman numerals were followed by either an \"A\" if the group was in the s- or p-block, or a \"B\" if the group was in the d-block. The roman numerals used correspond to the last digit of today's naming convention (e.g. the group 4 elements were group IVB, and the group 14 elements were group IVA). In Europe, the lettering was similar, except that \"A\" was used if the group was before group 10, and \"B\" was used for groups including and after group 10. In addition, groups 8, 9 and 10 used to be treated as one triple-sized group, known collectively in both notations as group VIII. In 1988, the new IUPAC naming system was put into use, and the old group names were deprecated.\nSome of these groups have been given trivial (unsystematic) names, as seen in the table below, although some are rarely used. Groups 3\u201310 have no trivial names and are referred to simply by their group numbers or by the name of the first member of their group (such as \"the scandium group\" for Group 3), since they display fewer similarities and/or vertical trends.\nElements in the same group tend to show patterns in atomic radius, ionization energy, and electronegativity. From top to bottom in a group, the atomic radii of the elements increase. Since there are more filled energy levels, valence electrons are found farther from the nucleus. From the top, each successive element has a lower ionization energy because it is easier to remove an electron since the atoms are less tightly bound. Similarly, a group has a top to bottom decrease in electronegativity due to an increasing distance between valence electrons and the nucleus. There are exceptions to these trends, however, an example of which occurs in group 11 where electronegativity increases farther down the group.\n\n\n=== Periods ===\n\nA period is a horizontal row in the periodic table. Although groups generally have more significant periodic trends, there are regions where horizontal trends are more significant than vertical group trends, such as the f-block, where the lanthanides and actinides form two substantial horizontal series of elements.\nElements in the same period show trends in atomic radius, ionization energy, electron affinity, and electronegativity. Moving left to right across a period, atomic radius usually decreases. This occurs because each successive element has an added proton and electron, which causes the electron to be drawn closer to the nucleus. This decrease in atomic radius also causes the ionization energy to increase when moving from left to right across a period. The more tightly bound an element is, the more energy is required to remove an electron. Electronegativity increases in the same manner as ionization energy because of the pull exerted on the electrons by the nucleus. Electron affinity also shows a slight trend across a period. Metals (left side of a period) generally have a lower electron affinity than nonmetals (right side of a period), with the exception of the noble gases.\n\n\n=== Blocks ===\n\nSpecific regions of the periodic table can be referred to as blocks in recognition of the sequence in which the electron shells of the elements are filled. Each block is named according to the subshell in which the \"last\" electron notionally resides. The s-block comprises the first two groups (alkali metals and alkaline earth metals) as well as hydrogen and helium. The p-block comprises the last six groups, which are groups 13 to 18 in IUPAC group numbering (3A to 8A in American group numbering) and contains, among other elements, all of the metalloids. The d-block comprises groups 3 to 12 (or 3B to 2B in American group numbering) and contains all of the transition metals. The f-block, often offset below the rest of the periodic table, has no group numbers and comprises lanthanides and actinides.\n\n\n=== Metals, metalloids and nonmetals ===\n\nAccording to their shared physical and chemical properties, the elements can be classified into the major categories of metals, metalloids and nonmetals. Metals are generally shiny, highly conducting solids that form alloys with one another and salt-like ionic compounds with nonmetals (other than the noble gases). The majority of nonmetals are coloured or colourless insulating gases; nonmetals that form compounds with other nonmetals feature covalent bonding. In between metals and nonmetals are metalloids, which have intermediate or mixed properties.\nMetal and nonmetals can be further classified into subcategories that show a gradation from metallic to non-metallic properties, when going left to right in the rows. The metals are subdivided into the highly reactive alkali metals, through the less reactive alkaline earth metals, lanthanides and actinides, via the archetypal transition metals, and ending in the physically and chemically weak post-transition metals. The nonmetals are simply subdivided into the polyatomic nonmetals, which, being nearest to the metalloids, show some incipient metallic character; the diatomic nonmetals, which are essentially nonmetallic; and the monatomic noble gases, which are nonmetallic and almost completely inert. Specialized groupings such as the refractory metals and the noble metals, which are subsets (in this example) of the transition metals, are also known and occasionally denoted.\nPlacing the elements into categories and subcategories based on shared properties is imperfect. There is a spectrum of properties within each category and it is not hard to find overlaps at the boundaries, as is the case with most classification schemes. Beryllium, for example, is classified as an alkaline earth metal although its amphoteric chemistry and tendency to mostly form covalent compounds are both attributes of a chemically weak or post transition metal. Radon is classified as a nonmetal and a noble gas yet has some cationic chemistry that is more characteristic of a metal. Other classification schemes are possible such as the division of the elements into mineralogical occurrence categories, or crystalline structures. Categorizing the elements in this fashion dates back to at least 1869 when Hinrichs wrote that simple boundary lines could be drawn on the periodic table to show elements having like properties, such as the metals and the nonmetals, or the gaseous elements.\n\n\n== Periodic trends ==\n\n\n=== Electron configuration ===\n\nThe electron configuration or organisation of electrons orbiting neutral atoms shows a recurring pattern or periodicity. The electrons occupy a series of electron shells (numbered shell 1, shell 2, and so on). Each shell consists of one or more subshells (named s, p, d, f and g). As atomic number increases, electrons progressively fill these shells and subshells more or less according to the Madelung rule or energy ordering rule, as shown in the diagram. The electron configuration for neon, for example, is 1s2 2s2 2p6. With an atomic number of ten, neon has two electrons in the first shell, and eight electrons in the second shell\u2014two in the s subshell and six in the p subshell. In periodic table terms, the first time an electron occupies a new shell corresponds to the start of each new period, these positions being occupied by hydrogen and the alkali metals.\n\nSince the properties of an element are mostly determined by its electron configuration, the properties of the elements likewise show recurring patterns or periodic behaviour, some examples of which are shown in the diagrams below for atomic radii, ionization energy and electron affinity. It is this periodicity of properties, manifestations of which were noticed well before the underlying theory was developed, that led to the establishment of the periodic law (the properties of the elements recur at varying intervals) and the formulation of the first periodic tables.\n\n\n=== Atomic radii ===\n\nAtomic radii vary in a predictable and explainable manner across the periodic table. For instance, the radii generally decrease along each period of the table, from the alkali metals to the noble gases; and increase down each group. The radius increases sharply between the noble gas at the end of each period and the alkali metal at the beginning of the next period. These trends of the atomic radii (and of various other chemical and physical properties of the elements) can be explained by the electron shell theory of the atom; they provided important evidence for the development and confirmation of quantum theory.\nThe electrons in the 4f-subshell, which is progressively filled from cerium (element 58) to ytterbium (element 70), are not particularly effective at shielding the increasing nuclear charge from the sub-shells further out. The elements immediately following the lanthanides have atomic radii that are smaller than would be expected and that are almost identical to the atomic radii of the elements immediately above them. Hence hafnium has virtually the same atomic radius (and chemistry) as zirconium, and tantalum has an atomic radius similar to niobium, and so forth. This is known as the lanthanide contraction. The effect of the lanthanide contraction is noticeable up to platinum (element 78), after which it is masked by a relativistic effect known as the inert pair effect. The d-block contraction, which is a similar effect between the d-block and p-block, is less pronounced than the lanthanide contraction but arises from a similar cause.\n\n\n=== Ionization energy ===\n\nThe first ionization energy is the energy it takes to remove one electron from an atom, the second ionization energy is the energy it takes to remove a second electron from the atom, and so on. For a given atom, successive ionization energies increase with the degree of ionization. For magnesium as an example, the first ionization energy is 738 kJ/mol and the second is 1450 kJ/mol. Electrons in the closer orbitals experience greater forces of electrostatic attraction; thus, their removal requires increasingly more energy. Ionization energy becomes greater up and to the right of the periodic table.\nLarge jumps in the successive molar ionization energies occur when removing an electron from a noble gas (complete electron shell) configuration. For magnesium again, the first two molar ionization energies of magnesium given above correspond to removing the two 3s electrons, and the third ionization energy is a much larger 7730 kJ/mol, for the removal of a 2p electron from the very stable neon-like configuration of Mg2+. Similar jumps occur in the ionization energies of other third-row atoms.\n\n\n=== Electronegativity ===\n\nElectronegativity is the tendency of an atom to attract electrons. An atom's electronegativity is affected by both its atomic number and the distance between the valence electrons and the nucleus. The higher its electronegativity, the more an element attracts electrons. It was first proposed by Linus Pauling in 1932. In general, electronegativity increases on passing from left to right along a period, and decreases on descending a group. Hence, fluorine is the most electronegative of the elements, while caesium is the least, at least of those elements for which substantial data is available.\nThere are some exceptions to this general rule. Gallium and germanium have higher electronegativities than aluminium and silicon respectively because of the d-block contraction. Elements of the fourth period immediately after the first row of the transition metals have unusually small atomic radii because the 3d-electrons are not effective at shielding the increased nuclear charge, and smaller atomic size correlates with higher electronegativity. The anomalously high electronegativity of lead, particularly when compared to thallium and bismuth, appears to be an artifact of data selection (and data availability)\u2014methods of calculation other than the Pauling method show the normal periodic trends for these elements.\n\n\n=== Electron affinity ===\n\nThe electron affinity of an atom is the amount of energy released when an electron is added to a neutral atom to form a negative ion. Although electron affinity varies greatly, some patterns emerge. Generally, nonmetals have more positive electron affinity values than metals. Chlorine most strongly attracts an extra electron. The electron affinities of the noble gases have not been measured conclusively, so they may or may not have slightly negative values.\nElectron affinity generally increases across a period. This is caused by the filling of the valence shell of the atom; a group 17 atom releases more energy than a group 1 atom on gaining an electron because it obtains a filled valence shell and is therefore more stable.\nA trend of decreasing electron affinity going down groups would be expected. The additional electron will be entering an orbital farther away from the nucleus. As such this electron would be less attracted to the nucleus and would release less energy when added. However, in going down a group, around one-third of elements are anomalous, with heavier elements having higher electron affinities than their next lighter congenors. Largely, this is due to the poor shielding by d and f electrons. A uniform decrease in electron affinity only applies to group 1 atoms.\n\n\n=== Metallic character ===\nThe lower the values of ionization energy, electronegativity and electron affinity, the more metallic character the element has. Conversely, nonmetallic character increases with higher values of these properties. Given the periodic trends of these three properties, metallic character tends to decrease going across a period (or row) and, with some irregularities (mostly) due to poor screening of the nucleus by d and f electrons, and relativistic effects, tends to increase going down a group (or column or family). Thus, the most metallic elements (such as caesium and francium) are found at the bottom left of traditional periodic tables and the most nonmetallic elements (oxygen, fluorine, chlorine) at the top right. The combination of horizontal and vertical trends in metallic character explains the stair-shaped dividing line between metals and nonmetals found on some periodic tables, and the practice of sometimes categorizing several elements adjacent to that line, or elements adjacent to those elements, as metalloids.\n\n\n== History ==\n\n\n=== First systemization attempts ===\n\nIn 1789, Antoine Lavoisier published a list of 33 chemical elements, grouping them into gases, metals, nonmetals, and earths. Chemists spent the following century searching for a more precise classification scheme. In 1829, Johann Wolfgang D\u00f6bereiner observed that many of the elements could be grouped into triads based on their chemical properties. Lithium, sodium, and potassium, for example, were grouped together in a triad as soft, reactive metals. D\u00f6bereiner also observed that, when arranged by atomic weight, the second member of each triad was roughly the average of the first and the third; this became known as the Law of Triads. German chemist Leopold Gmelin worked with this system, and by 1843 he had identified ten triads, three groups of four, and one group of five. Jean-Baptiste Dumas published work in 1857 describing relationships between various groups of metals. Although various chemists were able to identify relationships between small groups of elements, they had yet to build one scheme that encompassed them all.\nIn 1857, German chemist August Kekul\u00e9 observed that carbon often has four other atoms bonded to it. Methane, for example, has one carbon atom and four hydrogen atoms. This concept eventually became known as valency; different elements bond with different numbers of atoms.\nIn 1862, Alexandre-Emile B\u00e9guyer de Chancourtois, a French geologist, published an early form of periodic table, which he called the telluric helix or screw. He was the first person to notice the periodicity of the elements. With the elements arranged in a spiral on a cylinder by order of increasing atomic weight, de Chancourtois showed that elements with similar properties seemed to occur at regular intervals. His chart included some ions and compounds in addition to elements. His paper also used geological rather than chemical terms and did not include a diagram; as a result, it received little attention until the work of Dmitri Mendeleev.\nIn 1864, Julius Lothar Meyer, a German chemist, published a table with 44 elements arranged by valency. The table showed that elements with similar properties often shared the same valency. Concurrently, William Odling (an English chemist) published an arrangement of 57 elements, ordered on the basis of their atomic weights. With some irregularities and gaps, he noticed what appeared to be a periodicity of atomic weights among the elements and that this accorded with \"their usually received groupings\". Odling alluded to the idea of a periodic law but did not pursue it. He subsequently proposed (in 1870) a valence-based classification of the elements.\n\nEnglish chemist John Newlands produced a series of papers from 1863 to 1866 noting that when the elements were listed in order of increasing atomic weight, similar physical and chemical properties recurred at intervals of eight; he likened such periodicity to the octaves of music. This so termed Law of Octaves, however, was ridiculed by Newlands' contemporaries, and the Chemical Society refused to publish his work. Newlands was nonetheless able to draft a table of the elements and used it to predict the existence of missing elements, such as germanium. The Chemical Society only acknowledged the significance of his discoveries five years after they credited Mendeleev.\nIn 1867, Gustavus Hinrichs, a Danish born academic chemist based in America, published a spiral periodic system based on atomic spectra and weights, and chemical similarities. His work was regarded as idiosyncratic, ostentatious and labyrinthine and this may have militated against its recognition and acceptance.\n\n\n=== Mendeleev's table ===\n\nRussian chemistry professor Dmitri Mendeleev and German chemist Julius Lothar Meyer independently published their periodic tables in 1869 and 1870, respectively. Mendeleev's table was his first published version; that of Meyer was an expanded version of his (Meyer's) table of 1864. They both constructed their tables by listing the elements in rows or columns in order of atomic weight and starting a new row or column when the characteristics of the elements began to repeat.\nThe recognition and acceptance afforded to Mendeleev's table came from two decisions he made. The first was to leave gaps in the table when it seemed that the corresponding element had not yet been discovered. Mendeleev was not the first chemist to do so, but he was the first to be recognized as using the trends in his periodic table to predict the properties of those missing elements, such as gallium and germanium. The second decision was to occasionally ignore the order suggested by the atomic weights and switch adjacent elements, such as tellurium and iodine, to better classify them into chemical families. Later in 1913, Henry Moseley determined experimental values of the nuclear charge or atomic number of each element, and showed that Mendeleev's ordering actually corresponds to the order of increasing atomic number.\nThe significance of atomic numbers to the organization of the periodic table was not appreciated until the existence and properties of protons and neutrons became understood. Mendeleev's periodic tables used atomic weight instead of atomic number to organize the elements, information determinable to fair precision in his time. Atomic weight worked well enough in most cases to (as noted) give a presentation that was able to predict the properties of missing elements more accurately than any other method then known. Substitution of atomic numbers, once understood, gave a definitive, integer-based sequence for the elements, and Moseley predicted (in 1913) that the only elements still missing between aluminium (Z=13) and gold (Z=79) were Z = 43, 61, 72 and 75, all of which were later discovered. The sequence of atomic numbers is still used today even as new synthetic elements are being produced and studied.\n\n\n=== Second version and further development ===\n\nIn 1871, Mendeleev published his periodic table in a new form, with groups of similar elements arranged in columns rather than in rows, and those columns numbered I to VIII corresponding with the element's oxidation state. He also gave detailed predictions for the properties of elements he had earlier noted were missing, but should exist. These gaps were subsequently filled as chemists discovered additional naturally occurring elements. It is often stated that the last naturally occurring element to be discovered was francium (referred to by Mendeleev as eka-caesium) in 1939. However, plutonium, produced synthetically in 1940, was identified in trace quantities as a naturally occurring element in 1971.\nThe popular periodic table layout, also known as the common or standard form (as shown at various other points in this article), is attributable to Horace Groves Deming. In 1923, Deming, an American chemist, published short (Mendeleev style) and medium (18-column) form periodic tables. Merck and Company prepared a handout form of Deming's 18-column medium table, in 1928, which was widely circulated in American schools. By the 1930s Deming's table was appearing in handbooks and encyclopaedias of chemistry. It was also distributed for many years by the Sargent-Welch Scientific Company.\nWith the development of modern quantum mechanical theories of electron configurations within atoms, it became apparent that each period (row) in the table corresponded to the filling of a quantum shell of electrons. Larger atoms have more electron sub-shells, so later tables have required progressively longer periods.\n\nIn 1945, Glenn Seaborg, an American scientist, made the suggestion that the actinide elements, like the lanthanides, were filling an f sub-level. Before this time the actinides were thought to be forming a fourth d-block row. Seaborg's colleagues advised him not to publish such a radical suggestion as it would most likely ruin his career. As Seaborg considered he did not then have a career to bring into disrepute, he published anyway. Seaborg's suggestion was found to be correct and he subsequently went on to win the 1951 Nobel Prize in chemistry for his work in synthesizing actinide elements.\nAlthough minute quantities of some transuranic elements occur naturally, they were all first discovered in laboratories. Their production has expanded the periodic table significantly, the first of these being neptunium, synthesized in 1939. Because many of the transuranic elements are highly unstable and decay quickly, they are challenging to detect and characterize when produced. There have been controversies concerning the acceptance of competing discovery claims for some elements, requiring independent review to determine which party has priority, and hence naming rights. The most recently accepted and named elements are flerovium (element 114) and livermorium (element 116), both named on 31 May 2012. In 2010, a joint Russia\u2013US collaboration at Dubna, Moscow Oblast, Russia, claimed to have synthesized six atoms of ununseptium (element 117), making it the most recently claimed discovery.\nOn December 30, 2015, elements 113, 115, 117, and 118 were formally recognized by IUPAC, completing the seventh row of the periodic table. Official names and symbols for each of these elements, which will replace temporary designations such as ununpentium (Uup) in the case of element 115, are expected to be announced later in 2016. On June 8, 2016, IUPAC announced the proposed names for each element. Nihonium (Nh) is the proposed name for element 113 due to its discovery at RIKEN in Japan. It will be the first element to be named after a location in East Asia. Element 115 has the proposed name of moscovium (Mc) after the location of the Joint Institute for Nuclear Research in Moscow, Russia. Element 117 has the proposed name of tennessine (Ts), referencing the state of Tennessee in the United States which is home to Oak Ridge National Laboratory. Lastly, element 118 has the proposed name of oganesson (Og), after in honour of the Russian nuclear physicist Yuri Oganessian who led the team that synthesized it.\n\n\n== Different periodic tables ==\n\n\n=== Group 3 constitution variants ===\nThere are three main variants of periodic table, each differing as to the constitution of group 3. Scandium and yttrium are uniformly shown as the first two members of this group; the differences hinge on the identity of the remaining members.\nGroup 3 is Sc, Y, and La, Ac. Lanthanum (La) and actinium (Ac) occupy the two positions below yttrium. This variant is the most common. It emphasizes similarities in periodic trends going down groups 1, 2 and 3, at the expense of discontinuities in periodic trends between groups 3 and 4 and fragmenting the lanthanides and actinides.\nGroup 3 is Sc, Y, and Lu, Lr. Lutetium (Lu) and lawrencium (Lr) occupy the two positions below yttrium. This variant retains a 14-column wide f-block while fragmenting the lanthanides and actinides. It emphasizes similarities in periodic trends between group 3 and the following groups at the expense of discontinuities in periodic trends between groups 2 and 3.\nGroup 3 is Sc, Y, and 15 lanthanides and 15 actinides. The two positions below yttrium contain the lanthanides and the actinides (possibly by footnote markers). This variant emphasizes similarities in the chemistry of the 15 lanthanide elements (La\u2013Lu), at the expense of ambiguity as to which elements occupy the two group 3 positions below yttrium, and seemingly a 15-column wide f block (there can only be 14 elements in any row of the f block).\nThe three variants originate from historical difficulties in placing the lanthanides in the periodic table, and arguments as to where the f block elements start and end. It has been claimed that such arguments are proof that, \"it is a mistake to break the [periodic] system into sharply delimited blocks\". Equally, some versions of the two markers table have been criticized for implying that all 15 lanthanides occupy the single box or place below yttrium, in breach of the basic principle of one place, one element. The controversy over which two elements occupy the Group 3 positions below scandium and yttrium is further discussed in the Open questions and controversies section of this article.\nThe Lu and Lr table is shown in the lead and overview section of this article. When compared to the La and Ac variant, there are fewer apparent exceptions to the regular filling of the 4f orbitals among the subsequent members of the series. Unlike the two markers variant, there is no ambiguity on the composition of group 3.\n\n\n=== Periodic tables by different structure ===\n\nWithin 100 years of the appearance of Mendeleev's table in 1869 it has been estimated that around 700 different periodic table versions were published. As well as numerous rectangular variations, other periodic table formats have been shaped, for example, like a circle, cube, cylinder, building, spiral, lemniscate, octagonal prism, pyramid, sphere, or triangle. Such alternatives are often developed to highlight or emphasize chemical or physical properties of the elements that are not as apparent in traditional periodic tables.\nThe modern periodic table is sometimes expanded into its long or 32-column form by reinstating the footnoted f-block elements into their natural position between the s- and d-blocks. Unlike the 18-column form this arrangement results in \"no interruptions in the sequence of increasing atomic numbers\". The relationship of the f-block to the other blocks of the periodic table also becomes easier to see. Jensen advocates a form of table with 32 columns on the grounds that the lanthanides and actinides are otherwise relegated in the minds of students as dull, unimportant elements that can be quarantined and ignored. Despite these advantages the 32-column form is generally avoided by editors on account of its undue rectangular ratio (compared to a book page ratio).\n\nA popular alternative structure is that of Theodor Benfey (1960). The elements are arranged in a continuous spiral, with hydrogen at the centre and the transition metals, lanthanides, and actinides occupying peninsulas.\nMost periodic tables are two-dimensional; however, three-dimensional tables are known to as far back as at least 1862 (pre-dating Mendeleev's two-dimensional table of 1869). More recent examples include Courtines' Periodic Classification (1925), Wringley's Lamina System (1949), Gigu\u00e8re's Periodic helix (1965) and Dufour's Periodic Tree (1996). Going one further, Stowe's Physicist's Periodic Table (1989) has been described as being four-dimensional (having three spatial dimensions and one colour dimension).\nThe various forms of periodic tables can be thought of as lying on a chemistry\u2013physics continuum. Towards the chemistry end of the continuum can be found, as an example, Rayner-Canham's \"unruly\" Inorganic Chemist's Periodic Table (2002), which emphasizes trends and patterns, and unusual chemical relationships and properties. Near the physics end of the continuum is Janet's Left-Step Periodic Table (1928). This has a structure that shows a closer connection to the order of electron-shell filling and, by association, quantum mechanics. A somewhat similar approach has been taken by Alper, albeit criticized by Scerri as disregarding the need to display chemical and physical periodicity. Somewhere in the middle of the continuum is the ubiquitous common or standard form of periodic table. This is regarded as better expressing empirical trends in physical state, electrical and thermal conductivity, and oxidation numbers, and other properties easily inferred from traditional techniques of the chemical laboratory.\n\n\n== Open questions and controversies ==\n\n\n=== Elements with unknown chemical properties ===\nAlthough all elements up to ununoctium have been discovered, of the elements above hassium (element 108), only copernicium (element 112) and flerovium (element 114) have known chemical properties. The other elements may behave differently from what would be predicted by extrapolation, due to relativistic effects; for example, flerovium has been predicted to possibly exhibit some noble-gas-like properties, even though it is currently placed in the carbon group. More recent experiments have suggested, however, that flerovium behaves chemically like lead, as expected from its periodic table position.\n\n\n=== Further periodic table extensions ===\n\nIt is unclear whether new elements will continue the pattern of the current periodic table as period 8, or require further adaptations or adjustments. Seaborg expected the eighth period to follow the previously established pattern exactly, so that it would include a two-element s-block for elements 119 and 120, a new g-block for the next 18 elements, and 30 additional elements continuing the current f-, d-, and p-blocks. More recently, physicists such as Pekka Pyykk\u00f6 have theorized that these additional elements do not follow the Madelung rule, which predicts how electron shells are filled and thus affects the appearance of the present periodic table.\n\n\n=== Element with the highest possible atomic number ===\nThe number of possible elements is not known. A very early suggestion made by Elliot Adams in 1911, and based on the arrangement of elements in each horizontal periodic table row, was that elements of atomic weight greater than 256\u00b1 (which would equate to between elements 99 and 100 in modern-day terms) did not exist. A higher\u2014more recent\u2014estimate is that the periodic table may end soon after the island of stability, which is expected to centre around element 126, as the extension of the periodic and nuclides tables is restricted by proton and neutron drip lines. Other predictions of an end to the periodic table include at element 128 by John Emsley, at element 137 by Richard Feynman, and at element 155 by Albert Khazan.\nBohr model\nThe Bohr model exhibits difficulty for atoms with atomic number greater than 137, as any element with an atomic number greater than 137 would require 1s electrons to be travelling faster than c, the speed of light. Hence the non-relativistic Bohr model is inaccurate when applied to such an element.\nRelativistic Dirac equation\nThe relativistic Dirac equation has problems for elements with more than 137 protons. For such elements, the wave function of the Dirac ground state is oscillatory rather than bound, and there is no gap between the positive and negative energy spectra, as in the Klein paradox. More accurate calculations taking into account the effects of the finite size of the nucleus indicate that the binding energy first exceeds the limit for elements with more than 173 protons. For heavier elements, if the innermost orbital (1s) is not filled, the electric field of the nucleus will pull an electron out of the vacuum, resulting in the spontaneous emission of a positron; however, this does not happen if the innermost orbital is filled, so that element 173 is not necessarily the end of the periodic table.\n\n\n=== Placement of hydrogen and helium ===\nSimply following electron configurations, hydrogen (electronic configuration 1s1) and helium (1s2) should be placed in groups 1 and 2, above lithium ([He]2s1) and beryllium ([He]2s2). However, such placing is rarely used outside of the context of electron configurations: When the noble gases (then called \"inert gases\") were first discovered around 1900, they were known as \"group 0\", reflecting no chemical reactivity of these elements known at that point, and helium was placed on the top that group, as it did share the extreme chemical inertness seen throughout the group. As the group changed its formal number, many authors continued to assign helium directly above neon, in group 18; one of the examples of such placing is the current IUPAC table.\nHydrogen's chemical properties are not very close to those of the alkali metals, which occupy group 1, and on that basis hydrogen is sometimes placed elsewhere: one of the most common alternatives is in group 17; one of the factors behind it is the strictly univalent predominantly non-metallic chemistry of hydrogen, and that of fluorine (the element placed on the top of group 17) is strictly univalent and non-metallic. Sometimes, to show how hydrogen has properties both corresponding to those of the alkali metals and the halogens, it may be shown in two columns simultaneously. Another suggestion is above carbon in group 14: placed that way, it fits well into the trend of increasing trends of ionization potential values and electron affinity values, and is not too stray from the electronegativity trend. Finally, hydrogen is sometimes placed separately from any group; this is based on how general properties of hydrogen differ from that of any group: unlike hydrogen, the other group 1 elements show extremely metallic behaviour; the group 17 elements commonly form salts (hence the term \"halogen\"); elements of any other group show some multivalent chemistry. The other period 1 element, helium, is sometimes placed separately from any group as well. The property that distinguishes helium from the rest of the noble gases (even though the extraordinary inertness of helium is extremely close to that of neon and argon) is that in its closed electron shell, helium has only two electrons in the outermost electron orbital, while the rest of the noble gases have eight.\n\n\n=== Groups included in the transition metals ===\nThe definition of a transition metal, as given by IUPAC, is an element whose atom has an incomplete d sub-shell, or which can give rise to cations with an incomplete d sub-shell. By this definition all of the elements in groups 3\u201311 are transition metals. The IUPAC definition therefore excludes group 12, comprising zinc, cadmium and mercury, from the transition metals category.\nSome chemists treat the categories \"d-block elements\" and \"transition metals\" interchangeably, thereby including groups 3\u201312 among the transition metals. In this instance the group 12 elements are treated as a special case of transition metal in which the d electrons are not ordinarily involved in chemical bonding. The 2007 report of mercury(IV) fluoride (HgF4), a compound in which mercury would use its d electrons for bonding, has prompted some commentators to suggest that mercury can be regarded as a transition metal. Other commentators, such as Jensen, have argued that the formation of a compound like HgF4 can occur only under highly abnormal conditions; indeed, its existence is currently disputed. As such, mercury could not be regarded as a transition metal by any reasonable interpretation of the ordinary meaning of the term.\nStill other chemists further exclude the group 3 elements from the definition of a transition metal. They do so on the basis that the group 3 elements do not form any ions having a partially occupied d shell and do not therefore exhibit any properties characteristic of transition metal chemistry. In this case, only groups 4\u201311 are regarded as transition metals.\n\n\n=== Period 6 and 7 elements in group 3 ===\nAlthough scandium and yttrium are always the first two elements in group 3 the identity of the next two elements is not settled. They are either lanthanum and actinium; or lutetium and lawrencium. Physical and chemical arguments have been made in support of the latter arrangement but not all authors have been convinced. Most working chemists are not aware there is any controversy. In December 2015 an IUPAC project was established to make a recommendation on the matter.\nLanthanum and actinium are traditionally depicted as the remaining group 3 members. It has been suggested that this layout originated in the 1940s, with the appearance of periodic tables relying on the electron configurations of the elements and the notion of the differentiating electron. The configurations of caesium, barium and lanthanum are [Xe]6s1, [Xe]6s2 and [Xe]5d16s2. Lanthanum thus has a 5d differentiating electron and this establishes \"it in group 3 as the first member of the d-block for period 6\". A consistent set of electron configurations is then seen in group 3: scandium [Ar]3d14s2, yttrium [Kr]4d15s2 and lanthanum [Xe]5d16s2. Still in period 6, ytterbium was assigned an electron configuration of [Xe]4f135d16s2 and lutetium [Xe]4f145d16s2, \"resulting in a 4f differentiating electron for lutetium and firmly establishing it as the last member of the f-block for period 6\". Matthias described the placement of lanthanum under yttrium as, \"a mistake in the periodic system\u2014unfortunately mostly propagated by the Welsh [Sargent-Welch] Company\u2026and\u2026everybody copied it\". Lavelle further argued for the retention of lanthanum under yttrium given several well-known reference books featured periodic tables with such an arrangement.\nIn other tables, lutetium and lawrencium are the remaining group 3 members. Early techniques for chemically separating scandium, yttrium and lutetium relied on the fact that these elements occurred together in the so-called \"yttrium group\" whereas La and Ac occurred together in the \"cerium group\". Accordingly, lutetium rather than lanthanum was assigned to group 3 by some chemists in the 1920s and 30s. Later spectroscopic work found that the electron configuration of ytterbium was in fact [Xe]4f146s2. This meant that ytterbium and lutetium\u2014the latter with [Xe]4f145d16s2\u2014both had 14 f-electrons, \"resulting in a d- rather than an f- differentiating electron\" for lutetium and making it an \"equally valid candidate\" with [Xe]5d16s2 lanthanum, for the group 3 periodic table position below yttrium. Several physicists in the 1950s and 60s opted for lutetium, in light of a comparison of several of its physical properties with those of lanthanum. This arrangement, in which lanthanum is the first member of the f-block, is disputed by some authors since lanthanum lacks any f-electrons. However, it has been argued that this is not valid concern given other periodic table anomalies\u2014thorium, for example, has no f-electrons yet is part of the f-block. As for lawrencium, its electron configuration was confirmed in 2015 as [Rn]5f147s27p1. Such a configuration represents another periodic table anomaly, regardless of whether lawrencium is located in the f-block or the d-block, as the only potentially applicable p-block position has been reserved for ununtrium with its predicted electron configuration of [Rn]5f146d107s27p1.\n\n\n=== Optimal form ===\nThe many different forms of periodic table have prompted the question of whether there is an optimal or definitive form of periodic table. The answer to this question is thought to depend on whether the chemical periodicity seen to occur among the elements has an underlying truth, effectively hard-wired into the universe, or if any such periodicity is instead the product of subjective human interpretation, contingent upon the circumstances, beliefs and predilections of human observers. An objective basis for chemical periodicity would settle the questions about the location of hydrogen and helium, and the composition of group 3. Such an underlying truth, if it exists, is thought to have not yet been discovered. In its absence, the many different forms of periodic table can be regarded as variations on the theme of chemical periodicity, each of which explores and emphasizes different aspects, properties, perspectives and relationships of and among the elements. The ubiquity of the standard or medium-long periodic table is thought to be a result of this layout having a good balance of features in terms of ease of construction and size, and its depiction of atomic order and periodic trends.\n\n\n== See also ==\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Bibliography ==\n\n\n== External links ==\nIUPAC. \"Periodic Table of the Elements\". Retrieved 24 May 2016. Note: There is no IUPAC ruling on the composition of Group 3. \nM. Dayah. \"Dynamic Periodic Table\". Retrieved 14 May 2012. \nBrady Haran. \"The Periodic Table of Videos\". University of Nottingham. Retrieved 14 May 2012. \nMark Winter. \"WebElements: the periodic table on the web\". University of Sheffield. Retrieved 14 May 2012. \nMark R. Leach. \"The INTERNET Database of Periodic Tables\". Retrieved 14 May 2012.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Periodic_table", 
                "title": "Periodic table"
            }, 
            {
                "snippet": " or synthetic silicon carbide. Henri Moissan also synthesized SiC and tungsten carbide in his electric arc furnace in Paris about the same time as Acheson", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from December 2010\nArticles with unsourced statements from November 2013\nCS1 maint: Multiple names: authors list\nCeramic engineering\nCeramic materials\nEngineering disciplines\nIndustrial processes\nMaterials science\nUse dmy dates from July 2013", 
                "pageContent": "Ceramic engineering is the science and technology of creating objects from inorganic, non-metallic materials. This is done either by the action of heat, or at lower temperatures using precipitation reactions from high-purity chemical solutions. The term includes the purification of raw materials, the study and production of the chemical compounds concerned, their formation into components and the study of their structure, composition and properties.\nCeramic materials may have a crystalline or partly crystalline structure, with long-range order on atomic scale. Glass ceramics may have an amorphous or glassy structure, with limited or short-range atomic order. They are either formed from a molten mass that solidifies on cooling, formed and matured by the action of heat, or chemically synthesized at low temperatures using, for example, hydrothermal or sol-gel synthesis.\nThe special character of ceramic materials gives rise to many applications in materials engineering, electrical engineering, chemical engineering and mechanical engineering. As ceramics are heat resistant, they can be used for many tasks for which materials like metal and polymers are unsuitable. Ceramic materials are used in a wide range of industries, including mining, aerospace, medicine, refinery, food and chemical industries, packaging science, electronics, industrial and transmission electricity, and guided lightwave transmission.\n\n\n== History ==\nThe word \"ceramic\" is derived from the Greek word \u03ba\u03b5\u03c1\u03b1\u03bc\u03b9\u03ba\u03cc\u03c2 (keramikos) meaning pottery. It is related to the older Indo-European language root \"to burn\",  \"Ceramic\" may be used as a noun in the singular to refer to a ceramic material or the product of ceramic manufacture, or as an adjective. The plural \"ceramics\" may be used to refer the making of things out of ceramic materials. Ceramic engineering, like many sciences, evolved from a different discipline by today's standards. Materials science engineering is grouped with ceramics engineering to this day.\n\nAbraham Darby first used coke in 1709 in Shropshire, England, to improve the yield of a smelting process. Coke is now widely used to produce carbide ceramics. Potter Josiah Wedgwood opened the first modern ceramics factory in Stoke-on-Trent, England, in 1759. Austrian chemist Carl Josef Bayer, working for the textile industry in Russia, developed a process to separate alumina from bauxite ore in 1888. The Bayer process is still used to purify alumina for the ceramic and aluminium industries. Brothers Pierre and Jacques Curie discovered piezoelectricity in Rochelle salt circa 1880. Piezoelectricity is one of the key properties of electroceramics.\nE.G. Acheson heated a mixture of coke and clay in 1893, and invented carborundum, or synthetic silicon carbide. Henri Moissan also synthesized SiC and tungsten carbide in his electric arc furnace in Paris about the same time as Acheson. Karl Schr\u00f6ter used liquid-phase sintering to bond or \"cement\" Moissan's tungsten carbide particles with cobalt in 1923 in Germany. Cemented (metal-bonded) carbide edges greatly increase the durability of hardened steel cutting tools. W.H. Nernst developed cubic-stabilized zirconia in the 1920s in Berlin. This material is used as an oxygen sensor in exhaust systems. The main limitation on the use of ceramics in engineering is brittleness. \n\n\n=== Military ===\n\nThe military requirements of World War II encouraged developments, which created a need for high-performance materials and helped speed the development of ceramic science and engineering. Throughout the 1960s and 1970s, new types of ceramics were developed in response to advances in atomic energy, electronics, communications, and space travel. The discovery of ceramic superconductors in 1986 has spurred intense research to develop superconducting ceramic parts for electronic devices, electric motors, and transportation equipment.\nThere is an increasing need in the military sector for high-strength, robust materials which have the capability to transmit light around the visible (0.4\u20130.7 micrometers) and mid-infrared (1\u20135 micrometers) regions of the spectrum. These materials are needed for applications requiring transparent armour. Transparent armor is a material or system of materials designed to be optically transparent, yet protect from fragmentation or ballistic impacts. The primary requirement for a transparent armour system is to not only defeat the designated threat but also provide a multi-hit capability with minimized distortion of surrounding areas. Transparent armour windows must also be compatible with night vision equipment. New materials that are thinner, lightweight, and offer better ballistic performance are being sought. Such solid-state components have found widespread use for various applications in the electro-optical field including: optical fibres for guided lightwave transmission, optical switches, laser amplifiers and lenses, hosts for solid-state lasers and optical window materials for gas lasers, and infrared (IR) heat seeking devices for missile guidance systems and IR night vision. \n\n\n== Modern industry ==\nNow a multibillion-dollar a year industry, ceramic engineering and research has established itself as an important field of science. Applications continue to expand as researchers develop new kinds of ceramics to serve different purposes.\nZirconium dioxide ceramics are used in the manufacture of knives. The blade of the ceramic knife will stay sharp for much longer than that of a steel knife, although it is more brittle and can be snapped by dropping it on a hard surface.\nCeramics such as alumina, boron carbide and silicon carbide have been used in bulletproof vests to repel small arms rifle fire. Such plates are known commonly as trauma plates. Similar material is used to protect cockpits of some military aircraft, because of the low weight of the material.\nSilicon nitride parts are used in ceramic ball bearings. Their higher hardness means that they are much less susceptible to wear and can offer more than triple lifetimes. They also deform less under load meaning they have less contact with the bearing retainer walls and can roll faster. In very high speed applications, heat from friction during rolling can cause problems for metal bearings; problems which are reduced by the use of ceramics. Ceramics are also more chemically resistant and can be used in wet environments where steel bearings would rust. The major drawback to using ceramics is a significantly higher cost. In many cases their electrically insulating properties may also be valuable in bearings.\nIn the early 1980s, Toyota researched production of an adiabatic ceramic engine which can run at a temperature of over 6000 \u00b0F (3300 \u00b0C). Ceramic engines do not require a cooling system and hence allow a major weight reduction and therefore greater fuel efficiency. Fuel efficiency of the engine is also higher at high temperature, as shown by Carnot's theorem. In a conventional metallic engine, much of the energy released from the fuel must be dissipated as waste heat in order to prevent a meltdown of the metallic parts. Despite all of these desirable properties, such engines are not in production because the manufacturing of ceramic parts in the requisite precision and durability is difficult. Imperfection in the ceramic leads to cracks, which can lead to potentially dangerous equipment failure. Such engines are possible in laboratory settings, but mass-production is not feasible with current technology.\nWork is being done in developing ceramic parts for gas turbine engines. Currently, even blades made of advanced metal alloys used in the engines' hot section require cooling and careful limiting of operating temperatures. Turbine engines made with ceramics could operate more efficiently, giving aircraft greater range and payload for a set amount of fuel.\n\nRecently, there have been advances in ceramics which include bio-ceramics, such as dental implants and synthetic bones. Hydroxyapatite, the natural mineral component of bone, has been made synthetically from a number of biological and chemical sources and can be formed into ceramic materials. Orthopaedic implants made from these materials bond readily to bone and other tissues in the body without rejection or inflammatory reactions. Because of this, they are of great interest for gene delivery and tissue engineering scaffolds. Most hydroxyapatite ceramics are very porous and lack mechanical strength and are used to coat metal orthopaedic devices to aid in forming a bond to bone or as bone fillers. They are also used as fillers for orthopaedic plastic screws to aid in reducing the inflammation and increase absorption of these plastic materials. Work is being done to make strong, fully dense nano crystalline hydroxyapatite ceramic materials for orthopaedic weight bearing devices, replacing foreign metal and plastic orthopaedic materials with a synthetic, but naturally occurring, bone mineral. Ultimately these ceramic materials may be used as bone replacements or with the incorporation of protein collagens, synthetic bones.\nHigh-tech ceramic is used in watch-making for producing watch cases. The material is valued by watchmakers for its light weight, scratch-resistance, durability and smooth touch. IWC is one of the brands that initiated the use of ceramic in watch-making. The case of the IWC 2007 Top Gun edition of the Pilot's Watch Double chronograph is crafted in high-tech black ceramic.\n\n\n== Glass-ceramics ==\n\nGlass-ceramic materials share many properties with both glasses and ceramics. Glass-ceramics have an amorphous phase and one or more crystalline phases and are produced by a so-called \"controlled crystallization\", which is typically avoided in glass manufacturing. Glass-ceramics often contain a crystalline phase which constitutes anywhere from 30% [m/m] to 90% [m/m] of its composition by volume, yielding an array of materials with interesting thermomechanical properties.\nIn the processing of glass-ceramics, molten glass is cooled down gradually before reheating and annealing. In this heat treatment the glass partly crystallizes. In many cases, so-called 'nucleation agents' are added in order to regulate and control the crystallization process. Because there is usually no pressing and sintering, glass-ceramics do not contain the volume fraction of porosity typically present in sintered ceramics.\nThe term mainly refers to a mix of lithium and aluminosilicates which yields an array of materials with interesting thermomechanical properties. The most commercially important of these have the distinction of being impervious to thermal shock. Thus, glass-ceramics have become extremely useful for countertop cooking. The negative thermal expansion coefficient (TEC) of the crystalline ceramic phase can be balanced with the positive TEC of the glassy phase. At a certain point (~70% crystalline) the glass-ceramic has a net TEC near zero. This type of glass-ceramic exhibits excellent mechanical properties and can sustain repeated and quick temperature changes up to 1000 \u00b0C.\n\n\n== Processing steps ==\nThe traditional ceramic process generally follows this sequence: Milling \u2192 Batching \u2192 Mixing \u2192 Forming \u2192 Drying \u2192 Firing \u2192 Assembly. \n\nMilling is the process by which materials are reduced from a large size to a smaller size. Milling may involve breaking up cemented material (in which case individual particles retain their shape) or pulverization (which involves grinding the particles themselves to a smaller size). Milling is generally done by mechanical means, including attrition (which is particle-to-particle collision that results in agglomerate break up or particle shearing), compression (which applies a forces that results in fracturing), and impact (which employs a milling medium or the particles themselves to cause fracturing). Attrition milling equipment includes the wet scrubber (also called the planetary mill or wet attrition mill), which has paddles in water creating vortexes in which the material collides and break up. Compression mills include the jaw crusher, roller crusher and cone crusher. Impact mills include the ball mill, which has media that tumble and fracture the material. Shaft impactors cause particle-to particle attrition and compression.\nBatching is the process of weighing the oxides according to recipes, and preparing them for mixing and drying.\nMixing occurs after batching and is performed with various machines, such as dry mixing ribbon mixers (a type of cement mixer), Mueller mixers, and pug mills. Wet mixing generally involves the same equipment.\nForming is making the mixed material into shapes, ranging from toilet bowls to spark plug insulators. Forming can involve: (1) Extrusion, such as extruding \"slugs\" to make bricks, (2) Pressing to make shaped parts, (3) Slip casting, as in making toilet bowls, wash basins and ornamentals like ceramic statues. Forming produces a \"green\" part, ready for drying. Green parts are soft, pliable, and over time will lose shape. Handling the green product will change its shape. For example, a green brick can be \"squeezed\", and after squeezing it will stay that way.\nDrying is removing the water or binder from the formed material. Spray drying is widely used to prepare powder for pressing operations. Other dryers are tunnel dryers and periodic dryers. Controlled heat is applied in this two-stage process. First, heat removes water. This step needs careful control, as rapid heating causes cracks and surface defects. The dried part is smaller than the green part, and is brittle, necessitating careful handling, since a small impact will cause crumbling and breaking.\nSintering is where the dried parts pass through a controlled heating process, and the oxides are chemically changed to cause bonding and densification. The fired part will be smaller than the dried part.\n\n\n== Forming methods ==\nCeramic forming techniques include throwing, slipcasting, tape casting, freeze-casting, injection moulding, dry pressing, isostatic pressing, hot isostatic pressing (HIP) and others. Methods for forming ceramic powders into complex shapes are desirable in many areas of technology. Such methods are required for producing advanced, high-temperature structural parts such as heat engine components and turbines. Materials other than ceramics which are used in these processes may include: wood, metal, water, plaster and epoxy\u2014most of which will be eliminated upon firing.\nThese forming techniques are well known for providing tools and other components with dimensional stability, surface quality, high (near theoretical) density and microstructural uniformity. The increasing use and diversity of speciality forms of ceramics adds to the diversity of process technologies to be used.\nThus, reinforcing fibres and filaments are mainly made by polymer, sol-gel, or CVD processes, but melt processing also has applicability. The most widely used speciality form is layered structures, with tape casting for electronic substrates and packages being pre-eminent. Photo-lithography is of increasing interest for precise patterning of conductors and other components for such packaging. Tape casting or forming processes are also of increasing interest for other applications, ranging from open structures such as fuel cells to ceramic composites.\nThe other major layer structure is coating, where melt spraying is very important, but chemical and physical vapour deposition and chemical (e.g., sol-gel and polymer pyrolysis) methods are all seeing increased use. Besides open structures from formed tape, extruded structures, such as honeycomb catalyst supports, and highly porous structures, including various foams, for example, reticulated foam, are of increasing use.\nDensification of consolidated powder bodies continues to be achieved predominantly by (pressureless) sintering. However, the use of pressure sintering by hot pressing is increasing, especially for non-oxides and parts of simple shapes where higher quality (mainly microstructural homogeneity) is needed, and larger size or multiple parts per pressing can be an advantage.\n\n\n== The sintering process ==\n\nThe principles of sintering-based methods are simple (\"sinter\" has roots in the English \"cinder\"). The firing is done at a temperature below the melting point of the ceramic. Once a roughly-held-together object called a \"green body\" is made, it is baked in a kiln, where atomic and molecular diffusion processes give rise to significant changes in the primary microstructural features. This includes the gradual elimination of porosity, which is typically accompanied by a net shrinkage and overall densification of the component. Thus, the pores in the object may close up, resulting in a denser product of significantly greater strength and fracture toughness.\nAnother major change in the body during the firing or sintering process will be the establishment of the polycrystalline nature of the solid. This change will introduce some form of grain size distribution, which will have a significant impact on the ultimate physical properties of the material. The grain sizes will either be associated with the initial particle size, or possibly the sizes of aggregates or particle clusters which arise during the initial stages of processing.\nThe ultimate microstructure (and thus the physical properties) of the final product will be limited by and subject to the form of the structural template or precursor which is created in the initial stages of chemical synthesis and physical forming. Hence the importance of chemical powder and polymer processing as it pertains to the synthesis of industrial ceramics, glasses and glass-ceramics.\nThere are numerous possible refinements of the sintering process. Some of the most common involve pressing the green body to give the densification a head start and reduce the sintering time needed. Sometimes organic binders such as polyvinyl alcohol are added to hold the green body together; these burn out during the firing (at 200\u2013350 \u00b0C). Sometimes organic lubricants are added during pressing to increase densification. It is common to combine these, and add binders and lubricants to a powder, then press. (The formulation of these organic chemical additives is an art in itself. This is particularly important in the manufacture of high performance ceramics such as those used by the billions for electronics, in capacitors, inductors, sensors, etc.)\nA slurry can be used in place of a powder, and then cast into a desired shape, dried and then sintered. Indeed, traditional pottery is done with this type of method, using a plastic mixture worked with the hands. If a mixture of different materials is used together in a ceramic, the sintering temperature is sometimes above the melting point of one minor component \u2013 a liquid phase sintering. This results in shorter sintering times compared to solid state sintering.\n\n\n== Strength of ceramics ==\nA material's strength is dependent on its microstructure. The engineering processes to which a material is subjected can alter its microstructure. The variety of strengthening mechanisms that alter the strength of a material include the mechanism of grain boundary strengthening. Thus, although yield strength is maximized with decreasing grain size, ultimately, very small grain sizes make the material brittle. Considered in tandem with the fact that the yield strength is the parameter that predicts plastic deformation in the material, one can make informed decisions on how to increase the strength of a material depending on its microstructural properties and the desired end effect.\nThe relation between yield stress and grain size is described mathematically by the Hall-Petch equation which is\n\n  \n    \n      \n        \n          \u03c3\n          \n            y\n          \n        \n        =\n        \n          \u03c3\n          \n            0\n          \n        \n        +\n        \n          \n            \n              k\n              \n                y\n              \n            \n            \n              \n                d\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\sigma _{y}=\\sigma _{0}+{k_{y} \\over {\\sqrt {d}}}}\n  \nwhere ky is the strengthening coefficient (a constant unique to each material), \u03c3o is a materials constant for the starting stress for dislocation movement (or the resistance of the lattice to dislocation motion), d is the grain diameter, and \u03c3y is the yield stress.\nTheoretically, a material could be made infinitely strong if the grains are made infinitely small. This is, unfortunately, impossible because the lower limit of grain size is a single unit cell of the material. Even then, if the grains of a material are the size of a single unit cell, then the material is in fact amorphous, not crystalline, since there is no long range order, and dislocations can not be defined in an amorphous material. It has been observed experimentally that the microstructure with the highest yield strength is a grain size of about 10 nanometres, because grains smaller than this undergo another yielding mechanism, grain boundary sliding. Producing engineering materials with this ideal grain size is difficult because of the limitations of initial particle sizes inherent to nanomaterials and nanotechnology.\n\n\n== Theory of chemical processing ==\n\n\n=== Microstructural uniformity ===\nIn the processing of fine ceramics, the irregular particle sizes and shapes in a typical powder often lead to non-uniform packing morphologies that result in packing density variations in the powder compact. Uncontrolled agglomeration of powders due to attractive van der Waals forces can also give rise to in microstructural inhomogeneities.\nDifferential stresses that develop as a result of non-uniform drying shrinkage are directly related to the rate at which the solvent can be removed, and thus highly dependent upon the distribution of porosity. Such stresses have been associated with a plastic-to-brittle transition in consolidated bodies, and can yield to crack propagation in the unfired body if not relieved.\nIn addition, any fluctuations in packing density in the compact as it is prepared for the kiln are often amplified during the sintering process, yielding inhomogeneous densification. Some pores and other structural defects associated with density variations have been shown to play a detrimental role in the sintering process by growing and thus limiting end-point densities. Differential stresses arising from inhomogeneous densification have also been shown to result in the propagation of internal cracks, thus becoming the strength-controlling flaws.\nIt would therefore appear desirable to process a material in such a way that it is physically uniform with regard to the distribution of components and porosity, rather than using particle size distributions which will maximize the green density. The containment of a uniformly dispersed assembly of strongly interacting particles in suspension requires total control over particle-particle interactions. Monodisperse colloids provide this potential.\nMonodisperse powders of colloidal silica, for example, may therefore be stabilized sufficiently to ensure a high degree of order in the colloidal crystal or polycrystalline colloidal solid which results from aggregation. The degree of order appears to be limited by the time and space allowed for longer-range correlations to be established.\nSuch defective polycrystalline colloidal structures would appear to be the basic elements of submicrometer colloidal materials science, and, therefore, provide the first step in developing a more rigorous understanding of the mechanisms involved in microstructural evolution in inorganic systems such as polycrystalline ceramics.\n\n\n=== Self-assembly ===\n\nSelf-assembly is the most common term in use in the modern scientific community to describe the spontaneous aggregation of particles (atoms, molecules, colloids, micelles, etc.) without the influence of any external forces. Large groups of such particles are known to assemble themselves into thermodynamically stable, structurally well-defined arrays, quite reminiscent of one of the 7 crystal systems found in metallurgy and mineralogy (e.g. face-centred cubic, body-centred cubic, etc.). The fundamental difference in equilibrium structure is in the spatial scale of the unit cell (or lattice parameter) in each particular case.\nThus, self-assembly is emerging as a new strategy in chemical synthesis and nanotechnology. Molecular self-assembly has been observed in various biological systems and underlies the formation of a wide variety of complex biological structures. Molecular crystals, liquid crystals, colloids, micelles, emulsions, phase-separated polymers, thin films and self-assembled monolayers all represent examples of the types of highly ordered structures which are obtained using these techniques. The distinguishing feature of these methods is self-organization in the absence of any external forces.\nIn addition, the principal mechanical characteristics and structures of biological ceramics, polymer composites, elastomers, and cellular materials are being re-evaluated, with an emphasis on bioinspired materials and structures. Traditional approaches focus on design methods of biological materials using conventional synthetic materials. This includes an emerging class of mechanically superior biomaterials based on microstructural features and designs found in nature. The new horizons have been identified in the synthesis of bioinspired materials through processes that are characteristic of biological systems in nature. This includes the nanoscale self-assembly of the components and the development of hierarchical structures.\n\n\n== Ceramic composites ==\n\nSubstantial interest has arisen in recent years in fabricating ceramic composites. While there is considerable interest in composites with one or more non-ceramic constituents, the greatest attention is on composites in which all constituents are ceramic. These typically comprise two ceramic constituents: a continuous matrix, and a dispersed phase of ceramic particles, whiskers, or short (chopped) or continuous ceramic fibres. The challenge, as in wet chemical processing, is to obtain a uniform or homogeneous distribution of the dispersed particle or fibre phase. \nConsider first the processing of particulate composites. The particulate phase of greatest interest is tetragonal zirconia because of the toughening that can be achieved from the phase transformation from the metastable tetragonal to the monoclinic crystalline phase, aka transformation toughening. There is also substantial interest in dispersion of hard, non-oxide phases such as SiC, TiB, TiC, boron, carbon and especially oxide matrices like alumina and mullite. There is also interest too incorporating other ceramic particulates, especially those of highly anisotropic thermal expansion. Examples include Al2O3, TiO2, graphite, and boron nitride.\n\nIn processing particulate composites, the issue is not only homogeneity of the size and spatial distribution of the dispersed and matrix phases, but also control of the matrix grain size. However, there is some built-in self-control due to inhibition of matrix grain growth by the dispersed phase. Particulate composites, though generally offer increased resistance to damage, failure, or both, are still quite sensitive to inhomogeneities of composition as well as other processing defects such as pores. Thus they need good processing to be effective.\nParticulate composites have been made on a commercial basis by simply mixing powders of the two constituents. Although this approach is inherently limited in the homogeneity that can be achieved, it is the most readily adaptable for existing ceramic production technology. However, other approaches are of interest.\n\nFrom the technological standpoint, a particularly desirable approach to fabricating particulate composites is to coat the matrix or its precursor onto fine particles of the dispersed phase with good control of the starting dispersed particle size and the resultant matrix coating thickness. One should in principle be able to achieve the ultimate in homogeneity of distribution and thereby optimize composite performance. This can also have other ramifications, such as allowing more useful composite performance to be achieved in a body having porosity, which might be desired for other factors, such as limiting thermal conductivity.\nThere are also some opportunities to utilize melt processing for fabrication of ceramic, particulate, whisker and short-fibre, and continuous-fibre composites. Clearly, both particulate and whisker composites are conceivable by solid-state precipitation after solidification of the melt. This can also be obtained in some cases by sintering, as for precipitation-toughened, partially stabilized zirconia. Similarly, it is known that one can directionally solidify ceramic eutectic mixtures and hence obtain uniaxially aligned fibre composites. Such composite processing has typically been limited to very simple shapes and thus suffers from serious economic problems due to high machining costs.\nClearly, there are possibilities of using melt casting for many of these approaches. Potentially even more desirable is using melt-derived particles. In this method, quenching is done in a solid solution or in a fine eutectic structure, in which the particles are then processed by more typical ceramic powder processing methods into a useful body. There have also been preliminary attempts to use melt spraying as a means of forming composites by introducing the dispersed particulate, whisker, or fibre phase in conjunction with the melt spraying process.\nOther methods besides melt infiltration to manufacture ceramic composites with long fibre reinforcement are chemical vapour infiltration and the infiltration of fibre preforms with organic precursor, which after pyrolysis yield an amorphous ceramic matrix, initially with a low density. With repeated cycles of infiltration and pyrolysis one of those types of ceramic matrix composites is produced. Chemical vapour infiltration is used to manufacture carbon/carbon and silicon carbide reinforced with carbon or silicon carbide fibres.\nBesides many process improvements, the first of two major needs for fibre composites is lower fibre costs. The second major need is fibre compositions or coatings, or composite processing, to reduce degradation that results from high-temperature composite exposure under oxidizing conditions.\n\n\n== Applications ==\n\nThe products of technical ceramics include tiles used in the Space Shuttle program, gas burner nozzles, ballistic protection, nuclear fuel uranium oxide pellets, bio-medical implants, jet engine turbine blades, and missile nose cones.\nIts products are often made from materials other than clay, chosen for their particular physical properties. These may be classified as follows:\nOxides: silica, alumina, zirconia\nNon-oxides: carbides, borides, nitrides, silicides\nComposites: particulate or whisker reinforced matrices, combinations of oxides and non-oxides (e.g. polymers).\nCeramics can be used in many technological industries. One application is the ceramic tiles on NASA's Space Shuttle, used to protect it and the future supersonic space planes from the searing heat of re-entry into the Earth's atmosphere. They are also used widely in electronics and optics. In addition to the applications listed here, ceramics are also used as a coating in various engineering cases. An example would be a ceramic bearing coating over a titanium frame used for an aircraft. Recently the field has come to include the studies of single crystals or glass fibres, in addition to traditional polycrystalline materials, and the applications of these have been overlapping and changing rapidly.\n\n\n=== Aerospace ===\nEngines; Shielding a hot running aircraft engine from damaging other components.\nAirframes; Used as a high-stress, high-temp and lightweight bearing and structural component.\nMissile nose-cones; Shielding the missile internals from heat.\nSpace Shuttle tiles\nSpace-debris ballistic shields \u2013 ceramic fiber woven shields offer better protection to hypervelocity (~7 km/s) particles than aluminium shields of equal weight.\nRocket nozzles, withstands and focuses the exhaust of the rocket booster.\nUnmanned Air Vehicles; Implications of ceramic engine utilization in aeronautical applications (such as Unmanned Air Vehicles) may result in enhanced performance characteristics and less operational costs.\n\n\n=== Biomedical ===\n\nArtificial bone; Dentistry applications, teeth.\nBiodegradable splints; Reinforcing bones recovering from osteoporosis\nImplant material\n\n\n=== Electronics ===\nCapacitors\nIntegrated circuit packages\nTransducers\nInsulators\n\n\n=== Optical ===\n\nOptical fibres, guided lightwave transmission\nSwitches\nLaser amplifiers\nLenses\nInfrared heat-seeking devices\n\n\n=== Automotive ===\nHeat shield\nExhaust heat management\n\n\n== Biomaterials ==\n\nSilicification is quite common in the biological world and occurs in bacteria, single-celled organisms, plants, and animals (invertebrates and vertebrates). Crystalline minerals formed in such environment often show exceptional physical properties (e.g. strength, hardness, fracture toughness) and tend to form hierarchical structures that exhibit microstructural order over a range of length or spatial scales. The minerals are crystallized from an environment that is undersaturated with respect to silicon, and under conditions of neutral pH and low temperature (0\u201340 \u00b0C). Formation of the mineral may occur either within or outside of the cell wall of an organism, and specific biochemical reactions for mineral deposition exist that include lipids, proteins and carbohydrates.\nMost natural (or biological) materials are complex composites whose mechanical properties are often outstanding, considering the weak constituents from which they are assembled. These complex structures, which have risen from hundreds of million years of evolution, are inspiring the design of novel materials with exceptional physical properties for high performance in adverse conditions. Their defining characteristics such as hierarchy, multifunctionality, and the capacity for self-healing, are currently being investigated.\nThe basic building blocks begin with the 20 amino acids and proceed to polypeptides, polysaccharides, and polypeptides\u2013saccharides. These, in turn, compose the basic proteins, which are the primary constituents of the \u2018soft tissues\u2019 common to most biominerals. With well over 1000 proteins possible, current research emphasizes the use of collagen, chitin, keratin, and elastin. The \u2018hard\u2019 phases are often strengthened by crystalline minerals, which nucleate and grow in a biomediated environment that determines the size, shape and distribution of individual crystals. The most important mineral phases have been identified as hydroxyapatite, silica, and aragonite. Using the classification of Wegst and Ashby, the principal mechanical characteristics and structures of biological ceramics, polymer composites, elastomers, and cellular materials have been presented. Selected systems in each class are being investigated with emphasis on the relationship between their microstructure over a range of length scales and their mechanical response.\nThus, the crystallization of inorganic materials in nature generally occurs at ambient temperature and pressure. Yet the vital organisms through which these minerals form are capable of consistently producing extremely precise and complex structures. Understanding the processes in which living organisms control the growth of crystalline minerals such as silica could lead to significant advances in the field of materials science, and open the door to novel synthesis techniques for nanoscale composite materials, or nanocomposites.\n\nHigh-resolution SEM observations were performed of the microstructure of the mother-of-pearl (or nacre) portion of the abalone shell. Those shells exhibit the highest mechanical strength and fracture toughness of any non-metallic substance known. The nacre from the shell of the abalone has become one of the more intensively studied biological structures in materials science. Clearly visible in these images are the neatly stacked (or ordered) mineral tiles separated by thin organic sheets along with a macrostructure of larger periodic growth bands which collectively form what scientists are currently referring to as a hierarchical composite structure. (The term hierarchy simply implies that there are a range of structural features which exist over a wide range of length scales).\nFuture developments reside in the synthesis of bio-inspired materials through processing methods and strategies that are characteristic of biological systems. These involve nanoscale self-assembly of the components and the development of hierarchical structures.\n\n\n== See also ==\n\n\n== References ==\n\n\n== External links ==\nThe American Ceramic Society\nCeramic Tile Institute of America\nCeramic Engineering Companies", 
                "titleUrl": "https://en.wikipedia.org/wiki/Ceramic_engineering", 
                "title": "Ceramic engineering"
            }
        ], 
        "phraseCharStart": "1539"
    }, 
    {
        "phraseCharEnd": "606", 
        "phraseIndex": "T33", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "ITER carbon", 
        "wikiSearchResults": [
            {
                "snippet": "replacing carbon components in the vacuum vessel with tungsten and beryllium ones, to bring JET's components more in line with those planned for ITER. Heating", 
                "pageCategories": "Coordinates on Wikidata\nEnergy in the European Union\nInterlanguage link template link number\nTokamaks\nUse dmy dates from April 2011", 
                "pageContent": "JET, the Joint European Torus, is the world's largest operational magnetic confinement plasma physics experiment, located at Culham Centre for Fusion Energy in Oxfordshire, UK. Based on a tokamak design, the fusion research facility is a joint European project with a main purpose of opening the way to future nuclear fusion grid energy. More advanced facilities are being developed to follow on the JET research, including the International Thermonuclear Experimental Reactor and the Demonstration Power Station.\n\n\n== Construction ==\nThe JET facilities are situated on a former Navy airfield near Culham, Oxfordshire \u2013 RNAS Culham (HMS Hornbill), in the UK. They are located alongside Culham Centre for Fusion Energy (the UK's fusion research laboratory, which opened in 1965). The construction of the buildings which house the project was undertaken by Tarmac Construction, starting in 1978 with the Torus Hall being completed in January 1982. Construction of the JET machine itself began immediately after the completion of the Torus Hall, with the first plasma experiments in 1983. The cost was 198.8 Million European Units of Account (predecessor to the Euro) or 438 million in 2014 US dollars.\nJET's power requirements during the plasma pulse are around 500 MW with peak in excess of 1000 MW. Because power draw from the main grid is limited to 575 MW, two large flywheel generators were constructed to provide this necessary power. Each 775-ton flywheel can spin up to 225 rpm and store 3.75 GJ. Each flywheel uses 8.8 MW to spin up and can generate 400 MW (briefly).\n\n\n== Operations ==\nIn 1970 the Council of the European Community decided in favour of a robust fusion programme and provided the necessary legal framework for a European fusion device to be developed. Three years later, the design work began for the JET machine. In 1977 the construction work began and at the end of the same year a former Fleet Air Arm airfield at Culham in the UK was selected as the site for the JET project. In 1978 the \"JET Joint Undertaking\" was established as a legal entity. Only five years later the construction was completed on time and on budget. On 25 June 1983 the very first JET plasma was achieved and on 9 April 1984 Queen Elizabeth II officially opened the facility.\nOn 9 November 1991 a Preliminary Tritium Experiment achieved the world\u2019s first controlled release of fusion power.\nSix years later, in 1997, another world record was achieved at JET: 16 mega watts of fusion power were produced.\nIn 1998 JET\u2019s engineers developed a remote handling system with which, for the first time, it was possible to exchange certain components using artificial hands only. A \u201cRemote Handling\u201d system is, in general, an essential tool for any subsequent fusion power plant and especially for the International Thermonuclear Experimental Reactor (ITER). This Remote Handling system was later to lead on to become RACE (Remote Applications in Challenging Environments).\nIn 1999 the European Fusion Development Agreement (EFDA) was established with responsibility for the future collective use of JET.\nJET operated throughout 2003, with the year culminating in experiments using small amounts of tritium.\nIn October 2009, a 15-month shutdown period was started, and improvements were made to the tokamak, including replacing carbon components in the vacuum vessel with tungsten and beryllium ones, to bring JET's components more in line with those planned for ITER. Heating power was also increased by 50%, bringing the neutral beam power available to the plasma up to 34MW, and diagnostic and control capabilities were improved. In total, over 86,000 components were changed in the torus during the shutdown. In mid-May 2011, the shutdown reached its end. The first experimental campaign after the installation of the \u201cITER-Like Wall\u201d started on 2 September 2011.\nOn 14 July 2014 the European Commission signed a contract worth \u20ac283m for another 5-year extension so more advanced higher energy research can be performed at JET.\n\n\n== Future ==\nScientists intend to start ITER operations with a full tungsten divertor and are also planning fusion tests using Deuterium-Tritium plasmas for 2018: they hope to break their own record of 16 megawatts of fusion power. Beyond 2018, the future of JET is uncertain due to the UK decision in June 2016 to leave the EU.\n\n\n== See also ==\n\nFusion power\nITER\n\n\n== References ==\n\n\n== External links ==\nJET pages on the EUROfusion web site\nPoloidal field coils diagram\nJET demonstrates alpha particle heating. Oct 2005 good graph\n\nCulham Centre for Fusion Energy\nThe United Kingdom Atomic Energy Authority\nIAEA's information about JET\nPhotos from JET Torus Hall\n\n\n=== Sources ===\nFusion reactors explained by HowStuffWorks\nT. Fujita, et al., \"High performance experiments in JT-60U reversed shear discharges\", Nuclear Fusion, Vol 39, Page 1627 (1999)", 
                "titleUrl": "https://en.wikipedia.org/wiki/Joint_European_Torus", 
                "title": "Joint European Torus"
            }, 
            {
                "snippet": "gases (GHGs). Mitigation may also be achieved by increasing the capacity of carbon sinks, e.g., through reforestation. Mitigation policies can substantially", 
                "pageCategories": "All articles containing potentially dated statements\nAll articles to be expanded\nAll articles with specifically marked weasel-worded phrases\nAll articles with unsourced statements\nArticles containing potentially dated statements from 2011\nArticles to be expanded from May 2015\nArticles using small message boxes\nArticles with Wayback Machine links\nArticles with specifically marked weasel-worded phrases from May 2010\nArticles with unsourced statements from April 2009", 
                "pageContent": "Climate change mitigation consists of actions to limit the magnitude or rate of long-term climate change. Climate change mitigation generally involves reductions in human (anthropogenic) emissions of greenhouse gases (GHGs). Mitigation may also be achieved by increasing the capacity of carbon sinks, e.g., through reforestation. Mitigation policies can substantially reduce the risks associated with human-induced global warming.\n\"Mitigation is a public good; climate change is a case of the 'tragedy of the commons'\". Effective climate change mitigation will not be achieved if each agent (individual, institution or country) acts independently in its own selfish interest, (See International Cooperation and Emissions Trading) suggesting the need for collective action. Some adaptation actions, on the other hand, have characteristics of a private good as benefits of actions may accrue more directly to the individuals, regions, or countries that undertake them, at least in the short term. Nevertheless, financing such adaptive activities remains an issue, particularly for poor individuals and countries.\"\nExamples of mitigation include phasing out fossil fuels by switching to low-carbon energy sources, such as renewable and nuclear energy, and expanding forests and other \"sinks\" to remove greater amounts of carbon dioxide from the atmosphere. Energy efficiency may also play a role, for example, through improving the insulation of buildings. Another approach to climate change mitigation is climate engineering.\nMost countries are parties to the United Nations Framework Convention on Climate Change (UNFCCC). The ultimate objective of the UNFCCC is to stabilize atmospheric concentrations of GHGs at a level that would prevent dangerous human interference of the climate system. Scientific analysis can provide information on the impacts of climate change, but deciding which impacts are dangerous requires value judgments.\nIn 2010, Parties to the UNFCCC agreed that future global warming should be limited to below 2.0 \u00b0C (3.6 \u00b0F) relative to the pre-industrial level. With the Paris Agreement of 2015 this was confirmed, but was revised with a new target laying down \"parties will do the best\" to achieve warming below 1.5 \u00b0C. The current trajectory of global greenhouse gas emissions does not appear to be consistent with limiting global warming to below 1.5 or 2 \u00b0C. Other mitigation policies have been proposed, some of which are more stringent or modest than the 2 \u00b0C limit.\n\n\n== Background ==\n\n\n=== Greenhouse gas concentrations and stabilization ===\n\nOne of the issues often discussed in relation to climate change mitigation is the stabilization of greenhouse gas concentrations in the atmosphere. The United Nations Framework Convention on Climate Change (UNFCCC) has the ultimate objective of preventing \"dangerous\" anthropogenic (i.e., human) interference of the climate system. As is stated in Article 2 of the Convention, this requires that greenhouse gas (GHG) concentrations are stabilized in the atmosphere at a level where ecosystems can adapt naturally to climate change, food production is not threatened, and economic development can proceed in a sustainable fashion.\nThere are a number of anthropogenic greenhouse gases. These include carbon dioxide (chemical formula: CO2), methane (CH\n4), nitrous oxide (N\n2O), and a group of gases referred to as halocarbons. The emissions reductions necessary to stabilize the atmospheric concentrations of these gases varies. CO2 is the most important of the anthropogenic greenhouse gases (see radiative forcing).\nThere is a difference between stabilizing CO2 emissions and stabilizing atmospheric concentrations of CO2. Stabilizing emissions of CO2 at current levels would not lead to a stabilization in the atmospheric concentration of CO2. In fact, stabilizing emissions at current levels would result in the atmospheric concentration of CO2 continuing to rise over the 21st century and beyond (see the graphs opposite).\nThe reason for this is that human activities are adding CO2 to the atmosphere far faster than natural processes can remove it (see carbon dioxide in Earth's atmosphere for a more complete explanation). This is analogous to a flow of water into a bathtub. So long as the tap runs water (analogous to the emission of carbon dioxide) into the tub faster than water escapes through the plughole (the natural removal of carbon dioxide from the atmosphere), then the level of water in the tub (analogous to the concentration of carbon dioxide in the atmosphere) will continue to rise.\nAccording to some studies, stabilizing atmospheric CO2 concentrations would require anthropogenic CO2 emissions to be reduced by 80% relative to the peak emissions level. An 80% reduction in emissions would stabilize CO2 concentrations for around a century, but even greater reductions would be required beyond this. Other research has found that, after leaving room for emissions for food production for 9 billion people and to keep the global temperature rise below 2 \u00b0C, emissions from energy production and transport will have to peak almost immediately in the developed world and decline at ca. 10% per annum until zero emissions are reached around 2030. In developing countries energy and transport emissions would have to peak by 2025 and then decline similarly.\nStabilizing the atmospheric concentration of the other greenhouse gases humans emit also depends on how fast their emissions are added to the atmosphere, and how fast the GHGs are removed. Stabilization for these gases is described in the later section on non-CO2 GHGs.\nProjections\nProjections of future greenhouse gas emissions are highly uncertain. In the absence of policies to mitigate climate change, GHG emissions could rise significantly over the 21st century.\nNumerous assessments have considered how atmospheric GHG concentrations could be stabilized. The lower the desired stabilization level, the sooner global GHG emissions must peak and decline. GHG concentrations are unlikely to stabilize this century without major policy changes.\n\n\n=== Energy consumption by power source ===\n\nTo create lasting climate change mitigation, the replacement of high carbon emission intensity power sources, such as conventional fossil fuels\u2014oil, coal and natural gas\u2014with low-carbon power sources is required. Fossil fuels supply humanity with the vast majority of our energy demands, and at a growing rate. In 2012 the IEA noted that coal accounted for half the increased energy use of the prior decade, growing faster than all renewable energy sources. Both hydroelectricity and nuclear power together provide the majority of the generated low-carbon power fraction of global total power consumption.\n\n\n== Methods and means ==\n\nAssessments often suggest that GHG emissions can be reduced using a portfolio of low-carbon technologies. At the core of most proposals is the reduction of greenhouse gas (GHG) emissions through reducing energy waste and switching to low-carbon power sources of energy. As the cost of reducing GHG emissions in the electricity sector appears to be lower than in other sectors, such as in the transportation sector, the electricity sector may deliver the largest proportional carbon reductions under an economically efficient climate policy.\n\"Economic tools can be useful in designing climate change mitigation policies.\" \"While the limitations of economics and social welfare analysis, including cost\u2013benefit analysis, are widely documented, economics nevertheless provides useful tools for assessing the pros and cons of taking, or not taking, action on climate change mitigation, as well as of adaptation measures, in achieving competing societal goals. Understanding these pros and cons can help in making policy decisions on climate change mitigation and can influence the actions taken by countries, institutions and individuals.\"\nOther frequently discussed means include energy conservation, increasing fuel economy in automobiles (which includes the use of electric hybrids), charging plug-in hybrids and electric cars by low-carbon electricity, making individual-lifestyle changes (e.g., cycling instead of driving), and changing business practices. Many fossil fuel driven vehicles can be converted to use electricity, the U.S. has an estimated capacity of supporting 73% light duty vehicles (LDV). In terms of transportation, the net result would be a 27% total reduction in emissions of the greenhouse gases carbon dioxide, methane, and nitrous oxide, a 31% total reduction in nitrogen oxides, a slight reduction in nitrous oxide emissions, an increase in particulate matter emissions, the same sulfur dioxide emissions, and the near elimination of carbon monoxide and volatile organic compound emissions (a 98% decrease in carbon monoxide and a 93% decrease in volatile organic compounds). The emissions would be displaced away from street level, where they have \"high human-health implications.\"\nA range of energy technologies may contribute to climate change mitigation. These include nuclear power and renewable energy sources such as biomass, hydroelectricity, wind power, solar power, geothermal power, ocean energy, and; the use of carbon sinks, and carbon capture and storage. For example, Pacala and Socolow of Princeton have proposed a 15 part program to reduce CO2 emissions by 1 billion metric tons per year \u2212 or 25 billion tons over the 50-year period using today's technologies as a type of Global warming game.\nAnother consideration is how future socio-economic development proceeds. Development choices (or \"pathways\") can lead differences in GHG emissions. Political and social attitudes may affect how easy or difficult it is to implement effective policies to reduce emissions.\n\n\n=== Alternative energy sources ===\n\n\n==== Renewable energy ====\n\nRenewable energy flows involve natural phenomena such as sunlight, wind, rain, tides, plant growth, and geothermal heat, as the International Energy Agency explains:\n\nRenewable energy is derived from natural processes that are replenished constantly. In its various forms, it derives directly from the sun, or from heat generated deep within the earth. Included in the definition is electricity and heat generated from solar, wind, ocean, hydropower, biomass, geothermal resources, and biofuels and hydrogen derived from renewable resources.\n\nClimate change concerns and the need to reduce carbon emissions are driving increasing growth in the renewable energy industries. Low-carbon renewable energy replaces conventional fossil fuels in three main areas: power generation, hot water/ space heating, and transport fuels. In 2011, the share of renewables in electricity generation worldwide grew for the fourth year in a row to 20.2%. Based on REN21's 2014 report, renewables contributed 19% to supply global energy consumption. This energy consumption is divided as 9% coming from burning biomass, 4.2% as heat energy (non-biomass), 3.8% hydro electricity and 2% as electricity from wind, solar, geothermal, and biomass thermal power plants.\nRenewable energy use has grown much faster than anyone anticipated. The Intergovernmental Panel on Climate Change (IPCC) has said that there are few fundamental technological limits to integrating a portfolio of renewable energy technologies to meet most of total global energy demand. At the national level, at least 30 nations around the world already have renewable energy contributing more than 20% of energy supply.\nAs of 2012, renewable energy accounts for almost half of new electricity capacity installed and costs are continuing to fall. Public policy and political leadership helps to \"level the playing field\" and drive the wider acceptance of renewable energy technologies. As of 2011, 118 countries have targets for their own renewable energy futures, and have enacted wide-ranging public policies to promote renewables. Leading renewable energy companies include BrightSource Energy, First Solar, Gamesa, GE Energy, Goldwind, Sinovel, Suntech, Trina Solar, Vestas and Yingli.\nThe incentive to use 100% renewable energy has been created by global warming and other ecological as well as economic concerns. Mark Z. Jacobson says producing all new energy with wind power, solar power, and hydropower by 2030 is feasible and existing energy supply arrangements could be replaced by 2050. Barriers to implementing the renewable energy plan are seen to be \"primarily social and political, not technological or economic\". Jacobson says that energy costs with a wind, solar, water system should be similar to today's energy costs. According to a 2011 projection by the (IEA)International Energy Agency, solar power generators may produce most of the world's electricity within 50 years, dramatically reducing harmful greenhouse gas emissions. Critics of the \"100% renewable energy\" approach include Vaclav Smil and James E. Hansen. Smil and Hansen are concerned about the variable output of solar and wind power, NIMBYism, and a lack of infrastructure.\nEconomic analysts expect market gains for renewable energy (and efficient energy use) following the 2011 Japanese nuclear accidents. In his 2012 State of the Union address, President Barack Obama restated his commitment to renewable energy and mentioned the long-standing Interior Department commitment to permit 10,000 MW of renewable energy projects on public land in 2012. Globally, there are an estimated 3 million direct jobs in renewable energy industries, with about half of them in the biofuels industry.\nSome countries, with favorable geography, geology and weather well suited to an economical exploitation of renewable energy sources, already get most of their electricity from renewables, including from geothermal energy in Iceland (100 percent), and Hydroelectric power in Brazil (85 percent), Austria (62 percent), New Zealand (65 percent), and Sweden (54 percent). Renewable power generators are spread across many countries, with wind power providing a significant share of electricity in some regional areas: for example, 14 percent in the U.S. state of Iowa, 40 percent in the northern German state of Schleswig-Holstein, and 20 percent in Denmark. Solar water heating makes an important and growing contribution in many countries, most notably in China, which now has 70 percent of the global total (180 GWth). Worldwide, total installed solar water heating systems meet a portion of the water heating needs of over 70 million households. The use of biomass for heating continues to grow as well. In Sweden, national use of biomass energy has surpassed that of oil. Direct geothermal heating is also growing rapidly. Renewable biofuels for transportation, such as ethanol fuel and biodiesel, have contributed to a significant decline in oil consumption in the United States since 2006. The 93 billion liters of biofuels produced worldwide in 2009 displaced the equivalent of an estimated 68 billion liters of gasoline, equal to about 5 percent of world gasoline production.\n\n\n==== Nuclear power ====\n\nSince about 2001 the term \"nuclear renaissance\" has been used to refer to a possible nuclear power industry revival, driven by rising fossil fuel prices and new concerns about meeting greenhouse gas emission limits. However, in March 2011 the Fukushima nuclear disaster in Japan and associated shutdowns at other nuclear facilities raised questions among some commentators over the future of nuclear power. Platts has reported that \"the crisis at Japan's Fukushima nuclear plants has prompted leading energy-consuming countries to review the safety of their existing reactors and cast doubt on the speed and scale of planned expansions around the world\".\nThe World Nuclear Association has reported that nuclear electricity generation in 2012 was at its lowest level since 1999. Several previous international studies and assessments, suggested that as part of the portfolio of other low-carbon energy technologies, nuclear power will continue to play a role in reducing greenhouse gas emissions. Historically, nuclear power usage is estimated to have prevented the atmospheric emission of 64 gigatonnes of CO2-equivalent as of 2013. Public concerns about nuclear power include the fate of spent nuclear fuel, nuclear accidents, security risks, nuclear proliferation, and a concern that nuclear power plants are very expensive. Of these concerns, nuclear accidents and disposal of long-lived radioactive fuel/\"waste\" have probably had the greatest public impact worldwide. Although generally unaware of it, both of these glaring public concerns are greatly diminished by present passive safety designs, the experimentally proven, \"melt-down proof\" EBR-II, future molten salt reactors, and the use of conventional and more advanced fuel/\"waste\" pyroprocessing, with the latter recycling or reprocessing not presently being commonplace as it is often considered to be cheaper to use a once-through nuclear fuel cycle in many countries, depending on the varying levels of intrinsic value given by a society in reducing the long-lived waste in their country, with France doing a considerable amount of reprocessing when compared to the US.\nNuclear power, with a 10.6% share of world electricity production as of 2013, is second only to hydroelectricity as the largest source of low-carbon power. Over 400 reactors generate electricity in 31 countries.\nA Yale University review published in the Journal of Industrial Ecology analyzing CO2 life cycle assessment(LCA) emissions from nuclear power(Light water reactors) determined that: \"The collective LCA literature indicates that life cycle GHG emissions from nuclear power are only a fraction of traditional fossil sources and comparable to renewable technologies.\" While some have raised uncertainty surrounding the future GHG emissions of nuclear power as a result of an extreme potential decline in uranium ore grade without a corresponding increase in the efficiency of enrichment methods. In a scenario analysis of future global nuclear development, as it could be effected by a decreasing global uranium market of average ore grade, the analysis determined that depending on conditions, median life cycle nuclear power GHG emissions could be between 9 and 110 g CO2-eq/kWh by 2050, with the latter high figure being derived from a \"worst-case scenario\" that is not \"considered very robust\" by the authors of the paper, as the \"ore grade\" in the scenario is lower than the uranium concentration in many lignite coal ashes.\nAlthough this future analyses primarily deals with extrapolations for present Generation II reactor technology, the same paper also summarizes the literature on \"FBRs\"/Fast Breeder Reactors, of which two are in operation as of 2014 with the newest being the BN-800, for these reactors it states that the \"median life cycle GHG emissions ... [are] similar to or lower than [present light water reactors] LWRs and purports to consume little or no uranium ore.\nIn their 2014 report, the IPCC comparison of energy sources global warming potential per unit of electricity generated, which notably included albedo effects, mirror the median emission value derived from the Warner and Heath Yale meta-analysis for the more common non-breeding Light water reactors, a CO2-equivalent value of 12 g CO2-eq/kWh, which is the lowest global warming forcing of all baseload power sources, with comparable low carbon power baseload sources, such as hydropower and biomass, producing substantially more global warming forcing 24 and 230 g CO2-eq/kWh respectively.\nIn 2014, Brookings Institution published The Net Benefits of Low and No-Carbon Electricity Technologies which states, after performing an energy and emissions cost analysis, that \"The net benefits of new nuclear, hydro, and natural gas combined cycle plants far outweigh the net benefits of new wind or solar plants\", with the most cost effective low carbon power technology being determined to be nuclear power.\nDuring his presidential campaign, Barack Obama stated, \"Nuclear power represents more than 70% of our noncarbon generated electricity. It is unlikely that we can meet our aggressive climate goals if we eliminate nuclear power as an option.\"\n\nAnalysis in 2015 by Professor and Chair of Environmental Sustainability Barry W. Brook and his colleagues on the topic of replacing fossil fuels entirely, from the electric grid of the world, has determined that at the historically modest and proven-rate at which nuclear energy was added to and replaced fossil fuels in France and Sweden during each nation's building programs in the 1980s, within 10 years nuclear energy could displace or remove fossil fuels from the electric grid completely, \"allow[ing] the world to meet the most stringent greenhouse-gas mitigation targets.\". In a similar analysis, Brook had earlier determined that 50% of all global energy, that is not solely electricity, but transportation synfuels etc. could be generated within approximately 30 years, if the global nuclear fission build rate was identical to each of these nation's already proven decadal rates(in units of installed nameplate capacity, GW per year, per unit of global GDP(GW/year/$).\nThis is in contrast to the completely conceptual paper-studies for a 100% renewable energy world, which would require an orders of magnitude more costly global investment per year, an investment rate that has no historical precedent, having never been attempted due to its prohibitive cost, and with far greater land area that would be required to be devoted to the wind, wave and solar projects, along with the inherent assumption that humanity will use less, and not more, energy in the future. As Brook notes the \"principal limitations on nuclear fission are not technical, economic or fuel-related, but are instead linked to complex issues of societal acceptance, fiscal and political inertia, and inadequate critical evaluation of the real-world constraints facing [the other] low-carbon alternatives.\"\nNuclear power may be uncompetitive compared with fossil fuel energy sources in countries without a carbon tax program, and in comparison to a fossil fuel plant of the same power output, nuclear power plants take a longer amount of time to construct.\nTwo new, first of their kind, EPR reactors under construction in Finland and France have been delayed and are running over-budget. However learning from experience, two further EPR reactors under construction in China are on, and ahead, of schedule respectively. As of 2013, according to the IAEA and the European Nuclear Society, worldwide there were 68 civil nuclear power reactors under construction in 15 countries. China has 29 of these nuclear power reactors under construction, as of 2013, with plans to build many more, while in the US the licenses of almost half its reactors have been extended to 60 years, and plans to build another dozen are under serious consideration. There are also a considerable number of new reactors being built in South Korea, India, and Russia. At least 100 older and smaller reactors will \"most probably be closed over the next 10\u201315 years\". This is probable only if one does not factor in the ongoing Light Water Reactor Sustainability Program, created to permit the extension of the life span of the USA's 104 nuclear reactors to 60 years. The licenses of almost half of the USA's reactors have been extended to 60 years as of 2008. Two new \"passive safety\" AP1000 reactors are, as of 2013, being constructed at Vogtle Electric Generating Plant.\nPublic opinion about nuclear power varies widely between countries. A poll by Gallup International (2011) assessed public opinion in 47 countries. The poll was conducted following a tsunami and earthquake which caused an accident at the Fukushima nuclear power plant in Japan. 49% stated that they held favourable views about nuclear energy, while 43% held an unfavourable view. Another global survey by Ipsos (2011) assessed public opinion in 24 countries. Respondents to this survey showed a clear preference for renewable energy sources over coal and nuclear energy (refer to graph opposite). Ipsos (2012) found that solar and wind were viewed by the public as being more environmentally friendly and more viable long-term energy sources relative to nuclear power and natural gas. However, solar and wind were viewed as being less reliable relative to nuclear power and natural gas. In 2012 a poll done in the UK found that 63% of those surveyed support nuclear power, and with opposition to nuclear power at 11%. In Germany, strong anti-nuclear sentiment led to eight of the seventeen operating reactors being permanently shut down following the March 2011 Fukushima nuclear disaster.\nNuclear fusion research, in the form of the International Thermonuclear Experimental Reactor is underway. Fusion powered electricity generation was initially believed to be readily achievable, as fission power had been. However, the extreme requirements for continuous reactions and plasma containment led to projections being extended by several decades. In 2010, more than 60 years after the first attempts, commercial power production was still believed to be unlikely before 2050. Although rather than an either, or, issue economical fusion-fission hybrid reactors could be built before any attempt at this more demanding commercial \"pure-fusion reactor\"/DEMO reactor takes place.\n\n\n==== Coal to gas fuel switching ====\n\nMost mitigation proposals imply\u2014rather than directly state\u2014an eventual reduction in global fossil fuel production. Also proposed are direct quotas on global fossil fuel production.\nNatural gas emits far fewer greenhouse gases (i.e. CO2 and methane\u2014CH4) than coal when burned at power plants, but evidence has been emerging that this benefit could be completely negated by methane leakage at gas drilling fields and other points in the supply chain.\nA study performed by the Environmental Protection Agency (EPA) and the Gas Research Institute (GRI) in 1997 sought to discover whether the reduction in carbon dioxide emissions from increased natural gas (predominantly methane) use would be offset by a possible increased level of methane emissions from sources such as leaks and emissions. The study concluded that the reduction in emissions from increased natural gas use outweighs the detrimental effects of increased methane emissions. More recent peer-reviewed studies have challenged the findings of this study, with researchers from the National Oceanic and Atmospheric Administration (NOAA) reconfirming findings of high rates of methane (CH4) leakage from natural gas fields.\nA 2011 study by noted climate research scientist, Tom Wigley, found that while carbon dioxide (CO2) emissions from fossil fuel combustion may be reduced by using natural gas rather than coal to produce energy, it also found that additional methane (CH4) from leakage adds to the radiative forcing of the climate system, offsetting the reduction in CO2 forcing that accompanies the transition from coal to gas. The study looked at methane leakage from coal mining; changes in radiative forcing due to changes in the emissions of sulfur dioxide and carbonaceous aerosols; and differences in the efficiency of electricity production between coal- and gas-fired power generation. On balance, these factors more than offset the reduction in warming due to reduced CO2 emissions. When gas replaces coal there is additional warming out to 2,050 with an assumed leakage rate of 0%, and out to 2,140 if the leakage rate is as high as 10%. The overall effects on global-mean temperature over the 21st century, however, are small. Petron et al. (2013) and Alvarez et al. (2012) note that estimated that leakage from gas infrastructure is likely to be underestimated. These studies indicate that the exploitation of natural gas as a \"cleaner\" fuel is questionable. A 2014 meta-study of 20 years of natural gas technical literature shows that methane emissions are consistently underestimated but on a 100-year scale, the climate benefits of coal to gas fuel switching are likely larger than the negative effects of natural gas leakage.\n\n\n==== Heat pump ====\n\nA heat pump is a device that provides heat energy from a source of heat to a destination called a \"heat sink\". Heat pumps are designed to move thermal energy opposite to the direction of spontaneous heat flow by absorbing heat from a cold space and releasing it to a warmer one. A heat pump uses some amount of external power to accomplish the work of transferring energy from the heat source to the heat sink.\nWhile air conditioners and freezers are familiar examples of heat pumps, the term \"heat pump\" is more general and applies to many HVAC (heating, ventilating, and air conditioning) devices used for space heating or space cooling. When a heat pump is used for heating, it employs the same basic refrigeration-type cycle used by an air conditioner or a refrigerator, but in the opposite direction\u2014releasing heat into the conditioned space rather than the surrounding environment. In this use, heat pumps generally draw heat from the cooler external air or from the ground. In heating mode, heat pumps are three to four times more efficient in their use of electric power than simple electrical resistance heaters.\nIt has been concluded that heat pumps are the single technology that could reduce the greenhouse gas emissions of households better than every other technology that is available on the market. With a market share of 30% and (potentially) clean electricity, heat pumps could reduce global CO2 emissions by 8% annually. Using ground source heat pumps could reduce around 60% of the primary energy demand and 90% of CO2 emissions in Europe in 2050 and make handling high shares of renewable energy easier. Using surplus renewable energy in heat pumps is regarded as the most effective household means to reduce global warming and fossil fuel depletion.\nWith significant amounts of fossil fuel used in electricity production, demands on the electrical grid also generate greenhouse gases. Without a high share of low-carbon electricity, a domestic heat pump will produce more carbon emissions than using natural gas.\n\n\n==== Fossil fuel phase-out: carbon neutral and negative fuels ====\n\nFossil fuel may be phased-out with carbon neutral and carbon negative pipeline and transportation fuels created with power to gas and gas to liquids technologies. Carbon dioxide from fossil fuel flue gas can be used to produce plastic lumber allowing carbon negative reforestation.\n\n\n=== Demand side management ===\n\n\n==== Energy efficiency and conservation ====\n\nEfficient energy use, sometimes simply called \"energy efficiency\", is the goal of efforts to reduce the amount of energy required to provide products and services. For example, insulating a home allows a building to use less heating and cooling energy to achieve and maintain a comfortable temperature. Installing fluorescent lights or natural skylights reduces the amount of energy required to attain the same level of illumination compared to using traditional incandescent light bulbs. Compact fluorescent lights use two-thirds less energy and may last 6 to 10 times longer than incandescent lights.\nEnergy efficiency has proved to be a cost-effective strategy for building economies without necessarily growing energy consumption. For example, the state of California began implementing energy-efficiency measures in the mid-1970s, including building code and appliance standards with strict efficiency requirements. During the following years, California's energy consumption has remained approximately flat on a per capita basis while national U.S. consumption doubled. As part of its strategy, California implemented a \"loading order\" for new energy resources that puts energy efficiency first, renewable electricity supplies second, and new fossil-fired power plants last.\nEnergy conservation is broader than energy efficiency in that it encompasses using less energy to achieve a lesser energy service, for example through behavioural change, as well as encompassing energy efficiency. Examples of conservation without efficiency improvements would be heating a room less in winter, driving less, or working in a less brightly lit room. As with other definitions, the boundary between efficient energy use and energy conservation can be fuzzy, but both are important in environmental and economic terms. This is especially the case when actions are directed at the saving of fossil fuels.\nReducing energy use is seen as a key solution to the problem of reducing greenhouse gas emissions. According to the International Energy Agency, improved energy efficiency in buildings, industrial processes and transportation could reduce the world's energy needs in 2050 by one third, and help control global emissions of greenhouse gases.\n\n\n==== Demand side switching sources ====\nFuel switching on the demand side refers to changing the type of fuel used to satisfy a need for an energy service. To meet deep decarbonization goals, like the 80% reduction by 2050 goal being discussed in California and the European Union, many primary energy changes are needed. Energy efficiency alone may not be sufficient to meet these goals, switching fuels used on the demand side will help lower carbon emissions. Progressively coal, oil and eventually natural gas for space and water heating in buildings will need to be reduced. For an equivalent amount of heat, burning natural gas produces about 45 per cent less carbon dioxide than burning coal. There are various ways in which this could happen, and different strategies will likely make sense in different locations. While the system efficiency of a gas furnace may be higher than the combination of natural gas power plant and electric heat, the combination of the same natural gas power plant and an electric heat pump has lower emissions per unit of heat delivered in all but the coldest climates. This is possible because of the very efficient coefficient of performance of heat pumps.\nAt the beginning of this century 70% of all electricity was generated by fossil fuels, and as carbon free sources eventually make up half of the generation mix, replacing gas or oil furnaces and water heaters with electric ones will have a climate benefit. In areas like Norway, Brazil and Quebec that have abundant hydroelectricity, electric heat and hot water is common.\nThe economics of switching the demand side from fossil fuels to electricity for heating, will depend on the price of fuels vs electricity and the relative prices of the equipment. The EIA Annual Energy Outlook 2014 suggests that domestic gas prices will rise faster than electricity prices which will encourage electrification in the coming decades. Electrifying heating loads may also provide a flexible resource that can participate in demand response. Since thermostatically controlled loads have inherent energy storage, electrification of heating could provide a valuable resource to integrate variable renewable resources into the grid.\nAlternatives to electrification, include decarbonizing pipeline gas through power to gas, biogas, or other carbon neutral fuels. A 2015 study by Energy+Environmental Economics shows that a hybrid approach of decarbonizing pipeline gas, electrification, and energy efficiency can meet carbon reduction goals at a similar cost as only electrification and energy efficiency in Southern California.\n\n\n==== Demand side grid management ====\nExpanding intermittent electrical sources such as wind power, creates a growing problem balancing grid fluctuations. Some of the plans include building pumped storage or continental super grids costing billions of dollars. However instead of building for more power,there are a variety of ways to affect the size and timing of electricity demand on the consumer side. Designing for reduced demands on a smaller power grid is more efficient and economic than having extra generation and transmission for intermittentcy, power failures and peak demands. Having these abilities is one of the chief aims of a smart grid.\nTime of use metering is a common way to motivate electricity users to reduce their peak load consumption. For instance, running dishwashers and laundry at night after the peak has passed, reduces electricity costs.\nDynamic demand plans have devices passively shut off when stress is sensed on the electrical grid. This method may work very well with thermostats, when power on the grid sags a small amount, a low power temperature setting is automatically selected reducing the load on the grid. For instance millions of refrigerators reduce their consumption when clouds pass over solar installations. Consumers would need to have a smart meter in order for the utility to calculate credits.\nDemand response devices could receive all sorts of messages from the grid. The message could be a request to use a low power mode similar to dynamic demand, to shut off entirely during a sudden failure on the grid, or notifications about the current and expected prices for power. This would allow electric cars to recharge at the least expensive rates independent of the time of day. The vehicle-to-grid suggestion would use a car's battery or fuel cell to supply the grid temporarily.\n\n\n==== Lifestyle and behavior ====\nThe IPCC Fifth Assessment Report emphasises that behaviour, lifestyle and cultural change have a high mitigation potential in some sectors, particularly when complementing technological and structural change. In general, higher consumption lifestyles have a greater environmental impact. Overall, food accounts for the largest share of consumption-based GHG emissions with nearly 20% of the global carbon footprint, followed by housing, mobility, services, manufactured products, and construction. Food and services are more significant in poor countries, while mobility and manufactured goods are more significant in rich countries.\n\n\n===== Dietary change =====\n\nA 2014 study into the real-life diets of British people estimates their greenhouse gas contributions (CO2eq) to be: 7.19 kg/day for high meat-eaters through to 3.81 kg/day for vegetarians and 2.89 kg/day for vegans. The widespread adoption of a vegetarian diet could cut food-related greenhouse gas emissions by 63% by 2050. China introduced new dietary guidelines in 2016 which aim to cut meat consumption by 50% and thereby reduce greenhouse gas emissions by 1 billion tonnes by 2030.\n\n\n=== Sinks and negative emissions ===\n\nA carbon sink is a natural or artificial reservoir that accumulates and stores some carbon-containing chemical compound for an indefinite period, such as a growing forest. A negative carbon dioxide emission on the other hand is a permanent removal of carbon dioxide out of the atmosphere, such as directly capturing carbon dioxide in the atmosphere and storing it in geologic formations underground.\nThe Antarctic Climate and Ecosystems Cooperative Research Centre (ACE-CRC) notes that one third of humankind\u2019s annual emissions of CO2 are absorbed by the oceans. However, this also leads to ocean acidification, with potentially significant impacts on marine life. Acidification lowers the level of carbonate ions available for calcifying organisms to form their shells. These organisms include plankton species that contribute to the foundation of the Southern Ocean food web. However acidification may impact on a broad range of other physiological and ecological processes, such as fish respiration, larval development and changes in the solubility of both nutrients and toxins.\n\n\n==== Reforestation and afforestation ====\n\nAlmost 20 percent (8 GtCO2/year) of total greenhouse-gas emissions were from deforestation in 2007. It is estimated that avoided deforestation reduces CO2 emissions at a rate of 1 ton of CO2 per $1-$5 in opportunity costs from lost agriculture. Reforestation and afforestation, where there was previously no forest, could save at least another 1GtCO2/year, at an estimated cost of $5-$15/tCO2.\nTransferring rights over land from public domain to its indigenous inhabitants is argued to be a cost effective strategy to conserve forests. This includes the protection of such rights entitled in existing laws, such as India\u2019s Forest Rights Act. The transferring of such rights in China, perhaps the largest land reform in modern times, has been argued to have increased forest cover. In Brazil, forested areas given tenure to indigenous groups have even lower rates of clearing than national parks.\nWith increased intensive agriculture and urbanization, there is an increase in the amount of abandoned farmland. By some estimates, for every half a hectare of original old-growth forest cut down, more than 20 hectares of new secondary forests are growing, even though they do not have the same biodiversity as the original forests and original forests store 60% more carbon than these new secondary forests. According to a study in Science, promoting regrowth on abandoned farmland could offset years of carbon emissions.\n\n\n==== Avoided desertification ====\n\nRestoring grasslands store CO2 from the air into plant material. Grazing livestock, usually not left to wander, would eat the grass and would minimize any grass growth. However, grass left alone would eventually grow to cover its own growing buds, preventing them from photosynthesizing and the dying plant would stay in place. A method proposed to restore grasslands uses fences with many small paddocks and moving herds from one paddock to another after a day a two in order to mimick natural grazers and allowing the grass to grow optimally. Additionally, when part of leaf matter is consumed by a herding animal, a corresponding amount of root matter is sloughed off too as it would not be able to sustain the previous amount of root matter and while most of the lost root matter would rot and enter the atmosphere, part of the carbon is sequestered into the soil. It is estimated that increasing the carbon content of the soils in the world\u2019s 3.5 billion hectares of agricultural grassland by 1% would offset nearly 12 years of CO2 emissions. Allan Savory, as part of holistic management, claims that while large herds are often blamed for desertification, prehistoric lands supported large or larger herds and areas where herds were removed in the United States are still desertifying.\n\n\n==== Carbon capture and storage ====\n\nCarbon capture and storage (CCS) is a method to mitigate climate change by capturing carbon dioxide (CO2) from large point sources such as power plants and subsequently storing it away safely instead of releasing it into the atmosphere. The Intergovernmental Panel on Climate Change says CCS could contribute between 10% and 55% of the cumulative worldwide carbon-mitigation effort over the next 90 years. The International Energy Agency says CCS is \"the most important single new technology for CO2 savings\" in power generation and industry. Though it requires up to 40% more energy to run a CCS coal power plant than a regular coal plant, CCS could potentially capture about 90% of all the carbon emitted by the plant. Norway, which first began storing CO2, has cut its emissions by almost a million tons a year, or about 3% of the country's 1990 levels. As of late 2011, the total CO2 storage capacity of all 14 projects in operation or under construction is over 33 million tonnes a year. This is broadly equivalent to preventing the emissions from more than six million cars from entering the atmosphere each year.\n\n\n==== Negative carbon dioxide emissions ====\n\nCreating negative carbon dioxide emissions literally removes carbon from the atmosphere. Examples are direct air capture, biochar, bio-energy with carbon capture and storage and enhanced weathering technologies. These processes are sometimes considered as variations of sinks or mitigation, and sometimes as geoengineering.\nIn combination with other mitigation measures, sinks in combination with negative carbon emissions are considered crucial for meeting the 350 ppm target, and even the less conservative 450 ppm target.\n\n\n=== Geoengineering ===\n\nGeoengineering is seen by some as an alternative to mitigation and adaptation, but by others as an entirely separate response to climate change. In a literature assessment, Barker et al. (2007) described geoengineering as a type of mitigation policy. IPCC (2007) concluded that geoengineering options, such as ocean fertilization to remove CO2 from the atmosphere, remained largely unproven. It was judged that reliable cost estimates for geoengineering had not yet been published.\nChapter 28 of the National Academy of Sciences report Policy Implications of Greenhouse Warming: Mitigation, Adaptation, and the Science Base (1992) defined geoengineering as \"options that would involve large-scale engineering of our environment in order to combat or counteract the effects of changes in atmospheric chemistry.\" They evaluated a range of options to try to give preliminary answers to two questions: can these options work and could they be carried out with a reasonable cost. They also sought to encourage discussion of a third question \u2014 what adverse side effects might there be. The following types of option were examined: reforestation, increasing ocean absorption of carbon dioxide (carbon sequestration) and screening out some sunlight. NAS also argued \"Engineered countermeasures need to be evaluated but should not be implemented without broad understanding of the direct effects and the potential side effects, the ethical issues, and the risks.\". In July 2011 a report by the United States Government Accountability Office on geoengineering found that \"[c]limate engineering technologies do not now offer a viable response to global climate change.\"\n\n\n==== Carbon dioxide removal ====\n\nCarbon dioxide removal has been proposed as a method of reducing the amount of radiative forcing. A variety of means of artificially capturing and storing carbon, as well as of enhancing natural sequestration processes, are being explored. The main natural process is photosynthesis by plants and single-celled organisms (see biosequestration). Artificial processes vary, and concerns have been expressed about the long-term effects of some of these processes.\nIt is notable that the availability of cheap energy and appropriate sites for geological storage of carbon may make carbon dioxide air capture viable commercially. It is, however, generally expected that carbon dioxide air capture may be uneconomic when compared to carbon capture and storage from major sources \u2014 in particular, fossil fuel powered power stations, refineries, etc. In such cases, costs of energy produced will grow significantly. However, captured CO2 can be used to force more crude oil out of oil fields, as Statoil and Shell have made plans to do. CO2 can also be used in commercial greenhouses, giving an opportunity to kick-start the technology. Some attempts have been made to use algae to capture smokestack emissions, notably the GreenFuel Technologies Corporation, who have now shut down operations.\n\n\n==== Solar radiation management ====\n\nThe main purpose of solar radiation management seek to reflect sunlight and thus reduce global warming. The ability of stratospheric sulfate aerosols to create a global dimming effect has made them a possible candidate for use in climate engineering projects.\n\n\n=== Non-CO2 greenhouse gases ===\nCO2 is not the only GHG relevant to mitigation, and governments have acted to regulate the emissions of other GHGs emitted by human activities (anthropogenic GHGs). The emissions caps agreed to by most developed countries under the Kyoto Protocol regulate the emissions of almost all the anthropogenic GHGs. These gases are CO2, methane (CH4), nitrous oxide (N2O), the hydrofluorocarbons (HFC), perfluorocarbons (PFC), and sulfur hexafluoride (SF6).\nStabilizing the atmospheric concentrations of the different anthropogenic GHGs requires an understanding of their different physical properties. Stabilization depends both on how quickly GHGs are added to the atmosphere and how fast they are removed. The rate of removal is measured by the atmospheric lifetime of the GHG in question (see the main GHG article for a list). Here, the lifetime is defined as the time required for a given perturbation of the GHG in the atmosphere to be reduced to 37% of its initial amount. Methane has a relatively short atmospheric lifetime of about 12 years, while N2O's lifetime is about 110 years. For methane, a reduction of about 30% below current emission levels would lead to a stabilization in its atmospheric concentration, while for N2O, an emissions reduction of more than 50% would be required.\nMethane is a significantly more potent greenhouse gas than carbon dioxide in the amount of heat it can trap, especially in the short term. Burning one molecule of methane generates one molecule of carbon dioxide, indicating there may be no net benefit in using gas as a fuel source. Reducing the amount of waste methane produced in the first place and moving away from use of gas as a fuel source will have a greater beneficial impact, as might other approaches to productive use of otherwise-wasted methane. In terms of prevention, vaccines are being developed in Australia to reduce the significant global warming contributions from methane released by livestock via flatulence and eructation.\nAnother physical property of the anthropogenic GHGs relevant to mitigation is the different abilities of the gases to trap heat (in the form of infrared radiation). Some gases are more effective at trapping heat than others, e.g., SF6 is 22,200 times more effective a GHG than CO2 on a per-kilogram basis. A measure for this physical property is the global warming potential (GWP), and is used in the Kyoto Protocol.\nAlthough not designed for this purpose, the Montreal Protocol has probably benefited climate change mitigation efforts. The Montreal Protocol is an international treaty that has successfully reduced emissions of ozone-depleting substances (for example, CFCs), which are also greenhouse gases.\n\n\n== By sector ==\n\n\n=== Transport ===\n\nModern energy-efficient technologies, such as plug-in hybrid electric vehicles, and development of new technologies, such as carbon-neutral synthetic gasoline & Jet fuel, may reduce the consumption of petroleum, land use changes and emissions of carbon dioxide. A shift from air transport and truck transport to electric rail transport would reduce emissions significantly. For electric vehicles, the reduction of carbon emissions will improve further if the way the required electricity is generated is low-carbon power in origin.\n\n\n=== Urban planning ===\n\nEffective urban planning to reduce sprawl would decrease Vehicle Miles Travelled (VMT), lowering emissions from transportation. Increased use of public transport can also reduce greenhouse gas emissions per passenger kilometer. Between 1982 and 1997, the amount of land consumed for urban development in the United States increased by 47 percent while the nation's population grew by only 17 percent. Inefficient land use development practices have increased infrastructure costs as well as the amount of energy needed for transportation, community services, and buildings.\nAt the same time, a growing number of citizens and government officials have begun advocating a smarter approach to land use planning. These smart growth practices include compact community development, multiple transportation choices, mixed land uses, and practices to conserve green space. These programs offer environmental, economic, and quality-of-life benefits; and they also serve to reduce energy usage and greenhouse gas emissions.\nApproaches such as New Urbanism and Transit-oriented development seek to reduce distances travelled, especially by private vehicles, encourage public transit and make walking and cycling more attractive options. This is achieved through \"medium-density\", mixed-use planning and the concentration of housing within walking distance of town centers and transport nodes.\nSmarter growth land use policies have both a direct and indirect effect on energy consuming behavior. For example, transportation energy usage, the number one user of petroleum fuels, could be significantly reduced through more compact and mixed use land development patterns, which in turn could be served by a greater variety of non-automotive based transportation choices.\n\n\n==== Building design ====\n\nEmissions from housing are substantial, and government-supported energy efficiency programmes can make a difference.\nFor institutions of higher learning in the United States, greenhouse gas emissions depend primarily on total area of buildings and secondarily on climate. If climate is not taken into account, annual greenhouse gas emissions due to energy consumed on campuses plus purchased electricity can be estimated with the formula, E=aSb, where a =0.001621 metric tonnes of CO2 equivalent/square foot or 0.0241 metric tonnes of CO2 equivalent/square meter and b = 1.1354.\nNew buildings can be constructed using passive solar building design, low-energy building, or zero-energy building techniques, using renewable heat sources. Existing buildings can be made more efficient through the use of insulation, high-efficiency appliances (particularly hot water heaters and furnaces), double- or triple-glazed gas-filled windows, external window shades, and building orientation and siting. Renewable heat sources such as shallow geothermal and passive solar energy reduce the amount of greenhouse gasses emitted. In addition to designing buildings which are more energy-efficient to heat, it is possible to design buildings that are more energy-efficient to cool by using lighter-coloured, more reflective materials in the development of urban areas (e.g. by painting roofs white) and planting trees. This saves energy because it cools buildings and reduces the urban heat island effect thus reducing the use of air conditioning.\n\n\n=== Agriculture ===\n\nAccording to the EPA, agricultural soil management practices can lead to production and emission of nitrous oxide (N2O), a major greenhouse gas and air pollutant. Activities that can contribute to N\n2O emissions include fertilizer usage, irrigation and tillage. The management of soils accounts for over half of the emissions from the Agriculture sector. Cattle livestocks account for one third of emissions, through methane emissions. Manure management and rice cultivation also produce gaseous emissions.\nMethods that significantly enhance carbon sequestration in soil include no-till farming, residue mulching, cover cropping, and crop rotation, all of which are more widely used in organic farming than in conventional farming. Because only 5% of US farmland currently uses no-till and residue mulching, there is a large potential for carbon sequestration.\nA 2015 study found that farming can deplete soil carbon and render soil incapable of supporting life. Instead the study showed that conservation farming can protect carbon in soils, and repair damage over time.\nThe farming practise of cover crops has been recognized as climate-smart agriculture by the White House.\n\n\n=== Societal controls ===\n\nAnother method being examined is to make carbon a new currency by introducing tradeable \"personal carbon credits\". The idea being it will encourage and motivate individuals to reduce their 'carbon footprint' by the way they live. Each citizen will receive a free annual quota of carbon that they can use to travel, buy food, and go about their business. It has been suggested that by using this concept it could actually solve two problems; pollution and poverty, old age pensioners will actually be better off because they fly less often, so they can cash in their quota at the end of the year to pay heating bills and so forth.\n\n\n==== Population ====\n\nVarious organizations promote population control as a means for mitigating global warming. Proposed measures include improving access to family planning and reproductive health care and information, reducing natalistic politics, public education about the consequences of continued population growth, and improving access of women to education and economic opportunities.\nPopulation control efforts are impeded by there being somewhat of a taboo in some countries against considering any such efforts. Also, various religions discourage or prohibit some or all forms of birth control.\nPopulation size has a different per capita effect on global warming in different countries, since the per capita production of anthropogenic greenhouse gases varies greatly by country.\n\n\n== Costs and benefits ==\n\n\n=== Costs ===\nThe Stern Review proposes stabilising the concentration of greenhouse-gas emissions in the atmosphere at a maximum of 550ppm CO2e by 2050. The Review estimates that this would mean cutting total greenhouse-gas emissions to three quarters of 2007 levels. The Review further estimates that the cost of these cuts would be in the range \u22121.0 to +3.5% of World GDP, (i.e. GWP), with an average estimate of approximately 1%. Stern has since revised his estimate to 2% of GWP. For comparison, the Gross World Product (GWP) at PPP was estimated at $74.5 trillion in 2010, thus 2% is approximately $1.5 trillion. The Review emphasises that these costs are contingent on steady reductions in the cost of low-carbon technologies. Mitigation costs will also vary according to how and when emissions are cut: early, well-planned action will minimise the costs.\nOne way of estimating the cost of reducing emissions is by considering the likely costs of potential technological and output changes. Policy makers can compare the marginal abatement costs of different methods to assess the cost and amount of possible abatement over time. The marginal abatement costs of the various measures will differ by country, by sector, and over time.\n\n\n=== Benefits ===\n\nYohe et al. (2007) assessed the literature on sustainability and climate change. With high confidence, they suggested that up to the year 2050, an effort to cap greenhouse gas (GHG) emissions at 550 ppm would benefit developing countries significantly. This was judged to be especially the case when combined with enhanced adaptation. By 2100, however, it was still judged likely that there would be significant effects of global warming. This was judged to be the case even with aggressive mitigation and significantly enhanced adaptive capacity.\n\n\n=== Sharing ===\nOne of the aspects of mitigation is how to share the costs and benefits of mitigation policies. There is no scientific consensus over how to share these costs and benefits (Toth et al., 2001). In terms of the politics of mitigation, the UNFCCC's ultimate objective is to stabilize concentrations of GHG in the atmosphere at a level that would prevent \"dangerous\" climate change (Rogner et al., 2007).\nGHG emissions are an important correlate of wealth, at least at present (Banuri et al., 1996, pp. 91\u201392). Wealth, as measured by per capita income (i.e., income per head of population), varies widely between different countries. Activities of the poor that involve emissions of GHGs are often associated with basic needs, such as heating to stay tolerably warm. In richer countries, emissions tend to be associated with things like cars, central heating, etc. The impacts of cutting emissions could therefore have different impacts on human welfare according to wealth.\n\n\n==== Distributing emissions abatement costs ====\nThere have been different proposals on how to allocate responsibility for cutting emissions (Banuri et al., 1996, pp. 103\u2013105):\nEgalitarianism: this system interprets the problem as one where each person has equal rights to a global resource, i.e., polluting the atmosphere.\nBasic needs: this system would have emissions allocated according to basic needs, as defined according to a minimum level of consumption. Consumption above basic needs would require countries to buy more emission rights. From this viewpoint, developing countries would need to be at least as well off under an emissions control regime as they would be outside the regime.\nProportionality and polluter-pays principle: Proportionality reflects the ancient Aristotelian principle that people should receive in proportion to what they put in, and pay in proportion to the damages they cause. This has a potential relationship with the \"polluter-pays principle\", which can be interpreted in a number of ways:\nHistorical responsibilities: this asserts that allocation of emission rights should be based on patterns of past emissions. Two-thirds of the stock of GHGs in the atmosphere at present is due to the past actions of developed countries (Goldemberg et al., 1996, p. 29).\nComparable burdens and ability to pay: with this approach, countries would reduce emissions based on comparable burdens and their ability to take on the costs of reduction. Ways to assess burdens include monetary costs per head of population, as well as other, more complex measures, like the UNDP's Human Development Index.\nWillingness to pay: with this approach, countries take on emission reductions based on their ability to pay along with how much they benefit from reducing their emissions.\n\n\n==== Specific proposals ====\nAd hoc: Lashof (1992) and Cline (1992) (referred to by Banuri et al., 1996, p. 106), for example, suggested that allocations based partly on GNP could be a way of sharing the burdens of emission reductions. This is because GNP and economic activity are partially tied to carbon emissions.\nEqual per capita entitlements: this is the most widely cited method of distributing abatement costs, and is derived from egalitarianism (Banuri et al., 1996, pp. 106\u2013107). This approach can be divided into two categories. In the first category, emissions are allocated according to national population. In the second category, emissions are allocated in a way that attempts to account for historical (cumulative) emissions.\nStatus quo: with this approach, historical emissions are ignored, and current emission levels are taken as a status quo right to emit (Banuri et al., 1996, p. 107). An analogy for this approach can be made with fisheries, which is a common, limited resource. The analogy would be with the atmosphere, which can be viewed as an exhaustible natural resource (Goldemberg et al., 1996, p. 27). In international law, one state recognized the long-established use of another state's use of the fisheries resource. It was also recognized by the state that part of the other state's economy was dependent on that resource.\n\n\n== Governmental and intergovernmental action ==\n\nMany countries, both developing and developed, are aiming to use cleaner technologies (World Bank, 2010, p. 192). Use of these technologies aids mitigation and could result in substantial reductions in CO2 emissions. Policies include targets for emissions reductions, increased use of renewable energy, and increased energy efficiency. It is often argued that the results of climate change are more damaging in poor nations, where infrastructures are weak and few social services exist. The Commitment to Development Index is one attempt to analyze rich country policies taken to reduce their disproportionate use of the global commons. Countries do well if their greenhouse gas emissions are falling, if their gas taxes are high, if they do not subsidize the fishing industry, if they have a low fossil fuel rate per capita, and if they control imports of illegally cut tropical timber.\n\n\n=== Kyoto Protocol ===\n\nThe main current international agreement on combating climate change is the Kyoto Protocol, which came into force on 16 February 2005. The Kyoto Protocol is an amendment to the United Nations Framework Convention on Climate Change (UNFCCC). Countries that have ratified this protocol have committed to reduce their emissions of carbon dioxide and five other greenhouse gases, or engage in emissions trading if they maintain or increase emissions of these gases.\n\n\n=== Temperature targets ===\n\nActions to mitigate climate change are sometimes based on the goal of achieving a particular temperature target. One of the targets that has been suggested is to limit the future increase in global mean temperature (global warming) to below 2 \u00b0C, relative to the pre-industrial level. The 2 \u00b0C target was adopted in 2010 by Parties to the United Nations Framework Convention on Climate Change. Most countries of the world are Parties to the UNFCCC. The target had been adopted in 1996 by the European Union Council.\nFeasibility of 2 \u00b0C\nTemperatures have increased by 0.8 \u00b0C compared to the pre-industrial level, and another 0.5\u20130.7 \u00b0C is already committed. The 2 \u00b0C rise is typically associated in climate models with a carbon dioxide equivalent concentration of 400\u2013500 ppm by volume; the current (January 2015) level of carbon dioxide alone is 400 ppm by volume, and rising at 1\u20133 ppm annually. Hence, to avoid a very likely breach of the 2 \u00b0C target, CO2 levels would have to be stabilised very soon; this is generally regarded as unlikely, based on current programs in place to date. The importance of change is illustrated by the fact that world economic energy efficiency is improving at only half the rate of world economic growth.\nViews in the literature\nThere is disagreement among experts over whether or not the 2 \u00b0C target can be met. For example, according to Anderson and Bows (2011), \"there is little to no chance\" of meeting the target. On the other hand, according to Alcamo et al. (2013):\nPolicies adopted by parties to the UNFCCC are too weak to meet a 2 or 1.5 \u00b0C target. However, these targets might still be achievable if more stringent mitigation policies are adopted immediately.\nCost-effective 2 \u00b0C scenarios project annual global greenhouse gas emissions to peak before the year 2020, with deep cuts in emissions thereafter, leading to a reduction in 2050 of 41% compared to 1990 levels.\nDiscussion on other targets\nScientific analysis can provide information on the impacts of climate change and associated policies, such as reducing GHG emissions. However, deciding what policies are best requires value judgements. For example, limiting global warming to 1 \u00b0C relative to pre-industrial levels may help to reduce climate change damages more than a 2 \u00b0C limit. However, a 1 \u00b0C limit may be more costly to achieve than a 2 \u00b0C limit.\nAccording to some analysts, the 2 \u00b0C \"guardrail\" is inadequate for the needed degree and timeliness of mitigation. On the other hand, some economic studies suggest more modest mitigation policies. For example, the emissions reductions proposed by Nordhaus (2010) might lead to global warming (in the year 2100) of around 3 \u00b0C, relative to pre-industrial levels.\nOfficial long-term target of 1.5 \u00b0C\nIn 2015, two official UNFCCC scientific expert bodies came to the conclusion that, \"in some regions and vulnerable ecosystems, high risks are projected even for warming above 1.5\u00b0C\". This expert position was, together with the strong diplomatic voice of the poorest countries and the island nations in the Pacific, the driving force leading to the decision of the Paris Conference 2015, to lay down this 1.5 \u00b0C long-term target on top of the existing 2 \u00b0C goal.\n\n\n=== Encouraging use changes ===\n\n\n==== Emissions tax ====\n\nAn emissions tax on greenhouse gas emissions requires individual emitters to pay a fee, charge or tax for every tonne of greenhouse gas released into the atmosphere. Most environmentally related taxes with implications for greenhouse gas emissions in OECD countries are levied on energy products and motor vehicles, rather than on CO2 emissions directly.\nEmission taxes can be both cost-effective and environmentally effective. Difficulties with emission taxes include their potential unpopularity, and the fact that they cannot guarantee a particular level of emissions reduction. Emissions or energy taxes also often fall disproportionately on lower income classes. In developing countries, institutions may be insufficiently developed for the collection of emissions fees from a wide variety of sources.\n\n\n==== Subsidies ====\nAccording to Mark Z. Jacobson, a program of subsidization balanced against expected flood costs could pay for conversion to 100% renewable power by 2030. Jacobson, and his colleague Mark Delucchi, suggest that the cost to generate and transmit power in 2020 will be less than 4 cents per kilowatt hour (in 2007 dollars) for wind, about 4 cents for wave and hydroelectric, from 4 to 7 cents for geothermal, and 8 cents per kWh for solar, fossil, and nuclear power.\n\n\n==== Investment ====\n\nAnother indirect method of encouraging uses of renewable energy, and pursue sustainability and environmental protection, is that of prompting investment in this area through legal means, something that is already being done at national level as well as in the field of international investment.\n\n\n==== Carbon emissions trading ====\n\nWith the creation of a market for trading carbon dioxide emissions within the Kyoto Protocol, it is likely that London financial markets will be the centre for this potentially highly lucrative business; the New York and Chicago stock markets may have a lower trade volume than expected as long as the US maintains its rejection of the Kyoto.\nHowever, emissions trading may delay the phase-out of fossil fuels.\nIn the north-east United States, a successful cap and trade program has shown potential for this solution.\nThe European Union Emission Trading Scheme (EU ETS) is the largest multi-national, greenhouse gas emissions trading scheme in the world. It commenced operation on 1 January 2005, and all 28 member states of the European Union participate in the scheme which has created a new market in carbon dioxide allowances estimated at 35 billion Euros (US$43 billion) per year. The Chicago Climate Exchange was the first (voluntary) emissions market, and is soon to be followed by Asia's first market (Asia Carbon Exchange). A total of 107 million metric tonnes of carbon dioxide equivalent have been exchanged through projects in 2004, a 38% increase relative to 2003 (78 Mt CO2e).\nTwenty three multinational corporations have come together in the G8 Climate Change Roundtable, a business group formed at the January 2005 World Economic Forum. The group includes Ford, Toyota, British Airways and BP. On 9 June 2005 the Group published a statement stating that there was a need to act on climate change and claiming that market-based solutions can help. It called on governments to establish \"clear, transparent, and consistent price signals\" through \"creation of a long-term policy framework\" that would include all major producers of greenhouse gases.\nThe Regional Greenhouse Gas Initiative is a proposed carbon trading scheme being created by nine North-eastern and Mid-Atlantic American states; Connecticut, Delaware, Maine, Massachusetts, New Hampshire, New Jersey, New York, Rhode Island and Vermont. The scheme was due to be developed by April 2005 but has not yet been completed.\n\n\n=== Implementation ===\nImplementation puts into effect climate change mitigation strategies and targets. These can be targets set by international bodies or voluntary action by individuals or institutions. This is the most important, expensive and least appealing aspect of environmental governance.\n\n\n==== Funding ====\nImplementation requires funding sources but is often beset by disputes over who should provide funds and under what conditions. A lack of funding can be a barrier to successful strategies as there are no formal arrangements to finance climate change development and implementation. Funding is often provided by nations, groups of nations and increasingly NGO and private sources. These funds are often channelled through the Global Environmental Facility (GEF). This is an environmental funding mechanism in the World Bank which is designed to deal with global environmental issues. The GEF was originally designed to tackle four main areas: biological diversity, climate change, international waters and ozone layer depletion, to which land degradation and persistent organic pollutant were added. The GEF funds projects that are agreed to achieve global environmental benefits that are endorsed by governments and screened by one of the GEF\u2019s implementing agencies.\n\n\n==== Problems ====\nThere are numerous issues which result in a current perceived lack of implementation. It has been suggested that the main barriers to implementation are, Uncertainty, Fragmentation, Institutional void, Short time horizon of policies and politicians and Missing motives and willingness to start adapting. The relationships between many climatic processes can cause large levels of uncertainty as they are not fully understood and can be a barrier to implementation. When information on climate change is held between the large numbers of actors involved it can be highly dispersed, context specific or difficult to access causing fragmentation to be a barrier. Institutional void is the lack of commonly accepted rules and norms for policy processes to take place, calling into question the legitimacy and efficacy of policy processes. The Short time horizon of policies and politicians often means that climate change policies are not implemented in favour of socially favoured societal issues. Statements are often posed to keep the illusion of political action to prevent or postpone decisions being made. Missing motives and willingness to start adapting is a large barrier as it prevents any implementation.\nThe issues that arise with a system which involves international government cooperation, such as Cap and Trade, could potentially be improved with a polycentric approach where the rules are enforced by many small sections of authority as apposed to one overall enforcement agency.\n\n\n==== Occurrence ====\nDespite a perceived lack of occurrence, evidence of implementation is emerging internationally. Some examples of this are the initiation of NAPA\u2019s and of joint implementation. Many developing nations have made National Adaptation Programs of Action (NAPAs) which are frameworks to prioritize adaption needs. The implementation of many of these is supported by GEF agencies. Many developed countries are implementing \u2018first generation\u2019 institutional adaption plans particularly at the state and local government scale. There has also been a push towards joint implementation between countries by the UNFCC as this has been suggested as a cost-effective way for objectives to be achieved.\n\n\n=== Territorial policies ===\n\n\n==== United States ====\n\nEfforts to reduce greenhouse gas emissions by the United States include energy policies which encourage efficiency through programs like Energy Star, Commercial Building Integration, and the Industrial Technologies Program. On 12 November 1998, Vice President Al Gore symbolically signed the Kyoto Protocol, but he indicated participation by the developing nations was necessary prior its being submitted for ratification by the United States Senate.\nIn 2007, Transportation Secretary Mary Peters, with White House approval, urged governors and dozens of members of the House of Representatives to block California\u2019s first-in-the-nation limits on greenhouse gases from cars and trucks, according to e-mails obtained by Congress. The U.S. Climate Change Science Program is a group of about twenty federal agencies and US Cabinet Departments, all working together to address global warming.\nThe Bush administration pressured American scientists to suppress discussion of global warming, according to the testimony of the Union of Concerned Scientists to the Oversight and Government Reform Committee of the U.S. House of Representatives. \"High-quality science\" was \"struggling to get out,\" as the Bush administration pressured scientists to tailor their writings on global warming to fit the Bush administration's skepticism, in some cases at the behest of an ex-oil industry lobbyist. \"Nearly half of all respondents perceived or personally experienced pressure to eliminate the words 'climate change,' 'global warming' or other similar terms from a variety of communications.\" Similarly, according to the testimony of senior officers of the Government Accountability Project, the White House attempted to bury the report \"National Assessment of the Potential Consequences of Climate Variability and Change,\" produced by U.S. scientists pursuant to U.S. law. Some U.S. scientists resigned their jobs rather than give in to White House pressure to underreport global warming.\nIn the absence of substantial federal action, state governments have adopted emissions-control laws such as the Regional Greenhouse Gas Initiative in the Northeast and the Global Warming Solutions Act of 2006 in California.\n\n\n==== Developing countries ====\nIn order to reconcile economic development with mitigating carbon emissions, developing countries need particular support, both financial and technical. One of the means of achieving this is the Kyoto Protocol's Clean Development Mechanism (CDM). The World Bank's Prototype Carbon Fund is a public private partnership that operates within the CDM.\nAn important point of contention, however, is how overseas development assistance not directly related to climate change mitigation is affected by funds provided to climate change mitigation. One of the outcomes of the UNFCC Copenhagen Climate Conference was the Copenhagen Accord, in which developed countries promised to provide US $30 million between 2010 and 2012 of new and additional resources. Yet it remains unclear what exactly the definition of additional is and the European Commission has requested its member states to define what they understand to be additional, and researchers at the Overseas Development Institute have found four main understandings:\nClimate finance classified as aid, but additional to (over and above) the \u20180.7%\u2019 ODA target;\nIncrease on previous year's Official Development Assistance (ODA) spent on climate change mitigation;\nRising ODA levels that include climate change finance but where it is limited to a specified percentage; and\nIncrease in climate finance not connected to ODA.\nThe main point being that there is a conflict between the OECD states budget deficit cuts, the need to help developing countries adapt to develop sustainably and the need to ensure that funding does not come from cutting aid to other important Millennium Development Goals.\nHowever, none of these initiatives suggest a quantitative cap on the emissions from developing countries. This is considered as a particularly difficult policy proposal as the economic growth of developing countries are proportionally reflected in the growth of greenhouse emissions. Critics of mitigation often argue that, the developing countries' drive to attain a comparable living standard to the developed countries would doom the attempt at mitigation of global warming. Critics also argue that holding down emissions would shift the human cost of global warming from a general one to one that was borne most heavily by the poorest populations on the planet.\nIn an attempt to provide more opportunities for developing countries to adapt clean technologies, UNEP and WTO urged the international community to reduce trade barriers and to conclude the Doha trade round \"which includes opening trade in environmental goods and services\".\n\n\n== Non-governmental approaches ==\nWhile many of the proposed methods of mitigating global warming require governmental funding, legislation and regulatory action, individuals and businesses can also play a part in the mitigation effort.\n\n\n=== Choices in personal actions and business operations ===\nEnvironmental groups encourage individual action against global warming, often aimed at the consumer. Common recommendations include lowering home heating and cooling usage, burning less gasoline, supporting renewable energy sources, buying local products to reduce transportation, turning off unused devices, and various others.\nA geophysicist at Utrecht University has urged similar institutions to hold the vanguard in voluntary mitigation, suggesting the use of communications technologies such as videoconferencing to reduce their dependence on long-haul flights.\n\n\n==== Air travel and shipment ====\nIn 2008, climate scientist Kevin Anderson raised concern about the growing effect of rapidly increasing global air transport on the climate in a paper, and a presentation, suggesting that reversing this trend is necessary to reduce emissions.\nPart of the difficulty is that when aviation emissions are made at high altitude, the climate impacts are much greater than otherwise. Others have been raising the related concerns of the increasing hypermobility of individuals, whether traveling for business or pleasure, involving frequent and often long distance air travel, as well as air shipment of goods.\n\n\n=== Business opportunities and risks ===\n\nOn 9 May 2005 Jeff Immelt, the chief executive of General Electric (GE), announced plans to reduce GE's global warming related emissions by one percent by 2012. \"GE said that given its projected growth, those emissions would have risen by 40 percent without such action.\"\nOn 21 June 2005 a group of leading airlines, airports and aerospace manufacturers pledged to work together to reduce the negative environmental impact of aviation, including limiting the impact of air travel on climate change by improving fuel efficiency and reducing carbon dioxide emissions of new aircraft by fifty percent per seat kilometre by 2020 from 2000 levels. The group aims to develop a common reporting system for carbon dioxide emissions per aircraft by the end of 2005, and pressed for the early inclusion of aviation in the European Union's carbon emission trading scheme.\n\n\n=== Investor response ===\n\nClimate change is also a concern for large institutional investors who have a long term time horizon and potentially large exposure to the negative impacts of global warming because of the large geographic footprint of their multi-national holdings. SRI (Socially responsible investing) Funds allow investors to invest in funds that meet high ESG (environmental, social, governance) standards as such funds invest in companies that are aligned with these goals. Proxy firms can be used to draft guidelines for investment managers that take these concerns into account.\n\n\n=== Legal action ===\n\nIn some countries, those affected by climate change may be able to sue major producers. Attempts at litigation have been initiated by entire peoples such as Palau and the Inuit, as well as non-governmental organizations such as the Sierra Club. Although proving that particular weather events are due specifically to global warming may never be possible, methodologies have been developed to show the increased risk of such events caused by global warming.\nFor a legal action for negligence (or similar) to succeed, \"Plaintiffs ... must show that, more probably than not, their individual injuries were caused by the risk factor in question, as opposed to any other cause. This has sometimes been translated to a requirement of a relative risk of at least two.\" Another route (though with little legal bite) is the World Heritage Convention, if it can be shown that climate change is affecting World Heritage Sites like Mount Everest.\nBesides countries suing one another, there are also cases where people in a country have taken legal steps against their own government. Legal action for instance has been taken to try to force the U.S. Environmental Protection Agency to regulate greenhouse gas emissions under the Clean Air Act, and against the Export-Import Bank and OPIC for failing to assess environmental impacts (including global warming impacts) under NEPA.\nIn the Netherlands and Belgium, organisations as Urgenda and the vzw Klimaatzaak in Belgium have also sued their governments as they believe their governments aren't meeting the emission reductions they agreed to. Urgenda has all ready won their case against the Dutch government.\nAccording to a 2004 study commissioned by Friends of the Earth, ExxonMobil and its predecessors caused 4.7 to 5.3 percent of the world's man-made carbon dioxide emissions between 1882 and 2002. The group suggested that such studies could form the basis for eventual legal action.\nIn 2015, Exxon, received a subpoena. According to the Washington Post and confirmed by the company, the attorney general of New York, Eric Schneiderman, opened an investigation into the possibility that the company had mislead the public and investors about the risks of climate change.\n\n\n== See also ==\n\n\n=== By country ===\nDebate over China's economic responsibilities for climate change mitigation\nEuropean Climate Change Programme\nMitigation of global warming in Australia\n\n\n== Notes ==\n\n\n== References ==\nAlcamo, J.; et al. (2013), The Emissions Gap Report 2013, Nairobi, Kenya: United Nations Environment Programme (UNEP)  Archived 13 April 2014.\nClarke, L.; et al. (July 2007), Scenarios of Greenhouse Gas Emissions and Atmospheric Concentrations. Sub-report 2.1A of Synthesis and Assessment Product 2.1 by the U.S. Climate Change Science Program and the Subcommittee on Global Change Research, Washington, DC., USA: Department of Energy, Office of Biological & Environmental Research \nGallup International (19 April 2011), Impact of Japan Earthquake on Views about nuclear energy. Findings from a Global Snap Poll in 47 countries by WIN-Gallup International (21 March \u2013 10 April 2011) (PDF) \nIAEA, Climate Change and Nuclear Power 2008. A report by the International Atomic Energy Agency (IAEA) (PDF) \nOpen access: Hansen, J.; et al. (2013), \"Assessing \"Dangerous Climate Change\": Required Reduction of Carbon Emissions to Protect Young People, Future Generations and Nature\", PLoS ONE, 8 (12): e81648, Bibcode:2013PLoSO...881648H, doi:10.1371/journal.pone.0081648 . Archived 14 August 2014.\nIpcc tar wg3 (2001), Metz, B.; Davidson, O.; Swart, R.; Pan, J., eds., Climate Change 2001: Mitigation, Contribution of Working Group III to the Third Assessment Report of the Intergovernmental Panel on Climate Change, Cambridge University Press, ISBN 0-521-80769-7  (pb: 0-521-01502-2).\nIpcc ar4 wg1 (2007), Solomon, S.; Qin, D.; Manning, M.; Chen, Z.; Marquis, M.; Averyt, K.B.; Tignor, M.; Miller, H.L., eds., Climate Change 2007: The Physical Science Basis, Contribution of Working Group I to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change, Cambridge University Press, ISBN 978-0-521-88009-1  (pb: 978-0-521-70596-7).\nIpcc ar4 wg2 (2007), Parry, M.L.; Canziani, O.F.; Palutikof, J.P.; van der Linden, P.J.; Hanson, C.E., eds., Climate Change 2007: Impacts, Adaptation and Vulnerability, Contribution of Working Group II to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change, Cambridge University Press, ISBN 978-0-521-88010-7  (pb: 978-0-521-70597-4).\nIpcc ar4 wg3 (2007), Metz, B.; Davidson, O.R.; Bosch, P.R.; Dave, R.; Meyer, L.A., eds., Climate Change 2007: Mitigation of Climate Change, Contribution of Working Group III (WG3) to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change (IPCC), Cambridge University Press, ISBN 978-0-521-88011-4  (pb: 978-0-521-70598-1).\nIpcc ar4 syr (2007), Core Writing Team; Pachauri, R.K; Reisinger, A., eds., Climate Change 2007: Synthesis Report, Contribution of Working Groups I, II and III to the Fourth Assessment Report of the Intergovernmental Panel on Climate Change, Geneva, Switzerland: IPCC, ISBN 92-9169-122-4 .\nIPCC AR5 WG2 A (2014), Field, C.B.; et al., eds., Climate Change 2014: Impacts, Adaptation, and Vulnerability. Part A: Global and Sectoral Aspects (GSA). Contribution of Working Group II (WG2) to the Fifth Assessment Report (AR5) of the Intergovernmental Panel on Climate Change (IPCC), Cambridge University Press . Archived 25 June 2014.\nIPCC AR5 WG3 (2014), Edenhofer, O.; et al., eds., Climate Change 2014: Mitigation of Climate Change. Contribution of Working Group III (WG3) to the Fifth Assessment Report (AR5) of the Intergovernmental Panel on Climate Change (IPCC), Cambridge University Press . Archived 29 June 2014.\nIpsos (23 June 2011), Global Citizen Reaction to the Fukushima Nuclear Plant Disaster (theme: environment / climate) Ipsos Global @dvisor (PDF) . Survey website: Ipsos MORI: Poll: Strong global opposition towards nuclear power\nLuderer, G.; et al. (2013), \"Economic mitigation challenges: how further delay closes the door for achieving climate targets\", Environ. Res. Lett., 8 (3): 034033, Bibcode:2013ERL.....8c4033L, doi:10.1088/1748-9326/8/3/034033 \nNordhaus, W.D. (14 June 2010), \"Economic aspects of global warming in a post-Copenhagen environment\", PNAS, 107: 11721\u201311726, Bibcode:2010PNAS..10711721N, doi:10.1073/pnas.1005985107 . Paper on Professor Nordhaus's website (archived 23 August 2014).\nUK Royal Society (September 2009), Geoengineering the climate: science, governance and uncertainty (PDF), London: UK Royal Society, ISBN 978-0-85403-773-5 , RS Policy document 10/09. Report website.\nUS NRC (2011), Climate Stabilization Targets: Emissions, Concentrations, and Impacts over Decades to Millennia. A report by the US National Research Council (US NRC), Washington, D.C., USA: National Academies Press \nUNEP (November 2012), The Emissions Gap Report 2012 (PDF), Nairobi, Kenya: United Nations Environment Programme (UNEP)  Executive summary in other languages\n This article incorporates public domain material from the US EPA document: US EPA (14 June 2012), Glossary of Climate Change Terms: Climate Change: US EPA, US Environmental Protection Agency (EPA) Climate Change Division \nvan Vuuren, D.P.; et al. (7 December 2009), Meeting the 2 degree target. From climate objective to emission reduction measures. PBL publication number 500114012 (PDF), Netherlands Environmental Assessment Agency (Planbureau voor de Leefomgeving (PBL)) . Archived 2 November 2013. Report website (archived 21 August 2014).\n\n\n== External links ==\nIntergovernmental Panel on Climate Change \u2013 Includes the Working Group III Report \"Mitigation of Climate Change\" as part of the Fourth Assessment Report\nWhy Black Carbon and Ozone Also Matter, in September/October 2009 Foreign Affairs with Veerabhadran Ramanathan and Jessica Seddon Wallack.\nThe Climate Threat We Can Beat, in May/June 2012 Foreign Affairs with David G. Victor, Charles F. Kennel, Veerabhadran Ramanathan.\nThe Radical Emission Reduction Conference\n\n\n=== European Union ===\nEuropean Union's European Climate Change Programme\nEU New Energy Policy\nEuropean Union Greenhouse Gas Emission Trading Scheme (EU ETS)\nCommunity strategy to reduce CO2 from light vehicles.\nUK's Climate Change Programme\nThe Stern Review on the economics of climate change \u2013 Parts III and IV of the Stern Review are on climate change mitigation\n\n\n=== USA ===\nEPA: What You Can Do.\nU.S. Mayors Climate Protection Agreement signed by 178 mayors representing nearly 40 million Americans\nCalifornia Climate Change Portal.\n\n\n=== Academic ===\nRivington M, Matthews KB, Buchan K and Miller D (2005) \"An integrated assessment approach to investigate options for mitigation and adaptation to climate change at the farm-scale\", NJF Seminar 380, Odense, Denmark, 7\u20138 November 2005.\nJacobson, M.Z.; Delucchi, M.A. (2009). \"A Plan to Power 100 Percent of the Planet with Renewables\" (originally published as \"A Path to Sustainable Energy by 2030\")\". Scientific American. 301 (5): 58\u201365. doi:10.1038/scientificamerican1109-58. PMID 19873905. \nLiving On a New Earth, Scientific American April 2010\nGlobal Warming Newswire \u2013 published scientific studies on global warming", 
                "titleUrl": "https://en.wikipedia.org/wiki/Climate_change_mitigation", 
                "title": "Climate change mitigation"
            }, 
            {
                "snippet": "have outlined numerous commitments to reduce carbon dioxide emissions. One such announcement was the Low Carbon Transition Plan launched by the Brown ministry", 
                "pageCategories": "All Wikipedia articles in need of updating\nAll articles with unsourced statements\nArticles with unsourced statements from March 2015\nCS1 maint: Multiple names: authors list\nCommons category with page title same as on Wikidata\nEnergy by country in the European Union\nEnergy in the United Kingdom\nPages using web citations with no URL\nUse dmy dates from March 2016\nWikipedia articles in need of updating from October 2012", 
                "pageContent": "Energy use in the United Kingdom stood at 2,249 TWh (193.4 million tonnes of oil equivalent) in 2014. This equates to energy consumption per capita of 34.82 MWh (3.00 tonnes of oil equivalent) compared to a 2010 world average of 21.54 MWh (1.85 tonnes of oil equivalent). Demand for electricity in 2014 was 34.42GW on average (301.7TWh over the year) coming from a total electricity generation of 335.0TWh.\nSuccessive UK governments have outlined numerous commitments to reduce carbon dioxide emissions. One such announcement was the Low Carbon Transition Plan launched by the Brown ministry in July 2009, which aimed to generate 30% electricity from renewable sources, and 40% from low carbon content fuels by 2020. Notably, the UK is one of the best sites in Europe for wind energy, and wind power production is its fastest growing supply, in 2014 it generated 9.3% of the UK's total electricity.\nGovernment commitments to reduce emissions are occurring against a backdrop of economic crisis across Europe. During the European financial crisis, Europe\u2019s consumption of electricity shrank by 5%, with primary production also facing a noticeable decline. Britain's trade deficit was reduced by 8% due to substantial cuts in energy imports. Between 2007 and 2015, the UK's peak electrical demand fell from 61.5 GW to 52.7.GW.\nUK government energy policy aims to play a key role in limiting greenhouse gas emissions, whilst meeting energy demand. Shifting availabilities of resources and development of technologies also change the country's energy mix through changes in costs. In 2016, the United Kingdom was ranked 12th in the World on the Environmental Performance Index, which measures how well a country carries through environmental policy.\n\n\n== Overview ==\nAs recently as 2004, the UK was a net exporter of energy, however by 2010, more than 25% of UK energy was imported.\n\n\n== Fossil fuels ==\n\nDuring 2008, the total energy consumed in the United Kingdom was 234.4 million tonnes of oil equivalent (which is around 9.85 EJ = 9.85\u00d71018J).\nConcerns over peak oil have been raised by high-profile voices in the United Kingdom such as Sir David King and the Industry Task-Force on Peak Oil and Energy Security. The latter's 2010 report states that \"The next five years will see us face another crunch - the oil crunch. This time, we do have the chance to prepare. The challenge is to use that time well.\" (Sir Richard Branson and Ian Marchant).\n\n\n=== Natural gas ===\nUnited Kingdom produced 60% of its consumed natural gas in 2010. In five years the United Kingdom moved from almost gas self-sufficient (see North Sea Gas) to 40% gas import in 2010. Gas was almost 40% of total primary energy supply (TPES) and electricity more than 45% in 2010. Underground storage was about 5% of annual demand and more than 10% of net imports. There is an alternative fuel obligation in the United Kingdom. (see Renewable Transport Fuel Obligation)\nGasfields include Amethyst gasfield, Armada gasfield, Easington Catchment Area, East Knapton, Everest gasfield and Rhum gasfield.\nA gas leak occurred in March 2012 at the Elgin-Franklin fields, where about 200,000 cubic metres of gas was escaping every day. Total missed out on about \u00a383m of potential income.\n\n\n== Electricity supply ==\n\nWith the development of the national grid, the switch to using electricity, United Kingdom electricity consumption increased by around 150% between the post war nationalisation of the industry in 1948 and the mid-1960s. During the 1960s growth slowed as the market became saturated. The United Kingdom is planning to reform its electricity market. It plans to introduce a capacity mechanism and contracts for difference to encourage the building of new generation.\n\n\n=== Fuel sources ===\n\nDuring the 1940s some 90% of the generating capacity was fired by coal, with oil providing most of the remainder.\nThe United Kingdom started to develop a nuclear generating capacity in the 1950s, with Calder Hall being connected to the grid on 27 August 1956. Though the production of weapons-grade plutonium was the main reason behind this power station, other civil stations followed, and 26% of the nation's electricity was generated from nuclear power at its peak in 1997.\nDespite the flow of North Sea oil from the mid-1970s, oil fuelled generation remained relatively small and continued to decline.\nStarting in 1993, and continuing through to the 1990s, a combination of factors led to a so-called Dash for Gas, during which the use of coal was scaled back in favour of gas-fuelled generation. This was sparked by the privatisation of the National Coal Board, British Gas and the Central Electricity Generating Board; the introduction of laws facilitating competition within the energy markets; and the availability of cheap gas from the North Sea. In 1990 just 1.09% of all gas consumed in the country was used in electricity generation; by 2004 the figure was 30.25%.\nBy 2004, coal use in power stations had fallen to 50.5 million tonnes, representing 82.4% of all coal used in 2004 (a fall of 43.6% compared to 1980 levels), though up slightly from its low in 1999. On several occasions in May 2016, Britain burned no coal for electricity for the first time since 1882.\nFrom the mid-1990s new renewable energy sources began to contribute to the electricity generated, adding to a small hydroelectricity generating capacity.\nIn 2014, total electricity production stood at 335 TWh (down from a peak of 385 TWh in 2005), generated from the following sources:\n\nGas: 30.2% (0.05% in 1990)\nCoal: 29.1% (67% in 1990)\nNuclear: 19.0% (19% in 1990)\nWind: 9.4% (0% in 1990)\nBio-Energy: 6.8% (0% in 1990)\nHydroelectric: 1.8% (2.6% in 1990)\nSolar: 1.2% (0% in 1990)\nOil and other: 2.5% (12% in 1990)\nThe UK Government energy policy had targeted a total contribution from renewables to achieve 10% by 2010, but it was not until 2012 that this figure was exceeded; renewable energy sources supplied 11.3% (41.3 TWh) of the electricity generated in the United Kingdom in 2012. The Scottish Government has a target of generating 17% to 18% of Scotland's electricity from renewables by 2010, rising to 40% by 2020.\n\n\n=== UK 'energy gap' ===\n\nIn the early years of the 2000s, concerns grew over the prospect of an 'energy gap' in United Kingdom generating capacity. This was forecast to arise because it was expected that a number of coal fired power stations would close due to being unable to meet the clean air requirements of the European Large Combustion Plant Directive (directive 2001/80/EC). In addition, the United Kingdom's remaining Magnox nuclear stations were to have closed by 2015. The oldest AGR nuclear power station has had its life extended by ten years, and it was likely many of the others could be life-extended, reducing the potential gap suggested by the current accounting closure dates of between 2014 and 2023 for the AGR power stations.\nA report from the industry in 2005 forecast that, without action to fill the gap, there would be a 20% shortfall in electricity generation capacity by 2015. Similar concerns were raised by a report published in 2000 by the Royal Commission on Environmental Pollution (Energy - The Changing Climate). The 2006 Energy Review attracted considerable press coverage - in particular in relation to the prospect of constructing a new generation of nuclear power stations, in order to prevent the rise in carbon dioxide emissions that would arise if other conventional power stations were to be built.\nAmong the public, according to a November 2005 poll conducted by YouGov for Deloitte, 35% of the population expect that by 2020 the majority of electricity generation will come from renewable energy (more than double the government's target, and far larger than the 5.5% generated as of 2008), 23% expect that the majority will come from nuclear power, and only 18% that the majority will come from fossil fuels. 92% thought the Government should do more to explore alternative power generation technologies to reduce carbon emissions.\nIn June 2013, the industry regulator Ofgem warned that the UK's energy sector faces \"unprecedented challenges\" and that \"spare electricity power production capacity could fall to 2% by 2015, increasing the risk of blackouts\". Proposed solutions \"could include negotiating with major power users for them to reduce demand during peak times in return for payment\".\n\n\n=== Plugging the energy gap ===\nThe first move to plug the United Kingdom's projected energy gap was the construction of the conventionally gas-fired Langage Power Station and Marchwood Power Station which became operational in 2010.\nIn 2007, proposals for the construction of two new coal-fired power stations were announced, in Tilbury, Essex and in Kingsnorth, Kent. If built, they will be the first coal-fired stations to be built in the United Kingdom in 20 years.\nBeyond these new plants, there are a number of options that might be used to provide the new generating capacity, while minimising carbon emissions and producing less residues and contamination. Fossil fuel power plants might provide a solution if there was a satisfactory and economical way of reducing their carbon emissions. Carbon capture might provide a way of doing this; however the technology is relatively untried and costs are relatively high. As yet (2006) there are no power plants in operation with a full carbon capture and storage system.\nHowever, due to reducing demand in the late-2000s recession removing any medium term gap, and high gas prices, in 2011 and 2012 over 2 GW of older, less efficient, gas generation plant was mothballed. In 2011 electricity demand dropped 4%, and about 6.5 GW of additional gas-fired capacity is being added over 2011 and 2012. Early in 2012 the reserve margin stood at the high level of 32%.\nAnother important factor in reduced electrical demand in recent years has come from the phasing out of Incandescent light bulbs and a switch to compact fluorescent and LED lighting. Research by the University of Oxford has shown that the average annual electrical consumption for lighting in a UK home fell from 720 kWh in 1997 to 508 kWh in 2012. Between 2007 and 2015, the UK's peak electrical demand fell from 61.5 GW to 52.7.GW.\n\n\n=== Regional differences ===\n\nWhile in some ways limited by which powers are devolved, the four countries of the United Kingdom have different energy mixes and ambitions. Scotland currently has a target of 80% of electricity from renewables by 2020, which was increased from an original ambition of 50% by 2020 after it exceeded its interim target of 31 per cent by 2011. It has a quarter of the EU's estimated offshore wind potential, and is at the forefront of testing various marine energy systems.\n\n\n=== Nuclear ===\n\nBritain's fleet of operational reactors consists of 14 advanced gas-cooled reactors on six discrete sites, along with two Magnox units at Wylfa, and one PWR unit at Sizewell B. Overall, the installed nuclear capacity in the United Kingdom is between 10 and 11 GW. In addition, the UK experimented with Fast Breeder reactor technologies at Dounreay in Scotland, however the last fast breeder (with 250MWe of capacity) was shut down in 1994.\nWhile nuclear power does not produce significant carbon dioxide in generation (though the construction, mining, waste handling and disposal, and decommissioning do generate some carbon emissions), it raises other environmental and security concerns. Despite this, it has enormous potential for generating electricity, when it is taken into consideration that uranium could last up to a hundred years. However, even with changes to the planning system to speed applications, there are doubts over whether the necessary timescale could be met, and over the financial viability of nuclear power with present oil and gas prices. With no nuclear plants having been constructed since Sizewell B in 1995, there are also likely to be capacity issues within the native nuclear industry. The existing privatised nuclear supplier, British Energy, had been in financial trouble in 2004.\nIn October 2010 the British Government gave the go-ahead for the construction of up to eight new nuclear power plants. However, the Scottish Government, with the backing of the Scottish Parliament, has stated that no new nuclear power stations will be constructed in Scotland. In March 2012, E.ON UK and RWE npower announced they would be pulling out of developing new nuclear power plants, placing the future of nuclear power in the United Kingdom in doubt.\nThere was an 11% increase in the use of nuclear power in 2011, which helped to bring greenhouse gas emissions down 7% on the previous year.\n\n\n=== Renewable energy ===\n\nIn 2007, the United Kingdom Government agreed to an overall European Union target of generating 20% of the European Union's energy supply from renewable sources by 2020. Each European Union member state was given its own allocated target; for the United Kingdom it is 15%. This was formalised in January 2009 with the passage of the EU Renewables Directive. As renewable heat and fuel production in the United Kingdom are at extremely low bases, RenewableUK estimates that this will require 35\u201340% of the United Kingdom's electricity to be generated from renewable sources by that date, to be met largely by 33\u201335 GW of installed wind capacity.\nThe total of all renewable electricity sources provided for 14.9% of the electricity generated in the United Kingdom in 2013, reaching 53.7 TWh of electricity generated.\n\n\n==== Wind power ====\n\nIn December 2007, the United Kingdom Government announced plans for a massive expansion of wind energy production, by conducting a Strategic Environmental Assessment of up to 25 GW worth of wind farm offshore sites in preparation for a new round of development. These proposed sites were in addition to the 8 GW worth of sites already awarded in the 2 earlier rounds of site allocations, Round 1 in 2001 and Round 2 in 2003. Taken together it was estimated that this would result in the construction of over 7,000 offshore wind turbines.\nWind power delivers a growing fraction of the energy in the United Kingdom and at the beginning of January 2015, wind power in the United Kingdom consisted of 5,958 wind turbines with a total installed capacity of just under 12 gigawatts: 7,950 megawatts of onshore capacity and 4,049 megawatts of offshore capacity.\n\n\n==== Solar ====\n\nAt the end of 2011, there were 230,000 solar power projects in the United Kingdom, with a total installed generating capacity of 750 megawatts (MW). By February 2012 the installed capacity had reached 1,000 MW. Solar power use has increased very rapidly in recent years, albeit from a small base, as a result of reductions in the cost of photovoltaic (PV) panels, and the introduction of a Feed-in tariff (FIT) subsidy in April 2010. In 2012, the government said that 4 million homes across the UK will be powered by the sun within eight years, representing 22,000 MW of installed solar power capacity by 2020.\n\n\n==== Biofuels ====\n\nGas from sewage and landfill (biogas) has already been exploited in some areas. In 2004 it provided 129.3 GW\u00b7h (up 690% from 1990 levels), and was the UK's leading renewable energy source, representing 39.4% of all renewable energy produced (including hydro). The UK has committed to a target of 10.3% of renewable energy in transport to comply with the Renewable Energy Directive of the European Union but has not yet implemented legislation to meet this target.\nOther biofuels can provide a close-to-carbon-neutral energy source, if locally grown. In South America and Asia, the production of biofuels for export has in some cases resulted in significant ecological damage, including the clearing of rainforest. In 2004 biofuels provided 105.9 GW\u00b7h, 38% of it wood. This represented an increase of 500% from 1990.\n\n\n==== Geothermal power ====\n\nInvestigations into the exploitation of Geothermal power in the United Kingdom, prompted by the 1973 oil crisis, were abandoned as fuel prices fell. Only one scheme is operational, in Southampton. In 2004 it was announced that a further scheme would be built to heat the UK's first geothermal energy model village near Eastgate, County Durham.\n\n\n==== Hydroelectric ====\n\nAs of 2012, hydroelectric power stations in the United Kingdom accounted for 1.67 GW of installed electrical generating capacity, being 1.9% of the UK's total generating capacity and 14% of UK's renewable energy generating capacity. Annual electricity production from such schemes is approximately 5,700 GWh, being about 1.5% of the UK's total electricity production.\nThere are also pumped-storage power stations in the UK. These power stations are net consumers of electrical energy however they contribute to balancing the grid, which can facilitate renewable generation elsewhere, for example by 'soaking up' surplus renewable output at off-peak times and release the energy when it is required.\n\n\n== Cogeneration ==\n\nCombined heat and power plants, where 'waste' hot water from generating is used for district heating, are also a well tried technology in other parts of Europe. While it heats about 50% of all houses in Denmark, Finland, Poland, Sweden and Slovakia, it currently only plays a small role in the United Kingdom. It has, however, been rising, with total generation standing at 27.9 TWh by 2008. This consisted of 1,439 predominantly gas-fired schemes with a total CHP electrical generating capacity of 5.47 GW, and contributing 7% of the UK's electricity supply. Heat generation utilisation has fallen however from a peak of 65 TWh in 1991 to 49 TWh in 2012.\n\n\n== Energy research ==\nHistorically, public sector support for energy research and development in the United Kingdom has been provided by a variety of bodies with little co-ordination between them. Problems experienced have included poor continuity of funding, and the availability of funding for certain parts of the research\u2014development\u2014commercialisation process but not others. Levels of public funding have also been low by international standards, and funding by the private sector has also been limited.\nResearch in the area of energy is carried out by a number of public and private sector bodies:\nThe Engineering and Physical Sciences Research Council funds an energy programme spanning energy and climate change research. It aims to \"develop, embrace and exploit sustainable, low carbon and/or energy efficient technologies and systems\" to enable the United Kingdom \"to meet the Government\u2019s energy and environmental targets by 2020\". Its research includes renewable, conventional, nuclear and fusion electricity supply as well as energy efficiency, fuel poverty and other topics.\nSince being established in 2004, the UK Energy Research Centre carries out research into demand reduction, future sources of energy, infrastructure and supply, energy systems, sustainability and materials for advanced energy systems.\nThe Energy Technologies Institute, expected to begin operating in 2008, is to 'accelerate the development of secure, reliable and cost-effective low-carbon energy technologies towards commercial deployment'.\nIn relation to buildings, the Building Research Establishment carries out some research into energy conservation.\nThere is currently international research being conducted into fusion power. The ITER reactor is currently being constructed at Cadarache in France. The United Kingdom contributes towards this project through membership of the European Union. Prior to this, an experimental fusion reactor (the Joint European Torus) had been built at Culham in Oxfordshire.\n\n\n== Energy efficiency ==\nThe United Kingdom government has instituted several policies intended to promote an increase in efficient energy use. These include the roll out of smart meters, the Green Deal, the CRC Energy Efficiency Scheme, the Energy Savings Opportunity Scheme and Climate Change Agreements.\nIn tackling the energy trilemma, saving energy is the cheapest of all measures. The Guardian newspaper reported that in 2012 that by 2050, Germany projects a 25% drop in electricity demand: the UK projects a rise of up to 66%. MP and Secretary of State for Energy and Climate Change Ed Davey pointed out in November 2012 that a modest 10% reduction in 2030 means five fewer power stations and \u00a34bn cut from bills.\nThe UK government has implemented measures aimed at cutting the UK's energy use by 11% by 2020. If achieved, this improvement would be sufficient to replace 22 UK power stations, while possibly providing a boost to the economy and living standards.\n\n\n== Climate change ==\nThe Committee on Climate Change publishes an annual progress report in respect to control the climate change in the United Kingdom.\n\n\n== See also ==\n\nEnergy policy of the United Kingdom\nEconomy of the United Kingdom\nEnergy conservation in the United Kingdom\nEnergy switching services in the UK\nGreenhouse gas emissions by the United Kingdom\nMeter Point Administration Number\nRenewable energy in the United Kingdom\nRenewable energy by country\n\n\n== References ==\n\n\n== External links ==\nIEA Graph: Evolution of Electricity Generation by Fuel from 1971 to 2003 (pdf)\nDTI UK Energy Statistics\nDTI 2006 Energy Review\nDEFRA Market Transformation Programme\nDEFRA The United Kingdom element of the European Union Emissions Trading Scheme\nNISP National Industrial Symbiosis Programme (NISP)\nUK Energy Research Centre\nMap of United Kingdom power stations\nFriends of the Earth: The Future starts here: the route to a low carbon economy\nThe Rise of the Machines: A review of energy using products in the home\nMarket Transformation Programme: Consumer Electronics\nEnergy Consumption & Production in the UK\nEnergy Analyses in UK\nMap of the UK oil and gas infrastructure\nGB electricity generation by fuel type Latest six months, 30-minute resolution. Includes France, Netherlands and NI Interconnects.\nEnergy Managers Association\nIn the media\nSeptember 2006, NewBuilder, Climate Change perceived as greater threat than terrorism\nMay 2006, BBC, Survey: Your electricity choices revealed\nMay 2006, The Times, Minister's links to nuclear lobby\nMay 2006, BBC, Blair backs nuclear power plans\nMarch 2006, The Independent, Global warming: Your chance to change the climate\nMarch 2006, BBC, Is DIY power generation going to be the next big thing?\nJanuary 2006, BBC, The UK's energy debate has been framed wrongly\nMay 2002, European Environmental Bureau, Biofuels not as green as they sound\nJune 2000, RCEP, Royal commission calls for transformation in the UK's use of energy to counter climate change", 
                "titleUrl": "https://en.wikipedia.org/wiki/Energy_in_the_United_Kingdom", 
                "title": "Energy in the United Kingdom"
            }, 
            {
                "snippet": "parliament votes for stronger climate targets The Guardian 5.2.2014  \"ITER & Beyond\". Iter.org. Retrieved 23 April 2011.\u00a0  \"International Energy Agency \u2013 Energy", 
                "pageCategories": "All Wikipedia articles in need of updating\nEconomy of the European Union\nEnergy economics\nEnergy in the European Union\nEnergy policies and initiatives of the European Union\nEnergy policy by country\nEnergy policy in Europe\nOfficial website not in Wikidata\nPolicies of the European Union\nPolitics of the European Union", 
                "pageContent": "Although the European Union has legislated in the area of energy policy for many years, the concept of introducing a mandatory and comprehensive European Union energy policy was only approved at the meeting of the informal European Council on 27 October 2005 at Hampton Court. The EU Treaty of Lisbon of 2007 legally includes solidarity in matters of energy supply and changes to the energy policy within the EU. Prior to the Treaty of Lisbon, EU energy legislation has been based on the EU authority in the area of the common market and environment. However, in practice many policy competencies in relation to energy remain at national member state level, and progress in policy at European level requires voluntary cooperation by members states.\nIn 2007, the EU was importing 82% of its oil and 57% of its gas, which then made it the world's leading importer of these fuels. Only 3% of the uranium used in European nuclear reactors was mined in Europe. Russia, Canada, Australia, Niger and Kazakhstan were the five largest suppliers of nuclear materials to the EU, supplying more than 75% of the total needs in 2009. In 2015, the EU imports 53% of the energy it consumes. In January 2014, the EU agreed to a 40% emissions reduction by 2030, compared to 1990 levels, and a 27% renewable energy target. The target is the most ambitious of any region in the world, and is expected to provide 70,000 full-time jobs and cut \u20ac33bn in fossil fuel imports.\nIn 2015, the Framework Strategy for Energy Union is launched as one of the European Commission's 10 Priorities.\n\n\n== Energy Union ==\nThe Energy Union Strategy is a project of the European Commission to coordinate the transformation of European energy supply. It was launched in February 2015, with the aim of providing secure, sustainable, competitive, affordable energy.\nDonald Tusk, President of the European Council, introduced the idea of an energy union when he was Prime Minister of Poland. Eurocommissioner Vice President Maro\u0161 \u0160ef\u010dovi\u010d called the Energy Union the biggest energy project since the European Coal and Steel Community. The EU's reliance on Russia for its energy, and the annexation of Crimea by Russia have been cited as strong reasons for the importance of this policy.\nThe European Council concluded on March 19, 2015 that the EU is committed to building an Energy Union with a forward-looking climate policy on the basis of the Commission's framework strategy, with five priority dimensions:\nEnergy security, solidarity and trust\nA fully integrated European energy market\nEnergy efficiency contributing to moderation of demand\nDecarbonising the economy\nResearch, innovation and competitiveness.\n\nThe strategy includes a minimum 10% electricity interconnection target for all member states by 2020, which the Commission hopes will put downward pressure onto energy prices, reduce the need to build new power plants, reduce the risk of black-outs or other forms of electrical grid instability, improve the reliability of renewable energy supply, and encourage market integration.\n\n\n== Earlier proposals ==\nThe possible principles of Energy Policy for Europe were elaborated at the Commission's green paper A European Strategy for Sustainable, Competitive and Secure Energy on 8 March 2006. As a result of the decision to develop a common energy policy, the first proposals, Energy for a Changing World were published by the European Commission, following a consultation process, on 10 January 2007.\nIt is claimed that they will lead to a 'post-industrial revolution', or a low-carbon economy, in the European Union, as well as increased competition in the energy markets, improved security of supply, and improved employment prospects. The Commission's proposals have been approved at a meeting of the European Council on 8 and 9 March 2007.\nKey proposals include:\nA cut of at least 20% in greenhouse gas emissions from all primary energy sources by 2020 (compared to 1990 levels), while pushing for an international agreement to succeed the Kyoto Protocol aimed at achieving a 30% cut by all developed nations by 2020.\nA cut of up to 95% in carbon emissions from primary energy sources by 2050, compared to 1990 levels.\nA minimum target of 10% for the use of biofuels by 2020.\nThat the energy supply and generation activities of energy companies should be 'unbundled' from their distribution networks to further increase market competition.\nImproving energy relations with the EU's neighbours, including Russia.\nThe development of a European Strategic Energy Technology Plan to develop technologies in areas including renewable energy, energy conservation, low-energy buildings, fourth generation nuclear reactor, clean coal and carbon capture and sequestration (CCS).\nDeveloping an Africa-Europe Energy partnership, to help Africa 'leap-frog' to low-carbon technologies and to help develop the continent as a sustainable energy supplier.\nMany underlying proposals are designed to limit global temperature changes to no more than 2 \u00b0C above pre-industrial levels, of which 0.8 \u00b0C has already taken place and another 0.5\u20130.7 \u00b0C is already committed. 2 \u00b0C is usually seen as the upper temperature limit to avoid 'dangerous global warming'. Due to only minor efforts in global Climate change mitigation it is highly likely that the world will not be able to reach this particular target. The EU might then not only be forced to accept a less ambitious global target. Because the planned emissions reductions in the European energy sector (95% by 2050) are derived directly from the 2 \u00b0C target since 2007, the EU will have to revise its energy policy paradigm.\nIn 2014, negotiations about binding EU energy and climate targets until 2030 are set to start. European Parliament voted in February 2014 in favour of binding 2030 targets on renewables, emissions and energy efficiency: a 40% cut in greenhouse gases, compared with 1990 levels; at least 30% of energy to come from renewable sources; and a 40% improvement in energy efficiency.\n\n\n== Current policies ==\n\n\n=== SET Plan ===\nThe FP7 research program only reserved a moderate amount of funding for energy research, although energy has recently emerged as one of the key issues of the European Union. A large part of FP7 energy funding is also devoted to fusion research, a technology that will not be able to help meet European climate and energy objectives until beyond 2050. The European Commission tried to redress this shortfall with the SET plan.\nThe Steering Group on the implementation of the Strategic Energy Technologies Plan (SET Plan) on 26 June 2008 will set the agenda for an EU energy technology policy. It will enhance the coordination of national and European research and innovation efforts to position the EU in the forefront of the low-carbon technologies markets.\nThe SET plan initiatives:\nEuropean Wind Initiative\nfocus on large turbines and large systems' validation and demonstration (relevant to on and off-shore applications).\n\nSolar Europe Initiative\nfocus on large-scale demonstration for photovoltaics and concentrated solar power\nBioenergy Europe Initiative\nfocus on 'next generation' biofuels within the context of an overall bio-energy use strategy.\n\nEuropean CO2 capture, transport and storage initiative\nfocus on the whole system requirements, including efficiency, safety and public acceptance, to prove the viability of zero emission fossil fuel power plants at industrial scale.\nEuropean electricity grid initiative\nfocus on the development of the smart electricity system, including storage, and on the creation of a European centre to implement a research programme for the European transmission network.\n\nSustainable nuclear fission initiative\nfocus on the development of Generation IV reactors technologies. This includes the ESNII which comprises the Allegro reactor, MYRRHA and ASTRID.\nThe budget for the SET plan is estimated at \u20ac71 billion.\nThe IEA raised its concern that demand-side technologies do not feature at all in the six priority areas of the SET Plan.\nThe EU resolution is available in EUR-Lex\n\n\n==== EERA ====\nThe European Energy Research Alliance (EERA) is founded by the leading research institutes in the European Union (EU), to expand and optimise EU energy research capabilities through the sharing of world-class national facilities and the joint realisation of national and European programmes. This new Research Alliance will be a key actor of the EU Strategic Energy Technology Plan (SET Plan) and will contribute to accelerate the development of new low carbon technologies for EU to move toward a low carbon economy.\n\n\n=== Energy sources ===\n\nThe climate and energy policy is a choice between democracy and autocracy. The choice is one between patronage-based oil-and-gas oligarchies on the one hand, and adaptive and innovative low-carbon economy on the other.\nUnder the requirements of the Directive on Electricity Production from Renewable Energy Sources, which entered into force in October 2001, the member states are expected to meet \"indicative\" targets for renewable energy production. Although there is significant variation in national targets, the average is that 22% of electricity should be generated by renewables by 2010 (compared to 13,9% in 1997). The European Commission has proposed in its Renewable Energy Roadmap21 a binding target of increasing the level of renewable energy in the EU's overall mix from less than 7% today to 20% by 2020.\nEurope spent on importing fossil fuels \u20ac406 billion in 2011 (bn) and \u20ac545 bn in 2012. This is around three times more than the cost of the Greek bailout up to 2013. In 2012, wind energy avoided \u20ac9.6 billion (bn) of fossil fuel costs. EWEA recommends binding renewable energy target to support in replacing fossil fuels with wind energy in Europe by providing a stable regulatory framework. In addition, it recommends set a minimum emission performance standard for all new-build power installations.\n\n\n=== Energy markets ===\nThe EU promotes electricity market liberalisation and security of supply through Directive 2009/72/EC.\nThe 2004 Gas Security Directive has been intended to improve security of supply in the natural gas sector.\n\n\n=== IPEEC ===\n\nAt the Heiligendamm Summit in June 2007, the G8 acknowledged an EU proposal for an international initiative on energy efficiency tabled in March 2007, and agreed to explore, together with the International Energy Agency, the most effective means to promote energy efficiency internationally. A year later, on 8 June 2008, the G8 countries, China, India, South Korea and the European Community decided to establish the International Partnership for Energy Efficiency Cooperation, at the Energy Ministerial meeting hosted by Japan in the frame of the 2008 G8 Presidency, in Aomori.\n\n\n=== Buildings ===\nBuildings account for around 40% of EU energy requirements and have been the focus of several initiatives. From 4 January 2006, the 2002 Directive on the energy performance of buildings requires member states to ensure that new buildings, as well as large existing building undergoing refurbishment, meet certain minimum energy requirements. It also requires that all buildings should undergo 'energy certification' prior to sale, and that boilers and air conditioning equipment should be regularly inspected.\nAs part of the EU's SAVE Programme, aimed at promoting energy efficiency and encouraging energy-saving behaviour, the Boiler Efficiency Directive specifies minimum levels of efficiency for boilers fired with liquid or gaseous fuels. Originally, from June 2007, all homes (and other buildings) in the UK would have to undergo Energy Performance Certification before they are sold or let, to meet the requirements of the European Energy Performance of Buildings Directive (Directive 2002/91/EC).\n\n\n=== Transport ===\nCarbon dioxide emissions from transport have risen rapidly in recent years, from 21% of the total in 1990 to 28% in 2004.\nEU policies include the voluntary ACEA agreement, signed in 1998, to cut carbon dioxide emissions for new cars sold in Europe to an average of 140 grams of CO2/km by 2008, a 25% cut from the 1995 level. Because the target was unlikely to be met, the European Commission published new proposals in February 2007, requiring a mandatory limit of 130 grams of CO2/km for new cars by 2012, with 'complementary measures' being proposed to achieve the target of 120 grams of CO2/km that had originally been expected.\nIn the area of fuels, the 2001 Biofuels Directive requires that 5,75% of all transport fossil fuels (petrol and diesel) should be replaced by biofuels by 31 December 2010, with an intermediate target of 2% by the end of 2005. In February 2007 the European Commission proposed that, from 2011, suppliers will have to reduce carbon emissions per unit of energy by 1% a year from 2010 levels, to result in a cut of 10% by 2020 Stricter fuel standards to combat climate change and reduce air pollution.\n\n\n==== Flights ====\nAirlines can be charged for their greenhouse gas emissions on flights to and from Europe according to a court ruling in October 2011.\n\n\n=== Industry ===\nThe European Union Emission Trading Scheme, introduced in 2005 under the 2003 Emission Trading Directive, sets national caps on greenhouse gas emissions for power plants and other large point sources.\n\n\n=== Consumer goods ===\nA further area of energy policy has been in the area of consumer goods, where energy labels were introduced to encourage consumers to purchase more energy-efficient appliances.\n\n\n=== External energy relations ===\n\nBeyond the bounds of the European Union, EU energy policy has included negotiating and developing wider international agreements, such as the Energy Charter Treaty, the Kyoto Protocol, the post-Kyoto regime and a framework agreement on energy efficiency; extension of the EC energy regulatory framework or principles to neighbours (Energy Community, Baku Initiative, Euro-Med energy cooperation) and the emission trading scheme to global partners; the promotion of research and the use of renewable energy.\nThe EU-Russia energy cooperation will be based on a new comprehensive framework agreement within the post-Partnership and Cooperation Agreement (PCA), which will be negotiated in 2007. The energy cooperation with other third energy producer and transit countries is facilitated with different tools, such as the PCAs, the existing and foreseen Memorandums of Understanding on Energy Cooperation (with Ukraine, Azerbaijan, Kazakhstan and Algeria), the Association Agreements with Mediterranean countries, the European Neighbourhood Policy Action Plans; Euromed energy cooperation; the Baku initiative; and the EU-Norway energy dialogue. For the cooperation with African countries, a comprehensive Africa-Europe Energy partnership would be launched at the highest level, with the integration of Europe's Energy and Development Policies.\nFor ensuring efficient follow-up and coherence in pursuing the initiatives and processes, for sharing information in case of an external energy crisis, and for assisting the EU's early response and reactions in case of energy security threats, the network of energy correspondents in the Member States was established in early 2007. After the Russian-Ukrainian Gas Crisis of 2009 the EU decided that the existing external measures regarding gas supply security should be supplemented by internal provisions for emergency prevention and response, such as enhancing gas storage and network capacity or the development of the technical prerequisites for reverse flow in transit pipelines.\n\n\n== Solar anti-dumping levies ==\nThe European Commission imposed anti-dumping levies on Chinese solar panel imports in June 2013. As reported earlier China was selling solar panels under production costs that made the operation without return also for the Chinese solar power companies. Chinese government guaranteed the operation of Chinese solar power companies.\n\n\n== Research and development ==\nThe European Union is also active in the areas of energy research, development and promotion, via initiatives such as CEPHEUS (ultra-low energy housing), and programs under the umbrella titles of SAVE (energy saving) ALTENER (new and renewable energy sources), STEER (transport) and COOPENER (developing countries). Through Fusion for Energy, the EU is participating in the ITER project.\n\n\n== Public opinion ==\nIn a poll carried out for the European Commission in October and November 2005, 47% of the citizens questioned in the 27 countries of the EU (including the 2 states that joined in 2007) were in favour of taking decisions on key energy policy issues at a European level. 37% favoured national decisions and 8% that they be tackled locally.\nA similar survey of 29,220 people in March and May 2006 indicated that the balance had changed in favour of national decisions in these areas (42% in favour), with 39% backing EU policy making and 12% preferring local decisions. There was significant national variation with this, with 55% in favour in the Netherlands, but only 15% in Finland.\nA comprehensive public opinion survey was performed in May and June 2006. The authors propose following conclusions:\nEnergy issues are considered to be important but not at first glance.\nEU citizens perceive great future promise in the use of renewable energies. Despite majority opposition, nuclear energy also has its place in the future energy mix.\nCitizens appear to opt for changing the energy structure, enhancing research and development and guaranteeing the stability of the energy field rather than saving energy as the way to meet energy challenges.\nThe possible future consequences of energy issues do not generate deep fears in Europeans\u2019 minds.\nEuropeans appear to be fairly familiar with energy issues, although their knowledge seems somewhat vague.\nEnergy issues touch everybody and it is therefore hard to distinguish clear groups with differing perceptions. Nevertheless, rough distinction between groups of citizens is sketched.\n\n\n== See also ==\n\nDirectorate-General for Energy (European Commission)\nEuropean Commissioner for Energy\nEnergy Community\nEnergy Charter Treaty\nINOGATE\nEuropean Atomic Forum\nEuropean Transport, Telecommunications and Energy Council\nCommission Green paper\nCHP Directive\nEco-Design of Energy-Using Products Directive\nEnergy conservation\nEuropean Climate Change Programme\nEuropean Pollutant Emission Register (EPER)\nGlobal strategic petroleum reserves\nElectricity in the European Union\nEnergy efficiency in europe\nRenewable energy in the European Union\nTransport in Europe\nEuropean countries by fossil fuel use (% of total energy)\nEuropean countries by electricity consumption per person\n\n\n=== Member states ===\nEnergy policy of the United Kingdom\n\n\n== References ==\n\n\n== External links ==\nOfficial website\nEuropean information campaign on the opening of the energy markets and on energy consumers' right.\nEuropean Strategic Energy Technology Plan, Towards A Low Carbon Future.\nEurostat \u2013 Statistics Explained \u2013 all articles on energy\nManagEnergy, for energy efficiency and renewable energies at the local and regional level.\nBBC Q&A: EU energy proposals\n2006 Energy Green Paper\nCollective Energy Security: A New Approach for Europe\nBerlin Forum on Fossil Fuels.\nThe post-industrial revolution.\nNetherlands Environmental Assessment Agency \u2013 Meeting the European Union 2 \u00b0C climate target: global and regional emission implications\nGerman Institute for International and Security Affairs \u2013 Perspectives for the European Union's External Energy Policy\nEnergy Law Research Forum\nThe Liberalisation of the Power Industry in the European Union and its Impact on Climate Change \u2013 A Legal Analysis of the Internal Market in Electricity.\nTorsten H. Fransson Lecture: Implementing Clean Energy Goals in the EU \u2013 European Union Center at the University of Illinois, Urbana-Champaign.\n\n\n=== In the media ===\n8 Sep 2008 New Europe (neurope.eu) : Energy security and Europe.\n10 Jan 2007, BBC: EU plans 'industrial revolution'\n10 Jan 2007, BBC: EU's energy plans \u2013 how revolutionary?\n10 Jan 2007, Reuters: EU puts climate change at heart of energy policy\n10 Jan 2007, Xinhua: EU to build competitive internal energy market\n10 Jan 2007, Deutsche Welle: Germany Wary About EU Energy Policy\n10 Jan 2007, UK Government: UK Ministers strongly welcome EU energy proposals\n9 Jan 2007, Greenpeace: Fine wrapping conceals lack of substance\n14 Dec 2006, opendemocracy.net: Russia, Germany and European energy policy\n20 Nov 2006, eupolitix.com: Barroso calls for strong EU energy policy", 
                "titleUrl": "https://en.wikipedia.org/wiki/Energy_policy_of_the_European_Union", 
                "title": "Energy policy of the European Union"
            }, 
            {
                "snippet": "ITER office is at ORNL with partners at Princeton Plasma Physics Laboratory and Savannah River National Laboratory. The US contribution to the ITER project", 
                "pageCategories": "1942 establishments in Tennessee\nAll articles with unsourced statements\nArticles with unsourced statements from October 2015\nBattelle Memorial Institute\nBuildings and structures in Anderson County, Tennessee\nBuildings and structures in Roane County, Tennessee\nCS1 errors: dates\nCoordinates on Wikidata\nHSA Foundation members\nLaboratories in the United States", 
                "pageContent": "Oak Ridge National Laboratory (ORNL) is an American multiprogram science and technology national laboratory managed for the United States Department of Energy (DOE) by UT-Battelle. ORNL is the largest science and energy national laboratory in the Department of Energy system by surface and by annual budget. ORNL is located in Oak Ridge, Tennessee, near Knoxville. ORNL's scientific programs focus on materials, neutron science, energy, high-performance computing, systems biology and national security.\nORNL partners with the state of Tennessee, universities and industries to solve challenges in energy, advanced materials, manufacturing, security and physics.\nThe laboratory is home to several of the world's top supercomputers including the world's third most powerful supercomputer ranked by the TOP500, Titan, and is a leading neutron science and nuclear energy research facility that includes the Spallation Neutron Source and High Flux Isotope Reactor. ORNL hosts the Center for Nanophase Materials Sciences, the BioEnergy Science Center, and the Consortium for Advanced Simulation of Light-Water Reactors.\n\n\n== Overview ==\n\nOak Ridge National Laboratory is managed by UT-Battelle, a limited liability partnership between the University of Tennessee and the Battelle Memorial Institute, formed in 2000 for that purpose. The annual budget is US$1.65 billion, 80% of which is from the Department of Energy; the remainder is from various sources paying for use of the facilities. As of 2012 there are 4,400 staff working at ORNL, 1600 of which are directly conducting research, and an additional 3000 guest researchers annually.\nThere are five campuses on the Department of Energy's Oak Ridge reservation; the National Laboratory, the Y-12 National Security Complex, the East Tennessee Technology Park (formerly the Oak Ridge Gaseous Diffusion Plant), the Oak Ridge Institute for Science and Education, and the developing Oak Ridge Science and Technology Park, although the four other facilities are unrelated to the National Laboratory. The total area of the reservation 150 square kilometres (58 sq mi) of which the lab takes up 18 square kilometres (7 sq mi).\n\n\n== History ==\n\nThe town of Oak Ridge was established by the Army Corps of Engineers as part of the Clinton Engineer Works in 1942 on isolated farm land as part of the Manhattan Project. In 1943, construction of the \"Clinton Laboratories\" was completed, later renamed to \"Oak Ridge National Laboratory\". The site was chosen for the X-10 Graphite Reactor, used to show that plutonium can be extracted from enriched uranium. Enrico Fermi and his colleagues developed the world's second self-sustaining nuclear reactor after Fermi's previous experiment Chicago Pile-1, the X-10 was the first designed for continuous operation. After the end of World War II the demand for weapons-grade plutonium fell and the reactor and the laboratory's 1000 employees were no longer involved in nuclear weapons, instead it was used for scientific research. In 1946 the first medical isotopes were produced in the X-10 reactor, by 1950 almost 20,000 samples had been shipped to various hospitals. As the demand for military science had fallen dramatically the future of the lab was uncertain. Management of the lab was contracted by the US government to Monsanto, however, they withdrew in 1947. The University of Chicago assumed responsibility but withdrew shortly after, until in December 1947 Union Carbide and Carbon Co., which already operated two other facilities at Oak Ridge, took control of the laboratory and Alvin Weinberg was named Director of Research ORNL, and in 1955 Director of the Laboratory.\n\nIn 1950 the Oak Ridge School of Reactor Technology was established with two courses in reactor operation and safety; almost 1000 students graduated. Much of the research performed at ORNL in the 1950s was relating to nuclear reactors as a form of energy production both for propulsion and electricity; more reactors were built in the 1950s than the rest of the ORNL's history combined.\nAnother Experiment was the world's first light water reactor. With its principles of neutron moderation and fuel cooling by ordinary water it is the direct ancestor of most modern nuclear power stations. The US Military funded much of its development, for nuclear-powered submarines and ships of the US Navy.\nThe US Army contracted portable nuclear reactors in 1953 for heat and electricity generation in remote military bases. The reactors were designed at ORNL, produced by American Locomotive Company and used in Greenland, the Panama Canal Zone and Antarctica. The United States Air Force (USAF) also contributed funding to three reactors; the lab's first computers; and its first particle accelerators. ORNL designed and tested a nuclear-powered aircraft in 1954 as a proof-of-concept for a proposed USAF fleet of long-range bombers, although it never flew.\nThe provision of radionuclides by X-10 for medicine grew steadily in the 1950s with more isotopes available; ORNL was the only Western source of californium-252. ORNL scientists lowered the immune systems of mice and performed the world's first successful bone marrow transplant.\nIn the early 1960s there was a large push at ORNL to develop nuclear-powered desalination plants where deserts met the sea to provide water. The project called Water for Peace was backed by John F. Kennedy and Lyndon B. Johnson and presented at a 1964 United Nations conference but increases in the cost of construction and public confidence in nuclear power falling caused the plan to fail. The Health Physics Research Reactor built in 1962 was used for radiation exposure experiments leading to more accurate dosage limits, dosimeters and improved radiation shielding.\nIn 1964 the Molten-Salt Reactor Experiment began with the construction of the reactor. It was operated from 1966 until 1969 (with six months down time to move from U-235 to U-233 fuel) and proved the viability of molten salt reactors while also producing fuel for other reactors as a byproduct of its own reaction.\nThe High Flux Isotope Reactor built in 1965 with the highest neutron flux of any reactor at the time. It improved upon the work of the X-10 reactor producing more medical isotopes as well as allowing higher fidelity of materials research.\nResearchers in the Biology Division studied the effects of chemicals on mice including petrol fumes, pesticides and tobacco.\nIn the late 1960s cuts in funding led to plans for another particle accelerator to be cancelled and the United States Atomic Energy Commission cut the breeder reactor program by two-thirds leading to a downsizing in staff from 5000 to 3800.\n\nIn the 1970s the prospect of fusion power was strongly considered sparking research at ORNL. A tokamak called ORMAK, made operational in 1971, was the first tokamak to achieve a plasma temperature of 20 million Kelvin. After the success of the fusion experiments it was enlarged and renamed ORMAK II in 1973; however, the experiments ultimately failed to lead to fusion power plants.\nThe US Atomic Energy Commission required improved safety standards in the early 1970s for nuclear reactors so ORNL staff wrote almost 100 requirements covering many factors including fuel transport and earthquake resistance. In 1972 the AEC held a series of public hearings where emergency cooling requirements were highlighted and the safety requirements became more stringent.\nORNL was involved in analysing the damage to the core of the Three Mile Island Nuclear Generating Station after the accident in 1979.\nAlso in 1972 Peter Mazur, a biologist at ORNL, froze with liquid nitrogen, thawed and implanted mouse embryos in a surrogate mother. The mouse pups were born healthy. The technique is popular in the livestock industry as it allows the embryos of valuable cattle to be transported easily and a prize cow can have multiple eggs extracted and thus through in vitro fertilisation have many more offspring than would naturally be possible.\nIn 1974 Alvin Weinberg, director of the lab for 19 years, was replaced by Herman Postma, a fusion scientist.\nIn 1977 construction began for 6 metre (20 foot) superconducting electromagnets intended to control fusion reactions. The project was an international effort, three electromagnets were produced in the US, one in Japan, one in Switzerland and the final by remaining European states and experimentation continued into the 1980s.\nThe 1980s brought more changes to ORNL, a focus on efficiency became paramount.\nAn accelerated climate simulation chamber was built that applied varying weather conditions to insulation to test its efficacy and durability faster than real time. Materials research into heat resistant ceramics for use in truck and high-tech car engines was performed, building upon the materials research that began in the nuclear reactors of the 1950s. In 1987 the High Temperature Materials Laboratory was established where ORNL and industry researchers cooperated on ceramic and alloy projects. The materials research budget at ORNL doubled after initial uncertainty regarding Reagan's economic policy of less government expenditure.\nIn 1981 the Holifield Heavy Ion Research Facility, a 25 MV particle accelerator, was opened at ORNL. At the time, Holifield had the widest range of ion species and was twice as powerful as other accelerators attracting hundreds of guest researchers each year.\nThe Department of Energy was concerned with the pollution surrounding ORNL and it began clean-up efforts. Burial trenches and leaking pipes had contaminated the groundwater beneath the lab and radiation tanks were sitting idle, full of waste. Estimates of the total cost of clean-up were into the hundreds of millions of US dollars.\nThe five older reactors were subjected to safety reviews in 1987, ordered to be deactivated until the reviews were complete. By 1989 when the High Flux Isotope Reactor was restarted the US supply of certain medical isotopes was depleted.\nIn 1989 the former executive officer of the American Association for the Advancement of Science, Alvin Trivelpiece, became director of ORNL, he remained in the role until 2000.\nIn 1992, a whistleblower, Charles Varnadore, filed complaints against ORNL, alleging safety violations and retaliation by his superiors. While an administrative law judge ruled in Varnadore's favor, the Secretary of Labor, Robert Reich, overturned that ruling. However, Varnadore's case saw prime contractor Martin Marietta cited for safety violations, and ultimately led to additional whistleblower protection within DOE.\n\n\n== Areas of research ==\nORNL conducts research and development activities that span a wide range of scientific disciplines. Many research areas have a significant overlap with each other; researchers often work in two or more of the fields listed here. The laboratory's major research areas are described briefly below.\nChemical sciences \u2013 ORNL conducts both fundamental and applied research in a number of areas, including catalysis, surface science and interfacial chemistry; molecular transformations and fuel chemistry; heavy element chemistry and radioactive materials characterization; aqueous solution chemistry and geochemistry; mass spectrometry and laser spectroscopy; separations chemistry; materials chemistry including synthesis and characterization of polymers and other soft materials; chemical biosciences; and neutron science.\nElectron microscopy \u2013 ORNL's electron microscopy program investigates key issues in condensed matter, materials, chemical and nanosciences.\nNuclear medicine \u2013 The laboratory's nuclear medicine research is focused on the development of improved reactor production and processing methods to provide medical radioisotopes, the development of new radionuclide generator systems, the design and evaluation of new radiopharmaceuticals for applications in nuclear medicine and oncology.\nPhysics \u2013 Physics research at ORNL is focused primarily on studies of the fundamental properties of matter at the atomic, nuclear, and subnuclear levels and the development of experimental devices in support of these studies.\nPopulation \u2013 ORNL provides federal, state and international organizations with a gridded population database, called Landscan, for estimating ambient population. LandScan is a raster image, or grid, of population counts, which provides human population estimates every 30 x 30 arc seconds, which translates roughly to population estimates for 1 kilometer square windows or grid cells at the equator, with cell width decreasing at higher latitudes. Though many population datasets exist, LandScan is the best spatial population dataset, which also covers the globe. Updated annually (although data releases are generally one year behind the current year) offers continuous, updated values of population, based on the most recent information. Landscan data are accessible through GIS applications and a USAID public domain application called Population Explorer.\n\n\n=== Energy ===\nThe laboratory has a long history of energy research; nuclear reactor experiments have been conducted since the end of World War II in 1945. Because of the availability of reactors and high-performance computing resources an emphasis on improving the efficiency of nuclear reactors is present. The programs develop more efficient materials, more accurate simulations of aging reactor cores, sensors and controls as well as safety procedures for regulatory authorities.\nThe Energy Efficiency and Electricity Technologies Program (EEETP) aims to improve air quality in the US and reduce dependence on foreign oil supplies. There are three key areas of research; electricity, manufacturing and mobility. The electricity division focuses on reducing electricity consumption and finding alternative sources for production. Buildings, which account for 39% of US electricity consumption as of 2012, are a key area of research as the program aims to create affordable, carbon-neutral homes by 2020. Research also takes place into higher efficiency solar panels, geothermal electricity and heating, lower cost wind generators and the economic and environmental feasibility of potential hydro power plants.\nFusion is another area with a history of research at ORNL, dating back to the 1970s. The Fusion Energy Division pursues short-term goals to develop components such as high temperature superconductors, high-speed hydrogen pellet injectors and suitable materials for future fusion research. Much research into the behaviour and maintenance of a plasma takes place at the Fusion Energy Division to further the understanding of plasma physics, a crucial area for developing a fusion power plant. The US ITER office is at ORNL with partners at Princeton Plasma Physics Laboratory and Savannah River National Laboratory. The US contribution to the ITER project is 9.09% which is expected to be in excess of US$1.6 billion throughout the contract.\n\n\n=== Biology ===\n\nOak Ridge National Laboratory's biological research covers genomics, computational biology, structural biology and bioinformatics. The BioEnergy Program aims to improve the efficiency of all stages of the biofuel process to improve the energy security of the United States. The program aims to make genetic improvements to the potential biomass used, formulate methods for refineries that can accept a diverse range of fuels and to improve the efficiency of energy delivery both to power plants and end users.\nThe Center for Molecular Biophysics conducts research into the behaviour of biological molecules in various conditions. The center hosts projects that examine cell walls for biofuel production, use neutron scattering to analyse protein folding and simulate the effect of catalysis on a conventional and quantum scale.\n\n\n=== Neutron science ===\nThere are three neutron sources at ORNL; the High Flux Isotope Reactor (HFIR), the Oak Ridge Electron Linear Accelerator (ORELA) and the Spallation Neutron Source. HFIR provides neutrons in a stable beam resulting from a constant nuclear reaction whereas ORELA and SNS produce pulses of neutrons as they are particle accelerators. HFIR went critical in 1965 and has been used for materials research and as a major sources of medical radioisotopes since. As of 2013, HFIR provides the world's highest constant neutron flux as a result of various upgrades. As part of a US non-proliferation effort the HFIR is scheduled to switch from highly enriched uranium (>90%, weapons grade) to low-enriched (3\u20134%) in 2020; the last reactor in the US to do so. Berkelium used to produce the world's first sample of ununseptium was produced in the High Flux Isotope Reactor as part of an international effort. HFIR is likely to operate until approximately 2060 before the reactor vessel is considered unsafe for continued use.\nThe Spallation Neutron Source (SNS) is a particle accelerator that has the highest intensity neutron pulses of any man-made neutron source. SNS was made operational in 2006 and has since been upgraded to 1 megawatts with plans to continue up to 3 megawatts. High power neutron pulses permit clearer images of the targets meaning smaller samples can be analysed and accurate results require fewer pulses.\n\n\n=== Materials ===\n\nOak Ridge National Laboratory conducts research into materials science in a range of areas. Between 2002 and 2008 ORNL partnered with Caterpillar Inc. (CAT) to form a new material for their diesel engines that can withstand large temperature fluctuations. The new steel, named CF8C Plus, is based on conventional CF8C stainless steel with added manganese and nitrogen; the result has better high\u2013temperature properties and is easier to cast at a similar cost. In 2003 the partners received an R&D 100 award from R&D magazine and in 2009 received an award for \"excellence in technology transfer\" from the Federal Laboratory Consortium for the commercialisation of the steel.\nThere is a high-temperature materials lab at ORNL that permits researchers from universities, private companies and other government initiatives to use their facilities. The lab is available for free if the results are published; private research is permitted but requires payment. A separate lab, the Shared Equipment User Facility, is one of three DOE sponsored facilities with nano-scale microscopy and tomography facilities.\nThe Center for Nanophase Materials Sciences (CNMS) researches the behaviour and fabrication of nanomaterials. The center emphasises discovery of new materials and the understanding of underlying physical and chemical interactions that enable creation of nanomaterials. In 2012, CNMS produced a lithium-sulfide battery with a theoretical energy density three to five times greater than existing lithium ion batteries.\n\n\n=== Security ===\nOak Ridge National Laboratory provides resources to the US Department of Homeland Security and other defense programs. The Global Security and Nonproliferation (GS&N) program develops and implements policies, both US based and international, to prevent the proliferation of nuclear material. The program has developed safeguards for nuclear arsenals, guidelines for dismantling arsenals, plans of action should nuclear material fall into unauthorised hands, detection methods for stolen or missing nuclear material and trade of nuclear material between the US and Russia. The GS&N's work overlaps with that of the Homeland Security Programs Office, providing detection of nuclear material and nonproliferation guidelines. Other areas concerning the Department Homeland Security include nuclear and radiological forensics, chemical and biological agent detection using mass spectrometry and simulations of potential national hazards.\n\n\n=== High-performance computing ===\nThroughout the history of the Oak Ridge National Laboratory it has been the site of various supercomputers, home to the fastest on several occasions. In 1953, ORNL partnered with the Argonne National Laboratory to build ORACLE (Oak Ridge Automatic Computer and Logical Engine), a computer to research nuclear physics, chemistry, biology and engineering. ORACLE had 2048 words (80 Kibit) of memory and took approximately 590 microseconds to perform addition or multiplications of integers. In the 1960s ORNL was also equipped with an IBM 360/91 and an IBM 360/65. In 1995 ORNL bought an Intel Paragon based computer called the Intel Paragon XP/S 150 that performed at 154 gigaFLOPS and ranked third on the TOP500 list of supercomputers. In 2005 Jaguar was built, a Cray XT3 based system that performed at 25 teraFLOPS and received incremental upgrades up to the XT5 platform that performed at 2.3 petaFLOPS in 2009. It was recognised as the world's fastest from November 2009 until November 2010.\nSince 1992 the National Center for Computational Sciences (NCCS) has overseen high performance computing at ORNL. It manages the Oak Ridge Leadership Computing Facility that contains the machines. In 2012, Jaguar was upgraded to the XK7 platform, a fundamental change as GPUs are used for the majority of processing, and renamed Titan. Titan performs at 17.59 petaFLOPS and holds the number 1 spot on the TOP500 list for November 2012. Other computers include a 77 node cluster to visualise data that the larger machines output in the Exploratory Visualization Environment for Research in Science and Technology (EVEREST), a visualisation room with a 10 by 3 metre (30 by 10 ft) wall that displays 35 megapixel projections. Smoky is an 80 node linux cluster used for application development. Research projects are refined and tested on Smoky before running on larger machines such as Titan.\nIn 1989 programmers at the Oak Ridge National Lab wrote the first version of Parallel Virtual Machine (PVM), software that enables distributed computing on machines of differing specifications. PVM is free software and has become the de facto standard for distributed computing. Jack Dongarra of ORNL and the University of Tennessee wrote the LINPACK software library and LINPACK benchmarks, used to calculate linear algebra and the standard method of measuring floating point performance of a supercomputer as used by the TOP500 organisation.\n\n\n== See also ==\nAmerican Museum of Science and Energy\nCenter for Nanophase Materials Sciences\nK-25 Gaseous Diffusion Plant\nKarl Z. Morgan\nList of Advanced Scientific Computing Research Leadership Computing Challenge allocations\nNational Transportation Research Center\nSandia National Laboratory\nSpallation Neutron Source\nThorium Energy Alliance\nY-12 National Security Complex\n\n\n== Footnotes ==\n\n\n== Further reading ==\nLindsey A. Freeman, Longing for the Bomb: Oak Ridge and Atomic Nostalgia. Chapel Hill, NC: University of North Carolina Press, 2015.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Oak_Ridge_National_Laboratory", 
                "title": "Oak Ridge National Laboratory"
            }, 
            {
                "snippet": "helium-4 as this reaction gives out the most net energy. Electric confinement (ITER), inertial confinement(heating by laser) and heating by strong electric currents", 
                "pageCategories": "All accuracy disputes\nAll articles needing cleanup\nAll articles with unsourced statements\nArticles needing cleanup from May 2010\nArticles with disputed statements from October 2016\nArticles with unsourced statements from September 2007\nCleanup tagged articles without a reason field from May 2010\nCommons category with local link same as on Wikidata\nEnergy development\nFuels", 
                "pageContent": "A fuel is any material that can be made to react with other substances so that it releases chemical or nuclear energy as heat or to be used for work. The concept was originally applied solely to those materials capable of releasing chemical energy but has since also been applied to other sources of heat energy such as nuclear energy (via nuclear fission or nuclear fusion).\nThe heat energy released by reactions of fuels is converted into mechanical energy via a heat engine. Other times the heat itself is valued for warmth, cooking, or industrial processes, as well as the illumination that comes with combustion. Fuels are also used in the cells of organisms in a process known as cellular respiration, where organic molecules are oxidized to release usable energy. Hydrocarbons and related oxygen-containing molecules are by far the most common source of fuel used by humans, but other substances, including radioactive metals, are also utilized.\nFuels are contrasted with other substances or devices storing potential energy, such as those that directly release electrical energy (such as batteries and capacitors) or mechanical energy (such as flywheels, springs, compressed air, or water in a reservoir).\n\n\n== History ==\nThe first known use of fuel was the combustion of wood or sticks by Homo erectus near 2,000,000 (two million) years ago. Throughout most of human history fuels derived from plants or animal fat were only used by humans . Charcoal, a wood derivative, has been used since at least 6,000 BCE for melting metals. It was only supplanted by coke, derived from coal, as European forests started to become depleted around the 18th century. Charcoal briquettes are now commonly used as a fuel for barbecue cooking.\nCoal was first used as a fuel around 1000 BCE in China. With the energy in the form of chemical energy that could be released through combustion, but the concept development of the steam engine in the United Kingdom in 1769, coal came into more common use as a power source. Coal was later used to drive ships and locomotives. By the 19th century, gas extracted from coal was being used for street lighting in London. In the 20th and 21st centuries, the primary use of coal is to generate electricity, providing 40% of the world's electrical power supply in 2005.\nFossil fuels were rapidly adopted during the industrial revolution, because they were more concentrated and flexible than traditional energy sources, such as water power. They have become a pivotal part of our contemporary society, with most countries in the world burning fossil fuels in order to produce power.\nCurrently the trend has been towards renewable fuels, such as biofuels like alcohols.\n\n\n== Chemical ==\nChemical fuels are substances that release energy by reacting with substances around them, most notably by the process of combustion. Most of the chemical energy released in combustion was not stored in the chemical bonds of the fuel, but in the weak double bond of molecular oxygen.\nChemical fuels are divided in two ways. First, by their physical properties, as a solid, liquid or gas. Secondly, on the basis of their occurrence: primary (natural fuel) and secondary (artificial fuel). Thus, a general classification of chemical fuels is:\n\n\n=== Solid fuel ===\n\nSolid fuel refers to various types of solid material that are used as fuel to produce energy and provide heating, usually released through combustion. Solid fuels include wood (see wood fuel), charcoal, peat, coal, Hexamine fuel tablets, and pellets made from wood (see wood pellets), corn, wheat, rye and other grains. Solid-fuel rocket technology also uses solid fuel (see solid propellants). Solid fuels have been used by humanity for many years to create fire. Coal was the fuel source which enabled the industrial revolution, from firing furnaces, to running steam engines. Wood was also extensively used to run steam locomotives. Both peat and coal are still used in electricity generation today. The use of some solid fuels (e.g. coal) is restricted or prohibited in some urban areas, due to unsafe levels of toxic emissions. The use of other solid fuels such as wood is decreasing as heating technology and the availability of good quality fuel improves. In some areas, smokeless coal is often the only solid fuel used. In Ireland, peat briquettes are used as smokeless fuel. They are also used to start a coal fire.\n\n\n=== Liquid fuels ===\n\nLiquid fuels are combustible or energy-generating molecules that can be harnessed to create mechanical energy, usually producing kinetic energy; they also must take the shape of their container. It is the fumes of liquid fuels that are flammable instead of the fluid.\nMost liquid fuels in widespread use are derived from the fossilized remains of dead plants and animals by exposure to heat and pressure in the Earth's crust. However, there are several types, such as hydrogen fuel (for automotive uses), ethanol, jet fuel and biodiesel which are all categorized as a liquid fuel. Emulsified fuels of oil-in-water such as orimulsion have been developed a way to make heavy oil fractions usable as liquid fuels. Many liquid fuels play a primary role in transportation and the economy.\nSome common properties of liquid fuels are that they are easy to transport, and can be handled with relative ease. Also they are relatively easy to use for all engineering applications, and home use. Fuels like kerosene are rationed in some countries, for example available in government subsidized shops in India for home use.\nConventional diesel is similar to gasoline in that it is a mixture of aliphatic hydrocarbons extracted from petroleum. Kerosene is used in kerosene lamps and as a fuel for cooking, heating, and small engines. Natural gas, composed chiefly of methane, can only exist as a liquid at very low temperatures (regardless of pressure), which limits its direct use as a liquid fuel in most applications. LP gas is a mixture of propane and butane, both of which are easily compressible gases under standard atmospheric conditions. It offers many of the advantages of compressed natural gas (CNG), but is denser than air, does not burn as cleanly, and is much more easily compressed. Commonly used for cooking and space heating, LP gas and compressed propane are seeing increased use in motorized vehicles; propane is the third most commonly used motor fuel globally.\n\n\n=== Gaseous fuels ===\n\nFuel gas is any one of a number of fuels that under ordinary conditions are gaseous. Many fuel gases are composed of hydrocarbons (such as methane or propane), hydrogen, carbon monoxide, or mixtures thereof. Such gases are sources of potential heat energy or light energy that can be readily transmitted and distributed through pipes from the point of origin directly to the place of consumption. Fuel gas is contrasted with liquid fuels and from solid fuels, though some fuel gases are liquefied for storage or transport. While their gaseous nature can be advantageous, avoiding the difficulty of transporting solid fuel and the dangers of spillage inherent in liquid fuels, it can also be dangerous. It is possible for a fuel gas to be undetected and collect in certain areas, leading to the risk of a gas explosion. For this reason, odorizers are added to most fuel gases so that they may be detected by a distinct smell. The most common type of fuel gas in current use is natural gas.\n\n\n=== Biofuels ===\n\nBiofuel can be broadly defined as solid, liquid, or gas fuel consisting of, or derived from biomass. Biomass can also be used directly for heating or power\u2014known as biomass fuel. Biofuel can be produced from any carbon source that can be replenished rapidly e.g. plants. Many different plants and plant-derived materials are used for biofuel manufacture.\nPerhaps the earliest fuel employed by humans is wood. Evidence shows controlled fire was used up to 1.5 million years ago at Swartkrans, South Africa. It is unknown which hominid species first used fire, as both Australopithecus and an early species of Homo were present at the sites. As a fuel, wood has remained in use up until the present day, although it has been superseded for many purposes by other sources. Wood has an energy density of 10\u201320 MJ/kg.\nRecently biofuels have been developed for use in automotive transport (for example Bioethanol and Biodiesel), but there is widespread public debate about how carbon efficient these fuels are.\n\n\n=== Fossil fuels ===\n\nFossil fuels are hydrocarbons, primarily coal and petroleum (liquid petroleum or natural gas), formed from the fossilized remains of ancient plants and animals by exposure to high heat and pressure in the absence of oxygen in the Earth's crust over hundreds of millions of years. Commonly, the term fossil fuel also includes hydrocarbon-containing natural resources that are not derived entirely from biological sources, such as tar sands. These latter sources are properly known as mineral fuels.\nFossil fuels contain high percentages of carbon and include coal, petroleum, and natural gas. They range from volatile materials with low carbon:hydrogen ratios like methane, to liquid petroleum to nonvolatile materials composed of almost pure carbon, like anthracite coal. Methane can be found in hydrocarbon fields, alone, associated with oil, or in the form of methane clathrates. Fossil fuels formed from the fossilized remains of dead plants by exposure to heat and pressure in the Earth's crust over millions of years. This biogenic theory was first introduced by German scholar Georg Agricola in 1556 and later by Mikhail Lomonosov in the 18th century.\nIt was estimated by the Energy Information Administration that in 2007 primary sources of energy consisted of petroleum 36.0%, coal 27.4%, natural gas 23.0%, amounting to an 86.4% share for fossil fuels in primary energy consumption in the world. Non-fossil sources in 2006 included hydroelectric 6.3%, nuclear 8.5%, and others (geothermal, solar, tidal, wind, wood, waste) amounting to 0.9%. World energy consumption was growing about 2.3% per year.\nFossil fuels are non-renewable resources because they take millions of years to form, and reserves are being depleted much faster than new ones are being made. So we must conserve these fuels and use them judiciously. The production and use of fossil fuels raise environmental concerns. A global movement toward the generation of renewable energy is therefore under way to help meet increased energy needs. The burning of fossil fuels produces around 21.3 billion tonnes (21.3 gigatonnes) of carbon dioxide (CO2) per year, but it is estimated that natural processes can only absorb about half of that amount, so there is a net increase of 10.65 billion tonnes of atmospheric carbon dioxide per year (one tonne of atmospheric carbon is equivalent to 44/12 or 3.7 tonnes of carbon dioxide). Carbon dioxide is one of the greenhouse gases that enhances radiative forcing and contributes to global warming, causing the average surface temperature of the Earth to rise in response, which the vast majority of climate scientists agree will cause major adverse effects. Fuels are a source of energy.\n\n\n== Nuclear ==\n\nNuclear fuel is any material that is consumed to derive nuclear energy. Technically speaking, All matter can be a nuclear fuel because any element under the right conditions will release nuclear energy, but the materials commonly referred to as nuclear fuels are those that will produce energy without being placed under extreme duress. Nuclear fuel is a material that can be 'burned' by nuclear fission or fusion to derive nuclear energy. Nuclear fuel can refer to the fuel itself, or to physical objects (for example bundles composed of fuel rods) composed of the fuel material, mixed with structural, neutron moderating, or neutron reflecting materials.\nMost nuclear fuels contain heavy fissile elements that are capable of nuclear fission. When these fuels are struck by neutrons, they are in turn capable of emitting neutrons when they break apart. This makes possible a self-sustaining chain reaction that releases energy with a controlled rate in a nuclear reactor or with a very rapid uncontrolled rate in a nuclear weapon.\nThe most common fissile nuclear fuels are uranium-235 (235U) and plutonium-239 (239Pu). The actions of mining, refining, purifying, using, and ultimately disposing of nuclear fuel together make up the nuclear fuel cycle. Not all types of nuclear fuels create power from nuclear fission. Plutonium-238 and some other elements are used to produce small amounts of nuclear power by radioactive decay in radioisotope thermoelectric generators and other types of atomic batteries. Also, light nuclides such as tritium (3H) can be used as fuel for nuclear fusion. Nuclear fuel has the highest energy density of all practical fuel sources.\n\n\n=== Fission ===\n\nThe most common type of nuclear fuel used by humans is heavy fissile elements that can be made to undergo nuclear fission chain reactions in a nuclear fission reactor; nuclear fuel can refer to the material or to physical objects (for example fuel bundles composed of fuel rods) composed of the fuel material, perhaps mixed with structural, neutron moderating, or neutron reflecting materials. The most common fissile nuclear fuels are 235U and 239Pu, and the actions of mining, refining, purifying, using, and ultimately disposing of these elements together make up the nuclear fuel cycle, which is important for its relevance to nuclear power generation and nuclear weapons.\n\n\n=== Fusion ===\nFuels that produce energy by the process of nuclear fusion are currently not utilized by humans but are the main source of fuel for stars. Fusion fuels tend to be light elements such as hydrogen which will combine easily. Energy is required to start fusion by raising temperature so high all materials would turn into plasma, and allow nuclei to collide and stick together with each other before repelling due to electric charge. This process is called fusion and it can give out energy.\nIn stars that undergo nuclear fusion, fuel consists of atomic nuclei that can release energy by the absorption of a proton or neutron. In most stars the fuel is provided by hydrogen, which can combine to form helium through the proton-proton chain reaction or by the CNO cycle. When the hydrogen fuel is exhausted, nuclear fusion can continue with progressively heavier elements, although the net energy released is lower because of the smaller difference in nuclear binding energy. Once iron-56 or nickel-56 nuclei are produced, no further energy can be obtained by nuclear fusion as these have the highest nuclear binding energies. The elements then on use up energy instead of giving off energy when fused. Therefore, fusion stops and the star dies. In attempts by humans, fusion is only carried out with hydrogen (isotope of 2 and 3) to form helium-4 as this reaction gives out the most net energy. Electric confinement (ITER), inertial confinement(heating by laser) and heating by strong electric currents are the popular methods used. .\n\n\n== World trade ==\n\nThe World Bank reported that the USA was the top fuel importer in 2005 followed by the EU and Japan.\n\n\n== See also ==\n\n\n== Footnotes ==\n\n\n== References ==\nRatcliff, Brian; et al. (2000). Chemistry 1. Cambridge University press. ISBN 0-521-78778-5. \n\n\n== Further reading ==\nDirective 1999/94/EC of the European Parliament and of the council of 13 December 1999, relating to the availability of consumer information on fuel economy and CO2 emissions in respect of the marketing of new passenger cars PDF (140 KB).\nCouncil Directive 80/1268/EEC Fuel consumption of motor vehicles.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Fuel", 
                "title": "Fuel"
            }, 
            {
                "snippet": "on May 26, 2007. Retrieved 2008-03-08.\u00a0  \"Principales resultados por localidad 2005 (ITER)\". Retrieved 2008-03-08.\u00a0   (Spanish) Municipal Official Site", 
                "pageCategories": "Articles with Spanish-language external links\nCoordinates on Wikidata\nMunicipalities of the State of Mexico\nOtomi settlements\nPopulated places in the State of Mexico", 
                "pageContent": "Jilotepec de Molina Enr\u00edquez and Jilotepec de Abasolo are a town and a municipality located northwest zone of the State of Mexico, in Mexico. However, both entities are interchangeably referred to as \"Jilotepec\". This name comes from N\u00e1huatl, meaning \"hill of corncobs\". It is located in hilly and forested terrain an hour from Mexico City, Toluca, 40 minutes from San Juan del R\u00edo, 30 minutes from Tula and 20 from Tepeji. The Mexico City\u2013Quer\u00e9taro and the new Transoceanic Freeways converge within its territory that unite the coasts of Mexico from Veracruz to Michoac\u00e1n.\n\n\n== The town of Jilotepec de Molina Enr\u00edquez ==\nThe city had a population of 10,503 as of 2005.\nThe region was originally inhabited by Otomis but were conquered in 1379 by Acamapichtli the Aztec tlatoani (chief). After the Spanish Conquest, Jilotepec was recorded in ecclesiastical records as a village with a singe priest, administrated by the Franciscans with the Brothers Alonso de Rangel and Antonio de Ciudad Rodrigo being the first to evangelize the area. Sometime in the middle of the 16th century, silver was discovered in Zacatecas and Guanajuato, leading to the construction of the Camino Real a Zacatecas (Royal Road to Zacatecas) with passed through>\n\n\n== The municipality of Jilotepec de Abasolo ==\nAs municipal seat, Jilotepec de Molina Enr\u00edquez has governing authority over the following communities:6a. Mza. San Miguel de la Victoria Palo Alto, Agua Escondida, Aldama, Barrio Pobre, Bosque de Canalejas (Manzana Sexta Canalejas), Calpulalpan, Canalejas, Coscomate del Progreso, Danxh\u00f3, Dedeni Dolores, Denjhi, Dexcani Alto, Dexcani Bajo, Doxhicho, Ej. San Pablo Huantepec (7a. Mza. San Pablo), Ejido Acazuchitl\u00e1n (Ejido San Juan Bautista), Ejido de Coscomate del Progreso, Ejido de Jilotepec, Ejido de San Lorenzo Octeyuco, Ejido las Manzanas, El \u00c1guila, El Barrete, El Durazno de Cuauht\u00e9moc, El Durazno de Guerrero, El Fresno, El Huisache (Tercera Manzana de Dexcani Alto), El Magueyal, El Majuay, El Path\u00e9, El Pedregal, El Quelite, El Quichi, El Rinc\u00f3n, El Rosal, El Saltillito, El Saltillo, El Xhitey, Emiliano Zapata, La Comunidad, La Dalia, La Fortaleza, La Manzanilla (3a. Manzana de Dexcani Bajo), La Maqueda (Ejido de San Lorenzo Nenamicoyan), Las \u00c1nimas, Las Huertas, Las Manzanas, Las Pilas, Llano Grande, Magueycitos, Mataxhi, Octeyuco Dos Mil, Ojo de Agua, Potrero Nuevo, Rancho el Tejocote, Rancho la Herradura (Rancho los Gorriones), Rancho la Laguna, Rancho la Providencia (La Noria), Rancho San Francisco, Rancho Santa Ana el Sauz, San Ignacio (San Ignacio de Loyola), San Jos\u00e9 Ejido de San Lorenzo, San Juan Acazuchitl\u00e1n (San Juanico), San Lorenzo Nenamicoyan, San Lorenzo Octeyuco, San Mart\u00edn Tuchicuitlapilco, San Miguel de la Victoria, San Pablo Huantepec, San Vicente, Santa Martha de la Cruz, Santiago Oxthoc, S\u00e9ptima Manzana de San Miguel de la Victoria, Tecolapan, Teupan, Xhim\u00f3jay, and Xhixhata.\nJilotepec is located at 1670 meters over sea level it has 586.53 km2 being the fourth largest municipality of Mexico State. According to INEGIs data Jilotepec de Abasolo has 71624 inhabitants. The municipality borders the municipalities of Polotitlan, Aculco, Timilpan, Chapa de Mota, Villa del Carb\u00f3n, Soyaniquilpan and the state of Hidalgo. At the end of the 18th century, Jilotepec was part of the municipality of Huichapan, in the District of Tula. On March 11, 1824, Jilotepec de Abasolo was created from parts of Huichapan Chapa de Mota, Villa del Carb\u00f3n, and Acambay.\n\n\n=== Economic activities ===\n6,416 hectares of the municipality is dedicated to the production of corn (the major crop), beans, wheat and livestock which is very important activity in the economic life of Jilotepec. Livestock includes cows pigs, sheep, and deer being a major dairy and meat producer. There are also farms with 8.5 millions birds for the production of meat and eggs. The raising of freshwater fish is a growing industry in the municipality as well.\nThe municipality has two types of industry, the first being family workshops which produce clothes, ceramics and pottery. The second are factories focused on the production and embroidery of clothes for men and the manufacturing of plastic containers and other wool products.\n\n\n== References ==\n\n\n== External links ==\n(Spanish) Municipal Official Site", 
                "titleUrl": "https://en.wikipedia.org/wiki/Jilotepec_de_Abasolo", 
                "title": "Jilotepec de Abasolo"
            }, 
            {
                "snippet": "Ragheb. As of 2001, about 235 naval reactors had been built  \"Beyond ITER\". The ITER Project. Information Services, Princeton Plasma Physics Laboratory", 
                "pageCategories": "All articles with dead external links\nAll articles with unsourced statements\nArticles containing video clips\nArticles with Russian-language external links\nArticles with Wayback Machine links\nArticles with dead external links from May 2016\nArticles with unsourced statements from April 2014\nArticles with unsourced statements from December 2015\nCS1 errors: dates\nCS1 errors: external links", 
                "pageContent": "Nuclear power is the use of nuclear reactions that release nuclear energy to generate heat, which most frequently is then used in steam turbines to produce electricity in a nuclear power plant. The term includes nuclear fission, nuclear decay and nuclear fusion. Presently, the nuclear fission of elements in the actinide series of the periodic table produce the vast majority of nuclear energy in the direct service of humankind, with nuclear decay processes, primarily in the form of geothermal energy, and radioisotope thermoelectric generators, in niche uses making up the rest.\nFission-electric power stations are one of the leading low carbon power generation methods of producing electricity, and in terms of total life-cycle greenhouse gas emissions per unit of energy generated, has emission values lower than \"renewable energy\" when the latter is taken as a single energy source. As all electricity supplying technologies use cement etc., during construction, emissions are yet to be brought to zero. A 2014 analysis of the carbon footprint literature by the Intergovernmental Panel on Climate Change (IPCC) reported that the embodied total life-cycle emission intensity of fission electricity has a median value of 12 g CO2 eq/kWh which is the lowest out of all commercial baseload energy sources, and second lowest out of all commercial electricity technologies known, after wind power which is an Intermittent energy source with embodied greenhouse gas emissions, per unit of energy generated of 11 g CO2eq/kWh. Each result is contrasted with coal & fossil gas at 820 and 490 g CO2 eq/kWh. With this translating into, from the beginning of Fission-electric power station commercialization in the 1970s, having prevented the emission of about 64 billion tonnes of carbon dioxide equivalent, greenhouse gases that would have otherwise resulted from the burning of fossil fuels in thermal power stations.\nThere is a social debate about nuclear power. Proponents, such as the World Nuclear Association and Environmentalists for Nuclear Energy, contend that nuclear power is a safe, sustainable energy source that reduces carbon emissions. Opponents, such as Greenpeace International and NIRS, contend that nuclear power poses many threats to people and the environment.\nFar-reaching fission power reactor accidents, or accidents that resulted in medium to long-lived fission product contamination of inhabited areas, have occurred in Generation I & II reactor designs, blueprinted between 1950 and 1980. These include the Chernobyl disaster which occurred in 1986, the Fukushima Daiichi nuclear disaster (2011), and the more contained Three Mile Island accident (1979). There have also been some nuclear submarine accidents. In terms of lives lost per unit of energy generated, analysis has determined that fission-electric reactors have caused fewer fatalities per unit of energy generated than the other major sources of energy generation. Energy production from coal, petroleum, natural gas and hydroelectricity has caused a greater number of fatalities per unit of energy generated due to air pollution and energy accident effects. Four years after the Fukushima-Daiichi accident, there have been no fatalities due to exposure to radiation, and no discernible increased incidence of radiation-related health effects are expected among exposed members of the public and their descendants. The Japan Times estimated 1,600 deaths were the result of evacuation, due to physical and mental stress stemming from long stays at shelters, a lack of initial care as a result of hospitals being disabled by the tsunami, and suicides.\nIn 2015:\nTen new reactors were connected to the grid.\nSeven reactors were permanently shut down.\n441 reactors had a worldwide net capacity of 382,855 megawatts of electricity.\n67 new nuclear reactors were under construction.\nMost of the new activity is in China where there is an urgent need to control pollution from coal plants.\n\n\n== History ==\n\n\n=== Origins ===\n\nIn 1932 physicist Ernest Rutherford discovered that when lithium atoms were \"split\" by protons from a proton accelerator, immense amounts of energy were released in accordance with the principle of mass\u2013energy equivalence. However, he and other nuclear physics pioneers Niels Bohr and Albert Einstein believed harnessing the power of the atom for practical purposes anytime in the near future was unlikely, with Rutherford labeling such expectations \"moonshine.\"\nThe same year, his doctoral student James Chadwick discovered the neutron, which was immediately recognized as a potential tool for nuclear experimentation because of its lack of an electric charge. Experimentation with bombardment of materials with neutrons led Fr\u00e9d\u00e9ric and Ir\u00e8ne Joliot-Curie to discover induced radioactivity in 1934, which allowed the creation of radium-like elements at much less the price of natural radium. Further work by Enrico Fermi in the 1930s focused on using slow neutrons to increase the effectiveness of induced radioactivity. Experiments bombarding uranium with neutrons led Fermi to believe he had created a new, transuranic element, which was dubbed hesperium.\n\nBut in 1938, German chemists Otto Hahn and Fritz Strassmann, along with Austrian physicist Lise Meitner and Meitner's nephew, Otto Robert Frisch, conducted experiments with the products of neutron-bombarded uranium, as a means of further investigating Fermi's claims. They determined that the relatively tiny neutron split the nucleus of the massive uranium atoms into two roughly equal pieces, contradicting Fermi. This was an extremely surprising result: all other forms of nuclear decay involved only small changes to the mass of the nucleus, whereas this process\u2014dubbed \"fission\" as a reference to biology\u2014involved a complete rupture of the nucleus. Numerous scientists, including Le\u00f3 Szil\u00e1rd, who was one of the first, recognized that if fission reactions released additional neutrons, a self-sustaining nuclear chain reaction could result. Once this was experimentally confirmed and announced by Fr\u00e9d\u00e9ric Joliot-Curie in 1939, scientists in many countries (including the United States, the United Kingdom, France, Germany, and the Soviet Union) petitioned their governments for support of nuclear fission research, just on the cusp of World War II, for the development of a nuclear weapon.\nIn the United States, where Fermi and Szil\u00e1rd had both emigrated, this led to the creation of the first man-made reactor, known as Chicago Pile-1, which achieved criticality on December 2, 1942. This work became part of the Manhattan Project, which made enriched uranium and built large reactors to breed plutonium for use in the first nuclear weapons, which were used on the cities of Hiroshima and Nagasaki.\n\nIn 1945, the pocketbook The Atomic Age heralded the untapped atomic power in everyday objects and depicted a future where fossil fuels would go unused. One science writer, David Dietz, wrote that instead of filling the gas tank of your car two or three times a week, you will travel for a year on a pellet of atomic energy the size of a vitamin pill. Glenn Seaborg, who chaired the Atomic Energy Commission, wrote \"there will be nuclear powered earth-to-moon shuttles, nuclear powered artificial hearts, plutonium heated swimming pools for SCUBA divers, and much more\". These overly optimistic predications remain unfulfilled.\nUnited Kingdom, Canada, and USSR proceeded over the course of the late 1940s and early 1950s. Electricity was generated for the first time by a nuclear reactor on December 20, 1951, at the EBR-I experimental station near Arco, Idaho, which initially produced about 100 kW. Work was also strongly researched in the US on nuclear marine propulsion, with a test reactor being developed by 1953 (eventually, the USS Nautilus, the first nuclear-powered submarine, would launch in 1955). In 1953, US President Dwight Eisenhower gave his \"Atoms for Peace\" speech at the United Nations, emphasizing the need to develop \"peaceful\" uses of nuclear power quickly. This was followed by the 1954 Amendments to the Atomic Energy Act which allowed rapid declassification of U.S. reactor technology and encouraged development by the private sector.\n\n\n=== Early years ===\nOn June 27, 1954, the USSR's Obninsk Nuclear Power Plant became the world's first nuclear power plant to generate electricity for a power grid, and produced around 5 megawatts of electric power.\nLater in 1954, Lewis Strauss, then chairman of the United States Atomic Energy Commission (U.S. AEC, forerunner of the U.S. Nuclear Regulatory Commission and the United States Department of Energy) spoke of electricity in the future being \"too cheap to meter\". Strauss was very likely referring to hydrogen fusion \u2014which was secretly being developed as part of Project Sherwood at the time\u2014but Strauss's statement was interpreted as a promise of very cheap energy from nuclear fission. The U.S. AEC itself had issued far more realistic testimony regarding nuclear fission to the U.S. Congress only months before, projecting that \"costs can be brought down... [to]... about the same as the cost of electricity from conventional sources...\"\nIn 1955 the United Nations' \"First Geneva Conference\", then the world's largest gathering of scientists and engineers, met to explore the technology. In 1957 EURATOM was launched alongside the European Economic Community (the latter is now the European Union). The same year also saw the launch of the International Atomic Energy Agency (IAEA).\n\nThe world's first commercial nuclear power station, Calder Hall at Windscale, England, was opened in 1956 with an initial capacity of 50 MW (later 200 MW). The first commercial nuclear generator to become operational in the United States was the Shippingport Reactor (Pennsylvania, December 1957).\nOne of the first organizations to develop nuclear power was the U.S. Navy, for the purpose of propelling submarines and aircraft carriers. The first nuclear-powered submarine, USS Nautilus (SSN-571), was put to sea in December 1954. As of 2016, the U.S. Navy submarine fleet is made up entirely of nuclear-powered vessels, with 75 submarines in service. Two U.S. nuclear submarines, USS Scorpion and USS Thresher, have been lost at sea. The Russian Navy is currently (2016) estimated to have 61 nuclear submarines in service; eight Soviet and Russian nuclear submarines have been lost at sea. This includes the Soviet submarine K-19 reactor accident in 1961 which resulted in 8 deaths and more than 30 other people were over-exposed to radiation. The Soviet submarine K-27 reactor accident in 1968 resulted in 9 fatalities and 83 other injuries. Moreover, Soviet submarine K-429 sank twice, but was raised after each incident. Several serious nuclear and radiation accidents have involved nuclear submarine mishaps.\nThe U.S. Army also had a nuclear power program, beginning in 1954. The SM-1 Nuclear Power Plant, at Fort Belvoir, Virginia, was the first power reactor in the U.S. to supply electrical energy to a commercial grid (VEPCO), in April 1957, before Shippingport. The SL-1 was a U.S. Army experimental nuclear power reactor at the National Reactor Testing Station in eastern Idaho. It underwent a steam explosion and meltdown in January 1961, which killed its three operators. In Soviet Union in The Mayak Production Association there were a number of accidents including an explosion that released 50-100 tonnes of high-level radioactive waste, contaminating a huge territory in the eastern Urals and causing numerous deaths and injuries. The Soviet regime kept this accident secret for about 30 years. The event was eventually rated at 6 on the seven-level INES scale (third in severity only to the disasters at Chernobyl and Fukushima).\n\n\n=== Development ===\n\nInstalled nuclear capacity initially rose relatively quickly, rising from less than 1 gigawatt (GW) in 1960 to 100 GW in the late 1970s, and 300 GW in the late 1980s. Since the late 1980s worldwide capacity has risen much more slowly, reaching 366 GW in 2005. Between around 1970 and 1990, more than 50 GW of capacity was under construction (peaking at over 150 GW in the late 1970s and early 1980s) \u2014 in 2005, around 25 GW of new capacity was planned. More than two-thirds of all nuclear plants ordered after January 1970 were eventually cancelled. A total of 63 nuclear units were canceled in the USA between 1975 and 1980.\nDuring the 1970s and 1980s rising economic costs (related to extended construction times largely due to regulatory changes and pressure-group litigation) and falling fossil fuel prices made nuclear power plants then under construction less attractive. In the 1980s (U.S.) and 1990s (Europe), flat load growth and electricity liberalization also made the addition of large new baseload capacity unattractive.\nThe 1973 oil crisis had a significant effect on countries, such as France and Japan, which had relied more heavily on oil for electric generation (39% and 73% respectively) to invest in nuclear power.\nSome local opposition to nuclear power emerged in the early 1960s, and in the late 1960s some members of the scientific community began to express their concerns. These concerns related to nuclear accidents, nuclear proliferation, high cost of nuclear power plants, nuclear terrorism and radioactive waste disposal. In the early 1970s, there were large protests about a proposed nuclear power plant in Wyhl, Germany. The project was cancelled in 1975 and anti-nuclear success at Wyhl inspired opposition to nuclear power in other parts of Europe and North America. By the mid-1970s anti-nuclear activism had moved beyond local protests and politics to gain a wider appeal and influence, and nuclear power became an issue of major public protest. Although it lacked a single co-ordinating organization, and did not have uniform goals, the movement's efforts gained a great deal of attention. In some countries, the nuclear power conflict \"reached an intensity unprecedented in the history of technology controversies\".\n\nIn France, between 1975 and 1977, some 175,000 people protested against nuclear power in ten demonstrations. In West Germany, between February 1975 and April 1979, some 280,000 people were involved in seven demonstrations at nuclear sites. Several site occupations were also attempted. In the aftermath of the Three Mile Island accident in 1979, some 120,000 people attended a demonstration against nuclear power in Bonn. In May 1979, an estimated 70,000 people, including then governor of California Jerry Brown, attended a march and rally against nuclear power in Washington, D.C. Anti-nuclear power groups emerged in every country that has had a nuclear power programme.\n\n\n=== Three mile Island and Chernobyl ===\n\nHealth and safety concerns, the 1979 accident at Three Mile Island, and the 1986 Chernobyl disaster played a part in stopping new plant construction in many countries, although the public policy organization, the Brookings Institution states that new nuclear units, at the time of publishing in 2006, had not been built in the U.S. because of soft demand for electricity, and cost overruns on nuclear plants due to regulatory issues and construction delays. By the end of the 1970s it became clear that nuclear power would not grow nearly as dramatically as once believed. Eventually, more than 120 reactor orders in the U.S. were ultimately cancelled and the construction of new reactors ground to a halt. A cover story in the February 11, 1985, issue of Forbes magazine commented on the overall failure of the U.S. nuclear power program, saying it \"ranks as the largest managerial disaster in business history\".\nUnlike the Three Mile Island accident, the much more serious Chernobyl accident did not increase regulations affecting Western reactors since the Chernobyl reactors were of the problematic RBMK design only used in the Soviet Union, for example lacking \"robust\" containment buildings. Many of these RBMK reactors are still in use today. However, changes were made in both the reactors themselves (use of a safer enrichment of uranium) and in the control system (prevention of disabling safety systems), amongst other things, to reduce the possibility of a duplicate accident.\nAn international organization to promote safety awareness and professional development on operators in nuclear facilities was created: WANO; World Association of Nuclear Operators.\nOpposition in Ireland and Poland prevented nuclear programs there, while Austria (1978), Sweden (1980) and Italy (1987) (influenced by Chernobyl) voted in referendums to oppose or phase out nuclear power. In July 2009, the Italian Parliament passed a law that cancelled the results of an earlier referendum and allowed the immediate start of the Italian nuclear program. After the Fukushima Daiichi nuclear disaster a one-year moratorium was placed on nuclear power development, followed by a referendum in which over 94% of voters (turnout 57%) rejected plans for new nuclear power.\n\n\n=== Nuclear renaissance ===\n\nSince about 2001 the term nuclear renaissance has been used to refer to a possible nuclear power industry revival, driven by rising fossil fuel prices and new concerns about meeting greenhouse gas emission limits. In 2012, the World Nuclear Association reported that nuclear electricity generation was at its lowest level since 1999. As of January 2016, however, 65 new nuclear power reactors were under construction. Over 150 were planned, equivalent to nearly half of capacity at that time.\n\n\n=== Fukushima Daiichi Nuclear Disaster ===\n\nJapan's 2011 Fukushima Daiichi nuclear accident, which occurred in a reactor design from the 1960s, prompted a re-examination of nuclear safety and nuclear energy policy in many countries. Germany plans to close all its reactors by 2022, and Italy has re-affirmed its ban on electric utilities generating, but not importing, fission derived electricity. In 2011 the International Energy Agency halved its prior estimate of new generating capacity to be built by 2035. In 2013 Japan signed a deal worth $22 billion, in which Mitsubishi Heavy Industries would build four modern Atmea reactors for Turkey. In August 2015, following 4 years of near zero fission-electricity generation, Japan began restarting its fission fleet, after safety upgrades were completed, beginning with Sendai fission-electric station.\nIn March 2011 the nuclear emergencies at Japan's Fukushima Daiichi Nuclear Power Plant and shutdowns at other nuclear facilities raised questions among some commentators over the future of the renaissance. China, Germany, Switzerland, Israel, Malaysia, Thailand, United Kingdom, Italy and the Philippines have reviewed their nuclear power programs. Indonesia and Vietnam still plan to build nuclear power plants.\nThe World Nuclear Association has said that \"nuclear power generation suffered its biggest ever one-year fall through 2012 as the bulk of the Japanese fleet remained offline for a full calendar year\". Data from the International Atomic Energy Agency showed that nuclear power plants globally produced 2346 TWh of electricity in 2012 \u2013 seven per cent less than in 2011. The figures illustrate the effects of a full year of 48 Japanese power reactors producing no power during the year. The permanent closure of eight reactor units in Germany was also a factor. Problems at Crystal River, Fort Calhoun and the two San Onofre units in the USA meant they produced no power for the full year, while in Belgium Doel 3 and Tihange 2 were out of action for six months. Compared to 2010, the nuclear industry produced 11% less electricity in 2012.\n\n\n=== Post-Fukushima controversy ===\n\nThe Fukushima Daiichi nuclear accident sparked controversy about the importance of the accident and its effect on nuclear's future. IAEA Director General Yukiya Amano said the Japanese nuclear accident \"caused deep public anxiety throughout the world and damaged confidence in nuclear power\", and the International Energy Agency halved its estimate of additional nuclear generating capacity to be built by 2035. But by 2015, the Agency's outlook had become more promising. \"Nuclear power is a critical element in limiting greenhouse gas emissions,\" the agency noted, and \"the prospects for nuclear energy remain positive in the medium to long term despite a negative impact in some countries in the aftermath of the [Fukushima-Daiichi] accident...it is still the second-largest source worldwide of low-carbon electricity. And the 72 reactors under construction at the start of last year were the most in 25 years.\" Though Platts reported in 2011 that \"the crisis at Japan's Fukushima nuclear plants has prompted leading energy-consuming countries to review the safety of their existing reactors and cast doubt on the speed and scale of planned expansions around the world\", Progress Energy Chairman/CEO Bill Johnson made the observation that \"Today there\u2019s an even more compelling case that greater use of nuclear power is a vital part of a balanced energy strategy\". In 2011, The Economist opined that nuclear power \"looks dangerous, unpopular, expensive and risky\", and that \"it is replaceable with relative ease and could be forgone with no huge structural shifts in the way the world works\". Earth Institute Director Jeffrey Sachs disagreed, claiming combating climate change would require an expansion of nuclear power. \"We won't meet the carbon targets if nuclear is taken off the table,\" he said. \"We need to understand the scale of the challenge.\"\nInvestment banks were critical of nuclear soon after the accident. Many disputed their impartiality, however, due to significant investments in renewable energy, perceived by some as a valid alternative to nuclear. In early April 2011, analysts at Swiss-based investment bank UBS said: \"At Fukushima, four reactors have been out of control for weeks, casting doubt on whether even an advanced economy can master nuclear safety...we believe the Fukushima accident was the most serious ever for the credibility of nuclear power\". UBS has helped to raise more than $20 billion since 2006 and advised on more than a dozen deals for renewable energy and cleantech companies. Deutsche Bank advised that \"the global impact of the Fukushima accident is a fundamental shift in public perception with regard to how a nation prioritizes and values its populations health, safety, security, and natural environment when determining its current and future energy pathways...renewable energy will be a clear long-term winner in most energy systems, a conclusion supported by many voter surveys conducted over the past few weeks. Deutsche Bank has over \u20ac1 billion in capital invested in renewables projects in Europe, North & South America, and Asia.\nManufacturers also recognized a profit opportunity in negative public perceptions about nuclear. In September 2011, German engineering giant Siemens announced it will withdraw entirely from the nuclear industry, as a response to the Fukushima nuclear accident in Japan, and said that it would no longer build nuclear power plants anywhere in the world. The company\u2019s chairman, Peter L\u00f6scher, said that \"Siemens was ending plans to cooperate with Rosatom, the Russian state-controlled nuclear power company, in the construction of dozens of nuclear plants throughout Russia over the coming two decades\". Renewable energy is a core component of Siemens's profit base. In February, 2016 the firm proposed a \u20ac10 billion renewable energy investment in Egypt.\nIn February 2012, the United States Nuclear Regulatory Commission approved the construction of two additional reactors at the Vogtle Electric Generating Plant, the first reactors to be approved in over 30 years since the Three Mile Island accident, but NRC Chairman Gregory Jaczko cast a dissenting vote citing safety concerns stemming from Japan's 2011 Fukushima nuclear disaster, and saying \"I cannot support issuing this license as if Fukushima never happened\". Jaczko resigned in April 2012. One week after Southern received the license to begin major construction on the two new reactors, a dozen environmental and anti-nuclear groups sued to stop the Plant Vogtle expansion project, saying \"public safety and environmental problems since Japan's Fukushima Daiichi nuclear reactor accident have not been taken into account\". In July 2012, the suit was rejected by the Washington, D.C. Circuit Court of Appeals.\nCountries such as Australia, Austria, Denmark, Greece, Ireland, Italy, Latvia, Liechtenstein, Luxembourg, Malta, Portugal, Israel, Malaysia, New Zealand, and Norway have no nuclear power reactors and remain opposed to nuclear power. However, by contrast, some countries remain in favor, and financially support nuclear fusion research, including EU wide funding of the ITER project.\n\n\n== Industry ==\n\n\n=== Companies ===\n\n\n=== Capacity and production ===\n\nNuclear power capacity remained relatively stable between the mid 1980s until the accident at the Fukushima Daiichi reactor in March 2011. In June 2015, Platts reported global nuclear generation increased by 1% in 2014, the first annual increase since Fukushima.\nThe United States produces the most nuclear energy, with nuclear power providing 19% of the electricity it consumes, while France produces the highest percentage of its electrical energy from nuclear reactors\u201480% as of 2006. In the European Union as a whole, nuclear energy provides 30% of the electricity. Nuclear energy policy differs among European Union countries, and some, such as Austria, Estonia, Ireland and Italy, have no active nuclear power stations. In comparison, France has a large number of these plants, with 16 multi-unit stations in current use.\nMany military and some civilian (such as some icebreaker) ships use nuclear marine propulsion, a form of nuclear propulsion. A few space vehicles have been launched using full-fledged nuclear reactors: 33 reactors belong to the Soviet RORSAT series and one was the American SNAP-10A.\nInternational research is continuing into safety improvements such as passively safe plants, the use of nuclear fusion, and additional uses of process heat such as hydrogen production (in support of a hydrogen economy), for desalinating sea water, and for use in district heating systems.\nNuclear (fission) power stations, excluding the contribution from naval nuclear fission reactors, provided 11% of the world's electricity in 2012, somewhat less than that generated by hydro-electric stations at 16%. Since electricity accounts for about 25% of humanity's energy usage with the majority of the rest coming from fossil fuel reliant sectors such as transport, manufacture and home heating, nuclear fission's contribution to the global final energy consumption is about 2.5%, a little more than the combined global electricity production from \"new renewables\"; wind, solar, biofuel and geothermal power, which together provided 2% of global final energy consumption in 2014.\nRegional differences in the use of fission energy are large. Fission energy generation, with a 20% share of the U.S. electricity production, is the single largest deployed technology among current low-carbon power sources in the country. In addition, two-thirds of the European Union's twenty-seven nations' low-carbon energy is produced by fission. Some of these nations have banned its generation, such as Italy, which ended the use of fission-electric generation, which started in 1963, in 1990. France is the largest user of nuclear energy, deriving 75% of its electricity from fission.\nIn 2013, the IAEA reported that there were 437 operational civil fission-electric reactors in 31 countries, although not every reactor was producing electricity. In addition, there were approximately 140 naval vessels using nuclear propulsion in operation, powered by some 180 reactors. As of 2013, attaining a net energy gain from sustained nuclear fusion reactions, excluding natural fusion power sources such as the Sun, remains an ongoing area of international physics and engineering research. With commercial fusion power production remaining unlikely before 2050.\nSince commercial nuclear energy began in the mid-1950s, 2008 was the first year that no new nuclear power plant was connected to the grid, although two were connected in 2009.\nIn 2015, the IAEA reported that worldwide there were 67 civil fission-electric power reactors under construction in 15 countries including Gulf states such as the United Arab Emirates (UAE). Over half of the 67 total being built were in Asia, with 28 in China. Eight new grid connections were completed by China in 2015 and the most recently completed reactor to be connected to the electrical grid, as of January 2016, was at the Kori Nuclear Power Plant in the Republic of Korea. In the US, four new Generation III reactors were under construction at Vogtle and Summer station, while a fifth was nearing completion at Watts Bar station, all five were expected to become operational before 2020. In 2013, four aging uncompetitive U.S reactors were closed. According to the World Nuclear Association, the global trend is for new nuclear power stations coming online to be balanced by the number of old plants being retired.\nAnalysis in 2015 by Professor and Chair of Environmental Sustainability Barry W. Brook and his colleagues on the topic of replacing fossil fuels entirely, from the electric grid of the world, has determined that at the historically modest and proven-rate at which nuclear energy was added to and replaced fossil fuels in France and Sweden during each nation's building programs in the 1980s, within 10 years nuclear energy could displace or remove fossil fuels from the electric grid completely, \"allow[ing] the world to meet the most stringent greenhouse-gas mitigation targets.\". In a similar analysis, Brook had earlier determined that 50% of all global energy, that is not solely electricity, but transportation synfuels etc. could be generated within approximately 30 years, if the global nuclear fission build rate was identical to each of these nation's already proven decadal rates(in units of installed nameplate capacity, GW per year, per unit of global GDP(GW/year/$).\nThis is in contrast to the completely conceptual paper-studies for a 100% renewable energy world, which would require an orders of magnitude more costly global investment per year, which has no historical precedent, having never been attempted due to its prohibitive cost, along with far greater land that would need to be devoted to the wind, wave and solar projects, and the inherent assumption that humanity will use less, and not more, energy in the future. As Brook notes the \"principal limitations on nuclear fission are not technical, economic or fuel-related, but are instead linked to complex issues of societal acceptance, fiscal and political inertia, and inadequate critical evaluation of the real-world constraints facing [the other] low-carbon alternatives.\"\n\n\n=== Economics ===\n\nNuclear power plants typically have high capital costs for building the plant, but low fuel costs. Although nuclear power plants can vary their output the electricity is generally less favorably priced when doing so. Nuclear power plants are therefore typically run as much as possible to keep the cost of the generated electrical energy as low as possible, supplying mostly base-load electricity.\nInternationally the price of nuclear plants rose 15% annually in 1970-1990. Yet, nuclear power has total costs in 2012 of about $96 per megawatt hour (MWh), most of which involves capital construction costs, compared with solar power at $130 per MWh, and natural gas at the low end at $64 per MWh.\nIn 2015, the Bulletin of the Atomic Scientists unveiled the Nuclear Fuel Cycle Cost Calculator, an online tool that estimates the full cost of electricity produced by three configurations of the nuclear fuel cycle. Two years in the making, this interactive calculator is the first generally accessible model to provide a nuanced look at the economic costs of nuclear power; it lets users test how sensitive the price of electricity is to a full range of components\u2014more than 60 parameters that can be adjusted for the three configurations of the nuclear fuel cycle considered by this tool (once-through, limited-recycle, full-recycle). Users can select the fuel cycle they would like to examine, change cost estimates for each component of that cycle, and even choose uncertainty ranges for the cost of particular components. This approach allows users around the world to compare the cost of different nuclear power approaches in a sophisticated way, while taking account of prices relevant to their own countries or regions.\nIn recent years there has been a slowdown of electricity demand growth. In Eastern Europe, a number of long-established projects are struggling to find finance, notably Belene in Bulgaria and the additional reactors at Cernavoda in Romania, and some potential backers have pulled out. Where the electricity market is competitive, cheap natural gas is available, and its future supply relatively secure, this also poses a major problem for nuclear projects and existing plants.\nAnalysis of the economics of nuclear power must take into account who bears the risks of future uncertainties. To date all operating nuclear power plants were developed by state-owned or regulated utility monopolies where many of the risks associated with construction costs, operating performance, fuel price, accident liability and other factors were borne by consumers rather than suppliers. In addition, because the potential liability from a nuclear accident is so great, the full cost of liability insurance is generally limited/capped by the government, which the U.S. Nuclear Regulatory Commission concluded constituted a significant subsidy. Many countries have now liberalized the electricity market where these risks, and the risk of cheaper competitors emerging before capital costs are recovered, are borne by plant suppliers and operators rather than consumers, which leads to a significantly different evaluation of the economics of new nuclear power plants.\nFollowing the 2011 Fukushima Daiichi nuclear disaster, costs are expected to increase for currently operating and new nuclear power plants, due to increased requirements for on-site spent fuel management and elevated design basis threats.\nThe economics of new nuclear power plants is a controversial subject, since there are diverging views on this topic, and multibillion-dollar investments ride on the choice of an energy source. Comparison with other power generation methods is strongly dependent on assumptions about construction timescales and capital financing for nuclear plants as well as the future costs of fossil fuels and renewables as well as for energy storage solutions for intermittent power sources. Cost estimates also need to take into account plant decommissioning and nuclear waste storage costs. On the other hand, measures to mitigate global warming, such as a carbon tax or carbon emissions trading, may favor the economics of nuclear power.\n\n\n=== Nuclear power organizations ===\nThere are multiple organizations which have taken a position on nuclear power and the nuclear power industry\u2013 some are proponents, and some are opponents.\n\n\n==== Proponents ====\nThe majority of pro-nuclear energy organizations and associations is either industry-supported or directly formed from industry members as advocacy groups or trade associations.\n\nEnvironmentalists for Nuclear Energy (International)\nNuclear Industry Association (United Kingdom)\nWorld Nuclear Association, a confederation of companies connected with nuclear power production. (International)\nInternational Atomic Energy Agency (IAEA)\nNuclear Energy Institute (United States)\nAmerican Nuclear Society (United States)\nUnited Kingdom Atomic Energy Authority (United Kingdom)\nEURATOM (Europe)\nEuropean Nuclear Education Network (Europe)\nAtomic Energy of Canada Limited (Canada)\nNuclear Matters (United States)\nBreakthrough Institute (United States)\nThorium Energy Alliance (United States)\nCalifornians for Green Nuclear Power (United States)\nSave Diablo Canyon (United States)\nThorium Now (United States)\nCategory:Nuclear industry organizations\n\n\n==== Opponents ====\n\nFriends of the Earth International, a network of environmental organizations.\nGreenpeace International, a non-governmental organization\nNuclear Information and Resource Service (International)\nWorld Information Service on Energy (International)\nSortir du nucl\u00e9aire (France)\nPembina Institute (Canada)\nInstitute for Energy and Environmental Research (United States)\nSayonara Nuclear Power Plants (Japan)\nCategory:Anti-nuclear organizations\n\n\n=== Future of the industry ===\n\nThe future of nuclear power varies greatly between countries, depending on government policies. Some countries, many of them in Europe, such as Germany, Belgium, and Lithuania, have adopted policies of nuclear power phase-out. At the same time, some Asian countries, such as China, South Korea, and India, have committed to rapid expansion of nuclear power. Many other countries, such as the United Kingdom and the United States, have policies in between. Japan was a major generator of nuclear power before the Fukushima accident, but as of August 2016, Japan has restarted only three of its nuclear plants, and the extent to which it will resume its nuclear program is uncertain.\nIn 2015, the International Energy Agency reported that the Fukushima accident had a strongly negative effect on nuclear power, yet \u201cthe prospects for nuclear energy remain positive in the medium to long term despite a negative impact in some countries in the aftermath of the accident.\u201d The IEA noted that at the start of 2014, there were 72 nuclear reactors under construction worldwide, the largest number in 25 years, and that China planned to increase nuclear power capacity from 17 gigawatts (GW) in 2014, to 58 GW in 2020.\nIn 2016, the US Energy Information Administration projected for its \u201cbase case\u201d that world nuclear power generation would increase from 2,344 billion kW-hr in 2012 to 4,501 billion kW-hr in 2040. Most of the predicted increase was expected to be in Asia.\nThe nuclear power industry in western nations has a history of construction delays, cost overruns, plant cancellations, and nuclear safety issues despite significant government subsidies and support. In December 2013, Forbes magazine cited a report which concluded that, in western countries, \"reactors are not a viable source of new power\". Even where they make economic sense, they are not feasible because nuclear\u2019s \"enormous costs, political and popular opposition, and regulatory uncertainty\". This view echoes the statement of former Exelon CEO John Rowe, who said in 2012 that new nuclear plants in the United States \"don\u2019t make any sense right now\" and won\u2019t be economically viable in the foreseeable future. John Quiggin, economics professor, also says the main problem with the nuclear option is that it is not economically-viable. Quiggin says that we need more efficient energy use and more renewable energy commercialization. Former NRC member Peter Bradford and Professor Ian Lowe made similar statements in 2011. However, some \"nuclear cheerleaders\" and lobbyists in the West continue to champion reactors, often with proposed new but largely untested designs, as a source of new power.\nMuch more new build activity is occurring in developing countries like South Korea, India and China. In March 2016, China had 30 reactors in operation, 24 under construction and plans to build more, However, according to a government research unit, China must not build \"too many nuclear power reactors too quickly\", in order to avoid a shortfall of fuel, equipment and qualified plant workers.\nIn the US, licenses of almost half its reactors have been extended to 60 years, Two new Generation III reactors are under construction at Vogtle, a dual construction project which marks the end of a 34-year period of stagnation in the US construction of civil nuclear power reactors. The station operator licenses of almost half the present 104 power reactors in the US, as of 2008, have been given extensions to 60 years. As of 2012, U.S. nuclear industry officials expect five new reactors to enter service by 2020, all at existing plants. In 2013, four aging, uncompetitive, reactors were permanently closed. Relevant state legislatures are trying to close Vermont Yankee and Indian Point Nuclear Power Plant.\nThe U.S. NRC and the U.S. Department of Energy have initiated research into Light water reactor sustainability which is hoped will lead to allowing extensions of reactor licenses beyond 60 years, provided that safety can be maintained, as the loss in non-CO2-emitting generation capacity by retiring reactors \"may serve to challenge U.S. energy security, potentially resulting in increased greenhouse gas emissions, and contributing to an imbalance between electric supply and demand.\"\nThere is a possible impediment to production of nuclear power plants as only a few companies worldwide have the capacity to forge single-piece reactor pressure vessels, which are necessary in the most common reactor designs. Utilities across the world are submitting orders years in advance of any actual need for these vessels. Other manufacturers are examining various options, including making the component themselves, or finding ways to make a similar item using alternate methods.\nAccording to the World Nuclear Association, globally during the 1980s one new nuclear reactor started up every 17 days on average, and in the year 2015 it was estimated that this rate could in theory eventually increase to one every 5 days, although no plans exist for that. As of 2007, Watts Bar 1 in Tennessee, which came on-line on February 7, 1996, was the last U.S. commercial nuclear reactor to go on-line. This is often quoted as evidence of a successful worldwide campaign for nuclear power phase-out. Electricity shortages, fossil fuel price increases, global warming, and heavy metal emissions from fossil fuel use, new technology such as passively safe plants, and national energy security may renew the demand for nuclear power plants.\n\n\n== Nuclear power plant ==\n\nJust as many conventional thermal power stations generate electricity by harnessing the thermal energy released from burning fossil fuels, nuclear power plants convert the energy released from the nucleus of an atom via nuclear fission that takes place in a nuclear reactor. The heat is removed from the reactor core by a cooling system that uses the heat to generate steam, which drives a steam turbine connected to a generator producing electricity.\n\n\n== Life cycle of nuclear fuel ==\n\nA nuclear reactor is only part of the life-cycle for nuclear power. The process starts with mining (see Uranium mining). Uranium mines are underground, open-pit, or in-situ leach mines. In any case, the uranium ore is extracted, usually converted into a stable and compact form such as yellowcake, and then transported to a processing facility. Here, the yellowcake is converted to uranium hexafluoride, which is then enriched using various techniques. At this point, the enriched uranium, containing more than the natural 0.7% U-235, is used to make rods of the proper composition and geometry for the particular reactor that the fuel is destined for. The fuel rods will spend about 3 operational cycles (typically 6 years total now) inside the reactor, generally until about 3% of their uranium has been fissioned, then they will be moved to a spent fuel pool where the short lived isotopes generated by fission can decay away. After about 5 years in a spent fuel pool the spent fuel is radioactively and thermally cool enough to handle, and it can be moved to dry storage casks or reprocessed.\n\n\n=== Conventional fuel resources ===\n\nUranium is a fairly common element in the Earth's crust. Uranium is approximately as common as tin or germanium in the Earth's crust, and is about 40 times more common than silver. Uranium is present in trace concentrations in most rocks, dirt, and ocean water, but can be economically extracted currently only only where it is present in high concentrations. Still, the world's present measured resources of uranium, economically recoverable at the arbitrary price ceiling of 130 USD/kg, are enough to last for between 70 and 100 years.\nAccording to the OECD in 2006, there was an expected 85 years worth of uranium in already identified resources, when that uranium is used in present reactor technology, in the OECD's red book of 2011, due to increased exploration, known uranium resources have grown by 12.5% since 2008, with this increase translating into greater than a century of uranium available if the metals usage rate were to continue at the 2011 level. The OECD also estimate 670 years of economically recoverable uranium in total conventional resources and phosphate ores, while also using present reactor technology, a resource that is recoverable from between 60-100 US$/kg of Uranium. In a similar manner to every other natural metal resource, for every tenfold increase in the cost per kilogram of uranium, there is a three-hundred fold increase in available lower quality ores that would then become economical. As the OECD note:\n\nEven if the nuclear industry expands significantly, sufficient fuel is available for centuries. If advanced breeder reactors could be designed in the future to efficiently utilize recycled or depleted uranium and all actinides, then the resource utilization efficiency would be further improved by an additional factor of eight.\n\nFor example, the OECD have determined that with a pure fast reactor fuel cycle with a burn up of, and recycling of, all the Uranium and actinides, actinides which presently make up the most hazardous substances in nuclear waste, there is 160,000 years worth of Uranium in total conventional resources and phosphate ore, at the price of 60-100 US$/kg of Uranium.\nCurrent light water reactors make relatively inefficient use of nuclear fuel, mostly fissioning only the very rare uranium-235 isotope. Nuclear reprocessing can make this waste reusable, and more efficient reactor designs, such as the currently under construction Generation III reactors achieve a higher efficiency burn up of the available resources, than the current vintage generation II reactors, which make up the vast majority of reactors worldwide.\n\n\n==== Breeding ====\n\nAs opposed to current light water reactors which use uranium-235 (0.7% of all natural uranium), fast breeder reactors use uranium-238 (99.3% of all natural uranium). It has been estimated that there is up to five billion years' worth of uranium-238 for use in these power plants.\nBreeder technology has been used in several reactors, but the high cost of reprocessing fuel safely, at 2006 technological levels, requires uranium prices of more than 200 USD/kg before becoming justified economically. Breeder reactors are still however being pursued as they have the potential to burn up all of the actinides in the present inventory of nuclear waste while also producing power and creating additional quantities of fuel for more reactors via the breeding process. In 2005, there were two breeder reactors producing power: the Ph\u00e9nix in France, which has since powered down in 2009 after 36 years of operation, and the BN-600 reactor, a reactor constructed in 1980 Beloyarsk, Russia which is still operational as of 2013. The electricity output of BN-600 is 600 MW \u2014 Russia plans to expand the nation's use of breeder reactors with the BN-800 reactor, was scheduled to become operational in 2014, but due to delays is not scheduled to produce power until 2017. The technical design of a yet larger breeder, the BN-1200 reactor was originally scheduled to be finalized in 2013, with construction slated for 2015 but has also been delayed. Japan's Monju breeder reactor restarted (having been shut down in 1995) in 2010 for 3 months, but shut down again after equipment fell into the reactor during reactor checkups, it is planned to become re-operational in late 2013. Both China and India are building breeder reactors. With the Indian 500 MWe Prototype Fast Breeder Reactor scheduled to become operational in 2014, with plans to build five more by 2020. The China Experimental Fast Reactor began producing power in 2011.\nAnother alternative to fast breeders is thermal breeder reactors that use uranium-233 bred from thorium as fission fuel in the thorium fuel cycle. Thorium is about 3.5 times more common than uranium in the Earth's crust, and has different geographic characteristics. This would extend the total practical fissionable resource base by 450%. India's three-stage nuclear power programme features the use of a thorium fuel cycle in the third stage, as it has abundant thorium reserves but little uranium.\n\n\n=== Solid waste ===\n\nThe most important waste stream from nuclear power plants is spent nuclear fuel. It is primarily composed of unconverted uranium as well as significant quantities of transuranic actinides (plutonium and curium, mostly). In addition, about 3% of it is fission products from nuclear reactions. The actinides (uranium, plutonium, and curium) are responsible for the bulk of the long-term radioactivity, whereas the fission products are responsible for the bulk of the short-term radioactivity.\n\n\n==== High-level radioactive waste ====\n\nHigh-level radioactive waste management concerns management and disposal of highly radioactive materials created during production of nuclear power. The technical issues in accomplishing this are daunting, due to the extremely long periods radioactive wastes remain deadly to living organisms. Of particular concern are two long-lived fission products, Technetium-99 (half-life 220,000 years) and Iodine-129 (half-life 15.7 million years), which dominate spent nuclear fuel radioactivity after a few thousand years. The most troublesome transuranic elements in spent fuel are Neptunium-237 (half-life two million years) and Plutonium-239 (half-life 24,000 years). Consequently, high-level radioactive waste requires sophisticated treatment and management to successfully isolate it from the biosphere. This usually necessitates treatment, followed by a long-term management strategy involving permanent storage, disposal or transformation of the waste into a non-toxic form.\nGovernments around the world are considering a range of waste management and disposal options, usually involving deep-geologic placement, although there has been limited progress toward implementing long-term waste management solutions. This is partly because the timeframes in question when dealing with radioactive waste range from 10,000 to millions of years, according to studies based on the effect of estimated radiation doses.\nSome proposed nuclear reactor designs however such as the American Integral Fast Reactor and the Molten salt reactor can use the nuclear waste from light water reactors as a fuel, transmutating it to isotopes that would be safe after hundreds, instead of tens of thousands of years. This offers a potentially more attractive alternative to deep geological disposal.\nAnother possibility is the use of thorium in a reactor especially designed for thorium (rather than mixing in thorium with uranium and plutonium (i.e. in existing reactors). Used thorium fuel remains only a few hundreds of years radioactive, instead of tens of thousands of years.\nSince the fraction of a radioisotope's atoms decaying per unit of time is inversely proportional to its half-life, the relative radioactivity of a quantity of buried human radioactive waste would diminish over time compared to natural radioisotopes (such as the decay chains of 120 trillion tons of thorium and 40 trillion tons of uranium which are at relatively trace concentrations of parts per million each over the crust's 3 * 1019 ton mass). For instance, over a timeframe of thousands of years, after the most active short half-life radioisotopes decayed, burying U.S. nuclear waste would increase the radioactivity in the top 2000 feet of rock and soil in the United States (10 million km2) by \u2248 1 part in 10 million over the cumulative amount of natural radioisotopes in such a volume, although the vicinity of the site would have a far higher concentration of artificial radioisotopes underground than such an average.\n\n\n==== Low-level radioactive waste ====\n\nThe nuclear industry also produces a large volume of low-level radioactive waste in the form of contaminated items like clothing, hand tools, water purifier resins, and (upon decommissioning) the materials of which the reactor itself is built. In the US, the Nuclear Regulatory Commission has repeatedly attempted to allow low-level materials to be handled as normal waste: landfilled, recycled into consumer items, etcetera.\n\n\n==== Comparing radioactive waste to industrial toxic waste ====\nIn countries with nuclear power, radioactive wastes comprise less than 1% of total industrial toxic wastes, much of which remains hazardous for long periods. Overall, nuclear power produces far less waste material by volume than fossil-fuel based power plants. Coal-burning plants are particularly noted for producing large amounts of toxic and mildly radioactive ash due to concentrating naturally occurring metals and mildly radioactive material from the coal. A 2008 report from Oak Ridge National Laboratory concluded that coal power actually results in more radioactivity being released into the environment than nuclear power operation, and that the population effective dose equivalent, or dose to the public from radiation from coal plants is 100 times as much as from the operation of nuclear plants. Indeed, coal ash is much less radioactive than spent nuclear fuel on a weight per weight basis, but coal ash is produced in much higher quantities per unit of energy generated, and this is released directly into the environment as fly ash, whereas nuclear plants use shielding to protect the environment from radioactive materials, for example, in dry cask storage vessels.\n\n\n==== Waste disposal ====\nDisposal of nuclear waste is often said to be the Achilles' heel of the industry. Presently, waste is mainly stored at individual reactor sites and there are over 430 locations around the world where radioactive material continues to accumulate. Some experts suggest that centralized underground repositories which are well-managed, guarded, and monitored, would be a vast improvement. There is an \"international consensus on the advisability of storing nuclear waste in deep geological repositories\", with the lack of movement of nuclear waste in the 2 billion year old natural nuclear fission reactors in Oklo, Gabon being cited as \"a source of essential information today.\"\nThere are no commercial scale purpose built underground repositories in operation. The Waste Isolation Pilot Plant (WIPP) in New Mexico has been taking nuclear waste since 1999 from production reactors, but as the name suggests is a research and development facility. A radiation leak at WIPP in 2014 brought renewed attention to the need for R&D on disposal or radioactive waste and spent fuel.\n\n\n=== Reprocessing ===\n\nReprocessing can potentially recover up to 95% of the remaining uranium and plutonium in spent nuclear fuel, putting it into new mixed oxide fuel. This produces a reduction in long term radioactivity within the remaining waste, since this is largely short-lived fission products, and reduces its volume by over 90%. Reprocessing of civilian fuel from power reactors is currently done in Europe, Russia, Japan, and India. The full potential of reprocessing has not been achieved because it requires breeder reactors, which are not commercially available.\nNuclear reprocessing reduces the volume of high-level waste, but by itself does not reduce radioactivity or heat generation and therefore does not eliminate the need for a geological waste repository. Reprocessing has been politically controversial because of the potential to contribute to nuclear proliferation, the potential vulnerability to nuclear terrorism, the political challenges of repository siting (a problem that applies equally to direct disposal of spent fuel), and because of its high cost compared to the once-through fuel cycle. Several different methods for reprocessing been tried, but many have had safety and practicality problems which have led to their discontinuation.\nIn the United States, the Obama administration stepped back from President Bush's plans for commercial-scale reprocessing and reverted to a program focused on reprocessing-related scientific research. Reprocessing is not allowed in the U.S. In the U.S., spent nuclear fuel is currently all treated as waste. A major recommendation of the Blue Ribbon Commission on America's Nuclear Future was that \"the United States should undertake an integrated nuclear waste management program that leads to the timely development of one or more permanent deep geological facilities for the safe disposal of spent fuel and high-level nuclear waste\".\n\n\n==== Depleted uranium ====\n\nUranium enrichment produces many tons of depleted uranium (DU) which consists of U-238 with most of the easily fissile U-235 isotope removed. U-238 is a tough metal with several commercial uses\u2014for example, aircraft production, radiation shielding, and armor\u2014as it has a higher density than lead. Depleted uranium is also controversially used in munitions; DU penetrators (bullets or APFSDS tips) \"self sharpen\", due to uranium's tendency to fracture along shear bands.\n\n\n== Accidents, attacks and safety ==\n\n\n=== Accidents ===\n\nSome serious nuclear and radiation accidents have occurred. Benjamin K. Sovacool has reported that worldwide there have been 99 accidents at nuclear power plants. Fifty-seven accidents have occurred since the Chernobyl disaster, and 57% (56 out of 99) of all nuclear-related accidents have occurred in the USA.\nNuclear power plant accidents include the Chernobyl accident (1986) with approximately 60 deaths so far attributed to the accident and a predicted, eventual total death toll, of from 4000 to 25,000 latent cancers deaths. The Fukushima Daiichi nuclear disaster (2011), has not caused any radiation related deaths, with a predicted, eventual total death toll, of from 0 to 1000, and the Three Mile Island accident (1979), no causal deaths, cancer or otherwise, have been found in follow up studies of this accident. Nuclear-powered submarine mishaps include the K-19 reactor accident (1961), the K-27 reactor accident (1968), and the K-431 reactor accident (1985). International research is continuing into safety improvements such as passively safe plants, and the possible future use of nuclear fusion.\nIn terms of lives lost per unit of energy generated, nuclear power has caused fewer accidental deaths per unit of energy generated than all other major sources of energy generation. Energy produced by coal, petroleum, natural gas and hydropower has caused more deaths per unit of energy generated, from air pollution and energy accidents. This is found in the following comparisons, when the immediate nuclear related deaths from accidents are compared to the immediate deaths from these other energy sources, when the latent, or predicted, indirect cancer deaths from nuclear energy accidents are compared to the immediate deaths from the above energy sources, and when the combined immediate and indirect fatalities from nuclear power and all fossil fuels are compared, fatalities resulting from the mining of the necessary natural resources to power generation and to air pollution. With these data, the use of nuclear power has been calculated to have prevented in the region of 1.8 million deaths between 1971 and 2009, by reducing the proportion of energy that would otherwise have been generated by fossil fuels, and is projected to continue to do so.\nAlthough according to Benjamin K. Sovacool, fission energy accidents ranked first in terms of their total economic cost, accounting for 41 percent of all property damage attributed to energy accidents. Analysis presented in the international Journal, Human and Ecological Risk Assessment found that coal, oil, Liquid petroleum gas and hydroelectric accidents(primarily due to the Banqiao dam burst) have resulted in greater economic impacts than nuclear power accidents.\nFollowing the 2011 Japanese Fukushima nuclear disaster, authorities shut down the nation's 54 nuclear power plants, but it has been estimated that if Japan had never adopted nuclear power, accidents and pollution from coal or gas plants would have caused more lost years of life. As of 2013, the Fukushima site remains highly radioactive, with some 160,000 evacuees still living in temporary housing, and some land will be unfarmable for centuries. The difficult Fukushima disaster cleanup will take 40 or more years, and cost tens of billions of dollars.\nForced evacuation from a nuclear accident may lead to social isolation, anxiety, depression, psychosomatic medical problems, reckless behavior, even suicide. Such was the outcome of the 1986 Chernobyl nuclear disaster in Ukraine. A comprehensive 2005 study concluded that \"the mental health impact of Chernobyl is the largest public health problem unleashed by the accident to date\". Frank N. von Hippel, a U.S. scientist, commented on the 2011 Fukushima nuclear disaster, saying that \"fear of ionizing radiation could have long-term psychological effects on a large portion of the population in the contaminated areas\". A 2015 report in Lancet explained that serious impacts of nuclear accidents were often not directly attributable to radiation exposure, but rather social and psychological effects. Evacuation and long-term displacement of affected populations created problems for many people, especially the elderly and hospital patients. But long-term displacement is not a unique feature to nuclear accidents, with hydropower and lignite surface mining projects routinely displacing thousands during normal, non-accident, operations, e.g Three Gorges Dam resp. Garzweiler surface mine.\n\n\n=== Attacks and sabotage ===\n\nTerrorists could target nuclear power plants in an attempt to release radioactive contamination into the community. The United States 9/11 Commission has said that nuclear power plants were potential targets originally considered for the September 11, 2001 attacks. An attack on a reactor\u2019s spent fuel pool could also be serious, as these pools are less protected than the reactor core. The release of radioactivity could lead to thousands of near-term deaths and greater numbers of long-term fatalities.\nIf nuclear power use is to expand significantly, nuclear facilities will have to be made extremely safe from attacks that could release massive quantities of radioactivity into the community. New reactor designs have features of passive safety, such as the flooding of the reactor core without active intervention by reactor operators. But these safety measures have generally been developed and studied with respect to accidents, not to the deliberate reactor attack by a terrorist group. However, the US Nuclear Regulatory Commission does now also require new reactor license applications to consider security during the design stage. In the United States, the NRC carries out \"Force on Force\" (FOF) exercises at all Nuclear Power Plant (NPP) sites at least once every three years. In the U.S., plants are surrounded by a double row of tall fences which are electronically monitored. The plant grounds are patrolled by a sizeable force of armed guards.\nInsider sabotage regularly occurs, because insiders can observe and work around security measures. Successful insider crimes depended on the perpetrators' observation and knowledge of security vulnerabilities. A fire caused 5\u201310 million dollars worth of damage to New York's Indian Point Energy Center in 1971. The arsonist turned out to be a plant maintenance worker. Sabotage by workers has been reported at many other reactors in the United States: at Zion Nuclear Power Station (1974), Quad Cities Nuclear Generating Station, Peach Bottom Nuclear Generating Station, Fort St. Vrain Generating Station, Trojan Nuclear Power Plant (1974), Browns Ferry Nuclear Power Plant (1980), and Beaver Valley Nuclear Generating Station (1981). Many reactors overseas have also reported sabotage by workers.\n\n\n== Nuclear proliferation ==\nMany technologies and materials associated with the creation of a nuclear power program have a dual-use capability, in that they can be used to make nuclear weapons if a country chooses to do so. When this happens a nuclear power program can become a route leading to a nuclear weapon or a public annex to a \"secret\" weapons program. The concern over Iran's nuclear activities is a case in point.\n\nA fundamental goal for American and global security is to minimize the nuclear proliferation risks associated with the expansion of nuclear power. If this development is \"poorly managed or efforts to contain risks are unsuccessful, the nuclear future will be dangerous\". The Global Nuclear Energy Partnership is one such international effort to create a distribution network in which developing countries in need of energy, would receive nuclear fuel at a discounted rate, in exchange for that nation agreeing to forgo their own indigenous develop of a uranium enrichment program. The France-based Eurodif/European Gaseous Diffusion Uranium Enrichment Consortium was/is one such program that successfully implemented this concept, with Spain and other countries without enrichment facilities buying a share of the fuel produced at the French controlled enrichment facility, but without a transfer of technology. Iran was an early participant from 1974, and remains a shareholder of Eurodif via Sofidif.\nAccording to Benjamin K. Sovacool, a \"number of high-ranking officials, even within the United Nations, have argued that they can do little to stop states using nuclear reactors to produce nuclear weapons\". A 2009 United Nations report said that:\n\nthe revival of interest in nuclear power could result in the worldwide dissemination of uranium enrichment and spent fuel reprocessing technologies, which present obvious risks of proliferation as these technologies can produce fissile materials that are directly usable in nuclear weapons.\n\nOn the other hand, one factor influencing the support of power reactors is due to the appeal that these reactors have at reducing nuclear weapons arsenals through the Megatons to Megawatts Program, a program which eliminated 425 metric tons of highly enriched uranium(HEU), the equivalent of 17,000 nuclear warheads, by diluting it with natural uranium making it equivalent to low enriched uranium(LEU), and thus suitable as nuclear fuel for commercial fission reactors. This is the single most successful non-proliferation program to date.\n\nThe Megatons to Megawatts Program, the brainchild of Thomas Neff of MIT, was hailed as a major success by anti-nuclear weapon advocates as it has largely been the driving force behind the sharp reduction in the quantity of nuclear weapons worldwide since the cold war ended. However without an increase in nuclear reactors and greater demand for fissile fuel, the cost of dismantling and down blending has dissuaded Russia from continuing their disarmament.\nCurrently, according to Harvard professor Matthew Bunn: \"The Russians are not remotely interested in extending the program beyond 2013. We've managed to set it up in a way that costs them more and profits them less than them just making new low-enriched uranium for reactors from scratch. But there are other ways to set it up that would be very profitable for them and would also serve some of their strategic interests in boosting their nuclear exports.\"\nUp to 2005, the Megatons to Megawatts Program had processed $8 billion of HEU/weapons grade uranium into LEU/reactor grade uranium, with that corresponding to the elimination of 10,000 nuclear weapons.\nFor approximately two decades, this material generated nearly 10 percent of all the electricity consumed in the United States (about half of all US nuclear electricity generated) with a total of around 7 trillion kilowatt-hours of electricity produced. Enough energy to energize the entire United States electric grid for about two years. In total it is estimated to have cost $17 billion, a \"bargain for US ratepayers\", with Russia profiting $12 billion from the deal. Much needed profit for the Russian nuclear oversight industry, which after the collapse of the Soviet economy, had difficulties paying for the maintenance and security of the Russian Federations highly enriched uranium and warheads.\nIn April 2012 there were thirty one countries that have civil nuclear power plants, of which nine have nuclear weapons, with the vast majority of these nuclear weapons states having first produced weapons, before commercial fission electricity stations. Moreover, the re-purposing of civilian nuclear industries for military purposes would be a breach of the Non-proliferation treaty, of which 190 countries adhere to.\n\n\n== Environmental issues ==\n\nLife cycle analysis (LCA) of carbon dioxide emissions show nuclear power as comparable to renewable energy sources. Emissions from burning fossil fuels are many times higher.\nAccording to the United Nations (UNSCEAR), regular nuclear power plant operation including the nuclear fuel cycle causes radioisotope releases into the environment amounting to 0.0002 millisieverts (mSv) per year of public exposure as a global average. (Such is small compared to variation in natural background radiation, which averages 2.4 mSv/a globally but frequently varies between 1 mSv/a and 13 mSv/a depending on a person's location as determined by UNSCEAR). As of a 2008 report, the remaining legacy of the worst nuclear power plant accident (Chernobyl) is 0.002 mSv/a in global average exposure (a figure which was 0.04 mSv per person averaged over the entire populace of the Northern Hemisphere in the year of the accident in 1986, although far higher among the most affected local populations and recovery workers).\n\n\n=== Climate change ===\n\nClimate change causing weather extremes such as heat waves, reduced precipitation levels and droughts can have a significant impact on all thermal power station infrastructure, including large biomass-electric and fission-electric stations alike, if cooling in these power stations, namely in the steam condenser is provided by certain freshwater sources. While many thermal stations use indirect seawater cooling or cooling towers that in comparison use little to no freshwater, those that were designed to heat exchange with rivers and lakes, can run into economic problems.\nThis presently infrequent generic problem may become increasingly significant over time. This can force nuclear reactors to be shut down, as happened in France during the 2003 and 2006 heat waves. Nuclear power supply was severely diminished by low river \ufb02ow rates and droughts, which meant rivers had reached the maximum temperatures for cooling reactors. During the heat waves, 17 reactors had to limit output or shut down. 77% of French electricity is produced by nuclear power and in 2009 a similar situation created a 8GW shortage and forced the French government to import electricity. Other cases have been reported from Germany, where extreme temperatures have reduced nuclear power production only 9 times due to high temperatures between 1979 and 2007. In particular:\nthe Unterweser nuclear power plant reduced output by 90% between June and September 2003\nthe Isar nuclear power plant cut production by 60% for 14 days due to excess river temperatures and low stream \ufb02ow in the river Isar in 2006 However the more modern Isar II station did not have to cut production, as unlike its sister station Isar I, Isar II was built with a cooling tower.\nSimilar events have happened elsewhere in Europe during those same hot summers. If global warming continues, this disruption is likely to increase or alternatively, station operators could instead retro-fit other means of cooling, like cooling towers, despite these frequently being large structures and therefore sometimes unpopular with the public.\n\n\n== Comparison with renewable energy ==\n\nAs of 2013, the World Nuclear Association has said \"There is unprecedented interest in renewable energy, particularly solar and wind energy, which provide electricity without giving rise to any carbon dioxide emission. Harnessing these for electricity depends on the cost and efficiency of the technology, which is constantly improving, thus reducing costs per peak kilowatt\".\nRenewable electricity production, from sources such as wind power and solar power, is frequently criticized for being intermittent or variable.\nLike nuclear energy, renewable electricity supply, of primarily hydropower, in the 20-50+% range has already been implemented in several European systems, albeit in the context of an integrated European grid system. In 2012, the share of electricity generated by all types of renewable sources in Germany was 21.9%, compared to 16.0% for nuclear power after Germany shut down 7-8 of its 18 nuclear reactors in 2011. In the United Kingdom, the amount of energy produced from renewable energy is expected to exceed that from nuclear power by 2018, and Scotland plans to obtain all electricity from renewable energy by 2020. The majority of installed renewable energy across the world is in the form of hydro power.\nThe IPCC has said that if governments were supportive, and the full complement of renewable energy technologies were deployed, renewable energy supply could account for almost 80% of the world's energy use within forty years. Rajendra Pachauri, chairman of the IPCC, said the necessary investment in renewables would cost only about 1% of global GDP annually. This approach could contain greenhouse gas levels to less than 450 parts per million, the safe level beyond which climate change becomes catastrophic and irreversible.\nIn 2014, Brookings Institution published The Net Benefits of Low and No-Carbon Electricity Technologies which states, after performing an energy and emissions cost analysis, that \"The net benefits of new nuclear, hydro, and natural gas combined cycle plants far outweigh the net benefits of new wind or solar plants\", with the most cost effective low carbon power technology being determined to be nuclear power.\nSimilarly, analysis in 2015 by Professor and Chair of Environmental Sustainability Barry W. Brook and his colleagues on the topic of replacing fossil fuels entirely, from the electric grid of the world, has determined that at the historically modest and proven-rate at which nuclear energy was added to and replaced fossil fuels in France and Sweden during each nation's building programs in the 1980s, within 10 years nuclear energy could displace or remove fossil fuels from the electric grid completely, \"allow[ing] the world to meet the most stringent greenhouse-gas mitigation targets.\". In a similar analysis, Brook had earlier determined that 50% of all global energy, that is not solely electricity, but transportation synfuels etc. could be generated within approximately 30 years, if the global nuclear fission build rate was identical to each of these nation's already proven decadal rates(in units of installed nameplate capacity, GW per year, per unit of global GDP(GW/year/$).\nThis is in contrast to the completely conceptual paper-studies for a 100% renewable energy world, which would require an orders of magnitude more costly global investment per year, which has no historical precedent, having never been attempted due to its prohibitive cost, along with far greater land that would have to be devoted to the wind, wave and solar projects, and the inherent assumption that humanity will use less, and not more, energy in the future. As Brook notes the \"principal limitations on nuclear fission are not technical, economic or fuel-related, but are instead linked to complex issues of societal acceptance, fiscal and political inertia, and inadequate critical evaluation of the real-world constraints facing [the other] low-carbon alternatives.\"\nWhile the cost of constructing established nuclear power reactor designs has followed an increasing trend due to regulations and court cases whereas the levelized cost of electricity is declining for wind power. In about 2011, wind power became as inexpensive as natural gas, and anti-nuclear groups have suggested that in 2010 solar power became cheaper than nuclear power. Data from the EIA in 2011 estimated that in 2016, solar will have a levelized cost of electricity almost twice that of nuclear (21\u00a2/kWh for solar, 11.39\u00a2/kWh for nuclear), and wind somewhat less (9.7\u00a2/kWh). However, the US EIA has also cautioned that levelized costs of intermittent sources such as wind and solar are not directly comparable to costs of \"dispatchable\" sources (those that can be adjusted to meet demand), as intermittent sources need costly large-scale back-up power supplies for when the weather changes.\nA 2010 study by the Global Subsidies Initiative compared global relative energy subsidies, or government financial aid to different energy sources, with this aid not solely funnelled into research and development but into bribing or \"incentivizing\" utilities to pursue renewable energy systems, over other options. Results show that fossil fuels receive about 1 US cents per kWh of energy they produce, nuclear energy receives 1.7 cents / kWh, renewable energy (excluding hydroelectricity) receives 5.0 cents / kWh and biofuels receive 5.1 cents / kWh in subsidies.\nThere is however no small volume of intensely radioactive spent fuel that needs to be stored or reprocessed with conventional renewable energy sources. A nuclear plant needs to be disassembled and removed. Much of the disassembled nuclear plant needs to be stored as low level nuclear waste for a few decades. However from a safety stand point, nuclear power, in terms of lives lost per unit of electricity delivered, is comparable to and in some cases, lower than many renewable energy sources.\n\n\n== Nuclear decommissioning ==\nThe financial costs of every nuclear power plant continues for some time after the facility has finished generating its last useful electricity. Once no longer economically viable, nuclear reactors and uranium enrichment facilities are generally decommissioned, returning the facility and its parts to a safe enough level to be entrusted for other uses, such as greenfield status. After a cooling-off period that may last decades, reactor core materials are dismantled and cut into small pieces to be packed in containers for interim storage or transmutation experiments. The consensus on how to approach the task is one that is relatively inexpensive, but it has the potential to be hazardous to the natural environment as it presents opportunities for human error, accidents or sabotage.\nIn the USA a Nuclear Waste Policy Act and Nuclear Decommissioning Trust Fund is legally required, with utilities banking 0.1 to 0.2 cents/kWh during operations to fund future decommissioning. They must report regularly to the NRC on the status of their decommissioning funds. About 70% of the total estimated cost of decommissioning all US nuclear power reactors has already been collected (on the basis of the average cost of $320 million per reactor-steam turbine unit).\nIn the U.S. in 2011, there are 13 reactors that had permanently shut down and are in some phase of decommissioning. With Connecticut Yankee Nuclear Power Plant and Yankee Rowe Nuclear Power Station having completed the process in 2006-2007, after ceasing commercial electricity production circa 1992. The majority of the 15 years, was used to allow the station to naturally cool-down on its own, which makes the manual disassembly process both safer and cheaper.Decommissioning at nuclear sites which have experienced a serious accident are the most expensive and time-consuming.\nWorking under an insurance framework that limits or structures accident liabilities in accordance with the Paris convention on nuclear third-party liability, the Brussels supplementary convention, and the Vienna convention on civil liability for nuclear damage and in the U.S. the Price-Anderson Act. It is often argued that this potential shortfall in liability represents an external cost not included in the cost of nuclear electricity; but the cost is small, amounting to about 0.1% of the levelized cost of electricity, according to a CBO study.\nThese beyond-regular-insurance costs for worst-case scenarios are not unique to nuclear power, as hydroelectric power plants are similarly not fully insured against a catastrophic event such as the Banqiao Dam disaster, where 11 million people lost their homes and from 30,000 to 200,000 people died, or large dam failures in general. As private insurers base dam insurance premiums on limited scenarios, major disaster insurance in this sector is likewise provided by the state.\n\n\n== Debate on nuclear power ==\n\nThe nuclear power debate concerns the controversy which has surrounded the deployment and use of nuclear fission reactors to generate electricity from nuclear fuel for civilian purposes. The debate about nuclear power peaked during the 1970s and 1980s, when it \"reached an intensity unprecedented in the history of technology controversies\", in some countries.\nProponents of nuclear energy contend that nuclear power is a sustainable energy source that reduces carbon emissions and increases energy security by decreasing dependence on imported energy sources. Proponents claim that nuclear power produces virtually no conventional air pollution, such as greenhouse gases and smog, in contrast to the chief viable alternative of fossil fuel. Nuclear power can produce base-load power unlike many renewables which are intermittent energy sources lacking large-scale and cheap ways of storing energy. M. King Hubbert saw oil as a resource that would run out, and proposed nuclear energy as a replacement energy source. Proponents claim that the risks of storing waste are small and can be further reduced by using the latest technology in newer reactors, and the operational safety record in the Western world is excellent when compared to the other major kinds of power plants.\nOpponents believe that nuclear power poses many threats to people and the environment. These threats include the problems of processing, transport and storage of radioactive nuclear waste, the risk of nuclear weapons proliferation and terrorism, as well as health risks and environmental damage from uranium mining. They also contend that reactors themselves are enormously complex machines where many things can and do go wrong; and there have been serious nuclear accidents. Critics do not believe that the risks of using nuclear fission as a power source can be fully offset through the development of new technology. They also argue that when all the energy-intensive stages of the nuclear fuel chain are considered, from uranium mining to nuclear decommissioning, nuclear power is neither a low-carbon nor an economical electricity source.\nArguments of economics and safety are used by both sides of the debate.\n\n\n== Use in space ==\n\nBoth fission and fusion appear promising for space propulsion applications, generating higher mission velocities with less reaction mass. This is due to the much higher energy density of nuclear reactions: some 7 orders of magnitude (10,000,000 times) more energetic than the chemical reactions which power the current generation of rockets.\nRadioactive decay has been used on a relatively small scale (few kW), mostly to power space missions and experiments by using radioisotope thermoelectric generators such as those developed at Idaho National Laboratory.\n\n\n== Research ==\n\n\n=== Advanced concepts ===\n\nCurrent fission reactors in operation around the world are second or third generation systems, with most of the first-generation systems having been retired some time ago. Research into advanced generation IV reactor types was officially started by the Generation IV International Forum (GIF) based on eight technology goals, including to improve nuclear safety, improve proliferation resistance, minimize waste, improve natural resource utilization, the ability to consume existing nuclear waste in the production of electricity, and decrease the cost to build and run such plants. Most of these reactors differ significantly from current operating light water reactors, and are generally not expected to be available for commercial construction before 2030.\nThe nuclear reactors to be built at Vogtle are new AP1000 third generation reactors, which are said to have safety improvements over older power reactors. However, John Ma, a senior structural engineer at the NRC, is concerned that some parts of the AP1000 steel skin are so brittle that the \"impact energy\" from a plane strike or storm driven projectile could shatter the wall. Edwin Lyman, a senior staff scientist at the Union of Concerned Scientists, is concerned about the strength of the steel containment vessel and the concrete shield building around the AP1000.\nThe Union of Concerned Scientists has referred to the EPR (nuclear reactor), currently under construction in China, Finland and France, as the only new reactor design under consideration in the United States that \"...appears to have the potential to be significantly safer and more secure against attack than today's reactors.\"\nOne disadvantage of any new reactor technology is that safety risks may be greater initially as reactor operators have little experience with the new design. Nuclear engineer David Lochbaum has explained that almost all serious nuclear accidents have occurred with what was at the time the most recent technology. He argues that \"the problem with new reactors and accidents is twofold: scenarios arise that are impossible to plan for in simulations; and humans make mistakes\". As one director of a U.S. research laboratory put it, \"fabrication, construction, operation, and maintenance of new reactors will face a steep learning curve: advanced technologies will have a heightened risk of accidents and mistakes. The technology may be proven, but people are not\".\n\n\n=== Hybrid nuclear fusion-fission ===\nHybrid nuclear power is a proposed means of generating power by use of a combination of nuclear fusion and fission processes. The concept dates to the 1950s, and was briefly advocated by Hans Bethe during the 1970s, but largely remained unexplored until a revival of interest in 2009, due to delays in the realization of pure fusion. When a sustained nuclear fusion power plant is built, it has the potential to be capable of extracting all the fission energy that remains in spent fission fuel, reducing the volume of nuclear waste by orders of magnitude, and more importantly, eliminating all actinides present in the spent fuel, substances which cause security concerns.\n\n\n=== Nuclear fusion ===\n\nNuclear fusion reactions have the potential to be safer and generate less radioactive waste than fission. These reactions appear potentially viable, though technically quite difficult and have yet to be created on a scale that could be used in a functional power plant. Fusion power has been under theoretical and experimental investigation since the 1950s.\nConstruction of the ITER facility began in 2007, but the project has run into many delays and budget overruns. The facility is now not expected to begin operations until the year 2027 \u2013 11 years after initially anticipated. A follow on commercial nuclear fusion power station, DEMO, has been proposed. There are also suggestions for a power plant based upon a different fusion approach, that of an inertial fusion power plant.\nFusion powered electricity generation was initially believed to be readily achievable, as fission-electric power had been. However, the extreme requirements for continuous reactions and plasma containment led to projections being extended by several decades. In 2010, more than 60 years after the first attempts, commercial power production was still believed to be unlikely before 2050.\n\n\n== See also ==\n\n\n== References ==\n\n\n== Further reading ==\n\nArmstrong, Robert C., Catherine Wolfram, Robert Gross, Nathan S. Lewis, and M.V. Ramana et al. The Frontiers of Energy, Nature Energy, Vol 1, 11 January 2016.\nClarfield, Gerald H. and William M. Wiecek (1984). Nuclear America: Military and Civilian Nuclear Power in the United States 1940-1980, Harper & Row.\nCooke, Stephanie (2009). In Mortal Hands: A Cautionary History of the Nuclear Age, Black Inc.\nCravens, Gwyneth (2007). Power to Save the World: the Truth about Nuclear Energy. New York: Knopf. ISBN 0-307-26656-7. \nElliott, David (2007). Nuclear or Not? Does Nuclear Power Have a Place in a Sustainable Energy Future?, Palgrave.\nFerguson, Charles D., (2007). Nuclear Energy: Balancing Benefits and Risks Council on Foreign Relations.\nHerbst, Alan M. and George W. Hopley (2007). Nuclear Energy Now: Why the Time has come for the World's Most Misunderstood Energy Source, Wiley.\nSchneider, Mycle, Steve Thomas, Antony Froggatt, Doug Koplow (2016). The World Nuclear Industry Status Report: World Nuclear Industry Status as of 1 January 2016.\nWalker, J. Samuel (1992). Containing the Atom: Nuclear Regulation in a Changing Environment, 1993-1971, Berkeley: University of California Press.\nWeart, Spencer R. The Rise of Nuclear Fear. Cambridge, MA: Harvard University Press, 2012. ISBN 0-674-05233-1\n\n\n== External links ==\nAlsos Digital Library for Nuclear Issues \u2014 Annotated Bibliography on Nuclear Power\nAn entry to nuclear power through an educational discussion of reactors\nArgonne National Laboratory\nA cost comparison of nuclear energy to other commercial energy sources\nBriefing Papers from the Australian EnergyScience Coalition\nBritish Energy \u2014 Understanding Nuclear Energy / Nuclear Power\nCoal Combustion: Nuclear Resource or Danger?\nCongressional Research Service report on Nuclear Energy Policy PDF (94.0 KB)\nEnergy Information Administration provides lots of statistics and information\nHow Nuclear Power Works\nIAEA Website The International Atomic Energy Agency\nIAEA's Power Reactor Information System (PRIS)\n\nNuclear Power: Climate Fix or Folly? (2009)\nNuclear Power Education\nNuclear Tourist.com, nuclear power information\nThe World Nuclear Industry Status Reports website\nTED Talk - Bill Gates on energy: Innovating to zero!\nLFTR in 5 Minutes - Creative Commons Film Compares PWR to Th-MSR/LFTR Nuclear Power. on YouTube", 
                "titleUrl": "https://en.wikipedia.org/wiki/Nuclear_power", 
                "title": "Nuclear power"
            }, 
            {
                "snippet": "source. This organization supports U.S. participation in the ITER project through the U.S. ITER Project Office, a partnership of Oak Ridge National Laboratory", 
                "pageCategories": "Science and technology in the United States\nUnited States Department of Energy", 
                "pageContent": "The Office of Science is a component of the United States Department of Energy (DOE). The Office of Science is the lead federal agency supporting fundamental scientific research for energy and the Nation\u2019s largest supporter of basic research in the physical sciences. The Office of Science portfolio has two principal thrusts: direct support of scientific research and direct support of the development, construction, and operation of unique, open-access scientific user facilities that are made available for use by external researchers.\nThe Office of Science manages this research portfolio through six interdisciplinary scientific program offices: Advanced Scientific Computing Research, Basic Energy Sciences, Biological and Environmental Research, Fusion Energy Sciences, High Energy Physics and Nuclear Physics. The Office of Science also has responsibility for 10 of the 17 United States Department of Energy National Laboratories.\nThe office is the predominant U.S. federal government sponsor for research in the physical sciences, including physics, chemistry, computer science, applied mathematics, materials science, nanoscience, and engineering, as well as systems biology and environmental sciences. The Office of Science makes extensive use of peer review and federal advisory committees to develop general directions for research investments, to identify priorities, and to determine the very best scientific proposals to support.\nThe 10 Office of Science national laboratories are: Ames Laboratory, Argonne National Laboratory, Brookhaven National Laboratory, Fermi National Accelerator Laboratory, Lawrence Berkeley National Laboratory, Oak Ridge National Laboratory, Pacific Northwest National Laboratory, Princeton Plasma Physics Laboratory, SLAC National Accelerator Laboratory, and the Thomas Jefferson National Accelerator Facility.\n\n\n== Program offices ==\nThe Office of Science includes six interdisciplinary science program offices:\nAdvanced Scientific Computing Research\nBasic Energy Sciences\nBiological and Environmental Research\nFusion Energy Sciences\nHigh Energy Physics\nNuclear Physics.\n\n\n=== Advanced Scientific Computing Research ===\nThe Office of Advanced Scientific Computing Research (ASCR) supports research and development in applied mathematics, computer science, and integrated network environments. The programs it supports represent the largest and most active computer science research effort within the U.S. federal government. Supercomputer facilities supported by ASCR include the National Energy Research Scientific Computing Center (NERSC) at Lawrence Berkeley National Laboratory in California, and the Leadership Computing Facility at Oak Ridge National Laboratory in Tennessee and Argonne National Laboratory in Illinois. The ASCR supports the Energy Sciences Network (ESnet), which interconnects more than 30 DOE sites at speeds up to 20 gigabits per second.\nESnet is a successor to a network that the Office of Science created in 1974 to connect geographically dispersed researchers through a single network. In the 1980s the Office of Science collaborated with DARPA, NSF and NASA to convert the agencies' separate networks into a single integrated communications network that became the basis for the commercial Internet.\n\n\n=== Biological and Environmental Research ===\nThe Office of Biological and Environmental Research (BER) supports research and scientific user facilities in the biological and environmental sciences to support DOE's missions in energy, environment, and basic research. BER initiated the Human Genome Project in 1986 and has continued to support activity in genomics-based systems biology and initiatives related to biotechnology applications. The Joint Genome Institute, formed in 1997, initially conducted sequencing of human DNA in support of the Human Genome Project. Its current focus is on sequencing the genomes of microbes, microbial communities, fungi, plants, and other organisms.\nEnvironmental efforts include research on the global carbon cycle and possible mitigation of the impacts of climate change. When it started in 1978, BER's Climate Change Research Program was the first U.S. research program to investigate the effects of greenhouse gases on climate and environment. The Office of Science climate change research program is now the third largest in the U.S.\n\n\n=== Fusion Energy Sciences ===\nThe Fusion Energy Sciences (FES) organization supports efforts to expand the fundamental understanding of plasma physics and the knowledge needed to develop a fusion energy source. This organization supports U.S. participation in the ITER project through the U.S. ITER Project Office, a partnership of Oak Ridge National Laboratory and Princeton Plasma Physics Laboratory.\n\n\n== Research funding ==\nMore than 90 percent of the Office of Science budget is allocated to research and scientific facilities. The fundamental research areas in which the Office of Science has programs include physics and other basic energy sciences, biological and environmental sciences, and computational science. Support is provided for research activities in the national laboratories and universities. The office is the principal (or the single largest) source of U.S. federal government support for research in high-energy physics, nuclear physics, fusion energy, materials science, and chemical sciences. The Office of Science is estimated to provide 40 percent of the funding for basic research in the physical sciences in the United States. It is also a major source of funding for government-supported research in climate change, geophysics, genomics, life sciences, and science education.\nIn constant dollars, Office of Science annual budgets for Basic Energy Science and Advanced Scientific Computing nearly doubled between fiscal years 1996 and 2009. Budgets for High Energy Physics and Biological and Energy Research remained relatively constant through that 14-year period. Nuclear Physics and Fusion Energy Sciences budgets were relatively static through most of the period, but had substantial increases in fiscal 2009. The increase in the Fusion budget reinstated the U.S. contribution to ITER, which was reduced significantly in the previous year.\n\n\n== History ==\n\nDOE's Office of Energy Research was a predecessor to the Office of Science.\nIn 2006, the Office of Science was placed under the oversight of the Under Secretary of Energy for Science, a new position created by the Energy Policy Act of 2005.\n\n\n=== Accomplishments and awards ===\nDOE lists 76 Nobel Prize winners as having been associated with Office of Science programs or facilities under DOE and its predecessor agencies.\n\n\n== See also ==\nList of Advanced Scientific Computing Research Leadership Computing Challenge allocations\nOffice of Scientific and Technical Information\n\n\n== References ==\n\n\n== External links ==\nOfficial website", 
                "titleUrl": "https://en.wikipedia.org/wiki/Office_of_Science", 
                "title": "Office of Science"
            }, 
            {
                "snippet": "GLADIS ion beam facility up to 23 MW/m2. Keywords: Tungsten coating; Carbon fibre composite (CFC); ITER-like wall; Magnetron sputtering; Ion implantation\u00a0", 
                "pageCategories": "Fusion power\nMaterials science", 
                "pageContent": "In nuclear fusion power research, the plasma-facing material (or materials) (PFM) is any material used to construct the plasma-facing components (PFC), those components exposed to the plasma within which nuclear fusion occurs, and particularly the material used for the lining or first wall of the reactor vessel.\nFusion reactor designs must consider three overall steps for energy generation:\nGenerating heat through fusion,\nCapturing heat in the first wall,\nTransferring heat at a faster rate than capturing heat.\nCurrently, fusion reactor research focuses on improving efficiency and reliability in heat generation, capture, and rate of transfer. Generating electricity from heat is beyond the scope of current research due to existing efficient heat-transfer cycles, such as heating water to operate steam turbines that drive electrical generators.\nCurrent fusion reactors are fueled by deuterium-tritium (D-T) fusion reactions, which produce high-energy neutrons that can damage the first wall. Tritium is not a commonly available isotope due to its short half-life but can be bred by the nuclear reaction of lithium (Li) isotopes with high-energy neutrons that collide with the first wall.\n\n\n== Requirements ==\nMost magnetic confinement fusion devices (MCFD) consist of several key components in their technical designs, including:\nMagnet system: confines the deuterium-tritium fuel in the form of plasma and in the shape of a torus.\nVacuum vessel: contains the core fusion plasma and maintains fusion conditions.\nFirst wall: positioned between the plasma and magnets in order to protect outer vessel components from radiation damage.\nCooling system: removes heat from the confinement and transfers heat from the first wall.\nThe core fusion plasma must not actually touch the first wall. ITER and many other current and projected fusion experiments, particularly those of the tokamak and stellarator designs, use intense magnetic fields in an attempt to achieve this, although plasma instability problems remain. Even with stable plasma confinement, however, the first wall material would be exposed to a neutron flux higher than in any current nuclear power reactor, which leads to two key problems in selecting the material:\nIt must withstand this neutron flux for a sufficient period of time to be economically viable.\nIt must not become sufficiently radioactive so as to produce unacceptable amounts of nuclear waste when lining replacement or plant decommissioning eventually occurs.\nThe lining material must also:\nAllow the passage of a large heat flux.\nBe compatible with intense and fluctuating magnetic fields.\nMinimize contamination of the plasma.\nBe produced and replaced at a reasonable cost.\nSome critical plasma-facing components, such as and in particular the divertor, are typically protected by a different material than that used for the major area of the first wall.\n\n\n== Proposed materials ==\nMaterials currently in use or under consideration include:\nBoron carbide.\nGraphite.\nCarbon fibre composite (CFC).\nBeryllium.\nTungsten.\nMolybdenum.\nLithium.\nMulti-layer tiles of several of these materials are also being considered and used, for example:\nA thin molybdenum layer on graphite tiles.\nA thin tungsten layer on graphite tiles.\nA tungsten layer on top of a molybdenum layer on graphite tiles.\nA boron carbide layer on top of CFC tiles.\nA liquid lithium layer on graphite tiles.\nA liquid lithium layer on top of a boron layer on graphite tiles.\nA liquid lithium layer on tungsten-based solid PFC surfaces or divertors.\nGraphite was used for the first wall material of the Joint European Torus (JET) at its startup (1983), in Tokamak \u00e0 configuration variable (1992) and in National Spherical Torus Experiment (NSTX, first plasma 1999).\nBeryllium was used to reline JET in 2009 in anticipation of its proposed use in ITER.\nTungsten is used for the divertor in JET, and will be used for the divertor in ITER. It is also used for the first wall in ASDEX Upgrade. Graphite tiles plasma sprayed with tungsten were used for the ASDEX Upgrade divertor.\nMolybdenum is used for the first wall material in Alcator C-Mod (1991).\nLiquid lithium (LL) used to coat the PFC of the Tokamak Fusion Test Reactor (TFTR, 1996).\n\n\n== Considerations ==\nDevelopment of satisfactory plasma-facing materials is one of the key problems still to be solved by current programs.\nPlasma-facing materials can be measured for performance in terms of:\nPower production for a given reactor size.\nCost to generate electricity.\nSelf-sufficiency of tritium production.\nAvailability of materials.\nDesign and fabrication of the PFC.\nSafety in waste disposal and in maintenance.\nThe International Fusion Materials Irradiation Facility (IFMIF) will particularly address this. Materials developed using IFMIF will be used in DEMO, the proposed successor to ITER.\nFrench Nobel laureate in physics, Pierre-Gilles de Gennes said of nuclear fusion, \"We say that we will put the sun into a box. The idea is pretty. The problem is, we don't know how to make the box.\"\n\n\n== Recent developments ==\nSolid plasma-facing materials are known to be susceptible to damage under large heat loads and high neutron flux. If damaged, these solids can contaminate the plasma and decrease plasma confinement stability. In addition, radiation can leak through defections in the solids and contaminate outer vessel components.\nLiquid metal plasma-facing components that enclose the plasma have been proposed to address challenges in the PFC. In particular, liquid lithium (LL) has been confirmed to have various properties that are attractive for fusion reactor performance.\n\n\n== Lithium ==\nLithium (Li) is an alkali metal with a low Z (atomic number). Li has a low first ionization energy of ~ 6 eV and is highly chemically reactive with ion species found in the plasma of fusion reactor cores. In particular, Li readily forms stable lithium compounds with hydrogen isotopes, oxygen, carbon, and other impurities found in D-T plasma.\nThe fusion reaction of D-T produces charged and neutral particles in the plasma. The charged particles remain magnetically confined to as a plasma. The neutral particles are not magnetically confined and will move toward the boundary between the hotter plasma and the colder PFC. Upon reaching the first wall, both neutral particles and charged particles that escaped the plasma become cold neutral particles in gaseous form. An outer edge of cold neutral gas is then \"recycled\", or mixed, with the hotter plasma. A temperature gradient between the cold neutral gas and the hot plasma is believed to be the principal cause of anomalous electron and ion transport from the magnetically confined plasma. As recycling decreases, the temperature gradient decreases and plasma confinement stability increases. With better conditions for fusion in the plasma, the reactor performance increases.\nInitial use of lithium in 1990s was motivated by a need for a low-recycling PFC. In 1996, ~ 0.02 grams of lithium coating was added to the PFC of TFTR, resulting in the fusion power output and the fusion plasma confinement to improve by a factor of two. On the first wall, lithium reacted with neutral particles to produce stable lithium compounds, resulting in low-recycling of cold neutral gas. In addition, lithium contamination in the plasma tended to be well below 1%.\nSince 1996, these results have been confirmed by a large number of magnetic confinement fusion devices (MCFD) that have also used lithium in their PFC, for example:\nTFTR (US), CDX-U (2005)/LTX(2010) (US), CPD (Japan), HT-7 (China), EAST (China), FTU (Italy).\nNSTX (US), T-10 (Russia), T-11M (Russia), TJ-II (Spain), RFX (Italy).\nThe primary energy generation in fusion reactor designs is from the absorption of high-energy neutrons. Results from these MCFD highlight additional benefits of liquid lithium coatings for reliable energy generation, including:\nAbsorb high-energy, or fast-moving, neutrons. About 80% of the energy produced in a fusion reaction of D-T is in the kinetic energy of the newly produced neutron.\nConvert kinetic energies of absorbed neutrons into heat on the first wall. The heat that is produced on the first wall can then be removed by coolants in ancillary systems that generate electricity.\nSelf-sufficient breeding of tritium by nuclear reaction with absorbed neutrons. Neutrons of varying kinetic energies will drive tritium-breeding reactions.\nLiquid lithium\nNewer developments in liquid lithium are currently being tested, for example:\nCoatings made of increasingly complex liquid lithium compounds.\nMulti-layered coatings of LL, B, F, and other low-Z metals.\nHigher density coatings of LL for use on PFC designed for greater heat loads and neutron flux.\n\n\n== See also ==\nInternational Fusion Materials Irradiation Facility#Background information.\nLithium Tokamak Experiment.\n\n\n== References ==\n\n\n== External links ==\nhttp://www.ipp.mpg.de/ippcms/eng/for/projekte/pfmc/index.html Max Planck Institute project page on PFM\nhttp://www.ipp.mpg.de/ippcms/eng/for/veranstaltungen/konferenzen/archiv/2011/03pfmc-13/index.html 13th International Workshop on Plasma-Facing Materials and Components for Fusion Applications / 1st International Conference on Fusion Energy Materials Science\n\"Development of W coatings for fusion applications\". Fusion Engineering and Design. 86: 1677\u20131680. doi:10.1016/j.fusengdes.2011.04.031. Retrieved 10 September 2012. Abstract: The paper gives a short overview on tungsten (W) coatings deposited by various methods on carbon materials (carbon fibre composite \u2013 CFC and fine grain graphite \u2013 FGG). Vacuum Plasma Spray (VPS), Chemical Vapor Deposition (CVD) and Physical Vapor Deposition (PVD)... A particular attention is paid to the Combined Magnetron Sputtering and Ion Implantation (CMSII) technique, which was developed during the last 4 years from laboratory to industrial scale and it is successfully applied for W coating (10\u201315 \u03bcm and 20\u201325 \u03bcm) of more than 2500 tiles for the ITER-like Wall project at JET and ASDEX Upgrade.... Experimentally, W/Mo coatings with a thickness up to 50 \u03bcm were produced and successfully tested in the GLADIS ion beam facility up to 23 MW/m2. Keywords: Tungsten coating; Carbon fibre composite (CFC); ITER-like wall; Magnetron sputtering; Ion implantation", 
                "titleUrl": "https://en.wikipedia.org/wiki/Plasma-facing_material", 
                "title": "Plasma-facing material"
            }
        ], 
        "phraseCharStart": "595"
    }, 
    {
        "phraseCharEnd": "1146", 
        "phraseIndex": "T34", 
        "phraseGoldStandardTag": "Material", 
        "phrase": "carbon", 
        "wikiSearchResults": [
            {
                "snippet": "article is about the chemical element. For other uses, see Carbon (disambiguation). Carbon (from Latin: carbo \"coal\") is a chemical element with symbol", 
                "pageCategories": "All articles with unsourced statements\nArticles containing Latin-language text\nArticles with unsourced statements from April 2016\nArticles with unsourced statements from June 2016\nBiology and pharmacology of chemical elements\nCarbon\nCarbon forms\nCarbonate minerals\nChemical elements\nGood articles", 
                "pageContent": "Carbon (from Latin: carbo \"coal\") is a chemical element with symbol C and atomic number 6. On the periodic table, it is the first (row 2) of six elements in column (group 14), which have in common the composition of their outer electron shell. It is nonmetallic and tetravalent\u2014making four electrons available to form covalent chemical bonds. Three isotopes occur naturally, 12C and 13C being stable while 14C is radioactive, decaying with a half-life of about 5,730 years. Carbon is one of the few elements known since antiquity.\nCarbon is the 15th most abundant element in the Earth's crust, and the fourth most abundant element in the universe by mass after hydrogen, helium, and oxygen. Carbon's abundance, its unique diversity of organic compounds, and its unusual ability to form polymers at the temperatures commonly encountered on Earth enables this element to serve as a common element of all known life. It is the second most abundant element in the human body by mass (about 18.5%) after oxygen.\nThe atoms of carbon can be bonded together in different ways, termed allotropes of carbon. The best known are graphite, diamond, and amorphous carbon. The physical properties of carbon vary widely with the allotropic form. For example, graphite is opaque and black while diamond is highly transparent. Graphite is soft enough to form a streak on paper (hence its name, from the Greek verb \"\u03b3\u03c1\u03ac\u03c6\u03b5\u03b9\u03bd\" which means \"to write\"), while diamond is the hardest naturally-occurring material known. Graphite is a good electrical conductor while diamond has a low electrical conductivity. Under normal conditions, diamond, carbon nanotubes, and graphene have the highest thermal conductivities of all known materials. All carbon allotropes are solids under normal conditions, with graphite being the most thermodynamically stable form. They are chemically resistant and require high temperature to react even with oxygen.\nThe most common oxidation state of carbon in inorganic compounds is +4, while +2 is found in carbon monoxide and transition metal carbonyl complexes. The largest sources of inorganic carbon are limestones, dolomites and carbon dioxide, but significant quantities occur in organic deposits of coal, peat, oil, and methane clathrates. Carbon forms a vast number of compounds, more than any other element, with almost ten million compounds described to date, and yet that number is but a fraction of the number of theoretically possible compounds under standard conditions.\n\n\n== Characteristics ==\n\nThe allotropes of carbon include graphite, one of the softest known substances, and diamond, the hardest naturally occurring substance. It bonds readily with other small atoms including other carbon atoms, and is capable of forming multiple stable covalent bonds with such atoms. Carbon is known to form almost ten million different compounds, a large majority of all chemical compounds. Carbon also has the highest sublimation point of all elements. At atmospheric pressure it has no melting point as its triple point is at 10.8 \u00b1 0.2 MPa and 4,600 \u00b1 300 K (~4,330 \u00b0C or 7,820 \u00b0F), so it sublimes at about 3,900 K.\nCarbon sublimes in a carbon arc which has a temperature of about 5,800 K (5,530 \u00b0C; 9,980 \u00b0F). Thus, irrespective of its allotropic form, carbon remains solid at higher temperatures than the highest melting point metals such as tungsten or rhenium. Although thermodynamically prone to oxidation, carbon resists oxidation more effectively than elements such as iron and copper that are weaker reducing agents at room temperature.\nCarbon compounds form the basis of all known life on Earth, and the carbon-nitrogen cycle provides some of the energy produced by the Sun and other stars. Although it forms an extraordinary variety of compounds, most forms of carbon are comparatively unreactive under normal conditions. At standard temperature and pressure, it resists all but the strongest oxidizers. It does not react with sulfuric acid, hydrochloric acid, chlorine or any alkalis. At elevated temperatures, carbon reacts with oxygen to form carbon oxides, and will rob oxygen from metal oxides to leave the elemental metal. This exothermic reaction is used in the iron and steel industry to smelt iron and to control the carbon content of steel:\nFe\n3O\n4 + 4 C(s) \u2192 3 Fe(s) + 4 CO(g)\nwith sulfur to form carbon disulfide and with steam in the coal-gas reaction:\nC(s) + H2O(g) \u2192 CO(g) + H2(g).\nCarbon combines with some metals at high temperatures to form metallic carbides, such as the iron carbide cementite in steel, and tungsten carbide, widely used as an abrasive and for making hard tips for cutting tools.\nAs of 2009, graphene appears to be the strongest material ever tested. The process of separating it from graphite will require some further technological development before it is economical for industrial processes.\nThe system of carbon allotropes spans a range of extremes:\n\n\n=== Allotropes ===\n\nAtomic carbon is a very short-lived species and, therefore, carbon is stabilized in various multi-atomic structures with different molecular configurations called allotropes. The three relatively well-known allotropes of carbon are amorphous carbon, graphite, and diamond. Once considered exotic, fullerenes are nowadays commonly synthesized and used in research; they include buckyballs, carbon nanotubes, carbon nanobuds and nanofibers. Several other exotic allotropes have also been discovered, such as lonsdaleite (questionable), glassy carbon, carbon nanofoam and linear acetylenic carbon (carbyne).\n\nThe amorphous form is an assortment of carbon atoms in a non-crystalline, irregular, glassy state, which is essentially graphite but not held in a crystalline macrostructure. It is present as a powder, and is the main constituent of substances such as charcoal, lampblack (soot) and activated carbon. At normal pressures, carbon takes the form of graphite, in which each atom is bonded trigonally to three others in a plane composed of fused hexagonal rings, just like those in aromatic hydrocarbons. The resulting network is 2-dimensional, and the resulting flat sheets are stacked and loosely bonded through weak van der Waals forces. This gives graphite its softness and its cleaving properties (the sheets slip easily past one another). Because of the delocalization of one of the outer electrons of each atom to form a \u03c0-cloud, graphite conducts electricity, but only in the plane of each covalently bonded sheet. This results in a lower bulk electrical conductivity for carbon than for most metals. The delocalization also accounts for the energetic stability of graphite over diamond at room temperature.\n\nAt very high pressures, carbon forms the more compact allotrope, diamond, having nearly twice the density of graphite. Here, each atom is bonded tetrahedrally to four others, forming a 3-dimensional network of puckered six-membered rings of atoms. Diamond has the same cubic structure as silicon and germanium, and because of the strength of the carbon-carbon bonds, it is the hardest naturally occurring substance measured by resistance to scratching. Contrary to the popular belief that \"diamonds are forever\", they are thermodynamically unstable under normal conditions and transform into graphite. Due to a high activation energy barrier, the transition into graphite is so slow at normal temperature that it is unnoticeable. Under some conditions, carbon crystallizes as lonsdaleite, a hexagonal crystal lattice with all atoms covalently bonded and properties similar to those of diamond.\nFullerenes are a synthetic crystalline formation with a graphite-like structure, but in place of hexagons, fullerenes are formed of pentagons (or even heptagons) of carbon atoms. The missing (or additional) atoms warp the sheets into spheres, ellipses, or cylinders. The properties of fullerenes (split into buckyballs, buckytubes, and nanobuds) have not yet been fully analyzed and represent an intense area of research in nanomaterials. The names \"fullerene\" and \"buckyball\" are given after Richard Buckminster Fuller, popularizer of geodesic domes, which resemble the structure of fullerenes. The buckyballs are fairly large molecules formed completely of carbon bonded trigonally, forming spheroids (the best-known and simplest is the soccerball-shaped C60 buckminsterfullerene). Carbon nanotubes are structurally similar to buckyballs, except that each atom is bonded trigonally in a curved sheet that forms a hollow cylinder. Nanobuds were first reported in 2007 and are hybrid bucky tube/buckyball materials (buckyballs are covalently bonded to the outer wall of a nanotube) that combine the properties of both in a single structure.\nOf the other discovered allotropes, carbon nanofoam is a ferromagnetic allotrope discovered in 1997. It consists of a low-density cluster-assembly of carbon atoms strung together in a loose three-dimensional web, in which the atoms are bonded trigonally in six- and seven-membered rings. It is among the lightest known solids, with a density of about 2 kg/m3. Similarly, glassy carbon contains a high proportion of closed porosity, but contrary to normal graphite, the graphitic layers are not stacked like pages in a book, but have a more random arrangement. Linear acetylenic carbon has the chemical structure -(C:::C)n-. Carbon in this modification is linear with sp orbital hybridization, and is a polymer with alternating single and triple bonds. This carbyne is of considerable interest to nanotechnology as its Young's modulus is forty times that of the hardest known material \u2013 diamond.\nIn 2015, a team at the North Carolina State University announced the development of another allotrope they have dubbed Q-carbon, created by a high energy low duration laser pulse on amorphous carbon dust. Q-carbon is reported to exhibit ferromagetism, fluorescence, and a hardness superior to diamonds.\n\n\n=== Occurrence ===\n\nCarbon is the fourth most abundant chemical element in the universe by mass after hydrogen, helium, and oxygen. Carbon is abundant in the Sun, stars, comets, and in the atmospheres of most planets. Some meteorites contain microscopic diamonds that were formed when the solar system was still a protoplanetary disk. Microscopic diamonds may also be formed by the intense pressure and high temperature at the sites of meteorite impacts.\nIn 2014 NASA announced a greatly upgraded database for tracking polycyclic aromatic hydrocarbons (PAHs) in the universe. More than 20% of the carbon in the universe may be associated with PAHs, complex compounds of carbon and hydrogen without oxygen. These compounds figure in the PAH world hypothesis where they are hypothesized to have a role in abiogenesis and formation of life. PAHs seem to have been formed \"a couple of billion years\" after the Big Bang, are widespread throughout the universe, and are associated with new stars and exoplanets.\nIt has been estimated that the solid earth as a whole contains 730 ppm of carbon, with 2000 ppm in the core and 120 ppm in the combined mantle and crust. Since the mass of the earth is 7024597200000000000\u26605.972\u00d71024 kg, this would imply 4360 million gigatonnes of carbon. This is much more than the amount of carbon in the oceans or atmosphere (below).\nIn combination with oxygen in carbon dioxide, carbon is found in the Earth's atmosphere (approximately 810 gigatonnes of carbon) and dissolved in all water bodies (approximately 36,000 gigatonnes of carbon). Around 1,900 gigatonnes of carbon are present in the biosphere. Hydrocarbons (such as coal, petroleum, and natural gas) contain carbon as well. Coal \"reserves\" (not \"resources\") amount to around 900 gigatonnes with perhaps 18 000 Gt of resources. Oil reserves are around 150 gigatonnes. Proven sources of natural gas are about 175 1012 cubic metres (containing about 105 gigatonnes of carbon), but studies estimate another 900 1012 cubic metres of \"unconventional\" deposits such as shale gas, representing about 540 gigatonnes of carbon.\nCarbon is also found in methane hydrates in polar regions and under the seas. Various estimates put this carbon between 500, 2500 Gt, or 3000 Gt.\nIn the past, quantities of hydrocarbons were greater. According to one source, in the period from 1751 to 2008 about 347 gigatonnes of carbon were released as carbon dioxide to the atmosphere from burning of fossil fuels. Another source puts the amount added to the atmosphere for the period since 1750 at 879 Gt, and the total going to the atmosphere, sea, and land (such as peat bogs) at almost 2000 Gt.\nCarbon is a constituent (about 12% by mass) of the very large masses of carbonate rock (limestone, dolomite, marble and so on). Coal is very rich in carbon (anthracite contains 92\u201398%) and is the largest commercial source of mineral carbon, accounting for 4,000 gigatonnes or 80% of fossil fuel.\nAs for individual carbon allotropes, graphite is found in large quantities in the United States (mostly in New York and Texas), Russia, Mexico, Greenland, and India. Natural diamonds occur in the rock kimberlite, found in ancient volcanic \"necks\", or \"pipes\". Most diamond deposits are in Africa, notably in South Africa, Namibia, Botswana, the Republic of the Congo, and Sierra Leone. Diamond deposits have also been found in Arkansas, Canada, the Russian Arctic, Brazil, and in Northern and Western Australia. Diamonds are now also being recovered from the ocean floor off the Cape of Good Hope. Diamonds are found naturally, but about 30% of all industrial diamonds used in the U.S. are now manufactured.\nCarbon-14 is formed in upper layers of the troposphere and the stratosphere at altitudes of 9\u201315 km by a reaction that is precipitated by cosmic rays. Thermal neutrons are produced that collide with the nuclei of nitrogen-14, forming carbon-14 and a proton.\nCarbon-rich asteroids are relatively preponderant in the outer parts of the asteroid belt in our solar system. These asteroids have not yet been directly sampled by scientists. The asteroids can be used in hypothetical space-based carbon mining, which may be possible in the future, but is currently technologically impossible.\n\n\n=== Isotopes ===\n\nIsotopes of carbon are atomic nuclei that contain six protons plus a number of neutrons (varying from 2 to 16). Carbon has two stable, naturally occurring isotopes. The isotope carbon-12 (12C) forms 98.93% of the carbon on Earth, while carbon-13 (13C) forms the remaining 1.07%. The concentration of 12C is further increased in biological materials because biochemical reactions discriminate against 13C. In 1961, the International Union of Pure and Applied Chemistry (IUPAC) adopted the isotope carbon-12 as the basis for atomic weights. Identification of carbon in nuclear magnetic resonance (NMR) experiments is done with the isotope 13C.\nCarbon-14 (14C) is a naturally occurring radioisotope, created in the upper atmosphere (lower stratosphere and upper troposphere) by interaction of nitrogen with cosmic rays. It is found in trace amounts on Earth of up to 1 part per trillion (0.0000000001%), mostly confined to the atmosphere and superficial deposits, particularly of peat and other organic materials. This isotope decays by 0.158 MeV \u03b2\u2212 emission. Because of its relatively short half-life of 5730 years, 14C is virtually absent in ancient rocks. The amount of 14C in the atmosphere and in living organisms is almost constant, but decreases predictably in their bodies after death. This principle is used in radiocarbon dating, invented in 1949, which has been used extensively to determine the age of carbonaceous materials with ages up to about 40,000 years.\nThere are 15 known isotopes of carbon and the shortest-lived of these is 8C which decays through proton emission and alpha decay and has a half-life of 1.98739x10\u221221 s. The exotic 19C exhibits a nuclear halo, which means its radius is appreciably larger than would be expected if the nucleus were a sphere of constant density.\n\n\n=== Formation in stars ===\n\nFormation of the carbon atomic nucleus requires a nearly simultaneous triple collision of alpha particles (helium nuclei) within the core of a giant or supergiant star which is known as the triple-alpha process, as the products of further nuclear fusion reactions of helium with hydrogen or another helium nucleus produce lithium-5 and beryllium-8 respectively, both of which are highly unstable and decay almost instantly back into smaller nuclei. This happens in conditions of temperatures over 100 megakelvin and helium concentration that the rapid expansion and cooling of the early universe prohibited, and therefore no significant carbon was created during the Big Bang.\nAccording to current physical cosmology theory, carbon is formed in the interiors of stars in the horizontal branch by the collision and transformation of three helium nuclei. When those stars die as supernova, the carbon is scattered into space as dust. This dust becomes component material for the formation of second or third-generation star systems with accreted planets. The Solar System is one such star system with an abundance of carbon, enabling the existence of life as we know it.\nThe CNO cycle is an additional fusion mechanisms that powers stars, wherein carbon operates as a catalyst.\nRotational transitions of various isotopic forms of carbon monoxide (for example, 12CO, 13CO, and 18CO) are detectable in the submillimeter wavelength range, and are used in the study of newly forming stars in molecular clouds.\n\n\n=== Carbon cycle ===\n\nUnder terrestrial conditions, conversion of one element to another is very rare. Therefore, the amount of carbon on Earth is effectively constant. Thus, processes that use carbon must obtain it from somewhere and dispose of it somewhere else. The paths of carbon in the environment form the carbon cycle. For example, photosynthetic plants draw carbon dioxide from the atmosphere (or seawater) and build it into biomass, as in the Calvin cycle, a process of carbon fixation. Some of this biomass is eaten by animals, while some carbon is exhaled by animals as carbon dioxide. The carbon cycle is considerably more complicated than this short loop; for example, some carbon dioxide is dissolved in the oceans; if bacteria do not consume it, dead plant or animal matter may become petroleum or coal, which releases carbon when burned.\n\n\n== Compounds ==\n\n\n=== Organic compounds ===\n\nCarbon can form very long chains of interconnecting C-C bonds, a property that is called catenation. Carbon-carbon bonds are strong and stable. Through catenation, carbon forms a countless number of compounds. A tally of unique compounds shows that more contain carbon that those that do not. A similar claim can be made for hydrogen because most organic compounds also contain hydrogen.\nThe simplest form of an organic molecule is the hydrocarbon\u2014a large family of organic molecules that are composed of hydrogen atoms bonded to a chain of carbon atoms. Chain length, side chains and functional groups all affect the properties of organic molecules.\nCarbon occurs in all known organic life and is the basis of organic chemistry. When united with hydrogen, it forms various hydrocarbons that are important to industry as refrigerants, lubricants, solvents, as chemical feedstock for the manufacture of plastics and petrochemicals, and as fossil fuels.\nWhen combined with oxygen and hydrogen, carbon can form many groups of important biological compounds including sugars, lignans, chitins, alcohols, fats, and aromatic esters, carotenoids and terpenes. With nitrogen it forms alkaloids, and with the addition of sulfur also it forms antibiotics, amino acids, and rubber products. With the addition of phosphorus to these other elements, it forms DNA and RNA, the chemical-code carriers of life, and adenosine triphosphate (ATP), the most important energy-transfer molecule in all living cells.\n\n\n=== Inorganic compounds ===\nCommonly carbon-containing compounds which are associated with minerals or which do not contain hydrogen or fluorine, are treated separately from classical organic compounds; the definition is not rigid (see reference articles above). Among these are the simple oxides of carbon. The most prominent oxide is carbon dioxide (CO2). This was once the principal constituent of the paleoatmosphere, but is a minor component of the Earth's atmosphere today. Dissolved in water, it forms carbonic acid (H\n2CO\n3), but as most compounds with multiple single-bonded oxygens on a single carbon it is unstable. Through this intermediate, though, resonance-stabilized carbonate ions are produced. Some important minerals are carbonates, notably calcite. Carbon disulfide (CS\n2) is similar.\nThe other common oxide is carbon monoxide (CO). It is formed by incomplete combustion, and is a colorless, odorless gas. The molecules each contain a triple bond and are fairly polar, resulting in a tendency to bind permanently to hemoglobin molecules, displacing oxygen, which has a lower binding affinity. Cyanide (CN\u2212), has a similar structure, but behaves much like a halide ion (pseudohalogen). For example, it can form the nitride cyanogen molecule ((CN)2), similar to diatomic halides. Other uncommon oxides are carbon suboxide (C\n3O\n2), the unstable dicarbon monoxide (C2O), carbon trioxide (CO3), cyclopentanepentone (C5O5), cyclohexanehexone (C6O6), and mellitic anhydride (C12O9).\nWith reactive metals, such as tungsten, carbon forms either carbides (C4\u2212), or acetylides (C2\u2212\n2) to form alloys with high melting points. These anions are also associated with methane and acetylene, both very weak acids. With an electronegativity of 2.5, carbon prefers to form covalent bonds. A few carbides are covalent lattices, like carborundum (SiC), which resembles diamond.\n\n\n=== Organometallic compounds ===\n\nOrganometallic compounds by definition contain at least one carbon-metal bond. A wide range of such compounds exist; major classes include simple alkyl-metal compounds (for example, tetraethyllead), \u03b72-alkene compounds (for example, Zeise's salt), and \u03b73-allyl compounds (for example, allylpalladium chloride dimer); metallocenes containing cyclopentadienyl ligands (for example, ferrocene); and transition metal carbene complexes. Many metal carbonyls exist (for example, tetracarbonylnickel); some workers consider the carbon monoxide ligand to be purely inorganic, and not organometallic.\nWhile carbon is understood to exclusively form four bonds, an interesting compound containing an octahedral hexacoordinated carbon atom has been reported. The cation of the compound is [(Ph3PAu)6C]2+. This phenomenon has been attributed to the aurophilicity of the gold ligands.\n\n\n== History and etymology ==\n\nThe English name carbon comes from the Latin carbo for coal and charcoal, whence also comes the French charbon, meaning charcoal. In German, Dutch and Danish, the names for carbon are Kohlenstoff, koolstof and kulstof respectively, all literally meaning coal-substance.\nCarbon was discovered in prehistory and was known in the forms of soot and charcoal to the earliest human civilizations. Diamonds were known probably as early as 2500 BCE in China, while carbon in the form of charcoal was made around Roman times by the same chemistry as it is today, by heating wood in a pyramid covered with clay to exclude air.\n\nIn 1722, Ren\u00e9 Antoine Ferchault de R\u00e9aumur demonstrated that iron was transformed into steel through the absorption of some substance, now known to be carbon. In 1772, Antoine Lavoisier showed that diamonds are a form of carbon; when he burned samples of charcoal and diamond and found that neither produced any water and that both released the same amount of carbon dioxide per gram. In 1779, Carl Wilhelm Scheele showed that graphite, which had been thought of as a form of lead, was instead identical with charcoal but with a small admixture of iron, and that it gave \"aerial acid\" (his name for carbon dioxide) when oxidized with nitric acid. In 1786, the French scientists Claude Louis Berthollet, Gaspard Monge and C. A. Vandermonde confirmed that graphite was mostly carbon by oxidizing it in oxygen in much the same way Lavoisier had done with diamond. Some iron again was left, which the French scientists thought was necessary to the graphite structure. In their publication they proposed the name carbone (Latin carbonum) for the element in graphite which was given off as a gas upon burning graphite. Antoine Lavoisier then listed carbon as an element in his 1789 textbook.\nA new allotrope of carbon, fullerene, that was discovered in 1985 includes nanostructured forms such as buckyballs and nanotubes. Their discoverers \u2013 Robert Curl, Harold Kroto and Richard Smalley \u2013 received the Nobel Prize in Chemistry in 1996. The resulting renewed interest in new forms lead to the discovery of further exotic allotropes, including glassy carbon, and the realization that \"amorphous carbon\" is not strictly amorphous.\n\n\n== Production ==\n\n\n=== Graphite ===\n\nCommercially viable natural deposits of graphite occur in many parts of the world, but the most important sources economically are in China, India, Brazil and North Korea. Graphite deposits are of metamorphic origin, found in association with quartz, mica and feldspars in schists, gneisses and metamorphosed sandstones and limestone as lenses or veins, sometimes of a metre or more in thickness. Deposits of graphite in Borrowdale, Cumberland, England were at first of sufficient size and purity that, until the 19th century, pencils were made simply by sawing blocks of natural graphite into strips before encasing the strips in wood. Today, smaller deposits of graphite are obtained by crushing the parent rock and floating the lighter graphite out on water.\nThere are three types of natural graphite\u2014amorphous, flake or crystalline flake, and vein or lump. Amorphous graphite is the lowest quality and most abundant. Contrary to science, in industry \"amorphous\" refers to very small crystal size rather than complete lack of crystal structure. Amorphous is used for lower value graphite products and is the lowest priced graphite. Large amorphous graphite deposits are found in China, Europe, Mexico and the United States. Flake graphite is less common and of higher quality than amorphous; it occurs as separate plates that crystallized in metamorphic rock. Flake graphite can be four times the price of amorphous. Good quality flakes can be processed into expandable graphite for many uses, such as flame retardants. The foremost deposits are found in Austria, Brazil, Canada, China, Germany and Madagascar. Vein or lump graphite is the rarest, most valuable, and highest quality type of natural graphite. It occurs in veins along intrusive contacts in solid lumps, and it is only commercially mined in Sri Lanka.\nAccording to the USGS, world production of natural graphite was 1.1 million tonnes in 2010, to which China contributed 800,000 t, India 130,000 t, Brazil 76,000 t, North Korea 30,000 t and Canada 25,000 t. No natural graphite was reported mined in the United States, but 118,000 t of synthetic graphite with an estimated value of $998 million was produced in 2009.\n\n\n=== Diamond ===\n\nThe diamond supply chain is controlled by a limited number of powerful businesses, and is also highly concentrated in a small number of locations around the world (see figure).\nOnly a very small fraction of the diamond ore consists of actual diamonds. The ore is crushed, during which care has to be taken in order to prevent larger diamonds from being destroyed in this process and subsequently the particles are sorted by density. Today, diamonds are located in the diamond-rich density fraction with the help of X-ray fluorescence, after which the final sorting steps are done by hand. Before the use of X-rays became commonplace, the separation was done with grease belts; diamonds have a stronger tendency to stick to grease than the other minerals in the ore.\nHistorically diamonds were known to be found only in alluvial deposits in southern India. India led the world in diamond production from the time of their discovery in approximately the 9th century BCE to the mid-18th century AD, but the commercial potential of these sources had been exhausted by the late 18th century and at that time India was eclipsed by Brazil where the first non-Indian diamonds were found in 1725.\nDiamond production of primary deposits (kimberlites and lamproites) only started in the 1870s after the discovery of the Diamond fields in South Africa. Production has increased over time and now an accumulated total of 4.5 billion carats have been mined since that date. About 20% of that amount has been mined in the last 5 years alone, and during the last ten years 9 new mines have started production while 4 more are waiting to be opened soon. Most of these mines are located in Canada, Zimbabwe, Angola, and one in Russia.\nIn the United States, diamonds have been found in Arkansas, Colorado and Montana. In 2004, a startling discovery of a microscopic diamond in the United States led to the January 2008 bulk-sampling of kimberlite pipes in a remote part of Montana.\nToday, most commercially viable diamond deposits are in Russia, Botswana, Australia and the Democratic Republic of Congo. In 2005, Russia produced almost one-fifth of the global diamond output, reports the British Geological Survey. Australia has the richest diamantiferous pipe with production reaching peak levels of 42 metric tons (41 long tons; 46 short tons) per year in the 1990s. There are also commercial deposits being actively mined in the Northwest Territories of Canada, Siberia (mostly in Yakutia territory; for example, Mir pipe and Udachnaya pipe), Brazil, and in Northern and Western Australia.\n\n\n== Applications ==\n\nCarbon is essential to all known living systems, and without it life as we know it could not exist (see alternative biochemistry). The major economic use of carbon other than food and wood is in the form of hydrocarbons, most notably the fossil fuel methane gas and crude oil (petroleum). Crude oil is distilled in refineries by the petrochemical industry to produce gasoline, kerosene, and other products. Cellulose is a natural, carbon-containing polymer produced by plants in the form of wood, cotton, linen, and hemp. Cellulose is used primarily for maintaining structure in plants. Commercially valuable carbon polymers of animal origin include wool, cashmere and silk. Plastics are made from synthetic carbon polymers, often with oxygen and nitrogen atoms included at regular intervals in the main polymer chain. The raw materials for many of these synthetic substances come from crude oil.\nThe uses of carbon and its compounds are extremely varied. It can form alloys with iron, of which the most common is carbon steel. Graphite is combined with clays to form the 'lead' used in pencils used for writing and drawing. It is also used as a lubricant and a pigment, as a molding material in glass manufacture, in electrodes for dry batteries and in electroplating and electroforming, in brushes for electric motors and as a neutron moderator in nuclear reactors.\nCharcoal is used as a drawing material in artwork, barbecue grilling, iron smelting, and in many other applications. Wood, coal and oil are used as fuel for production of energy and heating. Gem quality diamond is used in jewelry, and industrial diamonds are used in drilling, cutting and polishing tools for machining metals and stone. Plastics are made from fossil hydrocarbons, and carbon fiber, made by pyrolysis of synthetic polyester fibers is used to reinforce plastics to form advanced, lightweight composite materials.\nCarbon fiber is made by pyrolysis of extruded and stretched filaments of polyacrylonitrile (PAN) and other organic substances. The crystallographic structure and mechanical properties of the fiber depend on the type of starting material, and on the subsequent processing. Carbon fibers made from PAN have structure resembling narrow filaments of graphite, but thermal processing may re-order the structure into a continuous rolled sheet. The result is fibers with higher specific tensile strength than steel.\nCarbon black is used as the black pigment in printing ink, artist's oil paint and water colours, carbon paper, automotive finishes, India ink and laser printer toner. Carbon black is also used as a filler in rubber products such as tyres and in plastic compounds. Activated charcoal is used as an absorbent and adsorbent in filter material in applications as diverse as gas masks, water purification, and kitchen extractor hoods, and in medicine to absorb toxins, poisons, or gases from the digestive system. Carbon is used in chemical reduction at high temperatures. Coke is used to reduce iron ore into iron (smelting). Case hardening of steel is achieved by heating finished steel components in carbon powder. Carbides of silicon, tungsten, boron and titanium, are among the hardest known materials, and are used as abrasives in cutting and grinding tools. Carbon compounds make up most of the materials used in clothing, such as natural and synthetic textiles and leather, and almost all of the interior surfaces in the built environment other than glass, stone and metal.\n\n\n=== Diamonds ===\nThe diamond industry falls into two categories: one dealing with gem-grade diamonds and the other, with industrial-grade diamonds. While a large trade in both types of diamonds exists, the two markets act in dramatically different ways.\nUnlike precious metals such as gold or platinum, gem diamonds do not trade as a commodity: there is a substantial mark-up in the sale of diamonds, and there is not a very active market for resale of diamonds.\nIndustrial diamonds are valued mostly for their hardness and heat conductivity, with the gemological qualities of clarity and color being mostly irrelevant. About 80% of mined diamonds (equal to about 100 million carats or 20 tonnes annually) are unsuitable for use as gemstones are relegated for industrial use (known as bort). synthetic diamonds, invented in the 1950s, found almost immediate industrial applications; 3 billion carats (600 tonnes) of synthetic diamond is produced annually.\nThe dominant industrial use of diamond is in cutting, drilling, grinding, and polishing. Most of these applications do not require large diamonds; in fact, most diamonds of gem-quality except for their small size can be used industrially. Diamonds are embedded in drill tips or saw blades, or ground into a powder for use in grinding and polishing applications. Specialized applications include use in laboratories as containment for high pressure experiments (see diamond anvil cell), high-performance bearings, and limited use in specialized windows. With the continuing advances in the production of synthetic diamonds, new applications are becoming feasible. Garnering much excitement is the possible use of diamond as a semiconductor suitable for microchips, and because of its exceptional heat conductance property, as a heat sink in electronics.\n\n\n== Precautions ==\n\nPure carbon has extremely low toxicity to humans and can be handled and even ingested safely in the form of graphite or charcoal. It is resistant to dissolution or chemical attack, even in the acidic contents of the digestive tract. Consequently, once it enters into the body's tissues it is likely to remain there indefinitely. Carbon black was probably one of the first pigments to be used for tattooing, and \u00d6tzi the Iceman was found to have carbon tattoos that survived during his life and for 5200 years after his death. Inhalation of coal dust or soot (carbon black) in large quantities can be dangerous, irritating lung tissues and causing the congestive lung disease, coalworker's pneumoconiosis. Diamond dust used as an abrasive can harmful if ingested or inhaled. Microparticles of carbon are produced in diesel engine exhaust fumes, and may accumulate in the lungs. In these examples, the harm may result from contaminants (e.g., organic chemicals, heavy metals) rather than from the carbon itself.\nCarbon generally has low toxicity to life on Earth; but carbon nanoparticles are deadly to Drosophila.\nCarbon may burn vigorously and brightly in the presence of air at high temperatures. Large accumulations of coal, which have remained inert for hundreds of millions of years in the absence of oxygen, may spontaneously combust when exposed to air in coal mine waste tips, ship cargo holds and coal bunkers, and storage dumps.\nIn nuclear applications where graphite is used as a neutron moderator, accumulation of Wigner energy followed by a sudden, spontaneous release may occur. Annealing to at least 250 \u00b0C can release the energy safely, although in the Windscale fire the procedure went wrong, causing other reactor materials to combust.\nThe great variety of carbon compounds include such lethal poisons as tetrodotoxin, the lectin ricin from seeds of the castor oil plant Ricinus communis, cyanide (CN\u2212), and carbon monoxide; and such essentials to life as glucose and protein.\n\n\n== Bonding to carbon ==\n\n\n== See also ==\nCarbon chauvinism\nCarbon footprint\nLow-carbon economy\nTimeline of carbon nanotubes\n\n\n== References ==\n\n\n== External links ==\n\nCarbon on In Our Time at the BBC. (listen now)\nCarbon at The Periodic Table of Videos (University of Nottingham)\nCarbon on Britannica\nExtensive Carbon page at asu.edu\nElectrochemical uses of carbon\nCarbon\u2014Super Stuff. Animation with sound and interactive 3D-models.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Carbon", 
                "title": "Carbon"
            }, 
            {
                "snippet": "Radiocarbon dating (also referred to as carbon dating or carbon-14 dating) is a method for determining the age of an object containing organic material", 
                "pageCategories": "All Wikipedia articles written in British (Oxford) English\nAmerican inventions\nCS1 maint: Explicit use of et al.\nCarbon\nConservation and restoration\nFeatured articles\nIsotopes of carbon\nRadioactivity\nRadiocarbon dating\nRadiometric dating", 
                "pageContent": "Radiocarbon dating (also referred to as carbon dating or carbon-14 dating) is a method for determining the age of an object containing organic material by using the properties of radiocarbon (14C), a radioactive isotope of carbon.\nThe method was developed by Willard Libby in the late 1940s and soon became a standard tool for archaeologists. Libby received the Nobel Prize for his work in 1960. The radiocarbon dating method is based on the fact that radiocarbon is constantly being created in the atmosphere by the interaction of cosmic rays with atmospheric nitrogen. The resulting radiocarbon combines with atmospheric oxygen to form radioactive carbon dioxide, which is incorporated into plants by photosynthesis; animals then acquire 14C by eating the plants. When the animal or plant dies, it stops exchanging carbon with its environment, and from that point onwards the amount of 14C it contains begins to decrease as the 14C undergoes radioactive decay. Measuring the amount of 14C in a sample from a dead plant or animal such as a piece of wood or a fragment of bone provides information that can be used to calculate when the animal or plant died. The older a sample is, the less 14C there is to be detected, and because the half-life of 14C (the period of time after which half of a given sample will have decayed) is about 5,730 years, the oldest dates that can be reliably measured by radiocarbon dating are around 50,000 years ago, although special preparation methods occasionally permit dating of older samples.\nThe idea behind radiocarbon dating is straightforward, but years of work were required to develop the technique to the point where accurate dates could be obtained. Research has been ongoing since the 1960s to determine what the proportion of 14C in the atmosphere has been over the past fifty thousand years. The resulting data, in the form of a calibration curve, is now used to convert a given measurement of radiocarbon in a sample into an estimate of the sample's calendar age. Other corrections must be made to account for the proportion of 14C in different types of organisms (fractionation), and the varying levels of 14C throughout the biosphere (reservoir effects). Additional complications come from the burning of fossil fuels such as coal and oil, and from the above-ground nuclear tests done in the 1950s and 1960s. Because the time it takes to convert biological materials to fossil fuels is substantially longer than the time it takes for its 14C to decay below detectable levels, they contain almost no 14C, and as a result there was a noticeable drop in the proportion of 14C in the atmosphere beginning in the late 19th century. Conversely, nuclear testing increased the amount of 14C in the atmosphere, which attained a maximum in 1963 of almost twice what it had been before the testing began.\nMeasurement of radiocarbon was originally done by beta-counting devices, which counted the amount of beta radiation emitted by decaying 14C atoms in a sample. More recently, accelerator mass spectrometry has become the method of choice; it counts all the 14C atoms in the sample and not just the few that happen to decay during the measurements; it can therefore be used with much smaller samples (as small as individual plant seeds), and gives results much more quickly. The development of radiocarbon dating has had a profound impact on archaeology. In addition to permitting more accurate dating within archaeological sites than previous methods, it allows comparison of dates of events across great distances. Histories of archaeology often refer to its impact as the \"radiocarbon revolution\". Radiocarbon dating has allowed key transitions in prehistory to be dated, such as the end of the last ice age, and the beginning of the Neolithic and Bronze Age in different regions.\n\n\n== Background ==\n\n\n=== History ===\nIn 1939, Martin Kamen and Samuel Ruben of the Radiation Laboratory at Berkeley began experiments to determine if any of the elements common in organic matter had isotopes with half-lives long enough to be of value in biomedical research. They synthesized 14C using the laboratory's cyclotron accelerator and soon discovered that the atom's half-life was far longer than had been previously thought. This was followed by a prediction by Serge A. Korff of New York University that the interaction of slow neutrons with 14N in the atmosphere was the main pathway by which 14C was created in nature. It had previously been thought that 14C would be more likely to be created by deuterons interacting with 13C. At some time during World War II, Willard Libby, who was then at Berkeley, learned of Korff's research; it was this paper that gave Libby the idea that radiocarbon dating might be possible.\nIn 1945, Libby moved to the University of Chicago where he began his work on radiocarbon dating. He published a paper in 1946 in which he proposed that the carbon in living matter might include 14C as well as non-radioactive carbon. Libby and several collaborators proceeded to experiment with methane collected from sewage works in Baltimore, and after isotopically enriching their samples they were able to demonstrate that they contained radioactive 14C. By contrast, methane created from petroleum showed no radiocarbon activity because of its age. The results were summarized in a paper in Science in 1947, in which the authors commented that their results implied it would be possible to date materials containing carbon of organic origin.\nLibby and James Arnold proceeded to test the radiocarbon dating theory by analyzing samples with known ages. For example, two samples taken from the tombs of two Egyptian kings, Zoser and Sneferu, independently dated to 2625 BC plus or minus 75 years, were dated by radiocarbon measurement to an average of 2800 BC plus or minus 250 years. These results were published in Science in 1949. Within 11 years of their announcement, more than 20 radiocarbon dating laboratories had been set up worldwide.\nIn 1960, Libby was awarded the Nobel Prize in Chemistry for this work.\n\n\n=== Physical and chemical details ===\n\nIn nature, carbon exists as two stable, nonradioactive isotopes: carbon-12 (12C), and carbon-13 (13C), and a radioactive isotope, carbon-14 (14C), also known as \"radiocarbon\". The half-life of 14C (the time it takes for half of a given amount of 14C to decay) is about 5,730 years, so its concentration in the atmosphere might be expected to reduce over thousands of years, but 14C is constantly being produced in the lower stratosphere and upper troposphere by cosmic rays, which generate neutrons that in turn create 14C when they strike nitrogen-14 (14N) atoms. The following nuclear reaction creates 14C:\nn + 14\n7N \u2192 14\n6C + p\nwhere n represents a neutron and p represents a proton.\nOnce produced, the 14C quickly combines with the oxygen in the atmosphere to form carbon dioxide (CO\n2). Carbon dioxide produced in this way diffuses in the atmosphere, is dissolved in the ocean, and is taken up by plants via photosynthesis. Animals eat the plants, and ultimately the radiocarbon is distributed throughout the biosphere. The ratio of 14C to 12C is approximately 1.5 parts of 14C to 1012 parts of 12C. In addition, about 1% of the carbon atoms are of the stable isotope 13C.\nThe equation for the radioactive decay of 14C is:\n14\n6C \u2192 14\n7N + e\u2212 + \u03bd\ne\nBy emitting a beta particle (an electron, e\u2212) and an electron antineutrino (\u03bd\ne), one of the neutrons in the 14C nucleus changes to a proton and the 14C nucleus reverts to the stable (non-radioactive) isotope 14N.\n\n\n=== Principles ===\nDuring its life, a plant or animal is exchanging carbon with its surroundings, so the carbon it contains will have the same proportion of 14C as the atmosphere. Once it dies, it ceases to acquire 14C, but the 14C within its biological material at that time will continue to decay, and so the ratio of 14C to 12C in its remains will gradually decrease. Because 14C decays at a known rate, the proportion of radiocarbon can be used to determine how long it has been since a given sample stopped exchanging carbon \u2013 the older the sample, the less 14C will be left.\nThe equation governing the decay of a radioactive isotope is:\n\n  \n    \n      \n        N\n        =\n        \n          N\n          \n            0\n          \n        \n        \n          e\n          \n            \u2212\n            \u03bb\n            t\n          \n        \n        \n      \n    \n    {\\displaystyle N=N_{0}e^{-\\lambda t}\\,}\n  \nwhere N0 is the number of atoms of the isotope in the original sample (at time t = 0, when the organism from which the sample was taken died), and N is the number of atoms left after time t. \u03bb is a constant that depends on the particular isotope; for a given isotope it is equal to the reciprocal of the mean-life \u2013 i.e. the average or expected time a given atom will survive before undergoing radioactive decay. The mean-life, denoted by \u03c4, of 14C is 8,267 years, so the equation above can be rewritten as:\n\n  \n    \n      \n        t\n        =\n        8267\n        \u22c5\n        ln\n        \u2061\n        (\n        \n          N\n          \n            0\n          \n        \n        \n          /\n        \n        N\n        )\n        y\n        e\n        a\n        r\n        s\n        =\n        19035\n        \u22c5\n        log\n        \u2061\n        (\n        \n          N\n          \n            0\n          \n        \n        \n          /\n        \n        N\n        )\n        y\n        e\n        a\n        r\n        s\n      \n    \n    {\\displaystyle t=8267\\cdot \\ln(N_{0}/N)years=19035\\cdot \\log(N_{0}/N)years}\n  \nThe sample is assumed to have originally had the same 14C/12C ratio as the ratio in the atmosphere, and since the size of the sample is known, the total number of atoms in the sample can be calculated, yielding N0, the number of 14C atoms in the original sample. Measurement of N, the number of 14C atoms currently in the sample, allows the calculation of t, the age of the sample, using the equation above.\nThe half-life of a radioactive isotope (usually denoted by t1/2) is a more familiar concept than the mean-life, so although the equations above are expressed in terms of the mean-life, it is more usual to quote the value of 14C's half-life than its mean-life. The currently accepted value for the half-life of 14C is 5,730 years. This means that after 5,730 years, only half of the initial 14C will remain; a quarter will remain after 11,460 years; an eighth after 17,190 years; and so on.\nThe above calculations make several assumptions, such as that the level of 14C in the atmosphere has remained constant over time. In fact, the level of 14C in the atmosphere has varied significantly and as a result the values provided by the equation above have to be corrected by using data from other sources. This is done by calibration curves, which convert a measurement of 14C in a sample into an estimated calendar age. The calculations involve several steps and include an intermediate value called the \"radiocarbon age\", which is the age in \"radiocarbon years\" of the sample: an age quoted in radiocarbon years means that no calibration curve has been used \u2212 the calculations for radiocarbon years assume that the 14C/12C ratio has not changed over time. Calculating radiocarbon ages also requires the value of the half-life for 14C, which for more than a decade after Libby's initial work was thought to be 5,568 years. This was revised in the early 1960s to 5,730 years, which meant that many calculated dates in papers published prior to this were incorrect (the error in the half-life is about 3%). For consistency with these early papers, and to avoid the risk of a double correction for the incorrect half-life, radiocarbon ages are still calculated using the incorrect half-life value. A correction for the half-life is incorporated into calibration curves, so even though radiocarbon ages are calculated using a half-life value that is known to be incorrect, the final reported calibrated date, in calendar years, is accurate. When a date is quoted, the reader should be aware that if it is an uncalibrated date (a term used for dates given in radiocarbon years) it may differ substantially from the best estimate of the actual calendar date, both because it uses the wrong value for the half-life of 14C, and because no correction (calibration) has been applied for the historical variation of 14C in the atmosphere over time.\n\n\n=== Carbon exchange reservoir ===\n\nCarbon is distributed throughout the atmosphere, the biosphere, and the oceans; these are referred to collectively as the carbon exchange reservoir, and each component is also referred to individually as a carbon exchange reservoir. The different elements of the carbon exchange reservoir vary in how much carbon they store, and in how long it takes for the 14C generated by cosmic rays to fully mix with them. This affects the ratio of 14C to 12C in the different reservoirs, and hence the radiocarbon ages of samples that originated in each reservoir. The atmosphere, which is where 14C is generated, contains about 1.9% of the total carbon in the reservoirs, and the 14C it contains mixes in less than seven years. The ratio of 14C to 12C in the atmosphere is taken as the baseline for the other reservoirs: if another reservoir has a lower ratio of 14C to 12C, it indicates that the carbon is older and hence that some of the 14C has decayed. The ocean surface is an example: it contains 2.4% of the carbon in the exchange reservoir, but there is only about 95% as much 14C as would be expected if the ratio were the same as in the atmosphere. The time it takes for carbon from the atmosphere to mix with the surface ocean is only a few years, but the surface waters also receive water from the deep ocean, which has more than 90% of the carbon in the reservoir. Water in the deep ocean takes about 1,000 years to circulate back through surface waters, and so the surface waters contain a combination of older water, with depleted 14C, and water recently at the surface, with 14C in equilibrium with the atmosphere.\nCreatures living at the ocean surface have the same 14C ratios as the water they live in, and as a result of the reduced 14C/12C ratio, the radiocarbon age of marine life is typically about 440 years. Organisms on land are in closer equilibrium with the atmosphere and have the same 14C/12C ratio as the atmosphere. These organisms contain about 1.3% of the carbon in the reservoir; sea organisms have a mass of less than 1% of those on land and are not shown on the diagram. Accumulated dead organic matter, of both plants and animals, exceeds the mass of the biosphere by a factor of nearly 3, and since this matter is no longer exchanging carbon with its environment, it has a 14C/12C ratio lower than that of the biosphere.\n\n\n== Dating considerations ==\n\nThe variation in the 14C/12C ratio in different parts of the carbon exchange reservoir means that a straightforward calculation of the age of a sample based on the amount of 14C it contains will often give an incorrect result. There are several other possible sources of error that need to be considered. The errors are of four general types:\nvariations in the 14C/12C ratio in the atmosphere, both geographically and over time;\nisotopic fractionation;\nvariations in the 14C/12C ratio in different parts of the reservoir;\ncontamination.\n\n\n=== Atmospheric variation ===\nIn the early years of using the technique, it was understood that it depended on the atmospheric 14C/12C ratio having remained the same over the preceding few thousand years. To verify the accuracy of the method, several artefacts that were datable by other techniques were tested; the results of the testing were in reasonable agreement with the true ages of the objects. Over time, however, discrepancies began to appear between the known chronology for the oldest Egyptian dynasties and the radiocarbon dates of Egyptian artefacts. Neither the pre-existing Egyptian chronology nor the new radiocarbon dating method could be assumed to be accurate, but a third possibility was that the 14C/12C ratio had changed over time. The question was resolved by the study of tree rings: comparison of overlapping series of tree rings allowed the construction of a continuous sequence of tree-ring data that spanned 8,000 years. (Since that time the tree-ring data series has been extended to 13,900 years.) In the 1960s, Hans Suess was able to use the tree-ring sequence to show that the dates derived from radiocarbon were consistent with the dates assigned by Egyptologists. This was possible because although annual plants, such as corn, have a 14C/12C ratio that reflects the atmospheric ratio at the time they were growing, trees only add material to their outermost tree ring in any given year, while the inner tree rings don't get their 14C replenished and instead start losing 14C through decay. Hence each ring preserves a record of the atmospheric 14C/12C ratio of the year it grew in. Carbon-dating the wood from the tree rings themselves provides the check needed on the atmospheric 14C/12C ratio: with a sample of known date, and a measurement of the value of N (the number of atoms of 14C remaining in the sample), the carbon-dating equation allows the calculation of N0 \u2013 the number of atoms of 14C in the sample at the time the tree ring was formed \u2013 and hence the 14C/12C ratio in the atmosphere at that time. Equipped with the results of carbon-dating the tree rings, it became possible to construct calibration curves designed to correct the errors caused by the variation over time in the 14C/12C ratio. These curves are described in more detail below.\n\nCoal and oil began to be burned in large quantities during the 19th century. Both are sufficiently old that they contain little detectable 14C and, as a result, the CO\n2 released substantially diluted the atmospheric 14C/12C ratio. Dating an object from the early 20th century hence gives an apparent date older than the true date. For the same reason, 14C concentrations in the neighbourhood of large cities are lower than the atmospheric average. This fossil fuel effect (also known as the Suess effect, after Hans Suess, who first reported it in 1955) would only amount to a reduction of 0.2% in 14C activity if the additional carbon from fossil fuels were distributed throughout the carbon exchange reservoir, but because of the long delay in mixing with the deep ocean, the actual effect is a 3% reduction.\nA much larger effect comes from above-ground nuclear testing, which released large numbers of neutrons and created 14C. From about 1950 until 1963, when atmospheric nuclear testing was banned, it is estimated that several tonnes of 14C were created. If all this extra 14C had immediately been spread across the entire carbon exchange reservoir, it would have led to an increase in the 14C/12C ratio of only a few per cent, but the immediate effect was to almost double the amount of 14C in the atmosphere, with the peak level occurring in about 1965. The level has since dropped, as this bomb pulse or \"bomb carbon\" (as it is sometimes called) percolates into the rest of the reservoir.\n\n\n=== Isotopic fractionation ===\nPhotosynthesis is the primary process by which carbon moves from the atmosphere into living things. In photosynthetic pathways 12C is absorbed slightly more easily than 13C, which in turn is more easily absorbed than 14C. The differential uptake of the three carbon isotopes leads to 13C/12C and 14C/12C ratios in plants that differ from the ratios in the atmosphere. This effect is known as isotopic fractionation.\nTo determine the degree of fractionation that takes place in a given plant, the amounts of both 12C and 13C isotopes are measured, and the resulting 13C/12C ratio is then compared to a standard ratio known as PDB. The 13C/12C ratio is used instead of 14C/12C because the former is much easier to measure, and the latter can be easily derived: the depletion of 13C relative to 12C is proportional to the difference in the atomic masses of the two isotopes, so the depletion for 14C is twice the depletion of 13C. The fractionation of 13C, known as \u03b413C, is calculated as follows:\n\n  \n    \n      \n        \n          \n            \u03b4\n            \n              13\n            \n          \n          C\n        \n        =\n        \n          \n            (\n          \n        \n        \n          \n            \n              \n                \n                  (\n                \n              \n              \n                \n                  \n                    \n                      \n                      \n                        13\n                      \n                    \n                    C\n                  \n                  \n                    \n                      \n                      \n                        12\n                      \n                    \n                    C\n                  \n                \n              \n              \n                \n                  \n                    )\n                  \n                \n                \n                  s\n                  a\n                  m\n                  p\n                  l\n                  e\n                \n              \n            \n            \n              \n                \n                  (\n                \n              \n              \n                \n                  \n                    \n                      \n                      \n                        13\n                      \n                    \n                    C\n                  \n                  \n                    \n                      \n                      \n                        12\n                      \n                    \n                    C\n                  \n                \n              \n              \n                \n                  \n                    )\n                  \n                \n                \n                  P\n                  D\n                  B\n                \n              \n            \n          \n        \n        \u2212\n        1\n        \n          \n            )\n          \n        \n        \u00d7\n        1000\n        \n           \n          \n            o\n          \n        \n        \n        \n          /\n        \n        \n          \n          \n            o\n            o\n          \n        \n      \n    \n    {\\displaystyle \\mathrm {\\delta ^{13}C} ={\\Biggl (}\\mathrm {\\frac {{\\bigl (}{\\frac {^{13}C}{^{12}C}}{\\bigr )}_{sample}}{{\\bigl (}{\\frac {^{13}C}{^{12}C}}{\\bigr )}_{PDB}}} -1{\\Biggr )}\\times 1000\\ ^{o}\\!/\\!_{oo}}\n  \nwhere the \u2030 sign indicates parts per thousand. Because the PDB standard contains an unusually high proportion of 13C, most measured \u03b413C values are negative.\n\nFor marine organisms, the details of the photosynthesis reactions are less well understood, and the \u03b413C values for marine photosynthetic organisms are dependent on temperature. At higher temperatures, CO\n2 has poor solubility in water, which means there is less CO\n2 available for the photosynthetic reactions. Under these conditions, fractionation is reduced, and at temperatures above 14 \u00b0C the \u03b413C values are correspondingly higher, while at lower temperatures, CO\n2 becomes more soluble and hence more available to marine organisms. The \u03b413C value for animals depends on their diet. An animal that eats food with high \u03b413C values will have a higher \u03b413C than one that eats food with lower \u03b413C values. The animal's own biochemical processes can also impact the results: for example, both bone minerals and bone collagen typically have a higher concentration of 13C than is found in the animal's diet, though for different biochemical reasons. The enrichment of bone 13C also implies that excreted material is depleted in 13C relative to the diet.\nSince 13C makes up about 1% of the carbon in a sample, the 13C/12C ratio can be accurately measured by mass spectrometry. Typical values of \u03b413C have been found by experiment for many plants, as well as for different parts of animals such as bone collagen, but when dating a given sample it is better to determine the \u03b413C value for that sample directly than to rely on the published values.\nThe carbon exchange between atmospheric CO\n2 and carbonate at the ocean surface is also subject to fractionation, with 14C in the atmosphere more likely than 12C to dissolve in the ocean. The result is an overall increase in the 14C/12C ratio in the ocean of 1.5%, relative to the 14C/12C ratio in the atmosphere. This increase in 14C concentration almost exactly cancels out the decrease caused by the upwelling of water (containing old, and hence 14C depleted, carbon) from the deep ocean, so that direct measurements of 14C radiation are similar to measurements for the rest of the biosphere. Correcting for isotopic fractionation, as is done for all radiocarbon dates to allow comparison between results from different parts of the biosphere, gives an apparent age of about 440 years for ocean surface water.\n\n\n=== Reservoir effects ===\nLibby's original exchange reservoir hypothesis assumed that the 14C/12C ratio in the exchange reservoir is constant all over the world, but it has since been discovered that there are several causes of variation in the ratio across the reservoir.\nMarine effect\nThe CO\n2 in the atmosphere transfers to the ocean by dissolving in the surface water as carbonate and bicarbonate ions; at the same time the carbonate ions in the water are returning to the air as CO\n2. This exchange process brings14C from the atmosphere into the surface waters of the ocean, but the 14C thus introduced takes a long time to percolate through the entire volume of the ocean. The deepest parts of the ocean mix very slowly with the surface waters, and the mixing is uneven. The main mechanism that brings deep water to the surface is upwelling, which is more common in regions closer to the equator. Upwelling is also influenced by factors such as the topography of the local ocean bottom and coastlines, the climate, and wind patterns. Overall, the mixing of deep and surface waters takes far longer than the mixing of atmospheric CO\n2 with the surface waters, and as a result water from some deep ocean areas has an apparent radiocarbon age of several thousand years. Upwelling mixes this \"old\" water with the surface water, giving the surface water an apparent age of about several hundred years (after correcting for fractionation). This effect is not uniform \u2013 the average effect is about 440 years, but there are local deviations of several hundred years for areas that are geographically close to each other. The effect also applies to marine organisms such as shells, and marine mammals such as whales and seals, which have radiocarbon ages that appear to be hundreds of years old.\nHemisphere effect\nThe northern and southern hemispheres have atmospheric circulation systems that are sufficiently independent of each other that there is a noticeable time lag in mixing between the two. The atmospheric 14C/12C ratio is lower in the southern hemisphere, with an apparent additional age of 30 years for radiocarbon results from the south as compared to the north. This is probably because the greater surface area of ocean in the southern hemisphere means that there is more carbon exchanged between the ocean and the atmosphere than in the north. Since the surface ocean is depleted in 14C because of the marine effect, 14C is removed from the southern atmosphere more quickly than in the north.\nOther effects\nIf the carbon in freshwater is partly acquired from aged carbon, such as rocks, then the result will be a reduction in the 14C/12C ratio in the water. For example, rivers that pass over limestone, which is mostly composed of calcium carbonate, will acquire carbonate ions. Similarly, groundwater can contain carbon derived from the rocks through which it has passed. These rocks are usually so old that they no longer contain any measurable 14C, so this carbon lowers the 14C/12C ratio of the water it enters, which can lead to apparent ages of thousands of years for both the affected water and the plants and freshwater organisms that live in it. This is known as the hard water effect because it is often associated with calcium ions, which are characteristic of hard water; other sources of carbon such as humus can produce similar results. The effect varies greatly and there is no general offset that can be applied; additional research is usually needed to determine the size of the offset, for example by comparing the radiocarbon age of deposited freshwater shells with associated organic material.\nVolcanic eruptions eject large amounts of carbon into the air. The carbon is of geological origin and has no detectable 14C, so the 14C/12C ratio in the vicinity of the volcano is depressed relative to surrounding areas. Dormant volcanoes can also emit aged carbon. Plants that photosynthesize this carbon also have lower 14C/12C ratios: for example, plants on the Greek island of Santorini, near the volcano, have apparent ages of up to a thousand years. These effects are hard to predict \u2013 the town of Akrotiri, on Santorini, was destroyed in a volcanic eruption thousands of years ago, but radiocarbon dates for objects recovered from the ruins of the town show surprisingly close agreement with dates derived from other means. If the dates for Akrotiri are confirmed, it would indicate that the volcanic effect in this case was minimal.\n\n\n=== Contamination ===\nAny addition of carbon to a sample of a different age will cause the measured date to be inaccurate. Contamination with modern carbon causes a sample to appear to be younger than it really is: the effect is greater for older samples. If a sample that is 17,000 years old is contaminated so that 1% of the sample is modern carbon, it will appear to be 600 years younger; for a sample that is 34,000 years old the same amount of contamination would cause an error of 4,000 years. Contamination with old carbon, with no remaining 14C, causes an error in the other direction independent of age \u2013 a sample contaminated with 1% old carbon will appear to be about 80 years older than it really is, regardless of the date of the sample.\n\n\n== Samples ==\n\nSamples for dating need to be converted into a form suitable for measuring the 14C content; this can mean conversion to gaseous, liquid, or solid form, depending on the measurement technique to be used. Before this can be done, the sample must be treated to remove any contamination and any unwanted constituents. This includes removing visible contaminants, such as rootlets that may have penetrated the sample since its burial. Alkali and acid washes can be used to remove humic acid and carbonate contamination, but care has to be taken to avoid destroying or damaging the sample.\n\n\n=== Material considerations ===\nIt is common to reduce a wood sample to just the cellulose component before testing, but since this can reduce the volume of the sample to 20% of its original size, testing of the whole wood is often performed as well. Charcoal is often tested but is likely to need treatment to remove contaminants.\nUnburnt bone can be tested; it is usual to date it using collagen, the protein fraction that remains after washing away the bone's structural material. Hydroxyproline, one of the constituent amino acids in bone, was once thought to be a reliable indicator as it was not known to occur except in bone, but it has since been detected in groundwater.\nFor burnt bone, testability depends on the conditions under which the bone was burnt. If the bone was heated under reducing conditions, it (and associated organic matter) may have been carbonized. In this case the sample is often usable.\nShells from both marine and land organisms consist almost entirely of calcium carbonate, either as aragonite or as calcite, or some mixture of the two. Calcium carbonate is very susceptible to dissolving and recrystallizing; the recrystallized material will contain carbon from the sample's environment, which may be of geological origin. If testing recrystallized shell is unavoidable, it is sometimes possible to identify the original shell material from a sequence of tests. It is also possible to test conchiolin, an organic protein found in shell, but it constitutes only 1\u20132% of shell material.\nThe three major components of peat are humic acid, humins, and fulvic acid. Of these, humins give the most reliable date as they are insoluble in alkali and less likely to contain contaminants from the sample's environment. A particular difficulty with dried peat is the removal of rootlets, which are likely to be hard to distinguish from the sample material.\nSoil contains organic material, but because of the likelihood of contamination by humic acid of more recent origin, it is very difficult to get satisfactory radiocarbon dates. It is preferable to sieve the soil for fragments of organic origin, and date the fragments with methods that are tolerant of small sample sizes.\nOther materials that have been successfully dated include ivory, paper, textiles, individual seeds and grains, straw from within mud bricks, and charred food remains found in pottery.\n\n\n=== Preparation and size ===\nParticularly for older samples, it may be useful to enrich the amount of 14C in the sample before testing. This can be done with a thermal diffusion column. The process takes about a month and requires a sample about ten times as large as would be needed otherwise, but it allows more precise measurement of the 14C/12C ratio in old material and extends the maximum age that can be reliably reported.\nOnce contamination has been removed, samples must be converted to a form suitable for the measuring technology to be used. Where gas is required, CO\n2 is widely used. For samples to be used in liquid scintillation counters, the carbon must be in liquid form; the sample is typically converted to benzene. For accelerator mass spectrometry, solid graphite targets are the most common, although iron carbide and gaseous CO\n2 can also be used.\nThe quantity of material needed for testing depends on the sample type and the technology being used. There are two types of testing technology: detectors that record radioactivity, known as beta counters, and accelerator mass spectrometers. For beta counters, a sample weighing at least 10 grams (0.35 ounces) is typically required. Accelerator mass spectrometry (AMS) is much more sensitive, and samples as small as 0.5 milligrams (0.0077 grains) can be used.\n\n\n== Measurement and results ==\n\nFor decades after Libby performed the first radiocarbon dating experiments, the only way to measure the 14C in a sample was to detect the radioactive decay of individual carbon atoms. In this approach, what is measured is the activity, in number of decay events per unit mass per time period, of the sample. This method is also known as \"beta counting\", because it is the beta particles emitted by the decaying 14C atoms that are detected. In the late 1970s an alternative approach became available: directly counting the number of 14C and 12C atoms in a given sample, via accelerator mass spectrometry, usually referred to as AMS. AMS counts the 14C/12C ratio directly, instead of the activity of the sample, but measurements of activity and 14C/12C ratio can be converted into each other exactly. For some time, beta counting methods were more accurate than AMS, but as of 2014 AMS is more accurate and has become the method of choice for radiocarbon measurements. In addition to improved accuracy, AMS has two further significant advantages over beta counting: it can perform accurate testing on samples much too small for beta counting; and it is much faster \u2013 an accuracy of 1% can be achieved in minutes with AMS, which is far quicker than would be achievable with the older technology.\n\n\n=== Beta counting ===\nLibby's first detector was a Geiger counter of his own design. He converted the carbon in his sample to lamp black (soot) and coated the inner surface of a cylinder with it. This cylinder was inserted into the counter in such a way that the counting wire was inside the sample cylinder, in order that there should be no material between the sample and the wire. Any interposing material would have interfered with the detection of radioactivity, since the beta particles emitted by decaying 14C are so weak that half are stopped by a 0.01 mm thickness of aluminium.\nLibby's method was soon superseded by gas proportional counters, which were less affected by bomb carbon (the additional 14C created by nuclear weapons testing). These counters record bursts of ionization caused by the beta particles emitted by the decaying 14C atoms; the bursts are proportional to the energy of the particle, so other sources of ionization, such as background radiation, can be identified and ignored. The counters are surrounded by lead or steel shielding, to eliminate background radiation and to reduce the incidence of cosmic rays. In addition, anticoincidence detectors are used; these record events outside the counter, and any event recorded simultaneously both inside and outside the counter is regarded as an extraneous event and ignored.\nThe other common technology used for measuring 14C activity is liquid scintillation counting, which was invented in 1950, but which had to wait until the early 1960s, when efficient methods of benzene synthesis were developed, to become competitive with gas counting; after 1970 liquid counters became the more common technology choice for newly constructed dating laboratories. The counters work by detecting flashes of light caused by the beta particles emitted by 14C as they interact with a fluorescing agent added to the benzene. Like gas counters, liquid scintillation counters require shielding and anticoincidence counters.\nFor both the gas proportional counter and liquid scintillation counter, what is measured is the number of beta particles detected in a given time period. Since the mass of the sample is known, this can be converted to a standard measure of activity in units of either counts per minute per gram of carbon (cpm/g C), or becquerels per kg (Bq/kg C, in SI units). Each measuring device is also used to measure the activity of a blank sample \u2013 a sample prepared from carbon old enough to have no activity. This provides a value for the background radiation, which must be subtracted from the measured activity of the sample being dated to get the activity attributable solely to that sample's 14C. In addition, a sample with a standard activity is measured, to provide a baseline for comparison.\n\n\n=== Accelerator mass spectrometry ===\n\nAMS counts the atoms of 14C and 12C in a given sample, determining the 14C/12C ratio directly. The sample, often in the form of graphite, is made to emit C\u2212 ions (carbon atoms with a single negative charge), which are injected into an accelerator. The ions are accelerated and passed through a stripper, which removes several electrons so that the ions emerge with a positive charge. The C3+ ions are then passed through a magnet that curves their path; the heavier ions are curved less than the lighter ones, so the different isotopes emerge as separate streams of ions. A particle detector then records the number of ions detected in the 14C stream, but since the volume of 12C (and 13C, needed for calibration) is too great for individual ion detection, counts are determined by measuring the electric current created in a Faraday cup. Some AMS facilities are also able to evaluate a sample's fractionation, another piece of data necessary for calculating the sample's radiocarbon age. The use of AMS, as opposed to simpler forms of mass spectrometry, is necessary because of the need to distinguish the carbon isotopes from other atoms or molecules that are very close in mass, such as 14N and 13CH. As with beta counting, both blank samples and standard samples are used. Two different kinds of blank may be measured: a sample of dead carbon that has undergone no chemical processing, to detect any machine background, and a sample known as a process blank made from dead carbon that is processed into target material in exactly the same way as the sample which is being dated. Any 14C signal from the machine background blank is likely to be caused either by beams of ions that have not followed the expected path inside the detector, or by carbon hydrides such as 12CH\n2 or 13CH. A 14C signal from the process blank measures the amount of contamination introduced during the preparation of the sample. These measurements are used in the subsequent calculation of the age of the sample.\n\n\n=== Calculations ===\n\nThe calculations to be performed on the measurements taken depend on the technology used, since beta counters measure the sample's radioactivity whereas AMS determines the ratio of the three different carbon isotopes in the sample.\nTo determine the age of a sample whose activity has been measured by beta counting, the ratio of its activity to the activity of the standard must be found. To determine this, a blank sample (of old, or dead, carbon) is measured, and a sample of known activity is measured. The additional samples allow errors such as background radiation and systematic errors in the laboratory setup to be detected and corrected for. The most common standard sample material is oxalic acid, such as the HOxII standard, 1,000 lb of which was prepared by NIST in 1977 from French beet harvests.\nThe results from AMS testing are in the form of ratios of 12C, 13C, and 14C, which are used to calculate Fm, the \"fraction modern\". This is defined as the ratio between the 14C/12C ratio in the sample and the 14C/12C ratio in modern carbon, which is in turn defined as the 14C/12C ratio that would have been measured in 1950 had there been no fossil fuel effect.\nBoth beta counting and AMS results have to be corrected for fractionation. This is necessary because different materials of the same age, which because of fractionation have naturally different 14C/12C ratios, will appear to be of different ages because the 14C/12C ratio is taken as the indicator of age. To avoid this, all radiocarbon measurements are converted to the measurement that would have been seen had the sample been made of wood, which has a known \u03b413C value of \u221225\u2030.\nOnce the corrected 14C/12C ratio is known, a \"radiocarbon age\" is calculated using:\n\n  \n    \n      \n        A\n        g\n        e\n        =\n        \u2212\n        8033\n        \u22c5\n        l\n        n\n        (\n        F\n        m\n        )\n      \n    \n    {\\displaystyle Age=-8033\\cdot ln(Fm)}\n  \nThe calculation uses Libby's half-life of 5,568 years, not the more accurate modern value of 5,730 years. Libby\u2019s value for the half-life is used to maintain consistency with early radiocarbon testing results; calibration curves include a correction for this, so the accuracy of final reported calendar ages is assured.\n\n\n=== Errors and reliability ===\nThe reliability of the results can be improved by lengthening the testing time. For example, if counting beta decays for 250 minutes is enough to give an error of \u00b1 80 years, with 68% confidence, then doubling the counting time to 500 minutes will allow a sample with only half as much 14C to be measured with the same error term of 80 years.\nRadiocarbon dating is generally limited to dating samples no more than 50,000 years old, as samples older than that have insufficient 14C to be measurable. Older dates have been obtained by using special sample preparation techniques, large samples, and very long measurement times. These techniques can allow measurement of dates up to 60,000 and in some cases up to 75,000 years before the present.\nRadiocarbon dates are generally presented with a range of one standard deviation (usually represented by the Greek letter sigma as 1\u03c3) on either side of the mean. However, a date range of 1\u03c3 represents only 68% confidence level, so the true age of the object being measured may lie outside the range of dates quoted. This was demonstrated in 1970 by an experiment run by the British Museum radiocarbon laboratory, in which weekly measurements were taken on the same sample for six months. The results varied widely (though consistently with a normal distribution of errors in the measurements), and included multiple date ranges (of 1\u03c3 confidence) that did not overlap with each other. The measurements included one with a range from about 4250 to about 4390 years ago, and another with a range from about 4520 to about 4690.\nErrors in procedure can also lead to errors in the results. If 1% of the benzene in a modern reference sample accidentally evaporates, scintillation counting will give a radiocarbon age that is too young by about 80 years.\n\n\n=== Calibration ===\n\nThe calculations given above produce dates in radiocarbon years: i.e. dates that represent the age the sample would be if the 14C/12C ratio had been constant historically. Although Libby had pointed out as early as 1955 the possibility that this assumption was incorrect, it was not until discrepancies began to accumulate between measured ages and known historical dates for artefacts that it became clear that a correction would need to be applied to radiocarbon ages to obtain calendar dates.\nTo produce a curve that can be used to relate calendar years to radiocarbon years, a sequence of securely dated samples is needed which can be tested to determine their radiocarbon age. The study of tree rings led to the first such sequence: individual pieces of wood show characteristic sequences of rings that vary in thickness because of environmental factors such as the amount of rainfall in a given year. These factors affect all trees in an area, so examining tree-ring sequences from old wood allows the identification of overlapping sequences. In this way, an uninterrupted sequence of tree rings can be extended far into the past. The first such published sequence, based on bristlecone pine tree rings, was created by Wesley Ferguson. Hans Suess used this data to publish the first calibration curve for radiocarbon dating in 1967. The curve showed two types of variation from the straight line: a long term fluctuation with a period of about 9,000 years, and a shorter term variation, often referred to as \"wiggles\", with a period of decades. Suess said he drew the line showing the wiggles by \"cosmic schwung\", by which he meant that the variations were caused by extraterrestrial forces. It was unclear for some time whether the wiggles were real or not, but they are now well-established. These short term fluctuations in the calibration curve are now known as de Vries effects, after Hessel de Vries.\nA calibration curve is used by taking the radiocarbon date reported by a laboratory, and reading across from that date on the vertical axis of the graph. The point where this horizontal line intersects the curve will give the calendar age of the sample on the horizontal axis. This is the reverse of the way the curve is constructed: a point on the graph is derived from a sample of known age, such as a tree ring; when it is tested, the resulting radiocarbon age gives a data point for the graph.\n\nOver the next thirty years many calibration curves were published using a variety of methods and statistical approaches. These were superseded by the INTCAL series of curves, beginning with INTCAL98, published in 1998, and updated in 2004, 2009, and 2013. The improvements to these curves are based on new data gathered from tree rings, varves, coral, plant macrofossils, speleothems, and foraminifera. The INTCAL13 data includes separate curves for the northern and southern hemispheres, as they differ systematically because of the hemisphere effect; there is also a separate marine calibration curve. For a set of samples with a known sequence and separation in time such as a sequence of tree rings, the samples' radiocarbon ages form a small subset of the calibration curve. The resulting curve can then be matched to the actual calibration curve by identifying where, in the range suggested by the radiocarbon dates, the wiggles in the calibration curve best match the wiggles in the curve of sample dates. This \"wiggle-matching\" technique can lead to more precise dating than is possible with individual radiocarbon dates. Wiggle-matching can be used in places where there is a plateau on the calibration curve, and hence can provide a much more accurate date than the intercept or probability methods are able to produce. The technique is not restricted to tree rings; for example, a stratified tephra sequence in New Zealand, known to predate human colonization of the islands, has been dated to 1314 AD \u00b1 12 years by wiggle-matching. The wiggles also mean that reading a date from a calibration curve can give more than one answer: this occurs when the curve wiggles up and down enough that the radiocarbon age intercepts the curve in more than one place, which may lead to a radiocarbon result being reported as two separate age ranges, corresponding to the two parts of the curve that the radiocarbon age intercepted.\nBayesian statistical techniques can be applied when there are several radiocarbon dates to be calibrated. For example, if a series of radiocarbon dates is taken from different levels in a given stratigraphic sequence, Bayesian analysis can help determine if some of the dates should be discarded as anomalies, and can use the information to improve the output probability distributions. When Bayesian analysis was introduced, its use was limited by the need to use mainframe computers to perform the calculations, but the technique has since been implemented on programs available for personal computers, such as OxCal.\n\n\n=== Reporting dates ===\nSeveral formats for citing radiocarbon results have been used since the first samples were dated. As of 2014, the standard format required by the journal Radiocarbon is as follows.\nUncalibrated dates should be reported as \"<laboratory>: <14C year> \u00b1 <range> BP\", where:\n<laboratory> identifies the laboratory that tested the sample, and the sample ID\n<14C year> is the laboratory's determination of the age of the sample, in radiocarbon years\n<range> is the laboratory's estimate of the error in the age, at 1\u03c3 confidence.\nBP stands for \"before present\", referring to a reference date of 1950, so that 500 BP means the year 1450 AD.\nFor example, the uncalibrated date \"UtC-2020: 3510 \u00b1 60 BP\" indicates that the sample was tested by the Utrecht van der Graaf Laboratorium, where it has a sample number of 2020, and that the uncalibrated age is 3510 years before present, \u00b1 60 years. Related forms are sometimes used: for example, \"10 ka BP\" means 10,000 radiocarbon years before present (i.e. 8,050 BC), and 14C yr BP might be used to distinguish the uncalibrated date from a date derived from another dating method such as thermoluminescence.\nCalibrated 14C dates are frequently reported as cal BP, cal BC, or cal AD, again with BP referring to the year 1950 as the zero date. Radiocarbon gives two options for reporting calibrated dates. A common format is \"cal <date-range> <confidence>\", where:\n<date-range> is the range of dates corresponding to the given confidence level\n<confidence> indicates the confidence level for the given date range.\nFor example, \"cal 1220\u20131281 AD (1\u03c3)\" means a calibrated date for which the true date lies between 1220 AD and 1281 AD, with the confidence level given as 1\u03c3, or one standard deviation. Calibrated dates can also be expressed as BP instead of using BC and AD. The curve used to calibrate the results should be the latest available INTCAL curve. Calibrated dates should also identify any programs, such as OxCal, used to perform the calibration. In addition, an article in Radiocarbon in 2014 about radiocarbon date reporting conventions recommends that information should be provided about sample treatment, including the sample material, pretreatment methods, and quality control measurements; that the citation to the software used for calibration should specify the version number and any options or models used; and that the calibrated date should be given with the associated probabilities for each range.\n\n\n== Use in archaeology ==\n\n\n=== Interpretation ===\nA key concept in interpreting radiocarbon dates is archaeological association: what is the true relationship between two or more objects at an archaeological site? It frequently happens that a sample for radiocarbon dating can be taken directly from the object of interest, but there are also many cases where this is not possible. Metal grave goods, for example, cannot be radiocarbon dated, but they may be found in a grave with a coffin, charcoal, or other material which can be assumed to have been deposited at the same time. In these cases a date for the coffin or charcoal is indicative of the date of deposition of the grave goods, because of the direct functional relationship between the two. There are also cases where there is no functional relationship, but the association is reasonably strong: for example, a layer of charcoal in a rubbish pit provides a date which has a relationship to the rubbish pit.\nContamination is of particular concern when dating very old material obtained from archaeological excavations and great care is needed in the specimen selection and preparation. In 2014, Tom Higham and co-workers suggested that many of the dates published for Neanderthal artefacts are too recent because of contamination by \"young carbon\".\nAs a tree grows, only the outermost tree ring exchanges carbon with its environment, so the age measured for a wood sample depends on where the sample is taken from. This means that radiocarbon dates on wood samples can be older than the date at which the tree was felled. In addition, if a piece of wood is used for multiple purposes, there may be a significant delay between the felling of the tree and the final use in the context in which it is found. This is often referred to as the \"old wood\" problem. One example is the Bronze Age trackway at Withy Bed Copse, in England; the trackway was built from wood that had clearly been worked for other purposes before being re-used in the trackway. Another example is driftwood, which may be used as construction material. It is not always possible to recognize re-use. Other materials can present the same problem: for example, bitumen is known to have been used by some Neolithic communities to waterproof baskets; the bitumen's radiocarbon age will be greater than is measurable by the laboratory, regardless of the actual age of the context, so testing the basket material will give a misleading age if care is not taken. A separate issue, related to re-use, is that of lengthy use, or delayed deposition. For example, a wooden object that remains in use for a lengthy period will have an apparent age greater than the actual age of the context in which it is deposited.\n\n\n=== Notable applications ===\n\n\n==== Pleistocene/Holocene boundary in Two Creeks Fossil Forest ====\nThe Pleistocene is a geological epoch that began about 2.6 million years ago. The Holocene, the current geological epoch, begins about 11,700 years ago, when the Pleistocene ends. Establishing the date of this boundary \u2212 which is defined by sharp climatic warming \u2212 as accurately as possible has been a goal of geologists for much of the 20th century. At Two Creeks, in Wisconsin, a fossil forest was discovered (Two Creeks Buried Forest State Natural Area), and subsequent research determined that the destruction of the forest was caused by the Valders ice readvance, the last southward movement of ice before the end of the Pleistocene in that area. Before the advent of radiocarbon dating, the fossilized trees had been dated by correlating sequences of annually deposited layers of sediment at Two Creeks with sequences in Scandinavia. This led to estimates that the trees were between 24,000 and 19,000 years old, and hence this was taken to be the date of the last advance of the Wisconsin glaciation before its final retreat marked the end of the Pleistocene in North America. In 1952 Libby published radiocarbon dates for several samples from the Two Creeks site and two similar sites nearby; the dates were averaged to 11,404 BP with a standard error of 350 years. This result was uncalibrated, as the need for calibration of radiocarbon ages was not yet understood. Further results over the next decade supported an average date of 11,350 BP, with the results thought to be most accurate averaging 11,600 BP. There was initial resistance to these results on the part of Ernst Antevs, the palaeobotanist who had worked on the Scandinavian varve series, but his objections were eventually discounted by other geologists. In the 1990s samples were tested with AMS, yielding (uncalibrated) dates ranging from 11,640 BP to 11,800 BP, both with a standard error of 160 years. Subsequently, a sample from the fossil forest was used in an interlaboratory test, with results provided by over 70 laboratories. These tests produced a median age of 11,788 \u00b1 8 BP (2\u03c3 confidence) which when calibrated gives a date range of 13,730 to 13,550 cal BP. The Two Creeks radiocarbon dates are now regarded as a key result in developing the modern understanding of North American glaciation at the end of the Pleistocene.\n\n\n==== Dead Sea Scrolls ====\n\nIn 1947, scrolls were discovered in caves near the Dead Sea that proved to contain writing in Hebrew and Aramaic, most of which are thought to have been produced by the Essenes, a small Jewish sect. These scrolls are of great significance in the study of Biblical texts because many of them contain the earliest known version of books of the Hebrew bible. A sample of the linen wrapping from one of these scrolls, the Great Isaiah Scroll, was included in a 1955 analysis by Libby, with an estimated age of 1,917 \u00b1 200 years. Based on an analysis of the writing style, palaeographic estimates were made of the age of 21 of the scrolls, and samples from most of these, along with other scrolls which had not been palaeographically dated, were tested by two AMS laboratories in the 1990s. The results ranged in age from the early 4th century BC to the mid 4th century AD. In many cases the scrolls were determined to be older than the palaeographically determined age. The Isaiah scroll was included in the testing and was found to have two possible date ranges at a 2\u03c3 confidence level, because of the shape of the calibration curve at that point: there is a 15% chance that it dates from 355\u2212295 BC, and an 84% chance that it dates from 210\u221245 BC. Subsequently, these dates were criticized on the grounds that before the scrolls were tested, they had been treated with modern castor oil in order to make the writing easier to read; it was argued that failure to remove the castor oil sufficiently would have caused the dates to be too young. Multiple papers have been published both supporting and opposing the criticism.\n\n\n=== Impact ===\nSoon after the publication of Libby's 1949 paper in Science, universities around the world began establishing radiocarbon-dating laboratories, and by the end of the 1950s there were more than 20 active 14C research laboratories. It quickly became apparent that the principles of radiocarbon dating were valid, despite certain discrepancies, the causes of which then remained unknown.\nThe development of radiocarbon dating has had a profound impact on archaeology - often described as the \"radiocarbon revolution\". In the words of anthropologist R. E. Taylor, \"14C data made a world prehistory possible by contributing a time scale that transcends local, regional and continental boundaries\". It provides more accurate dating within sites than previous methods, which usually derived either from stratigraphy or from typologies (e.g. of stone tools or pottery); it also allows comparison and synchronization of events across great distances. The advent of radiocarbon dating may even have led to better field methods in archaeology, since better data recording leads to firmer association of objects with the samples to be tested. These improved field methods were sometimes motivated by attempts to prove that a 14C date was incorrect. Taylor also suggests that the availability of definite date information freed archaeologists from the need to focus so much of their energy on determining the dates of their finds, and led to an expansion of the questions archaeologists were willing to research. For example, from the 1970s questions about the evolution of human behaviour were much more frequently seen in archaeology.\nThe dating framework provided by radiocarbon led to a change in the prevailing view of how innovations spread through prehistoric Europe. Researchers had previously thought that many ideas spread by diffusion through the continent, or by invasions of peoples bringing new cultural ideas with them. As radiocarbon dates began to prove these ideas wrong in many instances, it became apparent that these innovations must sometimes have arisen locally. This has been described as a \"second radiocarbon revolution\", and with regard to British prehistory, archaeologist Richard Atkinson has characterized the impact of radiocarbon dating as \"radical ... therapy\" for the \"progressive disease of invasionism\". More broadly, the success of radiocarbon dating stimulated interest in analytical and statistical approaches to archaeological data. Taylor has also described the impact of AMS, and the ability to obtain accurate measurements from very small samples, as ushering in a third radiocarbon revolution.\nOccasionally, radiocarbon dating techniques date an object of popular interest, for example the Shroud of Turin, a piece of linen cloth thought by some to bear an image of Jesus Christ after his crucifixion. Three separate laboratories dated samples of linen from the Shroud in 1988; the results pointed to 14th-century origins, raising doubts about the shroud's authenticity as an alleged 1st-century relic.\nResearchers have studied other radioactive isotopes created by cosmic rays to determine if they could also be used to assist in dating objects of archaeological interest; such isotopes include 3He, 10Be, 21Ne, 26Al, and 36Cl. With the development of AMS in the 1980s it became possible to measure these isotopes precisely enough for them to be the basis of useful dating techniques, which have been primarily applied to dating rocks. Naturally occurring radioactive isotopes can also form the basis of dating methods, as with potassium\u2013argon dating, argon\u2013argon dating, and uranium series dating. Other dating techniques of interest to archaeologists include thermoluminescence, optically stimulated luminescence, electron spin resonance, and fission track dating, as well as techniques that depend on annual bands or layers, such as dendrochronology, tephrochronology, and varve chronology.\nIn 2016, the development of radiocarbon dating was recognized as a National Historic Chemical Landmark for its contributions to chemistry and society by the American Chemical Society.\n\n\n== See also ==\nDating methodologies in archaeology\n\n\n== Notes ==\n\n\n== References ==\n\n\n== Sources ==\nAitken, M.J. (1990). Science-based Dating in Archaeology. London: Longman. ISBN 0-582-49309-9. \nAitken, Martin J. (2003). \"Radiocarbon Dating\". In Ellis, Linda. Archaeological Method and Theory. New York: Garland Publishing. pp. 505\u2212508. \nBianchi, Thomas S.; Canuel, Elizabeth A. (2011). Chemical Markers in Aquatic Ecosystems. Princeton: Princeton University Press. ISBN 978-0-691-13414-7. \nBousman, C. Britt; Vierra, Bradley J. (2012). \"Chronology, Environmental Setting, and Views of the Terminal Pleistocene and Early Holocene Cultural Transitions in North America\". In Bousman, C. Britt; Vierra, Bradley J. From the Pleistocene to the Holocene: Human Organization and Cultural Transformations in Prehistoric North America. College Station, Texas: Texas A&M University Press. pp. 1\u201315. ISBN 978-1-60344-760-7. \nBowman, Sheridan (1995) [1990]. Radiocarbon Dating. London: British Museum Press. ISBN 0-7141-2047-2. \nCronin, Thomas M. (2010). Paleoclimates: Understanding Climate Change Past and Present. New York: Columbia University Press. ISBN 978-0-231-14494-0. \nDass, Chhabil (2007). Fundamentals of Contemporary Mass Spectrometry. Hoboken, New Jersey: John Wiley & Sons. ISBN 978-0-471-68229-5. \nEriksson Stenstr\u00f6m, Kristina; Skog, G\u00f6ran; Georgiadou, Elisavet; Genberg, Johan; Johansson, Anette (2011). A guide to radiocarbon units and calculations. Lund: Lund University. \nFerronsky, V.I.; Polyakov, V.A. (2012). Isotopes of the Earth's Hydrosphere. New York: Springer. ISBN 978-94-007-2855-4. \nKillick, David (2014). \"Using evidence from natural sciences in archaeology\". In Chapman, Robert; Alison, Wylie. Material Evidence: Learning From Archaeological Practice. Abingdon, UK: Routledge. pp. 159\u2013172. ISBN 978-0-415-83745-3. \nL'Annunziata, Michael F. (2007). Radioactivity: Introduction and History. Amsterdam: Elsevier. ISBN 978-0-444-52715-8. \nL'Annunziata, Michael F.; Kessler, Michael J. (2012). \"Liquid scintillation analysis: principles and practice\". In L'Annunziata, Michael F. Handbook of Radioactivity Analysis (3rd ed.). Oxford: Academic Press. pp. 423\u2013573. ISBN 978-0-12-384873-4. \nLibby, Willard F. (1965) [1952]. Radiocarbon Dating (2nd (1955) ed.). Chicago: Phoenix. \nMacdougall, Doug (2008). Nature's Clocks: How Scientists Measure the Age of Almost Everything. Berkeley, California: University of California Press. ISBN 978-0-520-24975-2. \nMalainey, Mary E. (2010). A Consumer's Guide to Archaeological Science. New York: Springer. ISBN 978-1-4419-5704-7. \nMaslin, Mark A.; Swann, George E.A. (2006). \"Isotopes in marine sediments\". In Leng, Melanie J. Isotopes in Palaeoenvironmental Research. Dordrecht: Springer. pp. 227\u2013290. ISBN 978-1-4020-2503-7. \nMook, W.G.; Waterbolk, H.T. (1985). Handbooks for Archaeologists: No. 3: Radiocarbon Dating. Strasbourg: European Science Foundation. ISBN 2-903148-44-9. \nPost, Wilfred M. (2001). \"Carbon cycle\". In Goudie, Andrew; Cuff, David J. Encyclopedia of Global Change: Environmental Change and Human Society, Volume 1. Oxford: Oxford University Press. pp. 127\u2013130. ISBN 0-19-514518-6. \nRenfrew, Colin (2014). \"Foreword\". In Taylor, R.E.; Bar-Yosef, Ofer. Radiocarbon Dating. Walnut Creek, California: Left Coast Press. pp. 12\u201314. ISBN 978-1-59874-590-0. \nSchoeninger, Margaret J. (2010). \"Diet reconstruction and ecology using stable isotope ratios\". In Larsen, Clark Spencer. A Companion to Biological Anthropology. Oxford: Blackwell. pp. 445\u2013464. ISBN 978-1-4051-8900-2. \n\u0160ilar, Jan (2004). \"Application of environmental radionuclides in radiochronology: Radiocarbon\". In Tykva, Richard; Berg, Dieter. Man-made and Natural Radioactivity in Environmental Pollution and Radiochronology. Dordrecht: Kluwer Academic Publishers. pp. 150\u2013179. ISBN 1-4020-1860-6. \nSuess, H.E. (1970). \"Bristlecone-pine calibration of the radiocarbon time-scale 5200 B.C. to the present\". In Olsson, Ingrid U. Radiocarbon Variations and Absolute Chronology. New York: John Wiley & Sons. pp. 303\u2013311. \nTaylor, R.E. (1987). Radiocarbon Dating. London: Academic Press. ISBN 0-12-433663-9. \nTaylor, R.E. (1997). \"Radiocarbon dating\". In Taylor, R.E.; Aitken, Martin J. Chronometric Dating in Archaeology. New York: Plenum Press. pp. 65\u201397. ISBN 0-306-45715-6. \nTaylor, R.E.; Bar-Yosef, Ofer (2014). Radiocarbon Dating (2nd ed.). Walnut Creek, California: Left Coast Press. ISBN 978-1-59874-590-0. \nTerasmae, J. (1984). \"Radiocarbon dating: some problems and potential developments\". In Mahaney, W.C. Quaternary Dating Methods. Amsterdam: Elsevier. pp. 1\u201315. ISBN 0-444-42392-3. \nTheod\u00f3rsson, P\u00e1ll (1996). Measurement of Weak Radioactivity. Singapore: World Scientific Publishing. ISBN 9810223153. \nTrumbore, Susan E. (1996). \"Applications of accelerator mass spectrometry to soil science\". In Boutton, Thomas W.; Yamasaki, Shin-ichi. Mass Spectrometry of Soils. New York: Marcel Dekker. pp. 311\u2013340. ISBN 0-8247-9699-3. \nTuniz, C.; Zoppi, U.; Barbetti, M. (2004). \"Radionuclide dating in archaeology by accelerator mass spectrometry\". In Martini, M.; Milazzo, M.; Piacentini, M. Physics Methods in Archaeometry. Amsterdam: IOS Press. pp. 385\u2013405. ISBN 978-1-58603-424-5. \nWalker, Mike (2005). Quaternary Dating Methods (PDF). Chichester: John Wiley & Sons. ISBN 978-0-470-86927-7. \nWarneck, Peter (2000). Chemistry of the Natural Atmosphere. London: Academic Press. ISBN 0-12-735632-0. \n\n\n== External links ==\nRADON \u2013 database for European 14C dates", 
                "titleUrl": "https://en.wikipedia.org/wiki/Radiocarbon_dating", 
                "title": "Radiocarbon dating"
            }, 
            {
                "snippet": "A carbon\u2013carbon bond is a covalent bond between two carbon atoms.[page\u00a0needed] The most common form is the single bond: a bond composed of two electrons", 
                "pageCategories": "All articles needing additional references\nArticles needing additional references from October 2013\nChemical bonding\nOrganic chemistry\nWikipedia articles needing page number citations from January 2015", 
                "pageContent": "A carbon\u2013carbon bond is a covalent bond between two carbon atoms. The most common form is the single bond: a bond composed of two electrons, one from each of the two atoms. The carbon\u2013carbon single bond is a sigma bond and is formed between one hybridized orbital from each of the carbon atoms. In ethane, the orbitals are sp3-hybridized orbitals, but single bonds formed between carbon atoms with other hybridisations do occur (e.g. sp2 to sp2). In fact, the carbon atoms in the single bond need not be of the same hybridisation. Carbon atoms can also form double bonds in compounds called alkenes or triple bonds in compounds called alkynes. A double bond is formed with an sp2-hybridized orbital and a p-orbital that isn't involved in the hybridization. A triple bond is formed with an sp-hybridized orbital and two p-orbitals from each atom. The use of the p-orbitals forms a pi bond.\nCarbon is one of the few elements that can form long chains of its own atoms, a property called catenation. This coupled with the strength of the carbon\u2013carbon bond gives rise to an enormous number of molecular forms, many of which are important structural elements of life, so carbon compounds have their own field of study: organic chemistry.\n\nBranching is also common in C\u2212C skeletons. Different carbon atoms can be identified with respect to the number of carbon neighbors:\nprimary carbon atom: one carbon neighbor\nsecondary carbon atom: two carbon neighbors\ntertiary carbon atom: three carbon neighbors\nquaternary carbon atom: four carbon neighbors\n\nIn \"structurally complex organic molecules\", it is the three-dimensional orientation of the carbon\u2013carbon bonds at quaternary loci which dictates the shape of the molecule. Further, quaternary loci are found in many biologically active small molecules, such as cortisone and morphine.\n\n\n== Synthesis ==\nCarbon\u2013carbon bond-forming reactions are organic reactions in which a new carbon\u2013carbon bond is formed. They are important in the production of many man-made chemicals such as pharmaceuticals and plastics.\nSome examples of reactions which form carbon\u2013carbon bonds are aldol reactions, Diels\u2013Alder reaction, the addition of a Grignard reagent to a carbonyl group, a Heck reaction, a Michael reaction and a Wittig reaction.\nThe directed synthesis of desired three-dimensional structures for tertiary carbons was largely solved during the late 20th century, but the same ability to direct quaternary carbon synthesis did not start to emerge until the first decade of the 21st century.\n\n\n== Bond strengths ==\nRelative to most bonds, a carbon\u2013carbon bond is very strong.\n\n\n== See also ==\nAn extensive list is presented here: list of carbon\u2013carbon bond-forming reactions\nThe chemistry of carbon bonded to other elements in the periodic table:\n\n\n== References ==", 
                "titleUrl": "https://en.wikipedia.org/wiki/Carbon\u2013carbon_bond", 
                "title": "Carbon\u2013carbon bond"
            }, 
            {
                "snippet": "alpha carbon (C\u03b1) in organic molecules refers to the first carbon atom that attaches to a functional group, such as a carbonyl. The second carbon atom", 
                "pageCategories": "Organic chemistry", 
                "pageContent": "The alpha carbon (C\u03b1) in organic molecules refers to the first carbon atom that attaches to a functional group, such as a carbonyl. The second carbon atom is called the beta carbon (C\u03b2),[1] and the system continues naming in alphabetical order with Greek letters.\nThe nomenclature can also be applied to the hydrogen atoms attached to the carbon atoms. A hydrogen atom attached to an alpha carbon atom is called an alpha-hydrogen atom, a hydrogen atom on the beta-carbon atom is a beta hydrogen atom, and so on.\nThis naming standard may not be in compliance with IUPAC nomenclature, which encourages that carbons be identified by number, not by Greek letter, but it nonetheless remains very popular, in particular because it is useful in identifying the relative location of carbon atoms to other functional groups.\nOrganic molecules with more than one functional group can be a source of confusion. Generally the functional group responsible for the name or type of the molecule is the 'reference' group for purposes of carbon-atom naming. For example, the molecules nitrostyrene and phenethylamine are very similar; the former can even be reduced into the latter. However, nitrostyrene's \u03b1-carbon atom is adjacent to the styrene group; in phenethylamine this same carbon atom is the \u03b2-carbon atom, as phenethylamine (being an amine rather than a styrene) counts its atoms from the opposite \"end\" of the molecule.\n\n\n== Examples ==\n\n\n=== Proteins and amino acids ===\nAlpha-carbon (\u03b1-carbon) is also a term that applies to proteins and amino acids. It is the backbone carbon before the carbonyl carbon. Therefore, reading along the backbone of a typical protein would give a sequence of \u2013[N\u2014C\u03b1\u2014carbonyl C]n\u2013 etc. (when reading in the N to C direction). The \u03b1-carbon is where the different substituents attach to each different amino acid. That is, the groups hanging off the chain at the \u03b1-carbon are what give amino acids their diversity. These groups give the \u03b1-carbon its stereogenic properties for every amino acid except for glycine. Therefore, the \u03b1-carbon is a stereocenter for every amino acid except glycine. Glycine also does not have a \u03b2-carbon, while every other amino acid does.\nThe \u03b1-carbon of an amino acid is significant in protein folding. When describing a protein, which is a chain of amino acids, one often approximates the location of each amino acid as the location of its \u03b1-carbon. In general, \u03b1-carbons of adjacent amino acids in a protein are about 3.8 \u00e5ngstr\u00f6ms (380 picometers) apart.\n\n\n=== Enols and enolates ===\nThe \u03b1-carbon is important for enol- and enolate-based carbonyl chemistry as well. Chemical transformations affected by the conversion to either an enolate or an enol, in general, lead to the \u03b1-carbon acting as a nucleophile, becoming, for example, alkylated in the presence of primary haloalkane. An exception is in reaction with silyl- chlorides, -bromides, and -iodides, where the oxygen acts as the nucleophile to produce silyl enol ether.\n\n\n== References ==\n\n^ Hackh's Chemical Dictionary, 1969, page 30.\n^ Hackh's Chemical Dictionary, 1969, page 95.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Alpha_and_beta_carbon", 
                "title": "Alpha and beta carbon"
            }, 
            {
                "snippet": "Carbon steel is steel in which the main interstitial alloying constituent is carbon in the range of 0.12\u20132.0%. The American Iron and Steel Institute (AISI)", 
                "pageCategories": "Metallurgical processes\nSteels\nUse dmy dates from June 2013\nWikipedia articles needing clarification from June 2016", 
                "pageContent": "Carbon steel is steel in which the main interstitial alloying constituent is carbon in the range of 0.12\u20132.0%. The American Iron and Steel Institute (AISI) definition says:\nSteel is considered to be carbon steel\n   when no minimum content is specified or required for chromium, cobalt, molybdenum, nickel, niobium, titanium, tungsten, vanadium or zirconium, or any other element to be added to obtain a desired alloying effect;\n   when the specified minimum for copper does not exceed 0.40 percent;\n   or when the maximum content specified for any of the following elements does not exceed the percentages noted: manganese 1.65, silicon 0.60, copper 0.60.\n\nThe term \"carbon steel\" may also be used in reference to steel which is not stainless steel; in this use carbon steel may include alloy steels.\nAs the carbon percentage content rises, steel has the ability to become harder and stronger through heat treating; however, it becomes less ductile. Regardless of the heat treatment, a higher carbon content reduces weldability. In carbon steels, the higher carbon content lowers the melting point.\n\n\n== Type ==\n\n\n=== Mild and low-carbon steel ===\nMild steel (steel containing a small percentage of carbon, strong and tough but not readily tempered), also known as plain-carbon steel, is now the most common form of steel because its price is relatively low while it provides material properties that are acceptable for many applications. Low-carbon steel contains approximately 0.05\u20130.25% carbon making it malleable and ductile. Mild steel has a relatively low tensile strength, but it is cheap and easy to form; surface hardness can be increased through carburizing.\nIt is often used when large quantities of steel are needed, for example as structural steel. The density of mild steel is approximately 7.85 g/cm3 (7850 kg/m3 or 0.284 lb/in3) and the Young's modulus is 200 GPa (29,000,000 psi).\nLow-carbon steels suffer from yield-point runout where the material has two yield points. The first yield point (or upper yield point) is higher than the second and the yield drops dramatically after the upper yield point. If a low-carbon steel is only stressed to some point between the upper and lower yield point then the surface develop L\u00fcder bands. Low-carbon steels contain less carbon than other steels and are easier to cold-form, making them easier to handle.\n\n\n=== Higher-carbon steels ===\nCarbon steels which can successfully undergo heat-treatment have a carbon content in the range of 0.30\u20131.70% by weight. Trace impurities of various other elements can have a significant effect on the quality of the resulting steel. Trace amounts of sulfur in particular make the steel red-short, that is, brittle and crumbly at working temperatures. Low-alloy carbon steel, such as A36 grade, contains about 0.05% sulfur and melts around 1,426\u20131,538 \u00b0C (2,599\u20132,800 \u00b0F). Manganese is often added to improve the hardenability of low-carbon steels. These additions turn the material into a low-alloy steel by some definitions, but AISI's definition of carbon steel allows up to 1.65% manganese by weight.\n\n\n== Types ==\n\nCarbon steel is broken down into four classes based on carbon content:\n\n\n=== Low-carbon steel ===\nUp to 0.3% carbon content.\n\n\n=== Medium-carbon steel ===\nApproximately 0.3\u20130.6% carbon content. Balances ductility and strength and has good wear resistance; used for large parts, forging and automotive components.\n\n\n=== High-carbon steel ===\nApproximately 0.6\u20131.0% carbon content. Very strong, used for springs, swords, and high-strength wires.\n\n\n=== Ultra-high-carbon steel ===\nApproximately 1.25\u20132.0% carbon content. Steels that can be tempered to great hardness. Used for special purposes like (non-industrial-purpose) knives, axles or punches. Most steels with more than 2.5% carbon content are made using powder metallurgy.\n\n\n== Heat treatment ==\n\nThe purpose of heat treating carbon steel is to change the mechanical properties of steel, usually ductility, hardness, yield strength, or impact resistance. Note that the electrical and thermal conductivity are only slightly altered. As with most strengthening techniques for steel, Young's modulus (elasticity) is unaffected. All treatments of steel trade ductility for increased strength and vice versa. Iron has a higher solubility for carbon in the austenite phase; therefore all heat treatments, except spheroidizing and process annealing, start by heating the steel to a temperature at which the austenitic phase can exist. The steel is then quenched (heat drawn out) at a high rate causing cementite to precipitate and finally the remaining pure iron to solidify. The rate at which the steel is cooled through the eutectoid temperature affects the rate at which carbon diffuses out of austenite and forms cementite. Generally speaking, cooling swiftly will leave iron carbide finely dispersed and produce a fine grained pearlite (until the martensite critical temperature is reached) and cooling slowly will give a coarser pearlite. Cooling a hypoeutectoid steel (less than 0.77 wt% C) results in a lamellar-pearlitic structure of iron carbide layers with \u03b1-ferrite (pure iron) between. If it is hypereutectoid steel (more than 0.77 wt% C) then the structure is full pearlite with small grains (larger than the pearlite lamella) of cementite scattered throughout. The relative amounts of constituents are found using the lever rule. The following is a list of the types of heat treatments possible:\nSpheroidizing: Spheroidite forms when carbon steel is heated to approximately 700 \u00b0C for over 30 hours. Spheroidite can form at lower temperatures but the time needed drastically increases, as this is a diffusion-controlled process. The result is a structure of rods or spheres of cementite within primary structure (ferrite or pearlite, depending on which side of the eutectoid you are on). The purpose is to soften higher carbon steels and allow more formability. This is the softest and most ductile form of steel. The image to the right shows where spheroidizing usually occurs.\nFull annealing: Carbon steel is heated to approximately 40 \u00b0C above Ac3? or Acm? for 1 hour; this ensures all the ferrite transforms into austenite (although cementite might still exist if the carbon content is greater than the eutectoid). The steel must then be cooled slowly, in the realm of 20 \u00b0C (36 \u00b0F) per hour. Usually it is just furnace cooled, where the furnace is turned off with the steel still inside. This results in a coarse pearlitic structure, which means the \"bands\" of pearlite are thick. Fully annealed steel is soft and ductile, with no internal stresses, which is often necessary for cost-effective forming. Only spheroidized steel is softer and more ductile.\nProcess annealing: A process used to relieve stress in a cold-worked carbon steel with less than 0.3 wt% C. The steel is usually heated up to 550\u2013650 \u00b0C for 1 hour, but sometimes temperatures as high as 700 \u00b0C. The image rightward shows the area where process annealing occurs.\nIsothermal annealing: It is a process in which hypoeutectoid steel is heated above the upper critical temperature and this temperature is maintained for a time and then the temperature is brought down below lower critical temperature and is again maintained. Then finally it is cooled at room temperature. This method rids any temperature gradient.\nNormalizing: Carbon steel is heated to approximately 55 \u00b0C above Ac3 or Acm for 1 hour; this ensures the steel completely transforms to austenite. The steel is then air-cooled, which is a cooling rate of approximately 38 \u00b0C (100 \u00b0F) per minute. This results in a fine pearlitic structure, and a more-uniform structure. Normalized steel has a higher strength than annealed steel; it has a relatively high strength and hardness.\nQuenching: Carbon steel with at least 0.4 wt% C is heated to normalizing temperatures and then rapidly cooled (quenched) in water, brine, or oil to the critical temperature. The critical temperature is dependent on the carbon content, but as a general rule is lower as the carbon content increases. This results in a martensitic structure; a form of steel that possesses a super-saturated carbon content in a deformed body-centered cubic (BCC) crystalline structure, properly termed body-centered tetragonal (BCT), with much internal stress. Thus quenched steel is extremely hard but brittle, usually too brittle for practical purposes. These internal stresses cause stress cracks on the surface. Quenched steel is approximately three to four (with more carbon) fold harder than normalized steel.\nMartempering (Marquenching): Martempering is not actually a tempering procedure, hence the term \"marquenching\". It is a form of isothermal heat treatment applied after an initial quench of typically in a molten salt bath at a temperature right above the \"martensite start temperature\". At this temperature, residual stresses within the material are relieved and some bainite may be formed from the retained austenite which did not have time to transform into anything else. In industry, this is a process used to control the ductility and hardness of a material. With longer marquenching, the ductility increases with a minimal loss in strength; the steel is held in this solution until the inner and outer temperatures equalize. Then the steel is cooled at a moderate speed to keep the temperature gradient minimal. Not only does this process reduce internal stresses and stress cracks, but it also increases the impact resistance.\nQuench and tempering: This is the most common heat treatment encountered, because the final properties can be precisely determined by the temperature and time of the tempering. Tempering involves reheating quenched steel to a temperature below the eutectoid temperature then cooling. The elevated temperature allows very small amounts of spheroidite to form, which restores ductility, but reduces hardness. Actual temperatures and times are carefully chosen for each composition.\nAustempering: The austempering process is the same as martempering, except the steel is held in the molten salt bath through the bainite transformation temperatures, and then moderately cooled. The resulting bainite steel has a greater ductility, higher impact resistance, and less distortion. The disadvantage of austempering is it can only be used on a few steels, and it requires a special salt bath.\n\n\n== Case hardening ==\n\nCase hardening processes harden only the exterior of the steel part, creating a hard, wear resistant skin (the \"case\") but preserving a tough and ductile interior. Carbon steels are not very hardenable; therefore thick pieces cannot be through-hardened. Alloy steels have a better hardenability, so they can through-harden and do not require case hardening. This property of carbon steel can be beneficial, because it gives the surface good wear characteristics but leaves the core tough.\n\n\n== Forging temperature of steel ==\n\n\n== See also ==\nCold working\nHot working\nWelding\nForging\n\n\n== References ==\n\n\n== Bibliography ==\nDegarmo, E. Paul; Black, J T.; Kohser, Ronald A. (2003), Materials and Processes in Manufacturing (9th ed.), Wiley, ISBN 0-471-65653-4. \nOberg, E.; et al. (1996), Machinery's Handbook (25th ed.), Industrial Press Inc, ISBN 0-8311-2599-3. \nSmith, William F.; Hashemi, Javad (2006), Foundations of Materials Science and Engineering (4th ed.), McGraw-Hill, ISBN 0-07-295358-6.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Carbon_steel", 
                "title": "Carbon steel"
            }, 
            {
                "snippet": "Carbon fibre-reinforced carbon (aka Carbon\u2013Carbon, abbreviated C/C or Reinforced Carbon-Carbon, abbreviated RCC or Carbon-Fiber-Reinforced Carbon, abbreviated", 
                "pageCategories": "All articles with unsourced statements\nArticles with unsourced statements from April 2014\nArticles with unsourced statements from June 2015\nComposite materials\nRefractory materials\nWikipedia articles needing clarification from April 2014", 
                "pageContent": "Carbon fibre-reinforced carbon (aka Carbon\u2013Carbon, abbreviated C/C or Reinforced Carbon-Carbon, abbreviated RCC or Carbon-Fiber-Reinforced Carbon, abbreviated CFRC) is a composite material consisting of carbon fiber reinforcement in a matrix of graphite. It was developed for the nose cones of intercontinental ballistic missiles, and is most widely known as the material for the nose cone and wing leading edges of the Space Shuttle orbiter. It has been used in the brake systems of Formula One racing cars since 1976; carbon\u2013carbon brake discs and pads are a standard component of Formula One brake systems.\nCarbon\u2013carbon is well-suited to structural applications at high temperatures, or where thermal shock resistance and/or a low coefficient of thermal expansion is needed. While it is less brittle than many other ceramics, it lacks impact resistance; Space Shuttle Columbia was destroyed during atmospheric re-entry after one of its RCC panels was broken by the impact of a piece of foam insulation from the Space Shuttle External Tank. This catastrophic failure was due in part to original shuttle design requirements which did not consider the likelihood of such violent impacts.\n\n\n== ProductionEdit ==\n\nThe material is made in three stages:\nFirst, material is laid up in its intended final shape, with carbon filament and/or cloth surrounded by an organic binder such as plastic or pitch. Often, coke or some other fine carbon aggregate is added to the binder mixture.\nSecond, the lay-up is heated, so that pyrolysis transforms the binder to relatively pure carbon. The binder loses volume in the process, so that voids form; the addition of aggregate reduces this problem, but does not eliminate it.\nThird, the voids are gradually filled by forcing a carbon-forming gas such as acetylene through the material at a high temperature, over the course of several days. This long heat treatment process also allows the carbon to form into larger graphite crystals, and is the major reason for the material's high cost, exceeding $100,000 per panel. The grey \"Reinforced Carbon\u2013Carbon (RCC)\" panels on the space shuttle's wing leading edges and nose cone cost NASA $100,000/sq ft to produce, although much of this cost was a result of the advanced geometry and research costs associated with the panels.\nC/C is a generally hard material that can be made highly resistant to thermal expansion, temperature gradients, and thermal cycling, depending on how the fibre scaffold is laid up and the quality/density of the matrix filler.\n\n\n== Mechanical propertiesEdit ==\nThe strength of carbon\u2013carbon with unidirectional reinforcement fibres is up to 700 MPa. Carbon\u2013carbon materials retain their properties above 2000 \u00b0C. This temperature may be exceeded with the help of protective coatings to prevent oxidation. The material has a density between 1.6\u20131.98 g/cm3.\n\n\n== Similar productsEdit ==\n\nCarbon fibre-reinforced silicon carbide (C/SiC) is a development of pure carbon\u2013carbon, and can be used in automotive applications, such as components of brake systems on high performance road cars, namely the brake disc and brake pads. C/SiC utilises silicon carbide with carbon fibre, and this compound is thought to be more durable than pure carbon-carbon, however heavier hence not used in F1.\nApplications initially included the Mercedes-Benz C215 Coupe F1 edition, and are standard fitment on the Bugatti Veyron and certain current Bentleys, Ferraris, Porsches, Corvette ZR1, ZO6 and Lamborghinis. They are also offered as an \"optional upgrade\" on certain high performance Audi cars, including the D3 S8, B7 RS4, C6 S6 and RS6, and the R8.\nCarbon brakes became widely available for commercial airplanes in the 1980s having been first used on the Concorde supersonic transport.\nA related non-ceramic carbon composite with uses in high tech racing automotives is the carbotanium carbon\u2013titanium composite used in the Zonda R and Huayra supercars made by the Italian motorcar company Pagani.\n\n\n== See alsoEdit ==\nBugatti Veyron\nBentley Brooklands\n\n\n== ReferencesEdit ==\n\n\n== External linksEdit ==\nCarbon brakes for Concorde", 
                "titleUrl": "https://en.wikipedia.org/wiki/Reinforced_carbon\u2013carbon", 
                "title": "Reinforced carbon\u2013carbon"
            }, 
            {
                "snippet": "involving carbon that powers some stars, see CNO cycle. For organic chemical ring-shaped structures, see Cyclic compounds.      The carbon cycle is the", 
                "pageCategories": "Articles with inconsistent citation formats\nBiogeochemical cycle\nBiogeography\nCarbon\nChemical oceanography\nCommons category with local link different than on Wikidata\nGeochemistry\nNumerical climate and weather models\nPhotosynthesis\nSoil biology", 
                "pageContent": "The carbon cycle is the biogeochemical cycle by which carbon is exchanged among the biosphere, pedosphere, geosphere, hydrosphere, and atmosphere of the Earth. Along with the nitrogen cycle and the water cycle, the carbon cycle comprises a sequence of events that are key to making the Earth capable of sustaining life; it describes the movement of carbon as it is recycled and reused throughout the biosphere, including carbon sinks.\nThe global carbon budget is the balance of the exchanges (incomes and losses) of carbon between the carbon reservoirs or between one specific loop (e.g., atmosphere <-> biosphere) of the carbon cycle. An examination of the carbon budget of a pool or reservoir can provide information about whether the pool or reservoir is functioning as a source or sink for carbon dioxide. The carbon cycle was initially discovered by Joseph Priestley and Antoine Lavoisier, and popularized by Humphry Davy.\n\n\n== Global climate ==\nCarbon-based molecules are crucial for life on Earth, because it is the main component of biological compounds. Carbon is also a major component of many minerals. Carbon also exists in various forms in the atmosphere. Carbon dioxide (CO2) is partly responsible for the greenhouse effect and is the most important human-contributed greenhouse gas.\nIn the past two centuries, human activities have seriously altered the global carbon cycle, most significantly in the atmosphere. Although carbon dioxide levels have changed naturally over the past several thousand years, human emissions of carbon dioxide into the atmosphere exceed natural fluctuations. Changes in the amount of atmospheric CO2 are considerably altering weather patterns and indirectly influencing oceanic chemistry. Current carbon dioxide levels in the atmosphere exceed measurements from the last 420,000 years and levels are rising faster than ever recorded, making it of critical importance to better understand how the carbon cycle works and what its effects are on the global climate.\n\n\n== Main components ==\nThe global carbon cycle is now usually divided into the following major reservoirs of carbon interconnected by pathways of exchange:\nThe atmosphere\nThe terrestrial biosphere\nThe oceans, including dissolved inorganic carbon and living and non-living marine biota\nThe sediments, including fossil fuels, fresh water systems and non-living organic material.\nThe Earth's interior, carbon from the Earth's mantle and crust. These carbon stores interact with the other components through geological processes\nThe carbon exchanges between reservoirs occur as the result of various chemical, physical, geological, and biological processes. The ocean contains the largest active pool of carbon near the surface of the Earth. The natural flows of carbon between the atmosphere, ocean, terrestrial ecosystems, and sediments is fairly balanced, so that carbon levels would be roughly stable without human influence.\n\n\n=== Atmosphere ===\n\nCarbon in the Earth's atmosphere exists in two main forms: carbon dioxide and methane. Both of these gases absorb and retain heat in the atmosphere and are partially responsible for the greenhouse effect. Methane produces a large greenhouse effect per volume as compared to carbon dioxide, but it exists in much lower concentrations and is more short-lived than carbon dioxide, making carbon dioxide the more important greenhouse gas of the two.\nCarbon dioxide leaves the atmosphere through photosynthesis, thus entering the terrestrial and oceanic biospheres. Carbon dioxide also dissolves directly from the atmosphere into bodies of water (oceans, lakes, etc.), as well as dissolving in precipitation as raindrops fall through the atmosphere. When dissolved in water, carbon dioxide reacts with water molecules and forms carbonic acid, which contributes to ocean acidity. It can then be absorbed by rocks through weathering. It also can acidify other surfaces it touches or be washed into the ocean.\nHuman activities over the past two centuries have significantly increased the amount of carbon in the atmosphere, mainly in the form of carbon dioxide, both by modifying ecosystems' ability to extract carbon dioxide from the atmosphere and by emitting it directly, e.g., by burning fossil fuels and manufacturing concrete.\n\n\n=== Terrestrial biosphere ===\n\nThe terrestrial biosphere includes the organic carbon in all land-living organisms, both alive and dead, as well as carbon stored in soils. About 500 gigatons of carbon are stored above ground in plants and other living organisms, while soil holds approximately 1,500 gigatons of carbon. Most carbon in the terrestrial biosphere is organic carbon, while about a third of soil carbon is stored in inorganic forms, such as calcium carbonate. Organic carbon is a major component of all organisms living on earth. Autotrophs extract it from the air in the form of carbon dioxide, converting it into organic carbon, while heterotrophs receive carbon by consuming other organisms.\nBecause carbon uptake in the terrestrial biosphere is dependent on biotic factors, it follows a diurnal and seasonal cycle. In CO2 measurements, this feature is apparent in the Keeling curve. It is strongest in the northern hemisphere, because this hemisphere has more land mass than the southern hemisphere and thus more room for ecosystems to absorb and emit carbon.\nCarbon leaves the terrestrial biosphere in several ways and on different time scales. The combustion or respiration of organic carbon releases it rapidly into the atmosphere. It can also be exported into the oceans through rivers or remain sequestered in soils in the form of inert carbon. Carbon stored in soil can remain there for up to thousands of years before being washed into rivers by erosion or released into the atmosphere through soil respiration. Between 1989 and 2008 soil respiration increased by about 0.1% per year. In 2008, the global total of CO2 released from the soil reached roughly 98 billion tonnes, about 10 times more carbon than humans are now putting into the atmosphere each year by burning fossil fuel. There are a few plausible explanations for this trend, but the most likely explanation is that increasing temperatures have increased rates of decomposition of soil organic matter, which has increased the flow of CO2. The length of carbon sequestering in soil is dependent on local climatic conditions and thus changes in the course of climate change. From pre-industrial era to 2010, the terrestrial biosphere represented a net source of atmospheric CO2 prior to 1940, switching subsequently to a net sink.\n\n\n=== Oceans ===\n\nOceans contain the greatest quantity of actively cycled carbon in this world and are second only to the lithosphere in the amount of carbon they store. The oceans' surface layer holds large amounts of dissolved inorganic carbon that is exchanged rapidly with the atmosphere. The deep layer's concentration of dissolved inorganic carbon (DIC) is about 15% higher than that of the surface layer. DIC is stored in the deep layer for much longer periods of time. Thermohaline circulation exchanges carbon between these two layers.\nCarbon enters the ocean mainly through the dissolution of atmospheric carbon dioxide, which is converted into carbonate. It can also enter the oceans through rivers as dissolved organic carbon. It is converted by organisms into organic carbon through photosynthesis and can either be exchanged throughout the food chain or precipitated into the ocean's deeper, more carbon rich layers as dead soft tissue or in shells as calcium carbonate. It circulates in this layer for long periods of time before either being deposited as sediment or, eventually, returned to the surface waters through thermohaline circulation.\nOceanic absorption of CO2 is one of the most important forms of carbon sequestering limiting the human-caused rise of carbon dioxide in the atmosphere. However, this process is limited by a number of factors. Because the rate of CO2 dissolution in the ocean is dependent on the weathering of rocks and this process takes place slower than current rates of human greenhouse gas emissions, ocean CO2 uptake will decrease in the future. CO2 absorption also makes water more acidic, which affects ocean biosystems. The projected rate of increasing oceanic acidity could slow the biological precipitation of calcium carbonates, thus decreasing the ocean's capacity to absorb carbon dioxide.\n\n\n=== Geological carbon cycle ===\nThe geologic component of the carbon cycle operates slowly in comparison to the other parts of the global carbon cycle. It is one of the most important determinants of the amount of carbon in the atmosphere, and thus of global temperatures.\nMost of the earth's carbon is stored inertly in the earth's lithosphere. Much of the carbon stored in the earth's mantle was stored there when the earth formed. Some of it was deposited in the form of organic carbon from the biosphere. Of the carbon stored in the geosphere, about 80% is limestone and its derivatives, which form from the sedimentation of calcium carbonate stored in the shells of marine organisms. The remaining 20% is stored as kerogens formed through the sedimentation and burial of terrestrial organisms under high heat and pressure. Organic carbon stored in the geosphere can remain there for millions of years.\nCarbon can leave the geosphere in several ways. Carbon dioxide is released during the metamorphosis of carbonate rocks when they are subducted into the earth's mantle. This carbon dioxide can be released into the atmosphere and ocean through volcanoes and hotspots. It can also be removed by humans through the direct extraction of kerogens in the form of fossil fuels. After extraction, fossil fuels are burned to release energy, thus emitting the carbon they store into the atmosphere.\n\n\n=== Human influence ===\n\nSince the industrial revolution, human activity has modified the carbon cycle by changing its component's functions and directly adding carbon to the atmosphere.\nThe largest human impact on the carbon cycle is through direct emissions from burning fossil fuels, which transfers carbon from the geosphere into the atmosphere. The rest of this increase is caused mostly by changes in land-use, particularly deforestation.\nAnother direct human impact on the carbon cycle is the chemical process of calcination of limestone for clinker production, which releases CO2. Clinker is an industrial precursor of cement.\nHumans also influence the carbon cycle indirectly by changing the terrestrial and oceanic biosphere. Over the past several centuries, direct and indirect human-caused land use and land cover change (LUCC) has led to the loss of biodiversity, which lowers ecosystems' resilience to environmental stresses and decreases their ability to remove carbon from the atmosphere. More directly, it often leads to the release of carbon from terrestrial ecosystems into the atmosphere. Deforestation for agricultural purposes removes forests, which hold large amounts of carbon, and replaces them, generally with agricultural or urban areas. Both of these replacement land cover types store comparatively small amounts of carbon, so that the net product of the process is that more carbon stays in the atmosphere.\nOther human-caused changes to the environment change ecosystems' productivity and their ability to remove carbon from the atmosphere. Air pollution, for example, damages plants and soils, while many agricultural and land use practices lead to higher erosion rates, washing carbon out of soils and decreasing plant productivity.\nHumans also affect the oceanic carbon cycle. Current trends in climate change lead to higher ocean temperatures, thus modifying ecosystems. Also, acid rain and polluted runoff from agriculture and industry change the ocean's chemical composition. Such changes can have dramatic effects on highly sensitive ecosystems such as coral reefs, thus limiting the ocean's ability to absorb carbon from the atmosphere on a regional scale and reducing oceanic biodiversity globally.\nArctic methane emissions indirectly caused by anthropogenic global warming also affect the carbon cycle, and contribute to further warming in what is known as climate change feedback.\nOn 12 November 2015, NASA scientists reported that human-made carbon dioxide (CO2) continues to increase above levels not seen in hundreds of thousands of years: currently, about half of the carbon dioxide released from the burning of fossil fuels remains in the atmosphere and is not absorbed by vegetation and the oceans.\n\n\n== See also ==\nBiochar\nCalvin cycle\nCarbon cycle re-balancing\nCarbon dioxide in Earth's atmosphere\nCarbon footprint\nDeficit irrigation\nNitrogen cycle\nOcean acidification\nPermafrost carbon cycle\nSnowball Earth and the \"Slow carbon cycle\"\nSoil plant atmosphere continuum\n\n\n== References ==\n\n\n== Further reading ==\nThe Carbon Cycle, updated primer by NASA Earth Observatory, 2011\nAppenzeller, Tim (2004). \"The case of the missing carbon\". National Geographic Magazine.  \u2013 article about the missing carbon sink\nBolin, Bert; Degens, E. T.; Kempe, S.; Ketner, P. (1979). The global carbon cycle. Chichester ; New York: Published on behalf of the Scientific Committee on Problems of the Environment (SCOPE) of the International Council of Scientific Unions (ICSU) by Wiley. ISBN 0-471-99710-2. Retrieved 2008-07-08. \nHoughton, R. A. (2005). \"The contemporary carbon cycle\". In William H Schlesinger (editor). Biogeochemistry. Amsterdam: Elsevier Science. pp. 473\u2013513. ISBN 0-08-044642-6. \nJanzen, H. H. (2004). \"Carbon cycling in earth systems\u2014a soil science perspective\". Agriculture, Ecosystems & Environment. 104 (3): 399\u2013417. doi:10.1016/j.agee.2004.01.040. \nMillero, Frank J. (2005). Chemical Oceanography (3 ed.). CRC Press. ISBN 0-8493-2280-4. \nSundquist, Eric; Broecker, Wallace S., eds. (1985). The Carbon Cycle and Atmospheric CO2: Natural variations Archean to Present. Geophysical Monographs Series. American Geophysical Union. \n\n\n== External links ==\nCarbon Cycle Science Program \u2013 an interagency partnership.\nNOAA's Carbon Cycle Greenhouse Gases Group\nGlobal Carbon Project \u2013 initiative of the Earth System Science Partnership\nUNEP \u2013 The present carbon cycle \u2013 Climate Change carbon levels and flows\nNASA's Orbiting Carbon Observatory\nCarboSchools, a European website with many resources to study carbon cycle in secondary schools.\nCarbon and Climate, an educational website with a carbon cycle applet for modeling your own projection.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Carbon_cycle", 
                "title": "Carbon cycle"
            }, 
            {
                "snippet": "\"Carbon fiber\" redirects here. For fibers of carbon, see carbon fibers.      Carbon fiber reinforced polymer, carbon fiber reinforced plastic or carbon", 
                "pageCategories": "Aerospace materials\nAll articles needing additional references\nAll articles with unsourced statements\nArticles needing additional references from June 2012\nArticles with unsourced statements from April 2015\nCarbon forms\nCommons category with local link same as on Wikidata\nComposite materials\nSynthetic fibers", 
                "pageContent": "Carbon fiber reinforced polymer, carbon fiber reinforced plastic or carbon fiber reinforced thermoplastic (CFRP, CRP, CFRTP or often simply carbon fiber, or even carbon), is an extremely strong and light fiber-reinforced plastic which contains carbon fibers. The spelling 'fibre' is common in British Commonwealth countries.\nCFRPs can be expensive to produce but are commonly used wherever high strength-to-weight ratio and rigidity are required, such as aerospace, automotive, civil engineering, sports goods and an increasing number of other consumer and technical applications.\nThe binding polymer is often a thermoset resin such as epoxy, but other thermoset or thermoplastic polymers, such as polyester, vinyl ester or nylon, are sometimes used. The composite may contain other fibers, such as an aramid (e.g. Kevlar, Twaron), aluminium, ultra-high-molecular-weight polyethylene (UHMWPE) or glass fibers, as well as carbon fiber. The properties of the final CFRP product can also be affected by the type of additives introduced to the binding matrix (the resin). The most frequent additive is silica, but other additives such as rubber and carbon nanotubes can be used. The material is also referred to as graphite-reinforced polymer or graphite fiber-reinforced polymer (GFRP is less common, as it clashes with glass-(fiber)-reinforced polymer). In product advertisements, it is sometimes referred to simply as graphite fiber for short.\n\n\n== Properties ==\nCFRPs are composite materials. In this case the composite consists of two parts: a matrix and a reinforcement. In CFRP the reinforcement is carbon fiber, which provides the strength. The matrix is usually a polymer resin, such as epoxy, to bind the reinforcements together. Because CFRP consists of two distinct elements, the material properties depend on these two elements.\nThe reinforcement will give the CFRP its strength and rigidity; measured by stress and elastic modulus respectively. Unlike isotropic materials like steel and aluminum, CFRP has directional strength properties. The properties of CFRP depend on the layouts of the carbon fiber and the proportion of the carbon fibers relative to the polymer. The two different equations governing the net elastic modulus of composite materials using the properties of the carbon fibers and the polymer matrix can also be applied to carbon fiber reinforced plastics. The following equation,\n\n  \n    \n      \n        \n          E\n          \n            c\n          \n        \n        =\n        \n          V\n          \n            m\n          \n        \n        \n          E\n          \n            m\n          \n        \n        +\n        \n          V\n          \n            f\n          \n        \n        \n          E\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle E_{c}=V_{m}E_{m}+V_{f}E_{f}}\n  \nis valid for composite materials with the fibers oriented in the direction of the applied load. \n  \n    \n      \n        \n          E\n          \n            c\n          \n        \n      \n    \n    {\\displaystyle E_{c}}\n   is the total composite modulus, \n  \n    \n      \n        \n          V\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle V_{m}}\n   and \n  \n    \n      \n        \n          V\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle V_{f}}\n   are the volume fractions of the matrix and fiber respectively in the composite, and \n  \n    \n      \n        \n          E\n          \n            m\n          \n        \n      \n    \n    {\\displaystyle E_{m}}\n   and \n  \n    \n      \n        \n          E\n          \n            f\n          \n        \n      \n    \n    {\\displaystyle E_{f}}\n   are the elastic moduli of the matrix and fibers respectively. The other extreme case of the elastic modulus of the composite with the fibers oriented transverse to the applied load can be found using the following equation:\n\n  \n    \n      \n        \n          E\n          \n            c\n          \n        \n        =\n        \n          \n            (\n            \n              \n                \n                  V\n                  \n                    m\n                  \n                \n                \n                  E\n                  \n                    m\n                  \n                \n              \n            \n            +\n            \n              \n                \n                  V\n                  \n                    f\n                  \n                \n                \n                  E\n                  \n                    f\n                  \n                \n              \n            \n            )\n          \n          \n            \u2212\n            1\n          \n        \n      \n    \n    {\\displaystyle E_{c}=\\left({\\frac {V_{m}}{E_{m}}}+{\\frac {V_{f}}{E_{f}}}\\right)^{-1}}\n  \nThe fracture toughness of carbon fiber reinforced plastics is governed by the following mechanisms: 1) debonding between the carbon fiber and polymer matrix, 2) fiber pull-out, and 3) delamination between the CFRP sheets. Typical epoxy-based CFRPs exhibit virtually no plasticity, with less than 0.5% strain to failure. Although CFRPs with epoxy have high strength and elastic modulus, the brittle fracture mechanics present unique challenges to engineers in failure detection since failure occurs catastrophically. As such, recent efforts to toughen CFRPs include modifying the existing epoxy material and finding alternative polymer matrix. One such material with high promise is PEEK, which exhibits an order of magnitude greater toughness with similar elastic modulus and tensile strength. However, PEEK is much more difficult to process and more expensive.\nDespite its high initial strength-to-weight ratio, a design limitation of CFRP is its lack of a definable fatigue endurance limit. This means, theoretically, that stress cycle failure cannot be ruled out. While steel and many other structural metals and alloys do have estimable fatigue endurance limits, the complex failure modes of composites mean that the fatigue failure properties of CFRP are difficult to predict and design for. As a result, when using CFRP for critical cyclic-loading applications, engineers may need to design in considerable strength safety margins to provide suitable component reliability over its service life.\nEnvironmental effects such as temperature and humidity can have profound effects on the polymer-based composites, including most CFRPs. While CFRPs demonstrate excellent corrosion resistance, the effect of moisture at wide ranges of temperatures can lead to degradation of the mechanical properties of CFRPs, particularly at the matrix-fiber interface. While the carbon fibers themselves are not affected by the moisture diffusing into the material, the moisture plasticizes the polymer matrix. The epoxy matrix used for engine fan blades are designed to be impervious against jet fuel, lubrication, and rain water, and external paint on the composites parts are applied to minimize damage from ultraviolet light.\nThe carbon fibers can cause galvanic corrosion when CRP parts are attached to aluminum.\n\n\n== Manufacture ==\n\nThe primary element of CFRP is a carbon filament; this is produced from a precursor polymer such as polyacrylonitrile (PAN), rayon, or petroleum pitch. For synthetic polymers such as PAN or rayon, the precursor is first spun into filament yarns, using chemical and mechanical processes to initially align the polymer chains in a way to enhance the final physical properties of the completed carbon fiber. Precursor compositions and mechanical processes used during spinning filament yarns may vary among manufacturers. After drawing or spinning, the polymer filament yarns are then heated to drive off non-carbon atoms (carbonization), producing the final carbon fiber. The carbon fibers filament yarns may be further treated to improve handling qualities, then wound on to bobbins.From these fibers, a unidirectional sheet is created. These sheets are layered onto each other in a quasi-isotropic layup, e.g. 0\u00b0, +60\u00b0 or \u221260\u00b0 relative to each other.\nFrom the elementary fiber, a bidirectional woven sheet can be created, i.e. a twill with a 2/2 weave. The process by which most CFRPs are made varies, depending on the piece being created, the finish (outside gloss) required, and how many of this particular piece are going to be produced. In addition, the choice of matrix can have a profound effect on the properties of the finished composite.\nMany CFRP parts are created with a single layer of carbon fabric that is backed with fiberglass. A tool called a chopper gun is used to quickly create these composite parts. Once a thin shell is created out of carbon fiber, the chopper gun cuts rolls of fiberglass into short lengths and sprays resin at the same time, so that the fiberglass and resin are mixed on the spot. The resin is either external mix, wherein the hardener and resin are sprayed separately, or internal mixed, which requires cleaning after every use. Manufacturing methods may include the following:\n\n\n=== Molding ===\nOne method of producing CFRP parts is by layering sheets of carbon fiber cloth into a mold in the shape of the final product. The alignment and weave of the cloth fibers is chosen to optimize the strength and stiffness properties of the resulting material. The mold is then filled with epoxy and is heated or air-cured. The resulting part is very corrosion-resistant, stiff, and strong for its weight. Parts used in less critical areas are manufactured by draping cloth over a mold, with epoxy either preimpregnated into the fibers (also known as pre-preg) or \"painted\" over it. High-performance parts using single molds are often vacuum-bagged and/or autoclave-cured, because even small air bubbles in the material will reduce strength. An alternative to the autoclave method is to use internal pressure via inflatable air bladders or EPS foam inside the non-cured laid-up carbon fiber.\n\n\n=== Vacuum bagging ===\nFor simple pieces of which relatively few copies are needed (1\u20132 per day), a vacuum bag can be used. A fiberglass, carbon fiber or aluminum mold is polished and waxed, and has a release agent applied before the fabric and resin are applied, and the vacuum is pulled and set aside to allow the piece to cure (harden). There are three ways to apply the resin to the fabric in a vacuum mold.\nThe first method is manual and called a wet layup, where the two-part resin is mixed and applied before being laid in the mold and placed in the bag. The other one is done by infusion, where the dry fabric and mold are placed inside the bag while the vacuum pulls the resin through a small tube into the bag, then through a tube with holes or something similar to evenly spread the resin throughout the fabric. Wire loom works perfectly for a tube that requires holes inside the bag. Both of these methods of applying resin require hand work to spread the resin evenly for a glossy finish with very small pin-holes.\nA third method of constructing composite materials is known as a dry layup. Here, the carbon fiber material is already impregnated with resin (pre-preg) and is applied to the mold in a similar fashion to adhesive film. The assembly is then placed in a vacuum to cure. The dry layup method has the least amount of resin waste and can achieve lighter constructions than wet layup. Also, because larger amounts of resin are more difficult to bleed out with wet layup methods, pre-preg parts generally have fewer pinholes. Pinhole elimination with minimal resin amounts generally require the use of autoclave pressures to purge the residual gases out.\n\n\n=== Compression molding ===\nA quicker method uses a compression mold. This is a two-piece (male and female) mold usually made out of aluminum or steel that is pressed together with the fabric and resin between the two. The benefit is the speed of the entire process. Some car manufacturers, such as BMW, claimed to be able to cycle a new part every 80 seconds. However, this technique has a very high initial cost since the molds require CNC machining of very high precision.\n\n\n=== Filament winding ===\nFor difficult or convoluted shapes, a filament winder can be used to make CFRP parts by winding filaments around a mandrel or a core.\n\n\n== Applications ==\nApplications for CFRP include the following:\n\n\n=== Aerospace engineering ===\n\nThe Airbus A350 XWB is built of 52% CFRP including wing spars and fuselage components, taking the lead from the Boeing 787 Dreamliner, for the aircraft with the highest weight ratio for CFRP, which was held at 50%. This, along with the Airbus A400M is one of the first commercial aircraft to have the wing spars made from composites, although the A400M isn't the first military aircraft to have structural composite components. Furthermore, the Airbus A380 is one of the first commercial airliner to have a central wing box made of CFRP; it is the first to have a smoothly contoured wing cross section instead of the wings being partitioned span-wise into sections. This flowing, continuous cross section optimises aerodynamic efficiency. Moreover, the trailing edge along with the rear bulkhead and Empennage and un-pressurized Fuselage are made of CFRP. However, many delays have pushed order delivery dates back because of problems with the manufacturing of these parts. Many aircraft that use CFRP have experienced delays with delivery dates due to the relatively new processes used to make CFRP components, whereas metallic structures have been studied and used on airframes for years and the processes are relatively well understood. A recurrent problem is the structural ageing monitoring, for which new methods are constantly investigated, due to the multi-material and anisotropy unusual nature of CFRP.\nIn 1968 a Hyfil carbon-fiber fan assembly was in service on the Rolls-Royce Conways of the Vickers VC10s operated by BOAC.\nSpecialist aircraft designer and manufacturer Scaled Composites have made extensive use of CFRP throughout their design range including the first private manned spacecraft Spaceship One. CFRP is widely used in micro air vehicles (MAVs) because of its high strength to weight ratio.\nSpaceX is using carbon fiber for the entire primary structure of their new super heavy-lift launch vehicle, the ITS launch vehicle\u2014as well as the two very large spacecraft that will be launched by it, the Interplanetary Spaceship and the ITS tanker. This is a particular challenge for the large liquid oxygen tank structure due to design challenges of such dense carbon/oxygen contact for long periods of time.\nUltralight aircraft (see SSDR) such as the E-Go, rely heavily on CFRP in order to meet the category weight compliance requirement of less than 115 kg (254 lb) without pilot or fuel.\n\n\n=== Automotive engineering ===\n\nCFRPs are extensively used in high-end automobile racing. The high cost of carbon fiber is mitigated by the material's unsurpassed strength-to-weight ratio, and low weight is essential for high-performance automobile racing. Race-car manufacturers have also developed methods to give carbon fiber pieces strength in a certain direction, making it strong in a load-bearing direction, but weak in directions where little or no load would be placed on the member. Conversely, manufacturers developed omnidirectional carbon fiber weaves that apply strength in all directions. This type of carbon fiber assembly is most widely used in the \"safety cell\" monocoque chassis assembly of high-performance race-cars.\nMany supercars over the past few decades have incorporated CFRP extensively in their manufacture, using it for their monocoque chassis as well as other components. As far back as 1971, the Citro\u00ebn SM offered optional lightweight carbon fiber wheels.\nUntil recently, the material has had limited use in mass-produced cars because of the expense involved in terms of materials, equipment, and the relatively limited pool of individuals with expertise in working with it. Recently, several mainstream vehicle manufacturers have started to use CFRP in everyday road cars.\nUse of the material has been more readily adopted by low-volume manufacturers who used it primarily for creating body-panels for some of their high-end cars due to its increased strength and decreased weight compared with the glass-reinforced polymer they used for the majority of their products.\nUse of carbon fiber in a vehicle can appreciably reduce the weight and hence the size of its frame. This will also facilitate designers' and engineers' creativity and allow more in-cabin space for commuters. A preference for carbon fiber can also reduce the amount of water and electricity used in manufacturing.\n\n\n=== Civil engineering ===\n\nCFRP has become a notable material in structural engineering applications. Studied in an academic context as to its potential benefits in construction, it has also proved itself cost-effective in a number of field applications strengthening concrete, masonry, steel, cast iron, and timber structures. Its use in industry can be either for retrofitting to strengthen an existing structure or as an alternative reinforcing (or pre-stressing) material instead of steel from the outset of a project.\nRetrofitting has become the increasingly dominant use of the material in civil engineering, and applications include increasing the load capacity of old structures (such as bridges) that were designed to tolerate far lower service loads than they are experiencing today, seismic retrofitting, and repair of damaged structures. Retrofitting is popular in many instances as the cost of replacing the deficient structure can greatly exceed its strengthening using CFRP.\nApplied to reinforced concrete structures for flexure, CFRP typically has a large impact on strength (doubling or more the strength of the section is not uncommon), but only a moderate increase in stiffness (perhaps a 10% increase). This is because the material used in this application is typically very strong (e.g., 3000 MPa ultimate tensile strength, more than 10 times mild steel) but not particularly stiff (150 to 250 GPa, a little less than steel, is typical). As a consequence, only small cross-sectional areas of the material are used. Small areas of very high strength but moderate stiffness material will significantly increase strength, but not stiffness.\nCFRP can also be applied to enhance shear strength of reinforced concrete by wrapping fabrics or fibers around the section to be strengthened. Wrapping around sections (such as bridge or building columns) can also enhance the ductility of the section, greatly increasing the resistance to collapse under earthquake loading. Such 'seismic retrofit' is the major application in earthquake-prone areas, since it is much more economic than alternative methods.\nIf a column is circular (or nearly so) an increase in axial capacity is also achieved by wrapping. In this application, the confinement of the CFRP wrap enhances the compressive strength of the concrete. However, although large increases are achieved in the ultimate collapse load, the concrete will crack at only slightly enhanced load, meaning that this application is only occasionally used.\nSpecialist ultra-high modulus CFRP (with tensile modulus of 420 GPa or more) is one of the few practical methods of strengthening cast-iron beams. In typical use, it is bonded to the tensile flange of the section, both increasing the stiffness of the section and lowering the neutral axis, thus greatly reducing the maximum tensile stress in the cast iron.\nWhen used as a replacement for steel, CFRP bars could be used to reinforce concrete structures; however, this application is not common.\nCFRP could be used as pre-stressing materials due to their high strength. The advantages of CFRP over steel as a pre-stressing material, namely its light weight and corrosion resistance, should enable the material to be used for niche applications such as in offshore environments. However, there are practical difficulties in anchorage of carbon fiber strands and applications of this are rare.\nIn the United States, pre-stressed concrete cylinder pipes (PCCP) account for a vast majority of water transmission mains. Due to their large diameters, failures of PCCP are usually catastrophic and affect large populations. Approximately 19,000 miles (31,000 km) of PCCP have been installed between 1940 and 2006. Corrosion in the form of hydrogen embrittlement has been blamed for the gradual deterioration of the pre-stressing wires in many PCCP lines. Over the past decade, CFRPs have been utilized to internally line PCCP, resulting in a fully structural strengthening system. Inside a PCCP line, the CFRP liner acts as a barrier that controls the level of strain experienced by the steel cylinder in the host pipe. The composite liner enables the steel cylinder to perform within its elastic range, to ensure the pipeline's long-term performance is maintained. CFRP liner designs are based on strain compatibility between the liner and host pipe.\nCFRP is a more costly material than its counterparts in the construction industry, glass fiber-reinforced polymer (GFRP) and aramid fiber-reinforced polymer (AFRP), though CFRP is, in general, regarded as having superior properties.\nMuch research continues to be done on using CFRP both for retrofitting and as an alternative to steel as a reinforcing or pre-stressing material. Cost remains an issue and long-term durability questions still remain. Some are concerned about the brittle nature of CFRP, in contrast to the ductility of steel. Though design codes have been drawn up by institutions such as the American Concrete Institute, there remains some hesitation among the engineering community about implementing these alternative materials. In part, this is due to a lack of standardization and the proprietary nature of the fiber and resin combinations on the market.\n\n\n=== Carbon fiber microelectrodes ===\nCarbon fibers are used for fabrication of carbon-fiber microelectrodes. In this application typically a single carbon fiber with diameter of 5\u20137 \u03bcm is sealed in a glass capillary. At the tip the capillary is either sealed with epoxy and polished to make carbon-fiber disk microelectrode or the fiber is cut to a length of 75\u2013150 \u03bcm to make carbon-fiber cylinder electrode. Carbon-fiber microelectrodes are used either in amperometry or fast-scan cyclic voltammetry for detection of biochemical signaling.\n\n\n=== Sports goods ===\n\nCFRP is now widely used in sports equipment. For the same strength, a CFRP bicycle frame weighs less than one of steel, aluminum, or titanium. The type and orientation of the carbon-fiber weave can be designed to maximize stiffness and minimize the chance of failure. The variety of shapes it can be built into has further increased stiffness and also allowed aerodynamic tube sections. CFRP frames, forks including suspension fork crowns and steerers, handlebars, seatposts, and crank arms are becoming more common on medium as well as higher-priced bicycles. CFRP wheels, while expensive, are also becoming popular. The higher yield strength of the CFRP rims compared to aluminium reduces the need to re-true a wheel, and the reduced mass of the rim also reduces the moment of inertia of the wheel, since it is a rotating component. Rarely, the spokes of the wheel can be made from CFRP, but most carbon wheelsets still use traditional stainless steel spokes. Some other less common uses of CFRP on bicycles include derailleur parts, brake and shifter levers and lever bodies, cassette sprocket carriers, suspension linkages, disc brake rotors, pedals, shoe soles, and saddle rails.\nCFRP is used in squash, tennis and badminton racquets, sport kite spars, high quality arrow shafts, hockey sticks, fishing rods, surfboards and rowing shells. Amputee athletes such as Oscar Pistorius use carbon fiber blades for running. It is used as a shank plate in some basketball sneakers to keep the foot stable, usually running the length of the shoe just above the sole and left exposed in some areas, usually in the arch.\nIn 2006, cricket bats with a thin carbon-fiber layer on the back were introduced and used in competitive matches by high-profile players including Ricky Ponting and Michael Hussey. The carbon fiber was claimed merely to increase the durability of the bats but was banned from all first-class matches by the ICC in 2007.\nAlthough lighter and stiffer than items made of traditional metals, CFRP may, under some circumstances, show significant rates of cracking and failure. This can occur because of impact or over-torquing or improper installation of components. It is possible for broken carbon bicycle frames to be repaired.\n\n\n=== Other applications ===\nThe fire resistance of polymers and thermo-set composites is significantly improved if a thin layer of carbon fibers is moulded near the surface because a dense, compact layer of carbon fibers efficiently reflects heat.\nCFRP is also finding application in an increasing number of high-end products that require stiffness and low weight, these include:\nGuitar Picks, such as those made by PickHeaven.\nLaptop cases by an increasing number of manufacturers.\nAudio components such as turntables and loudspeakers.\nMusical instruments, including violin bows, guitar pick-guards, drum shells, bagpipe chanters and entire musical instruments such as Luis and Clark's carbon fiber cellos, violas and violins; and Blackbird Guitars' acoustic guitars and ukuleles.\nKite systems use carbon fiber reinforced rods to obtain shapes and performances previously not possible.\nFirearms use it to replace certain metal, wood, and fiberglass components but many of the internal parts are still limited to metal alloys as current reinforced plastics are unsuitable.\nHigh-performance radio-controlled vehicle and aircraft components such as helicopter rotor blades.\nTripod legs, tent poles, fishing rods, billiards cues, walking sticks.\nMany other light and durable consumer items such as the handles of high-end knives.\nPoles for high reach, e.g. poles used by window cleaners and water fed poles.\nIn dentistry, carbon fiber posts are used in restoring root canal treated teeth.\nRailed train bogies for passenger service. This reduces the weight by up to 50% compared to metal bogies, which contributes to energy savings.\n\n\n== Disposal and recycling ==\nCFRPs have a long service lifetime when protected from the sun. When it is time to decommission CFRPs, they cannot be melted down in air like many metals. When free of vinyl (PVC or polyvinyl chloride) and other halogenated polymers, CFRPs can be thermally decomposed via thermal depolymerization in an oxygen-free environment. This can be accomplished in a refinery in a one-step process. Capture and reuse of the carbon and monomers is then possible. CFRPs can also be milled or shredded at low temperature to reclaim the carbon fiber; however, this process shortens the fibers dramatically. Just as with downcycled paper, the shortened fibers cause the recycled material to be weaker than the original material. There are still many industrial applications that do not need the strength of full-length carbon fiber reinforcement. For example, chopped reclaimed carbon fiber can be used in consumer electronics, such as laptops. It provides excellent reinforcement of the polymers used even if it lacks the strength-to-weight ratio of an aerospace component.\n\n\n== Carbon nano-tube reinforced polymer (CNRP) ==\nIn 2009, Zyvex Technologies introduced carbon nanotube-reinforced epoxy and carbon pre-pregs. Carbon nanotube reinforced polymer (CNRP) is several times stronger and tougher than CFRP and was used in the Lockheed Martin F-35 Lightning II as a structural material for aircraft. CNRP still uses carbon fiber as the primary reinforcement, but the binding matrix is a carbon nano-tube filled epoxy.\n\n\n== See also ==\nCarbon (fiber)\nCarbon nanotube\nComposite repairs\nFiber-reinforced plastic\nThe Mechanics of Running Blades\n\n\n== References ==\n\n\n== External links ==\nJapan Carbon Fiber Manufacturers Association (English)\nArticle on the basis of Carbon Fiber\nEngineers design composite bracing system for injured Hokie running back Cedric Humes\nThe New Steel a 1968 Flight article on the announcement of carbon fiber\nCarbon Fibres \u2013 the First Five Years A 1971 Flight article on carbon fiber in the aviation field", 
                "titleUrl": "https://en.wikipedia.org/wiki/Carbon_fiber_reinforced_polymer", 
                "title": "Carbon fiber reinforced polymer"
            }, 
            {
                "snippet": "\"Carbon-15\" redirects here. For the firearm, see Carbon 15. Carbon (C) has 15 known isotopes, from 8C to 22C, 2 of which (12C and 13C) are stable. The", 
                "pageCategories": "CS1 errors: dates\nCarbon\nIsotopes of carbon\nLists of isotopes by element\nPages with login required references or sources", 
                "pageContent": "Carbon (C) has 15 known isotopes, from 8C to 22C, 2 of which (12C and 13C) are stable. The longest-lived radioisotope is 14C, with a half-life of 5,700 years. This is also the only carbon radioisotope found in nature\u2014trace quantities are formed cosmogenically by the reaction 14N + 1n \u2192 14C + 1H. The most stable artificial radioisotope is 11C, which has a half-life of 20.334 minutes. All other radioisotopes have half-lives under 20 seconds, most less than 200 milliseconds. The least stable isotope is 8C, with a half-life of 2.0 x 10\u221221 s. Averaging over natural abundances, the relative atomic mass for carbon is 12.0107(8).\n\n\n== Carbon-11 ==\nCarbon-11 or 11C is a radioactive isotope of carbon that decays to boron-11. This decay mainly occurs due to positron emission; however, around 0.19\u20130.23% of the time, it is a result of electron capture. It has a half-life of 20.334 minutes.\n11C \u2192 11B + e+ + \u03bd\ne + 6987153808942752000\u26600.96 MeV\n11C + e\u2212 \u2192 11B + \u03bd\ne + 6987317230944426000\u26601.98 MeV\nIt is produced from nitrogen in a cyclotron by the reaction\n14N + p \u2192 11C + 4He\nCarbon-11 is commonly used as a radioisotope for the radioactive labeling of molecules in positron emission tomography. Among the many molecules used in this context is the radioligand [11C]DASB.\n\n\n== Natural isotopes ==\n\nThere are three naturally occurring isotopes of carbon: 12, 13, and 14. 12C and 13C are stable, occurring in a natural proportion of approximately 99:1. 14C is produced by thermal neutrons from cosmic radiation in the upper atmosphere, and is transported down to earth to be absorbed by living biological material. Isotopically, 14C constitutes a negligible part; but, since it is radioactive with a half-life of 5,700 years, it is radiometrically detectable. Since dead tissue doesn't absorb 14C, the amount of 14C is one of the methods used within the field of archeology for radiometric dating of biological material.\n\n\n== Paleoclimate ==\n12C and 13C are measured as the isotope ratio \u03b413C in benthic foraminifera and used as a proxy for nutrient cycling and the temperature dependent air-sea exchange of CO2 (ventilation) (Lynch-Stieglitz et al., 1995). Plants find it easier to use the lighter isotopes (12C) when they convert sunlight and carbon dioxide into food. So, for example, large blooms of plankton (free-floating organisms) absorb large amounts of 12C from the oceans. Originally, the 12C was mostly incorporated into the seawater from the atmosphere. If the oceans that the plankton live in are stratified (meaning that there are layers of warm water near the top, and colder water deeper down), then the surface water does not mix very much with the deeper waters, so that when the plankton dies, it sinks and takes away 12C from the surface, leaving the surface layers relatively rich in 13C. Where cold waters well up from the depths (such as in the North Atlantic), the water carries 12C back up with it. So, when the ocean was less stratified than today, there was much more 12C in the skeletons of surface-dwelling species. Other indicators of past climate include the presence of tropical species, coral growths rings, etc.\n\n\n== Tracing food sources and diets ==\nThe quantities of the different isotopes can be measured by mass spectrometry and compared to a standard; the result (e.g. the delta of the 13C = \u03b413C) is expressed as parts per thousand (\u2030).\n\n  \n    \n      \n        \n          \u03b4\n          \n            13\n          \n        \n        C\n        =\n        \n          \n            (\n          \n        \n        \n          \n            \n              \n                \n                  (\n                \n              \n              \n                \n                  \n                    \n                      \n                      \n                        13\n                      \n                    \n                    C\n                  \n                  \n                    \n                      \n                      \n                        12\n                      \n                    \n                    C\n                  \n                \n              \n              \n                \n                  \n                    )\n                  \n                \n                \n                  s\n                  a\n                  m\n                  p\n                  l\n                  e\n                \n              \n            \n            \n              \n                \n                  (\n                \n              \n              \n                \n                  \n                    \n                      \n                      \n                        13\n                      \n                    \n                    C\n                  \n                  \n                    \n                      \n                      \n                        12\n                      \n                    \n                    C\n                  \n                \n              \n              \n                \n                  \n                    )\n                  \n                \n                \n                  s\n                  t\n                  a\n                  n\n                  d\n                  a\n                  r\n                  d\n                \n              \n            \n          \n        \n        \u2212\n        1\n        \n          \n            )\n          \n        \n        \u00d7\n        1000\n        \n           \n          \n            o\n          \n        \n        \n        \n          /\n        \n        \n          \n          \n            o\n            o\n          \n        \n      \n    \n    {\\displaystyle \\delta ^{13}C={\\Biggl (}{\\frac {{\\bigl (}{\\frac {^{13}C}{^{12}C}}{\\bigr )}_{sample}}{{\\bigl (}{\\frac {^{13}C}{^{12}C}}{\\bigr )}_{standard}}}-1{\\Biggr )}\\times 1000\\ ^{o}\\!/\\!_{oo}}\n  \nStable carbon isotopes in carbon dioxide are utilized differentially by plants during photosynthesis. Grasses in temperate climates (barley, rice, wheat, rye and oats, plus sunflower, potato, tomatoes, peanuts, cotton, sugar beet, and most trees and their nuts/fruits, roses and Kentucky bluegrass) follow a C3 photosynthetic pathway that will yield \u03b413C values averaging about \u221226.5\u2030. Grasses in hot arid climates (maize in particular, but also millet, sorghum, sugar cane and crabgrass) follow a C4 photosynthetic pathway that produces \u03b413C values averaging about \u221212.5\u2030.\nIt follows that eating these different plants will affect the \u03b413C values in the consumer's body tissues. If an animal (or human) eats only C3 plants, their \u03b413C values will be from \u221218.5 to \u221222.0\u2030 in their bone collagen and \u221214.5\u2030 in the hydroxylapatite of their teeth and bones.\nIn contrast, C4 feeders will have bone collagen with a value of \u22127.5\u2030 and hydroxylapatite value of \u22120.5\u2030.\nIn actual case studies, millet and maize eaters can easily be distinguished from rice and wheat eaters. Studying how these dietary preferences are distributed geographically through time can illuminate migration paths of people and dispersal paths of different agricultural crops. However, human groups have often mixed C3 and C4 plants (northern Chinese historically subsisted on wheat and millet), or mixed plant and animal groups together (for example, southeastern Chinese subsisting on rice and fish).\n\n\n== Table ==\n\n\n=== Notes ===\nThe precision of the isotope abundances and atomic mass is limited through variations. The given ranges should be applicable to any normal terrestrial material.\nValues marked # are not purely derived from experimental data, but at least partly from systematic trends. Spins with weak assignment arguments are enclosed in parentheses.\nUncertainties are given in concise form in parentheses after the corresponding last digits. Uncertainty values denote one standard deviation, except isotopic composition and standard atomic mass from IUPAC, which use expanded uncertainties.\nCarbon-12 nuclide is of particular importance as it is used as the standard from which atomic masses of all nuclides are expressed: its atomic mass is by definition 12 Da.\nNuclide masses are given by IUPAP Commission on Symbols, Units, Nomenclature, Atomic Masses and Fundamental Constants (SUNAMCO).\nIsotope abundances are given by IUPAC Commission on Isotopic Abundances and Atomic Weights.\n\n\n== See also ==\nradiocarbon dating\nCosmogenic isotopes\nEnvironmental isotopes\nIsotopic signature\n\n\n== References ==\nIsotope masses from:\nG. Audi; A. H. Wapstra; C. Thibault; J. Blachot; O. Bersillon (2003). \"The NUBASE evaluation of nuclear and decay properties\" (PDF). Nuclear Physics A. 729: 3\u2013128. Bibcode:2003NuPhA.729....3A. doi:10.1016/j.nuclphysa.2003.11.001. \n\nIsotopic compositions and standard atomic masses from:\nJ. R. de Laeter; J. K. B\u00f6hlke; P. De Bi\u00e8vre; H. Hidaka; H. S. Peiser; K. J. R. Rosman; P. D. P. Taylor (2003). \"Atomic weights of the elements. Review 2000 (IUPAC Technical Report)\". Pure and Applied Chemistry. 75 (6): 683\u2013800. doi:10.1351/pac200375060683. \nM. E. Wieser (2006). \"Atomic weights of the elements 2005 (IUPAC Technical Report)\". Pure and Applied Chemistry. 78 (11): 2051\u20132066. doi:10.1351/pac200678112051. Lay summary. \n\nHalf-life, spin, and isomer data selected from the following sources. See editing notes on this article's talk page.\nG. Audi; A. H. Wapstra; C. Thibault; J. Blachot; O. Bersillon (2003). \"The NUBASE evaluation of nuclear and decay properties\" (PDF). Nuclear Physics A. 729: 3\u2013128. Bibcode:2003NuPhA.729....3A. doi:10.1016/j.nuclphysa.2003.11.001. \nNational Nuclear Data Center. \"NuDat 2.1 database\". Brookhaven National Laboratory. Retrieved September 2005.  \nN. E. Holden (2004). \"Table of the Isotopes\". In D. R. Lide. CRC Handbook of Chemistry and Physics (85th ed.). CRC Press. Section 11. ISBN 978-0-8493-0485-9.", 
                "titleUrl": "https://en.wikipedia.org/wiki/Isotopes_of_carbon", 
                "title": "Isotopes of carbon"
            }, 
            {
                "snippet": "Orbiting Carbon Observatory 2 (OCO-2) is an American environmental science satellite which launched on 2 July 2014. A NASA mission, it is a replacement", 
                "pageCategories": "2014 in the United States\nArticles containing video clips\nCommons category with local link same as on Wikidata\nEarth observation satellites\nGreenhouse gases\nSatellites of the United States\nSpacecraft launched by Delta II rockets\nSpacecraft launched in 2014\nStart-date transclusions with invalid parameters\nUse dmy dates from September 2016", 
                "pageContent": "Orbiting Carbon Observatory 2 (OCO-2) is an American environmental science satellite which launched on 2 July 2014. A NASA mission, it is a replacement for the Orbiting Carbon Observatory which was lost in a launch failure in 2009.\n\n\n== Mission description ==\nThe OCO-2 satellite was built by Orbital Sciences Corporation, based around the LEOStar-2 bus. The spacecraft is being used to study carbon dioxide concentrations and distributions in the atmosphere.\nOCO-2 was ordered after the original OCO spacecraft failed to achieve orbit. During the first satellite's launch atop a Taurus-XL in February 2009, the payload fairing failed to separate from around the spacecraft and the rocket did not have sufficient power to enter orbit with its additional mass. Although a Taurus launch was initially contracted for the reflight, the launch contract was cancelled after the same malfunction occurred on the launch of the Glory satellite two years later.\n\nUnited Launch Alliance launched OCO-2 using a Delta II rocket at the beginning of a 30-second launch window at 09:56:23 UTC (2:56:23 PDT) on 2 July 2014. Flying in the 7320-10C configuration, the rocket launched from Space Launch Complex 2W at Vandenberg Air Force Base. The initial launch attempt on 1 July at 09:56:44 UTC was scrubbed at 46 seconds on the countdown clock due to a faulty valve on the water suppression system, used to flow water on the launch pad to dampen the acoustic energy during launch.\nOCO-2 joined the A-train satellite constellation, becoming the sixth satellite in the group. Members of the A-train fly very close together in sun-synchronous orbit, to make nearly simultaneous measurements of Earth. A particularly short launch window of 30 seconds was necessary to achieve a proper position in the train. As of 19 September 2016 it was in an orbit with a perigee of 701.1 km (435.6 mi), an apogee of 703.8 km (437.3 mi) and a 98.2 degree inclination.\nThe mission is expected to cost US$467.7 million, including design, development, launch and operations.\n\n\n== Column CO2 measurements ==\n\nOCO-2 makes measurements in three different spectral bands over four to eight different footprints of approximately 1.29 km \u00d7 2.25 km (0.80 mi \u00d7 1.40 mi) each. About 24 soundings are collected per second while in sunlight and over 10% of these are sufficiently cloud free for further analysis. One spectral band is used for column measurements of oxygen (A-band 0.765 microns), and two are used for column measurements of carbon dioxide (weak band 1.61 microns, strong band 2.06 microns).\nIn the retrieval algorithm measurements from the three bands are combined to yield column-averaged dry-air mole fractions of carbon dioxide. Because these are dry-air mole fractions, these measurements do not change with water content or surface pressure. Because the molecular oxygen content of the atmosphere (i.e. excluding the oxygen in water vapour) is well known to be 20.95%, oxygen is used as a measure of the total dry air column. To ensure these measurements are traceable to the World Meteorological Organization, OCO-2 measurements are carefully compared with measurements by the Total Carbon Column Observing Network (TCCON).\n\n\n== References ==\n\n\n=== Bibliography ===\nOsterman, Gregory; et al. (30 March 2015). \"Orbiting Carbon Observatory\u20132 (OCO-2): Data Product User's Guide, Operational L1 and L2 Data Versions 6 and 6R\" (PDF). NASA. OCO D-55208. Retrieved 14 May 2015. \n\n\n== External links ==\n Media related to Orbiting Carbon Observatory-2 at Wikimedia Commons\nOrbiting Carbon Observatory at NASA.gov\nOrbiting Carbon Observatory by the Jet Propulsion Laboratory\nOrbiting Carbon Observatory by the JPL Science Division", 
                "titleUrl": "https://en.wikipedia.org/wiki/Orbiting_Carbon_Observatory_2", 
                "title": "Orbiting Carbon Observatory 2"
            }
        ], 
        "phraseCharStart": "1140"
    }
]